{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.http import MediaIoBaseUpload\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import io\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Google Drive\n",
    "def authenticate_google_drive():\n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "    drive_service = build('drive', 'v3', credentials=creds)\n",
    "    return drive_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_service = authenticate_google_drive()\n",
    "parent_folder_id = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a log file in Google Drive\n",
    "def create_log_file(drive_service, folder_id, log_filename='checkpoint.txt'):\n",
    "    log_file_metadata = {\n",
    "        'name': log_filename,\n",
    "        'parents': [folder_id],\n",
    "        'mimeType': 'text/plain'\n",
    "    }\n",
    "    log_file = drive_service.files().create(body=log_file_metadata, fields='id').execute()\n",
    "    print(f\"Created log file: {log_filename} with ID: {log_file.get('id')}\")\n",
    "    return log_file.get('id')\n",
    "\n",
    "# Function to append to the log file in Google Drive\n",
    "def append_to_log_file(drive_service, log_file_id, text):\n",
    "    # Get the log file content (if any)\n",
    "    request = drive_service.files().get_media(fileId=log_file_id)\n",
    "    log_content = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(log_content, request)\n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()\n",
    "    log_content.seek(0)\n",
    "    \n",
    "    # Append the new log content\n",
    "    new_content = log_content.read().decode('utf-8') + text + '\\n'\n",
    "    \n",
    "    # Write the new content back to the log file\n",
    "    media_body = MediaFileUpload(io.BytesIO(new_content.encode('utf-8')), mimetype='text/plain')\n",
    "    drive_service.files().update(fileId=log_file_id, media_body=media_body).execute()\n",
    "    print(f\"Updated log file {log_file_id} with new entry: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list files in a specific folder\n",
    "def list_files_in_folder(drive_service, folder_id):\n",
    "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "    result = drive_service.files().list(q=query).execute()\n",
    "    return result.get('files', [])\n",
    "\n",
    "# Function to create or update the checkpoint.txt file in each folder\n",
    "def create_or_update_checkpoint_file(drive_service, folder_id, files):\n",
    "    log_filename = 'checkpoint.txt'\n",
    "    \n",
    "    # Check if the checkpoint file already exists in this folder\n",
    "    existing_log_files = list_files_in_folder(drive_service, folder_id)\n",
    "    log_file_id = None\n",
    "    for file in existing_log_files:\n",
    "        if file['name'] == log_filename:\n",
    "            log_file_id = file['id']\n",
    "            break\n",
    "\n",
    "    # If the log file doesn't exist, create it\n",
    "    if log_file_id is None:\n",
    "        log_file_id = create_log_file(drive_service, folder_id, log_filename)\n",
    "\n",
    "    # Prepare the content of the checkpoint file (file names)\n",
    "    log_content = '\\n'.join([f['name'] for f in files])\n",
    "\n",
    "    # Update the checkpoint file with the list of files\n",
    "    media_body = MediaFileUpload(io.BytesIO(log_content.encode('utf-8')), mimetype='text/plain')\n",
    "    drive_service.files().update(fileId=log_file_id, media_body=media_body).execute()\n",
    "    print(f\"Checkpoint updated for folder {folder_id}\")\n",
    "\n",
    "# Function to recursively traverse folders and log files\n",
    "def traverse_and_log_files(drive_service, parent_folder_id):\n",
    "    # List all event folders (subfolders of ACLanthology)\n",
    "    event_folders = list_files_in_folder(drive_service, parent_folder_id)\n",
    "\n",
    "    for event_folder in event_folders:\n",
    "        event_folder_id = event_folder['id']\n",
    "        event_name = event_folder['name']\n",
    "        print(f\"Processing event: {event_name}\")\n",
    "\n",
    "        # List all volumes within the event folder\n",
    "        volume_folders = list_files_in_folder(drive_service, event_folder_id)\n",
    "        \n",
    "        for volume_folder in volume_folders:\n",
    "            volume_folder_id = volume_folder['id']\n",
    "            volume_name = volume_folder['name']\n",
    "            print(f\"Processing volume: {volume_name}\")\n",
    "\n",
    "            # List all papers (files) within the volume folder\n",
    "            paper_files = list_files_in_folder(drive_service, volume_folder_id)\n",
    "\n",
    "            # Create or update the checkpoint.txt for this volume\n",
    "            create_or_update_checkpoint_file(drive_service, volume_folder_id, paper_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list files in a specific folder\n",
    "def list_files_in_folder(drive_service, folder_id):\n",
    "    query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "    result = drive_service.files().list(q=query).execute()\n",
    "    return result.get('files', [])\n",
    "\n",
    "# Function to create or update the checkpoint.txt file in each folder\n",
    "def create_or_update_checkpoint_file(drive_service, folder_id, files):\n",
    "    log_filename = 'checkpoint.txt'\n",
    "    \n",
    "    # Check if the checkpoint file already exists in this folder\n",
    "    existing_log_files = list_files_in_folder(drive_service, folder_id)\n",
    "    log_file_id = None\n",
    "    for file in existing_log_files:\n",
    "        if file['name'] == log_filename:\n",
    "            log_file_id = file['id']\n",
    "            break\n",
    "\n",
    "    # Prepare the content of the checkpoint file (file names)\n",
    "    log_content = '\\n'.join([f['name'] for f in files])\n",
    "\n",
    "    # Write the content to a temporary file\n",
    "    with open(log_filename, 'w') as temp_log_file:\n",
    "        temp_log_file.write(log_content)\n",
    "\n",
    "    # Upload or update the checkpoint file\n",
    "    media_body = MediaFileUpload(log_filename, mimetype='text/plain')\n",
    "    \n",
    "    if log_file_id:\n",
    "        # Update the existing log file\n",
    "        drive_service.files().update(fileId=log_file_id, media_body=media_body).execute()\n",
    "    else:\n",
    "        # Create a new log file in the folder\n",
    "        file_metadata = {'name': log_filename, 'parents': [folder_id]}\n",
    "        drive_service.files().create(body=file_metadata, media_body=media_body).execute()\n",
    "\n",
    "    # Remove the temporary local file after uploading\n",
    "    os.remove(log_filename)\n",
    "\n",
    "    print(f\"Checkpoint updated for folder {folder_id}\")\n",
    "\n",
    "# Function to recursively traverse folders and log files\n",
    "def traverse_and_log_files(drive_service, parent_folder_id):\n",
    "    # List all event folders (subfolders of ACLanthology)\n",
    "    event_folders = list_files_in_folder(drive_service, parent_folder_id)\n",
    "\n",
    "    for event_folder in event_folders:\n",
    "        event_folder_id = event_folder['id']\n",
    "        event_name = event_folder['name']\n",
    "        print(f\"Processing event: {event_name}\")\n",
    "\n",
    "        # List all volumes within the event folder\n",
    "        volume_folders = list_files_in_folder(drive_service, event_folder_id)\n",
    "        \n",
    "        for volume_folder in volume_folders:\n",
    "            volume_folder_id = volume_folder['id']\n",
    "            volume_name = volume_folder['name']\n",
    "            print(f\"Processing volume: {volume_name}\")\n",
    "\n",
    "            # List all papers (files) within the volume folder\n",
    "            paper_files = list_files_in_folder(drive_service, volume_folder_id)\n",
    "\n",
    "            # Create or update the checkpoint.txt for this volume\n",
    "            create_or_update_checkpoint_file(drive_service, volume_folder_id, paper_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing event: conll\n",
      "Processing volume: 2023.conll-1\n",
      "Checkpoint updated for folder 1LwCWYKgw5zYv6Va0EQkjJrcSQ6-u-EVF\n",
      "Processing event: cl\n",
      "Processing volume: 2024.cl-1\n",
      "Checkpoint updated for folder 1fQPv5aivNulmduEedwzWkKWLsF19MtJ6\n",
      "Processing event: acl\n",
      "Processing volume: 2024.acl-long\n",
      "Checkpoint updated for folder 1X-OaGgPSH76t2dHLicjSQLECMG63Uflg\n",
      "Processing event: anlp\n",
      "Processing volume: A00-1\n",
      "Checkpoint updated for folder 1wRSZ9tRIo3QN8fo4hBX9qyfWR5GLueje\n",
      "Processing event: aacl\n",
      "Processing volume: 2020.aacl-demo\n",
      "Checkpoint updated for folder 1r72V_mkJ29yLhJJCL-0k6xZ5KqkPeF0u\n",
      "Processing volume: 2020.aacl-srw\n",
      "Checkpoint updated for folder 1JCeaLyLXLhV43lGZJHAy4C_R3R9S7O2g\n",
      "Processing volume: 2020.aacl-main\n",
      "Checkpoint updated for folder 1oPX2tp3_AoFlL8GMuKw1TbqtYBYeT0oO\n",
      "Processing volume: 2022.aacl-tutorials\n",
      "Checkpoint updated for folder 1c4RxX4XKNQQNuRRHGftOBvepqzXvsixW\n",
      "Processing volume: 2022.aacl-demo\n",
      "Checkpoint updated for folder 1ANib5a05yzlynBr8cYbL2fluS0uDMpqZ\n",
      "Processing volume: 2022.aacl-srw\n",
      "Checkpoint updated for folder 1Qn-JNnVovr-f_luSWC5kf_Mcb8NxESP0\n",
      "Processing volume: 2022.aacl-short\n",
      "Checkpoint updated for folder 1fLbKEK4LVHPxmE-6GPRAT6xTEeFreCd6\n",
      "Processing volume: 2022.aacl-main\n",
      "Checkpoint updated for folder 1RsUTINKxDHbytz66MbVCPo-CU_w_DlQh\n",
      "Processing volume: 2023.ijcnlp-tutorials\n",
      "Checkpoint updated for folder 1wlhkn5TGDeJVHEty-j_oy9wv07onAA3Y\n",
      "Processing volume: 2023.ijcnlp-srw\n",
      "Checkpoint updated for folder 1DrmmKBhT4mwJ_CH5vfJIRlPjEOowKcdU\n",
      "Processing volume: 2023.ijcnlp-short\n",
      "Checkpoint updated for folder 1lliayV_D4JBZ5HlQ4fxifPlNqpwTaVQY\n",
      "Processing volume: 2023.ijcnlp-main\n",
      "Checkpoint updated for folder 1mwGHzrYI3YQKfaa3_vKxyEdNapY8jKoS\n"
     ]
    }
   ],
   "source": [
    "traverse_and_log_files(drive_service, parent_folder_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5872 entries, 0 to 5871\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   location     5872 non-null   object \n",
      " 1   paper_name   5872 non-null   object \n",
      " 2   paper_pages  3071 non-null   float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 137.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3071.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.834582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.989754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>121.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       paper_pages\n",
       "count  3071.000000\n",
       "mean     10.834582\n",
       "std       6.989754\n",
       "min       0.000000\n",
       "25%       7.000000\n",
       "50%      10.000000\n",
       "75%      12.000000\n",
       "max     121.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"acl_anthology_dataset_with_pages.csv\")\n",
    "df.head()\n",
    "df.columns\n",
    "df.shape\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repaeted= pd.read_csv(\"acl_anthology_repeated_papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original= pd.read_csv(\"acl_anthology_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 5872\n",
      "Dataset size after removing duplicates: 5787\n",
      "Removed 85 duplicate entries\n"
     ]
    }
   ],
   "source": [
    "# Load the original dataset\n",
    "df_original = pd.read_csv(\"acl_anthology_dataset.csv\")\n",
    "\n",
    "# Print initial info\n",
    "print(f\"Original dataset size: {len(df_original)}\")\n",
    "\n",
    "# Remove duplicates based on paper_name (or any other relevant columns)\n",
    "df_original_clean = df_original.drop_duplicates(subset=['paper_name'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Dataset size after removing duplicates: {len(df_original_clean)}\")\n",
    "print(f\"Removed {len(df_original) - len(df_original_clean)} duplicate entries\")\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df_original_clean.to_csv(\"acl_anthology_dataset_clean.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
