[
  {
    "title": "A Framework for Robust Semantic Interpretation A Framework for Robust Semantic Interpretation",
    "abstract": "This paper describes AUTOSEM, a robust semantic interpretation framework that can operate both at parse time and repair time. The evaluation demonstrates that AUTOSEM achieves a high level of robustness efficiently and without requiring any hand-coded knowledge dedicated to repair.",
    "content": "Introduction In order for an approach to robust interpretation to be practical it must be efficient, address the ma- jor types of disfluencies that plague spontaneously produced language input, and be domain indepen- dent so that achieving robustness in a particular do- main does not require an additional knowledge en- gineering effort. This paper describes AUTOSEM, a semantic interpretation framework that possesses these three qualities. While previous approaches to robust interpretation have offered robust parsers paired with separate repair modules, with separate knowledge sources for each, AUTOSEM is a single unified framework that can operate both at parse time and repair time. AUTOSEM is integrated with the LCFLEX robust parser (Rosé and Lavie, to ap- pear; Lavie and Rosé, 2000). Together AUTOSEM and LCFLEX constitute the robust understanding engine within the CARMEL natural language un- derstanding component developed in the context of the Atlas intelligent tutoring project (Freedman at al., to appear). The evaluation reported here demon- strates that AUTOSEM's repair approach operates 200 times faster than the most similar competing ap- proach while producing hypotheses of better quality. AUTOSEM provides an interface to allow seman- tic interpretation to operate in parallel with syntac- tic interpretation at parse time in a lexicon driven fashion. Domain specific semantic knowledge is en- coded declaratively within a meaning representation specification. Semantic constructor functions are compiled automatically from this specification and then linked into lexical entries as in the Glue Lan- guage Semantics approach to interpretation (Dal- rymple, 1999). Based on syntactic head/argument relationships assigned at parse time, the construc- tor functions enforce semantic selectional restric- tions and assemble meaning representation struc- tures by composing the meaning representation asso- ciated with the constructor function with the mean- ing representation of each of its arguments. AUTOSEM first attempts to construct analy- ses that satisfy both syntactic and semantic well- formedness conditions. The LCFLEX parser has the ability to efficiently relax syntactic constraints as needed and as allowed by its parameterized flexi- bility settings. For sentences remaining beyond the parser's coverage, AUTOSEM's repair algorithm re- lies entirely on semantic knowledge to compose the partial analyses produced by the parser. Each se- mantic representation built by AUTOSEM's inter- pretation framework contains a pointer to the con- structor function that built it. Thus, each partial analysis can be treated as a constructor function with built in knowledge about how the associated partial analysis can be combined with other par- tial analyses in a semantically meaningful way. Ge- netic programming search (Koza, 1992; Koza, 1994) is used to efficiently compose the fragments pro- duced by the parser. The function definitions com- piled from the meaning representation specification allow the genetic search to use semantic constraints to make effective use of its search space. Thus, AU- TOSEM operates efficiently, free of any hand coded repair rules or any knowledge specifically dedicated to repair unlike other approaches to recovery from parser failure (Danieli and Gerbino, 1995; Van No- ord, 1997; Kasper et al., 1999). 2 The Meaning Representation Specification At the heart of AUTOSEM is its interpretation framework composed of semantic constructor func- tions compiled from a meaning representation spec- ification. These semantic constructor functions can be used at parse time to build up semantic represen- tations. These same constructor functions can then be used in a repair stage to compose the fragments returned by the parser in the cases where the parser is not able to obtain a complete analysis for an ex- (:type <*state> :isa (<>) : instances nil :vars (entity time duration polarity) :spec ((who <*entity> entity) (when <*when> time) (how-long <*time-length> duration) (negation [+/-) polarity))) (:type <*personal-state> :isa (<*state>) : instances nil :vars () :spec ((who <*who> entity))) (:type <busy> :isa (<*personal-state>) : instances nil :vars (activity) : spec ((frame *busy) (event <*event> activity))) (:type [+/-] :isa (<>) : instances (+ -) : vars nil : spec nil) Figure 1: Sample meaning representation specification entries tragrammatical input sentence. The meaning representation specification pro- vides a venue for expressing domain specific se- mantic information declaratively. AUTOSEM pro- duces frame-based meaning representation struc- tures. Thus, each domain specific meaning repre- sentation specification must define a set of semantic types that together specify the set of frames and atomic feature values that make up the domain spe- cific frame-based language, which slots are associ- ated with each frame, and what range of frames and atomic feature values may fill each of those slots. AUTOSEM provides a simple formalism for defin- ing meaning representations. Each entry corre- sponds to a semantic type and contains five fields: :type, :isa, :instances, :vars, and :spec. Some sample entries for the appointment scheduling do- main are displayed in Figure 1. Some details are omitted for simplicity. The :type field simply con- tains the name of the type. The :vars field contains a list of variables, each corresponding to a semantic role. The :spec field associates a frame and set of slots with a type. For each slot, the :spec field con- tains the name of the slot, the most general type re- striction on the slot, and a specification of where the slot filler comes from. This third piece of information can be either a variable name, indicating that what- ever is bound to that variable is what should fill that slot, or a function call to another semantic construc- tor function, allowing types to specify constraints at more than one level of embedding. Similar to the : spec field, the :instances field associates a list of atomic values with a type. Inheritance relations are defined via the :isa field. Types inherit the values of each subsuming type's :instances, :vars, and : spec fields. 3 Semantic Interpretation at Parse Time (:type <cancel> :isa (<*event>) : instances nil :vars (agent activity time polarity) : spec ((frame *cancel) (engagement <*event> activity))) Figure 2: Meaning representation definition of <cancel> (:morph cancel :syntax ((cat vlex) (root cancel) (vform bare) (irreg-past +) (irreg-pastpart +) (irreg-prespart +) (subcat (*or* intrans np)) (semtag cancel1)) : semantics (canceli <cancel> ((subject agent) (object activity) (tempadjunct time) (negation polarity)))) Figure 3: Lexical entry for the verb \"cancel\" As an extension to LCFLEX's LFG-like pseudo- unification grammar formalism, AUTOSEM pro- vides the insert-role function as an interface to allow semantic interpretation to operate in parallel with syntactic interpretation at parse time. When the insert-role function is used to insert a child constituent into the slot corresponding to its syntac- tic functional role in a parent constituent, the child constituent's semantic representation is passed in to the parent constituent's semantic constructor func- tion as in the Glue Language Semantics approach to interpretation (Dalrymple, 1999). AUTOSEM's lexicon formalism allows semantic constructor func- tions to be linked into lexical entries by means of the semtag feature. Each semtag feature value cor- responds to a semantic constructor function and mappings between syntactic functional roles such as subject, direct object, and indirect object and semantic roles such as agent, activity, or time. See Figures 2 and 3 discussed further be- low. Note that the syntactic features that appear in this example are taken from the COMLEX lex- icon (Grishman et al., 1994). In order to provide consistent input to the semantic constructor func- tions, AUTOSEM assumes a syntactic approach in which deep syntactic functional roles are assigned as in CARMEL's syntactic parsing grammar evaluated in (Freedman et al., to appear). In this way, for ex- ample, the roles assigned within an active sentence and its corresponding passive sentence remain the same. Since the same constructor function is called with different arguments a number of times in order to construct an analysis incrementally, an argument is included in every constructor function that allows a \"result so far\" to be passed in and augmented. Its default value, which is used the first time the constructor function is executed, is the representa- tion associated with the corresponding type in the absence of any arguments being instantiated. Each time the constructor function is executed, each of its arguments that are instantiated are first checked to be certain that the structures they are instantiated with match all of the type restrictions on all of the slots that are bound to that argument. If they are, the instantiated arguments' structures are inserted into the corresponding slots in the \"result so far\". Otherwise the constructor function fails. Take as an example the sentence \"The meeting I had scheduled was canceled by you.\" as it is pro- cessed by AUTOSEM using the CARMEL grammar and lexicon, which is built on top of the COMLEX lexicon (Grishman et al., 1994). The grammar as- signs deep syntactic functional roles to constituents. Thus, \"you\" is the deep subject of \"cancel\", and \"the meeting\" is the direct object both of \"cancel\" and of \"schedule\". The detailed subcategorization classes associated with verbs, nouns, and adjectives in COMLEX make it possible to determine what these relationships should be. The meaning repre- sentation entry for <cancel> as well as the lexical entry for the verb \"cancel\" are found in Figures 2 and 3 respectively. Some details are left out for sim- plicity. When \"the meeting I had scheduled\" is ana- lyzed as the surface subject of \"was canceled\", it is assigned the deep syntactic role of object since \"was canceled\" is passive. The verb \"cancel\" has cancell as its semtag value in the lexicon. cancel1 is defined there as being associated with the type <cancel>, and the object syntactic role is associated with the activity argument. Thus, the <cancel> function is called with its activity argument instantiated with the meaning of \"the meeting I had scheduled\". Next, when \"by you\" is attached, \"you\" is assigned the deep syntactic role of subject of \"cancel\". The subject role is associated with the agent argument in the definition of cancelli. Thus, <cancel> is called a again, this time with \"you\" instantiating the agent argument and the result from the last call to <cancel> passed in through the \"result so far\" argument. 4 Semantic Interpretation for Repair While the LCFLEX parser has been demonstrated to robustly parse a variety of disfluencies found in spontaneously generated language (Rosé and Lavie, to appear), sentences still remain that are beyond its coverage. Previous research involving the ear- lier GLR* parser (Lavie, 1995) and an earlier repair module (Rosé, 1997) has demonstrated that divid- ing the task of robust interpretation into two stages, namely parsing and repair, provides a better trade off between run time and coverage than attempt- ing to place the entire burden of robustness on the parser alone (Rosé, 1997). Thus, when the flexibility allowed at parse time is not sufficient to construct an analysis of an entire sentence for any reason, a frag- mentary analysis is passed into the repair module. For each pair of vertices in the chart the best single clause level and noun phrase level analysis accord- ing to LCFLEX's statistical disambiguation scores is included in the set of fragmentary analyses passed on to the repair stage. An example¹ is displayed in Figure 4. Here the sentence \"Why don't we make it from like eleven to one?\" failed to parse. In this case, the problem is that the insertion of \"like\" causes the sentence to be ungrammatical. When the parser's flexibility settings are such that it is constrained to build analyses only for contiguous portions of text, such an insertion would prevent the parser from con- structing an analysis covering the entire sentence. Nevertheless, it is able to construct analyses for a number of grammatical subsets of it. Genetic programming search (Koza, 1992; Koza, 1994) is used to search for different ways to combine the fragments. Genetic programming is an oppor- tunistic search algorithm used for constructing com- puter programs to solve particular problems. Among its desirable properties is its ability to search a large space efficiently by first sampling widely and shal- lowly, and then narrowing in on the regions sur- rounding the most promising looking points. 1 This example was generated with the grammar used in the evaluation. See Section 6. The AUTOSEM repair algo- rithm can be used with grammars that do not make use of AUTOSEM's parse-time interface by using a simple conver- sion program that automatically builds a function for each partial analysis corresponding to its semantic type. Sentence: Why don't we make it from like eleven to one? Functions: (<SCHEDULE> NIL T NIL NIL NIL NIL NIL NIL T NIL NIL :STR ((FRAME *SCHEDULE) (ATTITUDE *LET-S) (WHAT ((FRAME *IT)))) :COV (6 5 4 3 2 1) : SCORE 1.4012985e-45) (<SCHEDULE> NIL NIL NIL NIL NIL NIL NIL NIL T NIL NIL :STR ((FRAME *SCHEDULE) (ATTITUDE *LET-S)) :COV (5 4 3 2 1) : SCORE 1.4012985e-45) (<INTERVAL> NIL T T T :STR ((END ((FRAME *SIMPLE-TIME) (HOUR 1))) (START ((FRAME *SIMPLE-TIME) (HOUR 11))) (INCL-EXCL INCLUSIVE) (FRAME *INTERVAL)) :COV (11 10 9) : SCORE 1.2102125e-38) (<PRO> NIL NIL NIL :STR ((FRAME *PRO)) :COV (11) : SCORE 2.2223547e-18) (<SIMPLE-TIME> NIL NIL NIL NIL NIL NIL NIL T NIL :STR ((FRAME *SIMPLE-TIME) (HOUR 11)) :COV (9) : SCORE 1.0090891e-22) Ideal Program: (<SCHEDULE> NIL NIL NIL NIL NIL NIL NIL (<INTERVAL> NIL NIL NIL NIL :STR ((END ((FRAME *SIMPLE-TIME) (HOUR 1))) (START ((FRAME *SIMPLE-TIME) (HOUR 11))) (INCL-EXCL INCLUSIVE) (FRAME *INTERVAL)) :COV (11 10 9) : SCORE 1.2102125e-38) NIL NIL NIL :STR ((FRAME *SCHEDULE) (ATTITUDE *LET-S) (WHAT ((FRAME *IT)))) :COV (6 5 4 3 2 1) : SCORE 1.4012985e-45) (<SCHEDULE> NIL T NIL NIL NIL T T NIL NIL NIL NIL :STR ((FRAME *SCHEDULE) (WHAT ((FRAME *IT))) (NEGATIVE +) (WHO ((FRAME *WE)))) :COV (6 5 4 3 2) : SCORE 3.05483e-43) (<SCHEDULE> NIL NIL NIL NIL NIL T T NIL NIL NIL NIL :STR ((FRAME *SCHEDULE) (NEGATIVE +) (WHO ((FRAME *WE)))) :COV (5 4 3 2) : SCORE 2.2584231e-37) (<SCHEDULE> NIL T NIL NIL NIL T NIL NIL NIL NIL NIL : STR ((FRAME *SCHEDULE) (WHAT ((FRAME *IT))) (WHO ((FRAME *WE)))) :COV (6 5 4) : SCORE 1.861576e-27) (<SIMPLE-TIME> NIL NIL NIL NIL NIL NIL T NIL : STR ((FRAME *SIMPLE-TIME) (HOUR 1)) :COV (11) : SCORE 2.2223547e-18) (<IT> :STR ((FRAME *IT)) :COV (6) : SCORE 2.593466e-13) Interpretation: Let's schedule it for from eleven o'clock till one o'clock Figure 4: Repair example It first takes a list of functions and terminal sym- bols and randomly generates a population of pro- grams. It then evaluates each program for its \"fit- ness\" according to some predetermined set of crite- ria. The most fit programs are then paired up and used to produce the next generation by means of a crossover operation whereby a pair of subprograms, one from each parent program, are swapped. The new generation is evaluated for its fitness, and the process continues for a preset number of generations. As mentioned earlier, because each semantic rep- resentation built by AUTOSEM contains a pointer to the constructor function that built it, each partial analysis can itself be treated as a constructor func- tion. Thus, the function set made available to the genetic programming search for each sentence need- ing repair is derived from the set of partial anal- yses extracted from the parser's chart. A number of the functions produced for the example are dis- played in Figure 4. Some functions have been omit- ted for brevity. The functions are displayed as func- tion calls, with the name of the function followed by its arguments. The name of each function corre- sponds to the semantic type from the meaning rep- resentation that corresponds to the associated par- tial analysis. Following this is a list of place holders corresponding to each argument position associated with the semantic type, as described in Section 2. Each place holder is either nil if it is an open place holder, or t if the position has already been filled in the corresponding partial analysis. The STR field contains the corresponding partial analysis. This is the \"result so far\" parameter discussed in Section 3. The COV field lists the positions in the sentence covered by the partial analysis. Note that in the example sentence, the word \"don't\" covers both po- sitions 2 and 3 since the parser expands the con- traction before parsing. The SCORE field contains the statistical score assigned by the parser's statis- tical disambiguation procedure described in (Rosé and Lavie, to appear). The repair process begins as the genetic program- ming algorithm composes the function definitions into programs that assemble the fragments produced by the parser. The genetic programming algorithm has access to a list of type restrictions that are placed on each argument position by the meaning repre- sentation specification. Thus, the algorithm ensures that the programs that are generated do not vio- late any of the meaning representation's type restric- tions. Once a population of programs is generated ran- domly, each program is evaluated for its fitness. A simple function implements a preference for pro- grams that cover more of the sentence with fewer steps while using the analyses the parser assigned the best statistical scores to. A score between 0 and 1 is first assigned to each program corresponding to the percentage of the input sentence it covers. A second score between 0 and 1 estimates how com- plicated the program is by dividing the number of function calls by the length of the sentence and sub- tracting this number from 1. A third score is as- signed the average of the statistical scores assigned by the parser to the fragments used in the program. Using coefficients based on an intuitive assignment of relative importance to the three scores, the final fit- ness value of each program is 1-[(.55 coverageS) + (.25 * complexityS) + (.2 * statisticalS)]. A typed version of the original crossover algorithm described in (Koza, 1992; Koza, 1994) was used to ensure that new programs would not violate any type restrictions or include more than one partial analysis covering the same span of text. This was accomplished by first making for each subprogram a list of the subprograms from the alternate program it could be inserted into without violating any seman- tic constraints. From these two lists it is possible to generate a list of all quadruples that specify a sub- program from each parent program to be removed and which subprogram from the alternate parent program they could be inserted into. From this list, all quadruples were removed that would either cause a span of text to be covered more than once in a resulting program or would require a subprogram to be inserted into a subprogram that would have been removed. From the remaining list, a quadruple was selected randomly. The corresponding crossover operation was then executed and the resulting two new programs were returned. While this typed ver- sion of crossover is more complex than the original crossover operation, it can be executed very rapidly in practice because the programs are relatively small and the semantic type restrictions ensure than the initial lists generated are correspondingly small. 5 Related Work Recent approaches to robust parsing focus on shal- low or partial parsing techniques (Van Noord, 1997; Worm, 1998; Ait-Mokhtar and Chanod, 1997; Ab- ney, 1996). Rather than attempting to construct a parse covering an entire ungrammatical sentence as in (Lehman, 1989; Hipp, 1992), these approaches at- tempt to construct analyses for maximal contiguous portions of the input. The weakness of these partial parsing approaches is that part of the original mean- ing of the utterance may be discarded with the por- tion(s) of the utterance that are skipped in order to find a parsable subset. Information communicated by the relationships between these fragments within the original text is lost if these fragments are not combined. Thus, these less powerful algorithms es- sentially trade effectiveness for efficiency. Their goal is to introduce enough flexibility to gain an accept- able level of coverage at an acceptable computational expense. Some partial parsing approaches have been cou- pled with a post-parsing repair stage (Danieli and Gerbino, 1995; Rosé and Waibel, 1994; Rosé, 1997; Van Noord, 1997; Kasper et al., 1999) The goal behind these two stage approaches is to increase the coverage over partial parsing alone at a rea- sonable computational cost. Until the introduction of AUTOSEM, the ROSE approach, introduced in (Rosé, 1997), was unique in that it achieved this goal without either requiring hand coded knowledge specifically dedicated to repair or excessive amounts of interaction with the user. However, although (Rosé, 1997) demonstrates that the two stage ROSE approach is significantly faster than attempting to achieve the same quality of results in a single stage parsing approach, our evaluation demonstrates that it remains computationally intractable, requiring on average 67 seconds to repair a single parse on a 330 MHz Gateway 2000. In contrast, we demonstrate that AUTOSEM is on average 200 times faster, tak- ing only .33 seconds on average to repair a single parse while achieving results of superior quality. 6 Evaluation An experiment was conducted to evaluate AU- TOSEM's robustness by comparing the effectiveness and efficiency of AUTOSEM's repair approach with that of the alternative ROSE approach. The test set used for this evaluation contains 750 sentences extracted from a corpus of spontaneous scheduling dialogues collected in English. For both repair ap- proaches we used the meaning representation devel- oped for the appointment scheduling domain that was used in previous evaluations of the ROSE ap- proach (Rosé, 1997). It consists of 260 semantic types, each expressing domain specific concepts for the appointment scheduling domain such as busy, cancel, and out-of-town. The ROSE meaning rep- resentation specification was easily converted to the format used in AUTOSEM. Because a pre-existing semantic grammar was available that parsed directly onto this meaning representation, that grammar was used in the parsing stage to construct analyses. The final meaning representation structures for the first 300 sentences were then passed to a generation com- ponent, and the resulting texts were graded by a human judge not involved in developing the research reported here. Each result was graded as either Bad, Okay, or Perfect, where Perfect indicates that the re- sult was fluent and communicated the idea from the original sentence. A grade of Okay indicates that the result communicated the correct information, but not fluently. Those graded Bad either communi- cated incorrect information or were missing part or all of the information communicated in the original sentence. Each sentence was parsed in two different modes. In LC w/restarts mode, the parser was allowed to construct analyses for contiguous portions of input starting at any point in the sentence. In LCFLEX mode, the parser was allowed to start an analysis at any point and skip up to three words within the anal- ysis. Because the AUTOSEM repair algorithm runs significantly faster than the ROSE repair algorithm, repair was attempted after every parse rather than only when a parse quality heuristic indicated a need as in the ROSE approach (Rosé, 1997). We com- pared the results of both AUTOSEM and ROSE in conjunction with the LC w/restarts parsing mode. The results are displayed in Figures 5 and 7. Be- cause the ROSE approach only runs the full repair algorithm when its parse quality heuristic indicates a need and the parser returns more than one par- tial analysis, it only attempted repair for 14% of the sentences in the corpus. Nevertheless, although the AUTOSEM repair algorithm ran for each sentence, Figure 5 demonstrates that processing time for pars- ing plus repair in the AUTOSEM condition was dra- matically faster on average than with ROSE. Aver- age processing time for the ROSE algorithm was 200 times slower than that for AUTOSEM on sentences where both repair algorithms were used. In addition to the advantage in terms of speed, the AUTOSEM repair approach achieved an acceptable grade (Okay or Perfect) on approximately 4% more sentences. Parsing in LC w/restarts mode plus repair was also compared with parsing in LCFLEX mode with skipping up to three words. Again, LC w/restarts + AUTOSEM repair achieved a slightly higher number of acceptable grades, although LCFLEX achieved a slightly higher number of Perfect grades. On long sentences (between 15 and 20 words), LCFLEX mode required almost three times as much time as LC w/restarts mode plus AUTOSEM re- pair. This evaluation confirms our previous results that two stage approaches offer a better processing time versus robustness trade-off. The primary difference between ROSE and AU- TOSEM is that ROSE uses a single repair function, MY-COMB, to combine any two fragments by referring to the meaning representation specification. While it is possible to obtain the same set of repair hy- potheses with ROSE as with AUTOSEM, the ROSE approach insulates the genetic search from the se- mantic restrictions imposed by the meaning repre- sentation. These restrictions are visible only locally within individual applications of the MY-COMB func- tion. Thus, MY-COMB must be able to cope with the case where the arguments passed in cannot be com- bined. Large portions of the programs generated by ROSE as repair hypotheses do not end up contribut- ing to the resulting output. Parse Time 45 40 ROSE CARMEL ------ 35 30 25 20 15 10 5 0 0 5 10 15 20 Sentence Length Figure 5: Processing Times for Alternative Strategies 14 12 10 8 6 4 2 0 0 5 10 15 20 Sentence Length Figure 6: Processing Times for Alternative Strategies LCFLEX + repair LCFLEX ---x--- LC w restarts + repair LC w restarts ated by ROSE must therefore be much larger than in AUTOSEM in order to obtain the same results. Fur- thermore, the fitness of each repair hypothesis can only be computed by executing the program to ob- tain a result. The combination of all of these things makes the process of fitness evaluation in ROSE far more costly than in AUTOSEM. In contrast, AU- TOSEM's constructor function definitions make it possible for the genetic search to make use of seman- tic restrictions to speed up the process of converging on a high quality repair hypothesis. The tremendous speed-up offered by the AUTOSEM approach makes it practical to apply repair more often and to use a larger generation size (50 individuals as opposed to 32) and a larger number of generations (5 as opposed to 4) for the genetic search. 7 Current Directions In this paper we described AUTOSEM, a new robust semantic interpretation framework. Our evaluation demonstrates that AUTOSEM achieves a greater level of robustness 200 times more efficiently than the most similar competing approach. In AUTOSEM, the mapping between syntactic Bad Okay Perfect Total Acceptable LC w/restarts 92 (30.7%) 61 (20.3%) 147 (49.0%) 209 (69.3%) LCFLEX 72 (24.0%) 67 (22.3%) 161 (53.7%) 228 (76.0%) LC w/restarts + ROSE 75 (25.0%) 68 (22.7%) 157 (52.3%) 225 (75.0%) LC w/restarts + AUTOSEM 64 (21.3%) 84 (28.0%) 152 (50.7%) 236 (78.7%) LCFLEX+ AUTOSEM 64 (21.3%) 78 (26.0%) 158 (52.7%) 236 (78.7%) Figure 7: Interpretation quality with and without repair functional roles and semantic arguments is com- pletely determined in the current version. In some cases, such as with copular constructions and with adjunct prepositional phrases, it would be useful to introduce some non-determinism so that, for exam- ple, semantic selectional restrictions between the ob- ject of the preposition and the semantic structure that the prepositional phrase is attaching to can more easily play a role in selecting the appropri- ate semantic relationship. Exploring approaches for achieving this non-determinism efficiently is one of our current objectives. 8 Acknowledgements Special thanks are due to the JANUS multi- lingual speech-to-speech translation project for mak- ing their interlingua specification and semantic pars- ing and generation grammars available for the eval- uation reported here. This research was supported by NSF Grant IRI-94-57637 and Grant 9720359 to CIRCLE, a center for research on intelligent tutor- ing. References S. Abney. 1996. Partial parsing via finite-state cascades. In Proceedings of the Eighth European Summer School In Logic, Language and Informa- tion, Prague, Czech Republic. S. Ait-Mokhtar and J. Chanod. 1997. Incremental finite-state parsing. In Proceedings of the Fifth Conference on Applied Natural Language Process- ing. M. Dalrymple. 1999. Semantics and Syntax in Lex- ical Functional Grammar. The MIT Press. M. Danieli and E. Gerbino. 1995. Metrics for evalu- ating dialogue strategies in a spoken language sys- tem. In Working Notes of the AAAI Spring Sym- posium on Empirical Methods in Discourse Inter- pretation and Generation. R. Freedman, C. P. Rosé, M. A. Ringenberg, and K. VanLehn. to appear. Its tools for natural language dialogue: A domain-independent parser and planner. In Proceedings of the Intelligent Tu- toring Systems Conference. R. Grishman, C. Macleod, and A. Meyers. 1994. COMLEX syntax: Building a computational lexi- con. In Proceedings of the 15th International Con- ference on Computational Linguistics (COLING- 94). D. R. Hipp. 1992. Design and Development of Spoken Natural-Language Dialog Parsing Systems. Ph.D. thesis, Dept. of Computer Science, Duke University. W. Kasper, B. Kiefer, H. Krieger, C. Rupp, and K. Worm. 1999. Charting the depths of robust speech parsing. In Proceedings of the 37th An- nual Meeting of the Association for Computa- tional Linguistics. J. Koza. 1992. Genetic Programming: On the Pro- gramming of Computers by Means of Natural Se- lection. MIT Press. J. Koza. 1994. Genetic Programming II. MIT Press. A. Lavie and C. P. Rosé. 2000. Optimal ambi- guity packing in unification-augmented context- free grammars. In Proceedings of the International Workshop on Parsing Technologies. A. Lavie. 1995. A Grammar Based Robust Parser For Spontaneous Speech. Ph.D. thesis, School of Computer Science, Carnegie Mellon University. J. F. Lehman. 1989. Adaptive Parsing: Self- Extending Natural Language Interfaces. Ph.D. thesis, School of Computer Science, Carnegie Mel- lon University. C. P. Rosé and A. Lavie. to appear. Balancing ro- bustness and efficiency in unification augmented context-free parsers for large practical applica- tions. In J. C. Junqua and G. Van Noord, editors, Robustness in Language and Speech Technologies. Kluwer Academic Press. C. P. Rosé and A. Waibel. 1994. Recovering from parser failures: A hybrid statistical/symbolic ap- proach. In Proceedings of The Balancing Act: Combining Symbolic and Statistical Approaches to Language workshop at the 32nd Annual Meeting of the ACL. C. P. Rosé. 1997. Robust Interactive Dialogue Inter- pretation. Ph.D. thesis, School of Computer Sci- ence, Carnegie Mellon University. G. Van Noord. 1997. An efficient implementation of the head-corner parser. Computational Linguis- tics, 23(3). K. Worm. 1998. A model of robust processing of spontaneous speech by integrating viable frag- ments. In Proceedings of COLING-ACL 98."
  },
  {
    "title": "Ambiguity Packing in Constraint-based Parsing - Practical Results",
    "abstract": "We describe a novel approach to 'packing' of local ambiguity in parsing with a wide-coverage HPSG grammar, and provide an empirical assessment of the interaction between various packing and parsing strategies. We present a linear-time, bidirectional subsumption test for typed feature structures and demonstrate that (a) subsumption- and equivalence-based packing is applicable to large HPSG grammars and (b) average parse complexity can be greatly reduced in bottom-up chart parsing with comprehensive HPSG implementations.",
    "content": "1 Background The ambiguity inherent in natural language means that during parsing, some segments of the input string may end up being analysed as the same type of linguistic object in several different ways. Each of these different ways must be recorded, but subse- quent parsing steps must treat the set of analyses as a single entity, otherwise the computation becomes theoretically intractable. Earley's algorithm (Ear- ley, 1970), for example, avoids duplication of parse items by maintaining pointers to alternative deriva- tions in association with the item. This process has been termed 'local ambiguity packing' (Tomita, 1985), and the structure built up by the parser, a 'parse forest' (Billot & Lang, 1989). Context free (CF) grammars represent linguistic objects in terms of atomic category symbols. The test for duplicate parse items and thus being able to pack the sub- analyses associated with them is equality of cate- gory symbols. In the final parse forest every differ- ent combination of packed nodes induces a distinct, valid parse tree. Most existing unification-based parsing systems either implicitly or explicitly contain a context-free core. For example, in the CLE (Alshawi, 1992) the (manually-assigned) functors of the Prolog terms forming the categories constitute a CF 'backbone'. In the Alvey Tools system (Carroll, 1993) each dis- tinct set of features is automatically given a unique identifier and this is associated with every category containing those features. The packing technique has been shown to work well in practice in these and similar unification-augmented CF systems: the parser first tests for CF category equality, and then either (a) checks that the existing feature structure subsumes the newly derived one (Moore & Alshawi, 1992), or (b) forms an efficiently processable disjunc- tion of the feature structures (Maxwell and Kaplan, 1995). Extracting parses from the parse forest is similar to the CF case, except that a global check for consistency of feature values between packed nodes or between feature structure disjuncts is required (this global validation is not required if the sub- sumption test is strengthened to feature structure equivalence). In contrast, there is essentially no CF compo- nent in systems which directly interpret HPSG gram- mars. Although HPSG feature structures are typed, an initial CF category equality test cannot be im- plemented straightforwardly in terms of the top- level types of feature structures since two compat- ible types need not be equal, but could stand in a subtype-supertype relationship. In addition, the feature structure subsumption test is potentially ex- pensive since feature structures are large, typically containing hundreds of nodes. It is therefore an open question whether parsing systems using grammars of this type can gain any advantage from local ambi- guity packing. The question is becoming increasingly impor- tant, though, as wide-coverage HPSG grammars are starting to be deployed in practical applications- for example for 'deep' analysis in the VerbMo- bil speech-to-speech translation system (Wahlster, 1997; Kiefer, Krieger, Carroll, & Malouf, 1999).1 In this paper we answer the question by demonstrating that (a) subsumption- and equivalence-based feature structure packing is applicable to large HPSG gram- mars, and (b) average complexity and time taken for the parsing task can be greatly reduced. In Section 2 we present a new, linear-time, bidirec- 1A significant body of work on efficient processing with such grammars has been building up recently, with investi- gations into efficient feature structure operations, abstract- machine-based compilation, CF backbone computation, and finite-state approximation of HPSG derivations, amongst oth- ers (Flickinger, Oepen, Uszkoreit, & Tsujii, 2000). tional subsumption test for typed feature structures, which we use in a bottom-up, chart-based parsing algorithm incorporating novel, efficient accounting mechanisms to guarantee minimal chart size (Sec- tion 3). We present a full-scale evaluation of the techniques on a large corpus (Section 4), and com- plete the picture with an empirically-based discus- sion of grammar restrictors and parsing strategies (Section 5). 2 Efficient Subsumption and Equivalence Algorithms Our feature structure subsumption algorithm² as- sumes totally well-typed structures (Carpenter, 1992) and employs similar machinery to the quasi-destructive unification algorithm described by Tomabechi (1991). In particular, it uses temporary pointers in dag nodes, each pointer tagged with a generation counter, to keep track of intermediate results in processing; incrementing the generation counter invalidates all temporary pointers in a sin- gle operation. But whereas quasi-destructive unifi- cation makes two passes (determining whether the unification will be successful and then copying out the intermediate representation) the subsumption algorithm makes only one pass, checking reentran- cies and type-supertype relationships at the same time. The algorithm, shown in Figure 1, also si- multaneously tests if both feature structures sub- sume each other (i.e. they are equivalent), if either subsumes the other, or if there is no subsumption relation between them in either direction. The top-level entry point dag-subsumes-p() and subsidiary function dag-subsumes-po() each return two values, held in variables forwardp and back- wardp, both initially true, recording whether it is possible that the first dag subsumes the second and/or vice-versa, respectively. When one of these possibilities has been ruled out the appropriate vari- able is set to false; in the statement of the algorithm the two returned values are notated as a pair, i.e. (forwardp, backwardp). If at any stage both vari- ables have become set to false the possibility of sub- sumption in both directions has been ruled out so the algorithm exits. The (recursive) subsidiary function dag-subsumes- po () does most of the work, traversing the two input 2 Although independently-developed implementations of essentially the same algorithm can be found in the source code of The Attribute Logic Engine (ALE) version 3.2 (Carpenter & Penn, 1999) and the SICStus Prolog term utilities library (Penn, personal communication), we believe that there is no previous published description of the algorithm. 3 Feature structure F subsumes feature structure G iff: (1) if path p is defined in F then p is also defined in G and the type of the value of p in F is a supertype or equal to the value in G, and (2) ali paths that are reentrant in F are also reentrant in G. dags in step. First, it checks whether the current node in either dag is involved in a reentrancy that is not present in the other: for each node visited in one dag it adds a temporary pointer (held in the 'copy' slot) to the corresponding node in the other dag. If a node is reached that already has a pointer then this is a point of reentrancy in the dag, and if the pointer is not identical to the other dag node then this reentrancy is not present in the other dag. In this case the possibility that the former dag sub- sumes the latter is ruled out. After the reentrancy check the type-supertype relationship between the types at the current nodes in the two dags is deter- mined, and if one type is not equal to or a supertype of the other then subsumption cannot hold in that direction. Finally, after successfully checking the type-supertype relationships, the function recurses into the arcs outgoing from each node that have the same label. Since we are assuming totally well-typed feature structures, it must be the case that either the sets of arc labels in the two dags are the same, or one is a strict superset of the other. Only arcs with the same labels need be processed; extra arcs need not since the type-supertype check at the two nodes will already have determined that the feature struc- ture containing the extra arcs must be subsumed by the other, and they merely serve to further specify it and cannot affect the final result. Our implementation of the algorithm contains ex- tra redundant but cheap optimizations which for rea- sons of clarity are not shown in figure 1; these in- clude tests that forwardp is true immediately before the first supertype check and that backwardp is true before the second.4 The use of temporary pointers means that the space complexity of the algorithm is linear in the sum of the sizes of the feature structures. However, in our implementation the 'copy' slot that the point- ers occupy is already present in each dag node (it is required for the final phase of unification to store new nodes representing equivalence classes), so in practice the subsumption test does not allocate any new storage. All pointer references take constant time since there are no chains of 'forwarded' point- ers (forwarding takes place only during the course of unification and no forwarded pointers are left after- wards). Assuming the supertype tests can be carried 4 There is scope for further optimisation of the algorithm in the case where dag1 and dag2 are identical: full processing in- side the structure is not required (since all nodes inside it will be identical between the two dags and any strictly internal reentrancies will necessarily be the same), but we would still need to assign temporary pointers inside it so that any exter- nal reentrancies into the structure would be treated correctly. In our tests we have found that as far as constituents that are candidates for local ambiguity packing are concerned there is in fact little equality of structures between them, so special equality processing does not justify the extra complication. 1 procedure dag-subsumes-p(dag1, dag2) = 2 {establish context for non-local exit} 3 catch with tag 'fail' dag-subsumes-p0(dag1, dag2, true, true); 4 {reset temporary 'copy' pointers} 5 return (forwardp, backwardp); 6 end 7 procedure dag-subsumes-p0(dag1, dag2, forwardp, backwardp) = 8 if (dag1.copy is empty) then dag1.copy ← dag2; {check reentrancies} 9 else if(dag1.copy ≠ dag2) then forwardp ← false; fi 10 if (dag2.copy is empty) then dag2.copy ← dag1; 11 else if (dag2.copy ≠ dagl) then backwardp ← false; fi 12 if (forwardp = false and backwardp = false) then 13 throw (false, false) with tag 'fail'; {reentrancy check failed} 14 fi 15 if (not supertype-or-equal-p(dag1.type, dag2.type)) then forwardp ← false; fi {check types} 16 17 if (not supertype-or-equal-p(dag2.type, dag1.type)) then backwardp ← false; fi if (forwardp = false and backwardp = false) then 18 throw (false, false) with tag 'fail'; {no subtype relations} 19 fi 20 21 for each arc in intersect(dag1.arcs, dag2.arcs) do {check shared arcs recursively} 22 dag-subsumes-p0(destination of arc for dagl, destination of arc for dag2, forwardp, backwardp); 23 od 24 return (forwardp, backwardp); {signal result to caller} 25 end Figure 1: Bidirectional, linear-time feature structure subsumption (and equivalence) algorithm. out in constant time (e.g. by table lookup), and that the grammar allows us to put a small constant upper bound on the intersection of outgoing arcs from each node, the processing in the body of dag-subsumes- p0() takes unit time. The body may be executed up to N times where N is the number of nodes in the smaller of the two feature structures. So overall the algorithm has linear time complexity. In practice, our implementation (in the environment described in Section 4) performs of the order of 34,000 top-level feature structure subsumption tests per second. 3 Ambiguity Packing in the Parser Moore and Alshawi (1992) and Carroll (1993) have investigated local ambiguity packing for unification grammars with CF backbones, using CF category equality and feature structure subsumption to test if a newly derived constituent can be packed. If a new constituent is equivalent to or subsumed by an existing constituent, then it can be packed into the existing one and will take no further part in pro- cessing. However, if the new constituent subsumes an existing one, the situation is not so straightfor- ward: either (a) no packing takes place and the new constituent forms a separate edge (Carroll, 1993), or (b) previous processing involving the old constituent is undone or invalidated, and it is packed into the new one (Moore & Alshawi, 1992; however, it is un- clear whether they achieve maximal compactness in practice: see Table 1). In the former case the parse forest produced will not be optimally compact; in the latter it will be, but maintaining chart consis- tency and parser correctness becomes a non-trivial problem. Packing of a new edge into an existing one we call proactive (or forward) packing; for the more complex situation involving a new edge subsuming an existing one we introduce the term retroactive (or backward) packing. Several issues arise when packing an old edge (old) into one that was newly derived (new) retroactively: (i) everything derived from old (called derivatives of old in the following) must be invalidated and ex- cluded from further processing (as new is known to generate more general derivatives); and (ii) all pending computation involving old and its deriva- tives has to be blocked efficiently. Derivatives of old that are invalidated because of retroactive pack- ing may already contain packed analyses, however, which still represent valid ambiguity. These need to be repacked into corresponding derivatives of new when those become available. In turn, derivatives of old may have been packed already, such that they need not be available in the chart for subsequent sub- sumption tests. Therefore, the parser cannot simply delete everything derived from old when it is packed; instead, derivatives must be preserved (but blocked) 1 procedure block(edge, mark) = 2 if (edge.frozen = false or mark = freeze) then edge.frozen ← mark; fi 3 for each parent in edge.parents do block(parent, freeze); od 4 end 5 procedure packed-edge-p(new) = 6 for each old in chart[new.start][new.end] do 7 (forwardp, backwardp) ← dag-subsumes-p(old.dag, new.dag); 8 if (forwardp = true and old.frozen = false) then 9 old.packed ← (new | old.packed); 10 return true; 11 fi 12 if (backwardp) then 13 new.packed ← (new.packed ⊕ old.packed); 14 old.packed(); 15 if (old.frozen = false) then new.packed ← (old | new.packed); fi 16 block(old, frost); 17 delete(old, chart); 18 fi 19 od 20 return false; 21 end {mark current edge} {recursively freeze derivatives} {passive edges with same span} {test category subsumption} {equivalent or proactive packing} {pack 'new' into 'old'} {return to caller; signal success} {retroactive packing} {raise all packings into new host} {pack 'old' into 'new'} {frost 'old' and freeze derivatives} {remove 'old' from the chart} {signal failure to pack 'new' to caller} Figure 2: Algorithm called on each newly derived edge to achieve maximal packing. until the derivations have been recomputed on the basis of new. As new is equivalent to or more gen- eral than old it is guaranteed to derive at least the same set of edges; furthermore, the derivatives of new will again be equivalent to or more general than the corresponding edges derived from old. The procedure packed-edge-p(), sketched in Fig- ure 2, achieves pro- and retroactive packing with- out significant overhead in the parser; the algorithm can be integrated with arbitrary bottom-up (chart- based) parsing strategies. The interface assumes that the parser calls packed-edge-p() on each new edge new as it is derived; a return value of true indi- cates that new was packed proactively and requires no further processing. Conversely, a false return value from packed-edge-p() signals that new should subsequently undergo regular processing. The sec- ond part of the interface builds on notions we call frosting and freezing, meaning temporary and per- mament invalidation of edges, respectively. As a side-effect of calls to packed-edge-p(), a new edge can cause retroactive packing, resulting in the dele- tion of one or more existing edges from the chart and blocking of derivatives. Whenever the parser accesses the chart (i.e. in trying to combine edges) or retrieves a task from the agenda, it is expected to ignore all edges and parser tasks involving such edges that have a non-null 'frozen' value. When an existing edge old is packed retroactively, it is frosted and ignored by the parser; as old now represents lo- cal ambiguity, it still has to be taken into account when the parse forest is unpacked. Derivatives of old, on the other hand, need to be invalidated in both further parsing and later unpacking, since they would otherwise give rise to spurious analyses; ac- cordingly, such derivatives are frozen permanently. Frosting and freezing is done in the subsidiary pro- cedure block () that walks up the parent link recur- sively, storing a mark into the 'frozen' slot of edges that distinguishes between temporary frosting (in the top-level call) and permanent freezing (in recur- sive calls). For a newly derived edge new, packed-edge-p() tests mutual subsumption against all passive edges that span the same portion of the input string. When forward subsumption (or equivalence) is de- tected and the existing edge old is not blocked, reg- ular proactive packing is performed (adding new to the packing list for old) and the procedure returns immediately.6 In the case of backward subsump- The situation is simpler in the CLE parser (Moore & Al- shawi, 1992) because constituents and dominance relations are separated in the chart. The CLE encoding, in fact, does not record the actual daughters used in building a phrase (e.g. as unique references or pointers, as we do), but instead preserves the category information (i.e. a description) of those daugh- ters. Hence, in extracting complete parses from the chart, the CLE has to perform (a limited) search with re-unification of categories; in this respect, the CLE parse forest still is an underspecified representation of the set of analyses, whereas our encoding (see below) facilitates unpacking without extra search. 6 Packing an edge e<sub>1</sub> into another edge e<sub>2</sub> logically means that e<sub>2</sub> will henceforth serve as a representative for e<sub>1</sub> and the derivation(s) that it encodes. In practice, e<sub>1</sub> is removed from the chart and ignored in subsequent parser action and subsumption tests. Only in unpacking the parse forest will No Chart Packing Pro- and Retroactive Packing 20000 20000 17500 • passive edges 17500 • passive edges 15000 15000 12500 12500 10000 10000 7500 7500 5000 5000 2500 2500 0 0 1 3 5 7 9 11 13 15 17 19 21 23 25 String Length (in words) 1 3 5 7 9 11 13 15 17 19 21 23 25 String Length (in words) Figure 3: Effects of maximal ambiguity packing on the total chart size (truncated above 25 words). tion, analyses packed into old are raised into new (using the append operator '' because new can at- tract multiple existing edges in the loop); old itself is only packed into new when it is not blocked already. Finally, old is frosted, its derivatives are recursively frozen, and old is deleted from the chart. In contrast to proactive packing, the top-level loop in the pro- cedure continues so that new can pick up additional edges retroactively. However, once a backward sub- sumption is detected, it follows that no proactive packing can be achieved for new, as the chart can- not contain an edge that is more general than old. 4 Empirical Results We have carried out an evaluation of the algo- rithms presented above using the LinGO grammar (Flickinger & Sag, 1998), a publicly-available, multi- purpose, broad-coverage HPSG of English developed at CSLI Stanford. With roughly 8,000 types, an av- erage feature structure size of around 300 nodes, and 64 lexical and grammar rules (fleshing out the inter- action of HPSG ID schemata, wellformedness prin- ciples, and LP constraints), LinGO is among the largest HPSG grammars available. We used the LKB system (Copestake, 1992, 1999) as an experimen- tation platform since it provides a parameterisable bottom-up chart parser and precise, fine-grained profiling facilities (Oepen & Flickinger, 1998).7 All of our results were obtained in this environment, running on a 300 Mhz UltraSparc, and using a bal- anced test set of 2,100 sentences extracted from VerbMobil corpora of transcribed speech: input lengths from 1 to 20 words are represented with 100 test items each; although sentences in the corpus range up to 36 words in length there are relatively few longer than 20 words. the category of e1 and its decomposition(s) in daughter edges (and corresponding subtrees) be used again, to multiply out and project local ambiguity. The LinGO grammar and LKB software are publicly avail- able at 'http://lingo.stanford.edu/'. Figure 3 compares total chart size (in all-paths mode) for the regular LKB parser and our variant with pro- and retroactive packing enabled. Factor- ing ambiguity reduces the number of passive edges by a factor of more than three on average, while for a number of cases the reduction is by a factor of 30 and more. Compared to regular parsing, the rate of increase of passive chart items with respect to sen- tence length is greatly diminished. To quantify the degree of packing we achieve in practice, we re-ran the experiment reported by Moore and Alshawi (1992): counting the number of nodes required to represent all readings for a simple declarative sentence containing zero to six preposi- tional phrase (PP) modifiers. The results reported by Moore and Alshawi (1992) (using the CLE gram- mar of English) and those obtained using pro- and retroactive packing with the LinGO grammar are presented in Table 1.8 Although the comparison involves different grammars we believe it to be in- structive, since (i) both grammars have comprehen- sive coverage, (ii) derive the same numbers of read- ings for all test sentences in this experiment, (iii) require (almost) the same number of nodes for the basic cases (zero and one PP), (iv) exhibit a similar size in nodes for one core PP (measured by the in- crement from n = 0 to n = 1), and (v) the syntactic simplicity of the test material hardly allows crosstalk 8 Moore and Alshawi (1992) use the terms 'node' and 'record' interchangeably in their discussion of packing, where the CLE chart is comprised of separate con(stituent) and ana(lysis) entries for category and dominance information, respectively. It is unclear whether the counting of 'packed nodes' in Moore and Alshawi (1992) includes con records or not, since only ana records are required in parse tree recovery. In any case, both types of chart record need to be checked by subsumption as new entries are added to the chart. Con- versely, in our setup each edge represents not only the node category, but also pointers to the daughter(s) that gave rise to this edge, and moreover, where applicable, a list of packed edges that are subsumed by the category (but not necessarily by the daughters). For the LKB, the column 'result edges' in Table 1 refers to the total number of edges in the chart that contribute to at least one complete analysis. Kim saw a cat (in the hotel)\" Moore & Alshawi Our Method CPU Time n readings packed nodes result edges parse unpack plain # ÷ # ÷ msec msec msec 0 1 10 1.0 11 1.0 210 10 180 1 2 21 2.1 23 2-1 340 40 290 2 5 38 3.8 38 3.5 460 80 530 3 14 62 6-2 56 5-1 600 200 1,180 4 42 94 9.4 77 7.0 870 590 2,990 5 132 135 13-5 101 9-2 1,150 1,860 8,790 6 429 186 18-6 128 11.6 1,460 5,690 28,160 Table 1: Comparison of retroactive packing vs. the method used by Moore and Alshawi (1992); columns labeled '' show the relative increase of packed nodes (result edges) normalised to the n = 0 baseline. with other grammatical phenomena. Comparing rel- ative packing efficiency with increasing ambiguity (the columns labeled '÷' in Table 1), our method ap- pears to produce a more compact representation of ambiguity than the CLE, and at the same time builds a more specific representation of the parse forest that can be unpacked without search. To give an impres- sion of parser throughput, Table 1 includes timings for our parsing and unpacking (validation) phases, contrasted with the plain, non-packing LKB parser: as would be expected, parse time increases linearly in the number of edges, while unpacking costs re- flect the exponential increase in total numbers of analyses; the figures show that our packing scheme achieves a very significant speedup, even when un- packing time is included in the comparison. 5 Choosing the Grammar Restrictor and Parsing Strategy In order for the subsumption relation to apply mean- ingfully to HPSG signs, two conditions must be met. Firstly, parse tree construction must not be dupli- cated in the feature structures (by means of the HPSG DTRS feature) but be left to the parser (i.e. recorded in the chart); this is achieved in a stan- dard way by feature structure restriction (Shieber, 1985) applied to all passive edges. Secondly, the pro- cessing of constraints that do not restrict the search space but build up new (often semantic) structure should be postponed, since they are likely to inter- fere with subsumption. For example, analyses that differ only with respect to PP attachment would have the same syntax, but differences in semantics may prevent them being packed. This problem can be overcome by using restriction to (temporarily) re- move such (semantic) attributes from lexical entries and also from the rule set, before they are input to the parser in the initial parse forest construction phase. The second, unpacking phase of the parser re- verts to the unrestricted constraint set, so we can al- low overgeneration in the first phase and filter glob- ally inconsistent analyses during unpacking. Thus, the right choice of grammar restrictor can be viewed as an empirical rather than analytical problem. Table 2 summarizes packing efficiency and parser performance for three different restrictors (labeled no, partial, and full semantics, respectively); to gauge effects of input complexity, the table is fur- ther subdivided by sentence length into two groups (of around 1,000 sentences each). Compared to reg- ular parsing, packing with the full semantics in place is not effective: the chart size is reduced slightly, but the extra cost for testing subsumption increases total parse times by a factor of more than four. Eliminat- ing all semantics (i.e. the entire HPSG CONT value), on the other hand, results in overgeneralisation: with less information in the feature structures we achieve the highest number of packings, but at the same time rules apply much more freely, resulting in a larger chart compared to parsing with a partial se- mantics; moreover, unpacking takes longer because the parse forest now contains inconsistent analyses. Restricting compositional semantics but preserving attributes that participate in selection and agree- ment results in minimal chart size and parsing time (shown in the partial semantics figures) for both di- visions of the test corpus. The majority of packings involve equivalent fea- ture structures which suggests that unpacking could be greatly simplified if the grammar restrictor was guaranteed to preserve the generative capacity of the grammar (in the first parsing phase); then, only packings involving actual subsumption would have to be validated in the unpacking phase. Finally, There is room for further investigation here: partly for theory-internal reasons, current development of the LinGO grammar is working towards a stricter separation of restrictive (selectional) and constructive (compositional) constraints in Parser Passive Packed Edges Trees Packings CPU Time (sec) Ξ コ コ parse unpack no semantics 116 0.9 15-5 4.1 2-6 1.8 0-37 0.05 1-10 words partial semantics full semantics 111 0.8 12-0 3.6 2.4 1.4 0-33 0.05 149 2.8 2-1 0.4 0-2 0-1 0-60 0.04 no packing 160 5-6 0-44 no semantics 622 1-2 179.0 42.1 23.8 26.0 2-37 0-70 > 10 words partial semantics full semantics 575 1.0 134-9 35-0 20-6 18-9 1-97 0-63 1693 33-9 38-3 3-4 2.9 3-2 29-40 0-56 no packing 2075 99.9 6.46 Table 2: Contrasting various grammar restrictors on short (top) and medium-length (bottom) inputs; all numbers are averaged over 1,000 items per class; packings are, from left to right: equivalence ('='), pro- ('') and retroactive ('C') packings, and the number of edges that were frozen ('1'). we note that the number of retroactive packings is relatively small, and on average each such packing leads to only one previously derived edge being in- validated. This, of course, is a function of the order in which edges are derived, i.e. the parsing strategy. All the results in Table 2 were obtained with a 'right corner' strategy which aims to exhaust compu- tation for any suffix of the input string before mov- ing the input pointer to the left; this is achieved by means of a scoring function end start (where start and end are the vertices of the derivation that would result from the computation, and n is the total input length) that orders parser tasks in the agenda. However, we have observed (Oepen & Callmeier, 2000) that HPSG-type, highly lexicalized grammars bene- fit greatly from a bidirectional, 'key'-driven, active parsing regime, since they often employ rules with underspecified arguments that are only instantiated by coreference with other daughters (where the 'key' daughter is the linguistic head in many but not all constructions). This requirement and the general non-predictability of categories derived for any to- ken substring (in particular with respect to unary rule applications), means that a particular parsing strategy may reduce retroactive packing but cannot avoid it in general. With pro- and retroactive pack- ing and the minimal accounting overhead, we find overall parser throughput to be very robust against variation in the parsing strategy. Lavie and Rosé (2000) present heuristics for ordering parser actions to achieve maximally compact parse forests-though only with respect to a CF category backbone-in the absence of retroactive packing; however, the tech- niques we have presented here allow local ambigu- ity packing and parser tuning-possibly including priority-driven best-first search-to be carried out mostly independently of each other. the grammar and underlying semantic theory. We expect that our approach to packing will benefit from these developments. 6 Conclusions We have presented novel algorithms for efficient sub- sumption checking and pro- and retroactive local ambiguity packing with large feature structures, and have provided strong empirical evidence that our approach can be applied beneficially to chart pars- ing with a large, broad-coverage HPSG of English. By comparison to previous work in unification-based parsing we have demonstrated that pro- and retroac- tive packing are well-suited to achieve optimal pack- ing; furthermore, experimental results obtained with a publicly-available HPSG processing platform con- firm that ambiguity packing can greatly reduce av- erage parse complexity for this type of grammars. In related work, Miyao (1999) describes an ap- proach to packing in which alternative feature struc- tures are represented as packed, distributed disjunc- tions of feature structure fragments. Although the approach may have potential, the shifting of com- plex accounting into the unification algorithm is at variance with the findings of Kiefer et al. (1999), who report large speed-ups from the elimination of disjunction processing during unification. Unfortu- nately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison. We intend to develop the approach presented in this paper in several directions. Firstly, we will en- hance the unpacking phase to take advantage of the large number of equivalence packings we observe. This will significantly reduce the amount of work it needs to do. Secondly, many application contexts and subsequent layers of semantic processing will not require unfolding the entire parse forest; here, we need to define a selective, incremental unpack- ing procedure. Finally, applications like VerbMo- bil favour prioritized best-first rather than all-paths parsing. Using slightly more sophisticated account- ing in the agenda, we plan to investigate priority propagation in a best-first variant of our parser. Acknowledgements We are grateful to Ulrich Callmeier, Ann Copestake, Dan Flickinger, and three anonymous reviewers for comments on a draft of the paper, to Bob Moore for a detailed explanation of the workings of the CLE parser, and to Gerald Penn for information about related implementations of the subsumption algo- rithm. The research was supported by the Deutsche Forschungsgemeinschaft as part of the Collaborative Research Division Resource-Adaptive Cognitive Pro- cesses, project B4 (PERFORM); and by a UK EPSRC Advanced Fellowship to the second author. References Alshawi, H. (Ed.). (1992). The Core Language En- gine. Cambridge, MA: MIT Press. Billot, S., & Lang, B. (1989). The structure of shared forests in ambiguous parsing. In Proceed- ings of the 27th Meeting of the Association for Computational Linguistics (pp. 143-151). Van- couver, BC. Carpenter, B. (1992). The logic of typed feature structures. Cambridge, UK: Cambridge Univer- sity Press. Carpenter, B., & Penn, G. (1999). ALE. The At- tribute Logic Engine. User's guide version 3.2. (Universität Tübingen: http://wwww.sfs.nphil .uni-tuebingen.de/~gpenn/ale.html) Carroll, J. (1993). Practical unification-based parsing of natural language (Technical Re- port # 314). Cambridge, UK: Computer Laboratory, Cambridge University. (Online at: ftp://ftp.cl.cam.ac.uk/papers/reports/ TR314-jac-practical-unif-parsing.ps.gz) Copestake, A. (1992). The ACQUILEX LKB. Rep- resentation issues in semi-automatic acquisition of large lexicons. In Proceedings of the 3rd ACL Con- ference on Applied Natural Language Processing (pp. 88-96). Trento, Italy. Copestake, A. (1999). The (new) LKB sys- tem. User's guide. (CSLI, Stanford Uni- versity: http://www-csli.stanford.edu/~aac/ lkb.html) Earley, J. (1970). An efficient context-free parsing algorithm. Communications of the ACM, 13 (2), 94-102. Flickinger, D., Oepen, S., Uszkoreit, H., & Tsu- jii, J. (Eds.). (2000). Journal of Natural Lan- guage Engineering. Special Issue on Efficient pro- cessing with HPSG: Methods, systems, evaluation. Cambridge, UK: Cambridge University Press. (in preparation) Flickinger, D. P., & Sag, I. A. (1998). Linguis- tic Grammars Online. A multi-purpose broad- coverage computational grammar of English. In CSLI Bulletin 1999 (pp. 64–68). Stanford, CA: CSLI Publications. Kiefer, B., Krieger, H.-U., Carroll, J., & Malouf, R. (1999). A bag of useful techniques for efficient and robust parsing. In Proceedings of the 37th Meeting of the Association for Computational Linguistics (pp. 473–480). College Park, MD. Lavie, A., & Rosé, C. (2000). Optimal ambiguity packing in context-free parsers with interleaved unification. In Proceedings of the 6th Interna- tional Workshop on Parsing Technologies (pp. 147–158). Trento, Italy. Maxwell III, J. T., & Kaplan, R. M. (1995). A method for disjunctive constraint satisfaction. In M. Dalrymple, R. M. Kaplan, J. T. Maxwell III, & A. Zaenen (Eds.), Formal issues in Lexical- Functional Grammar (pp. 381–401). Stanford, CA: CSLI Publications. Miyao, Y. (1999). Packing of feature structures for efficient unification of disjunctive feature struc- tures. In Proceedings of the 37th Meeting of the Association for Computational Linguistics (pp. 579–84). College Park, MD. Moore, R. C., & Alshawi, H. (1992). Syntactic and semantic processing. In H. Alshawi (Ed.), The Core Language Engine (pp. 129–148). Cam- bridge, MA: MIT Press. Oepen, S., & Callmeier, U. (2000). Measure for measure: Parser cross-fertilization. Towards in- creased component comparability and exchange. In Proceedings of the 6th International Workshop on Parsing Technologies (pp. 183–194). Trento, Italy. Oepen, S., & Flickinger, D. P. (1998). Towards sys- tematic grammar profiling. Test suite technology ten years after. Journal of Computer Speech and Language, 12 (4) (Special Issue on Evaluation), 411–436. Shieber, S. M. (1985). Using restriction to extend parsing algorithms for complex feature-based for- malisms. In Proceedings of the 23rd Meeting of the Association for Computational Linguistics (pp. 145–152). Chicago, IL. Tomabechi, H. (1991). Quasi-destructive graph uni- fication. In Proceedings of the 29th Meeting of the Association for Computational Linguistics (pp. 315–322). Berkeley, CA. Tomita, M. (1985). An efficient context-free parsing algorithm for natural languages. In Proceedings of the 9th International Joint Conference on Artifi- cial Intelligence (pp. 756–764). Los Angeles, CA. Wahlster, W. (1997). VerbMobil — Erken- nung, Analyse, Transfer, Generierung und Syn- these von Spontansprache (VerbMobil Report # 198). Saarbrücken, Germany: Deutsches Forschungszentrum für Künstliche Intelligenz GmbH."
  },
  {
    "title": "The Automatic Translation of Discourse Structures",
    "abstract": "We empirically show that there are significant differences between the discourse structure of Japanese texts and the discourse structure of their corresponding English translations. To improve translation quality, we propose a computational model for rewriting discourse structures. When we train our model on a parallel corpus of manually built Japanese and English discourse structure trees, we learn to rewrite Japanese trees as trees that are closer to the natural English rendering than the original ones.",
    "content": "1 Motivation Almost all current MT systems process text one sen- tence at a time. Because of this limited focus, MT systems cannot re-group and re-order the clauses and sentences of an input text to achieve the most natural rendering in a target language. Yet, even between languages as close as English and French, there is a 10% mismatch in number of sentences — what is said in two sentences in one language is said in only one, or in three, in the other (Gale and Church, 1993). For distant language pairs, such as Japanese and English, the differences are more significant. Consider, for example, Japanese sentence (1), a word-by-word \"gloss\" of it (2), and a two-sentence translation of it that was produced by a professional translator (3). [厚生省が昨年公表した] [人口の将来推計では、] (1) [将来、一·四九九人を最低に、] [その後は上昇に 転ずると] [推計していたが、] [早くも予想がはす れる] [見通しとなった。] [In its future population estimates¹] [made (3) public last year,²] [the Ministry of Health and Welfare predicted that the SAB would drop to a new low of 1.499 in the future,³] [but would make a comeback after that,¹] [increasing once again.⁵] [However, it looks as if that prediction will be quickly shattered.⁶] The labeled spans of text represent elementary discourse units (edus), i.e., minimal text spans that have an unambiguous discourse function (Mann and Thompson, 1988). If we analyze the text frag- ments closely, we will notice that in translating sen- tence (1), a professional translator chose to realize the information in Japanese unit 2 first (unit 2 in text (1) corresponds roughly to unit 1 in text (3)); to realize then some of the information in Japanese unit 1 (part of unit 1 in text (1) corresponds to unit 2 in text (3)); to fuse then information given in units 1, 3, and 5 in text (1) and realize it in English as unit 3; and so on. Also, the translator chose to re- package the information in the original Japanese sen- tence into two English sentences. At the elementary unit level, the correspondence between Japanese sentence (1) and its English trans- lation (3) can be represented as in (4), where j Ce denotes the fact that the semantic content of unit j is realized fully in unit e; j ⊇ e denotes the fact that the semantic content of unit e is realized fully in unit j; j = e denotes the fact that units j and e are semantically equivalent; and j ∩ e denotes the fact that there is a semantic overlap between units j and e, but neither proper inclusion nor proper equiv- alence. j1 ⊇ e2; j1 ⊇ e3; j2 = e1; j3 ⊂ e3; j4 ⊇ e4; j4 ⊇ e5; j5 ⊇ e3; j6 ⊂ e6; (4) [The Ministry of Health and Welfare last year (2) revealed¹] [population of future estimate ac- cording to²] [in future 1.499 persons as the lowest³] [that after *SAB* rising to turn that¹] [*they* estimated but⁵] [already the estimate artribution eaborarior-coject-attribute-e (1) 厚生省が昨年公表 (The Ministry of Health and Welfare last year revealed) (2) 入口の将来推計で (population - of future estimate acccrding - to) temaga after (3) 将来一・ロ九九 人を最低に `ifutur e 1499 persons a: the lowest) (4) その後は上昇に転す ると that after (SAB) rising - to turn that) Concessio e aberatior-object-attribute-e (5) (6) (7) 推計していむが早くも予想がはずれ 見通しとなった hey] estimated but) alreacy the rediction estinate miss a point) became) elaboration-object-attribute-e (1) In its future population estimates (2) made public last year, Contrast (3) the Ministry of Health and Welfare predicted that the SAB would drop to a new low of 1.499 in the future. concession. elaboration-additional (4) but would make a comeback after that, (5) increasing once again. (6) However, it looks as if that prediction will be quickly shattered. Figure 1: The discourse structures of texts (1) and (3). Hence, the mappings in (4) provide an explicit representation of the way information is re-ordered and re-packaged when translated from Japanese into English. However, when translating text, it is also the case that the rhetorical rendering changes. What is realized in Japanese using an CONTRAST relation can be realized in English using, for example, a com- PARISON or a CONCESSION relation. Figure 1 presents in the style of Mann and Thomp- son (1988) the discourse structures of text frag- ments (1) and (3). Each discourse structure is a tree whose leaves correspond to the edus and whose internal nodes correspond to contiguous text spans. Each node is characterized by a status (NUCLEUS or SATELLITE) and a rhetorical relation, which is a re- lation that holds between two non-overlapping text spans. The distinction between nuclei and satellites comes from the empirical observation that the nu- cleus expresses what is more essential to the writer's intention than the satellite; and that the nucleus of a rhetorical relation is comprehensible independent of the satellite, but not vice versa. When spans are equally important, the relation is multinuclear: for example, the CONTRAST relation that holds between unit [3] and span [4.5] in the rhetorical structure of the English text in figure 1 is multinuclear. Rhetor- ical relations that end in the suffix \"-e\" denote re- stituents. For example, the ELABORATION-OBJECT- ATTRIBUTE-E relation that holds between units 2 and 1 in the English discourse structure corresponds to a restrictive relative. If one knows the mappings at the edu level, one can determine the mappings at the span (dis- course constituent) level as well. For example, us- ing the elementary mappings in (4), one can deter- mine that Japanese span [1,2] corresponds to English span [1,2]. Japanese unit [4] to English span [4,5], Japanese span [6.7] to English unit [6], Japanese span [1.5] to English span [1.5], and so on. As Fig- ure 1 shows, the CONCESSION relation that holds be- tween spans [1.5] and [6,7] in the Japanese tree corre- sponds to a similar relation that holds between span [1.5] and unit [6] in the English tree (modulo the fact that, in Japanese, the relation holds between sen- tence fragments, while in English it holds between full sentences). However, the TEMPORAL-AFTER re- lation that holds between units [3] and [4] in the Japanese tree is realized as a CONTRAST relation between unit [3] and span [4.5] in the English tree. And because Japanese units [6] and [7] are fused into unit [6] in English, the relation ELABORATION- OBJECT-ATTRIBUTE-E is no longer made explicit in the English text. Some of the differences between the two discourse Corpus ku (#) ks (#) kn (#) kr (#) Japanese 0.856 (80) 0.785 (3377) 0.724 (3377) 0.650 (3377) English 0.925 (60) 0.866 (1826) 0.839 (1826) 0.748 (1826) Table 1: Tagging reliability in MT systems at the syntactic level. For exam- ple, the re-ordering of units 1 and 2, can be dealt with using only syntactic models. However, as we will see in Section 2, there are significant differences between Japanese and English with respect to the way information is packaged and organized rhetori- cally not only at the sentence level, but also, at the paragraph and text levels. More specifically, as hu- mans translate Japanese into English, they re-order the clauses, sentences, and paragraphs of Japanese texts, they re-package the information into clauses, sentences, and paragraphs that are not a one-to-one mapping of the original Japanese units, and they rhetorically re-organize the structure of the trans- lated text so as to reflect rhetorical constraints spe- cific to English. If a translation system is to produce text that is not only grammatical but also coherent, it will have to ensure that the discourse structure of the target text reflects the natural renderings of the target language, and not that of the source language. In Section 2, we empirically show that there are significant differences between the rhetorical struc- ture of Japanese texts and their corresponding En- glish translations. These differences justify our in- vestigation into developing computational models for discourse structure rewriting. In Section 3, we present such a rewriting model, which re-orders the edus of the original text, determines English-specific clause, sentence, and paragraph boundaries, and re- builds the Japanese discourse structure of a text us- ing English-specific rhetorical renderings. In Sec- tion 4, we evaluate the performance of an imple- mentation of this model. We end with a discussion. 2 Experiment In order to assess the role of discourse structure in MT, we built manually a corpus of discourse trees for 40 Japanese texts and their corresponding trans- lations. The texts were selected randomly from the ARPA corpus (White and O'Connell, 1994). On av- erage, each text had about 460 words. The Japanese texts had a total of 335 paragraphs and 773 sen- tences. The English texts had a total of 337 para- graphs and 827 sentences. We developed a discourse annotation protocol for Japanese and English along the lines followed by Marcu et al. (1999). We used Marcu's discourse an- notation tool (1999) in order to manually construct the discourse structure of all Japanese and English texts in the corpus. 10% of the Japanese and En- The tool and the annotation protocol are available at http://www.isi.edu/~marcu/software/. The an- notation procedure yielded over the entire corpus 2641 Japanese edus and 2363 English edus. We computed the reliability of the annotation us- ing Marcu et al. (1999)'s method for computing kappa statistics (Siegel and Castellan, 1988) over hi- erarchical structures. Table 1 displays average kappa statistics that reflect the reliability of the annota- tion of elementary discourse units, ku, hierarchical discourse spans, ks, hierarchical nuclearity assign- ments, kn, and hierarchical rhetorical relation as- signments, kr. Kappa figures higher than 0.8 corre- spond to good agreement; kappa figures higher than 0.6 correspond to acceptable agreement. All kappa statistics were statistically significant at levels higher than a = 0.01. In addition to the kappa statis- tics, table 1 also displays in parentheses the average number of data points per document, over which the kappa statistics were computed. For each pair of Japanese-English discourse struc- tures, we also built manually an alignment file, which specified in the notation discussed on page 1 the correspondence between the edus of the Japanese text and the edus of its English translation. We computed the similarity between English and Japanese discourse trees using labeled recall and pre- cision figures that reflected the resemblance of the Japanese and English discourse structures with re- spect to their assignment of edu boundaries, hierar- chical spans, nuclearity, and rhetorical relations. Because the trees we compared differ from one language to the other in the number of elementary units, the order of these units, and the way the units are grouped recursively into discourse spans, we computed two types of recall and precision figures. In computing Position-Dependent (P-D) recall and precision figures, a Japanese span was considered to match an English span when the Japanese span con- tained all the Japanese edus that corresponded to the edus in the English span, and when the Japanese and English spans appeared in the same position with respect to the overall structure. For example, the English tree in figure 1 is characterized by 10 sub- sentential spans: [1], [2], [3], [4], [5], [6], [1,2], [4,5], [3,5], and [1,5]. (Span [1,6] subsumes 2 sentences, so it is not sub-sentential.) The Japanese discourse tree has only 4 spans that could be matched in the same positions with English spans, namely spans [1,2], [4], [5], and [1,5]. Hence the similarity between Level Units Spans Status/Nuclearity Relations P-DRP-DPP-DR P-DP P-DR P-D PP-DRP-DP Sentence 29.1 25.0 27.2 22.7 21.3 17.7 14.9 12.4 Paragraph 53.9 53.4 46.8 47.3 38.6 39.0 31.9 32.3 Text 41.3 42.6 31.5 32.6 28.8 29.9 26.1 27.1 Weighted Average 36.0 32.5 31.8 28.4 26.0 23.1 20.1 17.9 All 8.2 7.4 5.9 5.3 4.4 3.9 3.3 3.0 P-I R P-I P P-I R P-I P P-IR P-I P P-I R P-I P Sentence 71.0 61.0 56.0 46.6 44.3 36.9 30.5 25.4 Paragraph 62.1 61.6 53.2 53.8 43.3 43.8 35.1 35.5 Text 74.1 76.5 54.4 56.5 48.5 50.4 41.1 42.7 Weighted Average 69.6 63.0 55.2 49.2 44.8 39.9 33.1 29.5 All 74.5 66.8 50.6 45.8 39.4 35.7 26.8 24.3 Table 2: Similarity of the Japanese and English discourse structures to their discourse structure below the sentence level has a recall of 4/10 and a precision of 4/11 (in Fig- ure 1, there are 11 sub-sentential Japanese spans). In computing Position-Independent (P-I) recall and precision figures, even when a Japanese span \"floated\" during the translation to a position in the English tree that was different from the position in the initial tree, the P-I recall and precision figures were not affected. The Position-Independent figures reflect the intuition that if two trees t₁ and t2 both have a subtree t, t₁ and t₂ are more similar than if they were if they didn't share any tree. At the sentence level, we hence assume that if, for exam- ple, the syntactic structure of a relative clause is translated appropriately (even though it is not ap- propriately attached), this is better than translating wrongly that clause. The Position-Independent fig- ures offer a more optimistic metric for comparing discourse trees. They span a wider range of values than the Position-Dependent figures, which enable a finer grained comparison, which in turn enables a better characterization of the differences between Japanese and English discourse structures. When one takes an optimistic stance, for the spans at the sub-sentential level in the trees in Table 1 the recall is 6/10 and the precision is 6/11 because in addition to spans [1,2], [4], [5], and [1,5], one can also match Japanese span [1] to English span [2] and Japanese span [2] to Japanese span [1]. In order to provide a better estimate of how close two discourse trees were, we computed Position- Dependent and -Independent recall and precision fig- ures for the sentential level (where units are given by edus and spans are given by sets of edus or single sen- tences); paragraph level (where units are given by sentences and spans are given by sets of sentences or single paragraphs); and text level (where units are given by paragraphs and spans are given by sets of paragraphs). These figures offer a detailed pic- ture of how discourse structures and relations are discourse levels, from sentence to text. The differ- ences at the sentence level can be explained by differ- ences between the syntactic structures of Japanese and English. The differences at the paragraph and text levels have a purely rhetorical explanation. As expected, when we computed the recall and precision figures with respect to the nuclearity and relation assignments, we also factored in the statuses and the rhetorical relations that labeled each pair of spans. Table 2 summarizes the results (P-D and P- I (R)ecall and (P)recision figures) for each level (Sentence, Paragraph, and Text). The numbers in the \"Weighted Average\" line report averages of the Sentence-, Paragraph-, and Text-specific figures, weighted according to the number of units at each level. The numbers in the \"All\" line reflect recall and precision figures computed across the entire trees, with no attention paid to sentence and paragraph boundaries. Given the significantly different syntactic struc- tures of Japanese and English, we were not surprised by the low recall and precision results that reflect the similarity between discourse trees built below the sentence level. However, as Table 2 shows, there are significant differences between discourse trees at the paragraph and text levels as well. For exam- ple, the Position-Independent figures show that only about 62% of the sentences and only about 53% of the hierarchical spans built across sentences could be matched between the two corpora. When one looks at the status and rhetorical relations associ- ated with the spans built across sentences at the paragraph level, the P-I recall and precision figures drop to about 43% and 35% respectively. The differences in recall and precision are ex- plained both by differences in the way information is packaged into paragraphs in the two languages and the way it is structured rhetorically both within and above the paragraph level. to translate Japanese into English on a sentence-by- sentence basis, it is likely that the resulting text will be unnatural from a discourse perspective. For ex- ample, if some information rendered using a CON- TRAST relation in Japanese is rendered using an ELABORATION relation in English, it would be in- appropriate to use a discourse marker like \"but\" in the English translation, although that would be con- sistent with the Japanese discourse structure. An inspection of the rhetorical mappings between Japanese and English revealed that some Japanese rhetorical renderings are consistently mapped into one or a few preferred renderings in English. For ex- ample, 34 of 115 CONTRAST relations in the Japanese texts are mapped into CONTRAST relations in En- glish; 27 become nuclei of relations such as ANTITHE- SIS and CONCESSION, 14 are translated as COMPAR- ISON relations, 6 as satellites of CONCESSION rela- tions, 5 as LIST relations, etc. Our goal is to learn these systematic discourse mapping rules and exploit them in a machine translation context. 3 Towards a discourse-based machine translation system 3.1 Overall architecture We are currently working towards building the mod- ules of a Discourse-Based Machine Translation sys- tem that works along the following lines. 1. A discourse parser, such as those described by Sumita et al. (1992), Kurohashi (1994), and Marcu (1999), initially derives the discourse structure of the text given as input. 2. A discourse-structure transfer module rewrites the discourse structure of the input text so as to reflect a discourse rendering that is natural to the target language. 3. A statistical module maps the input text into the target language using translation and language models that incorporate discourse- specific features, which are extracted from the outputs of the discourse parser and discourse transfer modules. In this paper, we focus only on the discourse- structure transfer module. That is, we investigate the feasibility of building such a module. 3.2 The discourse-based transfer model In order to learn to rewrite discourse structure trees, we first address a related problem, which we define below: Definition 3.1 Given two trees T, and T₁ and a correspondence Table C defined between T, and Ti at the leaf level in terms of =, C, D, and relations, find a sequence of actions that rewrites the tree Ts If for any tuple (T., T., C) such a sequence of actions can be derived, it is then possible to use a corpus of (T, T₁, C) tuples in order to automatically learn to derive from an unseen tree T.,, which has the same structural properties as the trees T., a tree Ti,, which has structural properties similar to those of the trees Tt. In order to solve the problem in definition 3.1, we extend the shift-reduce parsing paradigm applied by Magerman (1995), Hermjakob and Mooney (1997), and Marcu (1999). In this extended paradigm, the transfer process starts with an empty Stack and an Input List that contains a sequence of elementary discourse trees edts, one edt for each edu in the tree Ts given as input. The status and rhetorical rela- tion associated with each edt is undefined. At each step, the transfer module applies an operation that is aimed at building from the units in T, the discourse tree Tr. In the context of our discourse-transfer mod- ule, we need 7 types of operations: • SHIFT operations transfer the first edt from the input list into the stack; • REDUCE operations pop the two discourse trees located at the top of the stack; combine them into a new tree updating the statuses and rhetorical relation names of the trees in- volved in the operation; and push the new tree on the top of the stack. These opera- tions are used to build the structure of the discourse tree in the target language. • BREAK operations are used in order to break the edt at the beginning of the input list into a predetermined number of units. These op- erations are used to ensure that the result- ing tree has the same number of edts as Tr. A BREAK operation is necessary whenever a Japanese edu is mapped into multiple English units. • CREATE-NEXT operations are used in order to create English discourse constituents that have no correspondent in the Japanese tree. • FUSE operations are used in order to fuse the edt at the top of the stack into the tree that immediately precedes it. These operations are used whenever multiple Japanese edus are mapped into one English edu. • SWAP operations swap the edt at the begin- ning of the input list with an edt found one or more positions to the right. These oper- ations are necessary for re-ordering discourse constituents. • ASSIGNTYPE operations assign one or more of the following types to the tree at the top of the stack: Unit, MultiUnit, Sentence, Para- Th erations are necessary in order to ensure sen- tence and paragraph boundaries that are spe- cific to the target language. For example, the first sentence of the English tree in Figure 1 can be obtained from the original Japanese sequence by following the sequence of actions (5), whose effects are shown in Figure 2. For the purpose of compactness, the figure does not illustrate the ef- fect of ASSIGNTYPE actions. For the same purpose, some lines correspond to more than one action. BREAK 2; SWAP 2; SHIFT; ASSIGNTYPE UNIT; SHIFT; REDUCE-NS-ELABORATION- OBJECT-ATTRIBUTE-E; ASSIGNTYPE MULTIUNIT; SHIFT; ASSIGNTYPE UNIT; SHIFT; ASSIGNTYPE UNIT; FUSE; ASSIGNTYPE UNIT; SWAP 2; SHIFT; ASSIGNTYPE UNIT; FUSE; BREAK 2; SHIFT; ASSIGNTYPE UNIT; SHIFT; (5) ASSIGNTYPE UNIT; REDUCE-NS- ELABORATION-ADDITIONAL; ASSIGNTYPE MULTIUNIT; REDUCE-NS-CONTRAST; ASSIGNTYPE MULTIUNIT; REDUCE-SN- BACKGROUND; ASSIGNTYPE SENTENCE. For our corpus, in order to enable a discourse- based transfer module to derive any English dis- course tree starting from any Japanese discourse tree, it is sufficient to implement: • one SHIFT operation; • 3 × 2 × 85 REDUCE operations; (For each of the three possible pairs of nuclear- ity assignments NUCLEUS-SATELLITE (NS), SATELLITE-NUCLEUS (SN), AND NUCLEUS- NUCLEUS (NN), there are two possible ways to reduce two adjacent trees (one results in a binary tree, the other in a non-binary tree (Marcu, 1999)), and 85 relation names.) • three types of BREAK operations; (In our cor- pus, a Japanese unit is broken into two, three, or at most four units.) • one type of CREATE-NEXT operation; • one type of FUSE operation; • eleven types of SWAP operations; (In our corpus, Japanese units are at most 11 posi- tions away from their location in an English- specific rendering.) • seven types of ASSIGNTYPE operations: Unit, MultiUnit, Sentence, MultiSentence, Para- These actions are sufficient for rewriting any tree Ts into any tree T₁, where T₁ may have a different number of edus, where the edus of T may have a different ordering than the edus of T., and where the hierarchical structures of the two trees may be different as well. 3.3 Learning the parameters of the discourse-transfer model We associate with each configuration of our trans- fer model a learning case. The cases were gener- ated by a program that automatically derived the sequence of actions that mapped the Japanese trees in our corpus into the sibling English trees, using the correspondences at the elementary unit level that were constructed manually. Overall, the 40 pairs of Japanese and English discourse trees yielded 14108 cases. To each learning example, we associated a set of features from the following classes: Operational and discourse features reflect the number of trees in the stack, the input list, and the types of the last five operations. They encode information pertaining to the types of the partial trees built up to a certain time and the rhetorical relations that hold be- tween these trees. Correspondence-based features reflect the nu- clearity, rhetorical relations, and types of the Japanese trees that correspond to the English-like partial trees derived up to a given time. Lexical features specify whether the Japanese spans that correspond to the structures de- rived up to a given time use potential dis- course markers, such as dakara (because) and no ni (although). The discourse transfer module uses the C4.5 pro- gram (Quinlan, 1993) in order to learn decision trees and rules that specify how Japanese discourse trees should be mapped into English-like trees. A ten-fold cross-validation evaluation of the classifier yielded an accuracy of 70.2% (±0.21). In order to better understand the strengths and weaknesses of the classifier, we also attempted to break the problem into smaller components. Hence, instead of learning all actions at once, we attempted to learn first whether the rewriting procedure should choose a SHIFT, REDUCE, BREAK, FUSE, SWAP, or ASSIGNTYPE operation (the \"Main Action Type\" classifier in table 3), and only then to refine this decision by determining what type of reduce opera- tion to perform, how many units to break a Japanese units into, how big the distance to the SWAP-ed unit should be, and what type of ASSIGNTYPE operation STACK INPUT LIST BREAK 2 1 2 3 4 5 6 7 ... SWAP 2 1 1\" 2 3 4 5 6 7 ... SHIFT 2 1\" 1\" 3 4 5 6 7 ... SHIFT 2 1\" 1\" 3 4 5 6 7 ... ELABORATION_OBJECT_ATTRIBUTE_E REDUCE-NS-ELABORATION-OBJECT-ATTRIBUTE-E SHIFT; SHIFT FUSE 2 1\" 1\" 3 4 5 6 7 ... ELABORATION_OBJECT_ATTRIBUTE_E SWAP 2; SHIFT; FUSE 2 1\" 1',3 4 5 6 7 ... ELABORATION_OBJECT_ATTRIBUTE_E 2 1\" 1',3,5 4 6 7 ... ELABORATION_OBJECT_ATTRIBUTE_E 2 1\" 1',3,5 4' 4\" 6 7 ... ELABORATION_OBJECT_ATTRIBUTE_E ELABORATION_ADDITIONAL 2 1\" 1',3,5 4' 4\" 6 7 ... ELABORATION_OBJECT_ATTRIBUTE_E 2 1\" 1',3,5 4' 4\" 6 7 ... CONTRAST BACKGROUND ELABORATION_OBJECT_ATTRIBUTE_E 2 1\" 1',3,5 4' 4\" 6 7 ... CONTRAST ELABORATION_ADDITIONAL BREAK 2; SHIFT; SHIFT REDUCE-NS-ELABORATION-ADDITIONAL REDUCE-NN-CONTRAST REDUCE-SN-BACKGROUND ASSIGNTYPE SENTENCE Figure 2: Example of incremental tree reconstruction. data set and the performance of each of these classi- fiers, as determined using a ten-fold cross-validation procedure. For the purpose of comparison, each clas- sifier is paired with a majority baseline. The results in Table 3 show that the most diffi- cult subtasks to learn are that of determining the number of units a Japanese unit should be broken into and that of determining the distance to the unit that is to be swapped. The features we used are not able to refine the baseline classifiers for these action types. The confusion matrix for the \"Main Action Type\" classifier (see Table 5) shows that the system has trouble mostly identifying BREAK and CREATE-NEXT actions. The system has difficulty learning what type of nuclearity ordering to pre- fer (the \"Nuclearity-Reduce\" classifier) and what re- lation to choose for the English-like structure (the \"Relation-Reduce\" classifier). Figure 3 shows a typical learning curve, the one that corresponds to the \"Reduce Relation\" classifier. Our learning curves suggest that more training data may improve performance. However, they also sug- gest that better features may be needed in order to improve performance significantly. Table 4 displays some learned rules. The first rule accounts for rhetorical mappings in which the or- der of the nucleus and satellite of an ATTRIBUTION relation is changed when translated from Japanese into English. The second rule was learned in order to map EXAMPLE Japanese satellites into EVIDENCE English satellites. Classifier # cases Accuracy (10-fold cross validation) | Majority baseline accuracy General 14108 70.20% (±0.21) 22.05% (on ASSIGNTYPE UNIT) (Learns all classes at once) Main Action Type 14108 82.53% (±0.25) 45.47% (on ASSIGNTYPE) Assign Type 6416 90.46% (±0.39) 57.30% (on ASSIGNTYPE Unit) Break 394 82.91% (±1.40) 82.91% (on BREAK 2) Nuclearity-Reduce 2388 67.43% (±1.03) 50.92% (on NS) Relation-Reduce 2388 48.20% (±1.01) 17.18% (on ELABORATION- OBJECT-ATTRIBUTE-E) Swap 842 62.98% (±1.62) 62.98% (on SWAP 1) Table 3: Performance of the classifiers Accuracy 48.00 46 00 44 00 Action (a) (b) (c) (d) (e) (f) (g) RelationReduce ASSIGNTYPE (a) 660 BREAK (b) 1 2 28 1 CREATE-NEXT (c) 1 8 FUSE (d) 69 8 3 REDUCE (e) 4 18 193 30 3 SHIFT (f) 1 4 15 44 243 25 SWAP (g) 3 4 14 43 25 42.00 40 00 38 00 3600 34 00 32.00 30 00 #Cases x 103 0.50 1.00 1 50 200 Figure 3: Learning curve for the Relation-Reduce classifier. if rhet RelOfStack-1InJapTree = ATTRIBUTION then rhet RelOfTopStackInEngTree ATTRIBUTION if rhet RelOfTopStackInJapTree = EXAMPLE A isSentence TheLastUnitInJapTreeOfTopStack = false then rhet RelOfTopStackInEngTree - EVIDENCE Table 4: Rule examples for the Relation-Reduce classifier. 4 Evaluation of the discourse-based transfer module By applying the General classifier or the other six classifiers successively, one can map any Japanese discourse tree into a tree whose structure comes closer to the natural rendering of English. To evalu- ate the discourse-based transfer module, we carried out a ten-fold cross-validation experiment. That is, we trained the classifiers on 36 pairs of manually built and aligned discourse structures, and we then used the learned classifiers in order to map 4 un- seen Japanese discourse trees into English-like trees. We measured the similarity of the derived trees with the English trees built manually, using the metrics discussed in Section 2. We repeated the procedure ten times, each time training and testing on different subsets of tree paire Table 5: Confusion matrix for the Main Action Type classifier. We take the results reported in Table 2 as a base- line for our model. The baseline corresponds to ap- plying no knowledge of discourse. Table 6 displays the absolute improvement (in percentage points) in recall and precision figures obtained when the Gen- eral classifier was used to map Japanese trees into English-looking trees. The General classifier yielded the best results. The results in Table 6 are averaged over a ten-fold cross-validation experiment. The results in Table 6 show that our model outperforms the baseline with respect to building English-like discourse structures for sentences, but it under-performs the baseline with respect to build- ing English-like structures at the paragraph and text levels. The main shortcoming of our model seems to come from its low performance in assigning para- graph boundaries. Because our classifier does not learn correctly which spans to consider paragraphs and which spans not, the recall and precision results at the paragraph and text levels are negatively af- fected. The poorer results at the paragraph and text levels can be also explained by errors whose effect cu- mulates during the step-by-step tree-reconstruction procedure; and by the fact that, for these levels, there is less data to learn from. However, if one ignores the sentence and para- graph boundaries and evaluates the discourse struc- tures overall, one can see that our model outper- forms the baseline on all accounts according to the Position-Dependent evaluation; outperforms the baseline with respect to the assignment of elemen- tary units, hierarchical spans, and nuclearity sta- tuses according to the Position-Independent evalu- ation and under performs the basoling only slightly Level Units Spans Status/Nuclearity Relations P-DR P-DP P-DR P-DP P-DR P-D P P-DR P-D P Sentence +9.1 +25.5 +2.0 +19.9 +0.4 +13.4 -0.01 +8.4 Paragraph -14.7 +1.4 -12.5 -1.7 -11.0 -2.4 -9.9 -3.3 Text -9.6 -13.5 -7.1 -11.1 -6.3 -10.0 -5.2 -8.8 Weighted Average +1.5 +14.1 -2.1 +9.9 -3.1 +6.4 -3.0 +3.9 All -1.2 +2.5 -0.1 +2.9 +0.6 +3.5 +0.7 +2.6 P-I R P-I P P-I R P-I P P-I R P-I P P-I R P-I P Sentence +13.4 +30.4 +3.1 +36.1 -6.3 +18.6 -10.1 +3.9 Paragraph -15.6 +0.6 -13.5 -0.8 -11.7 -1.8 -10.3 -2.8 Text -15.4 -23.3 -13.0 -20.4 -13.2 -19.5 -11.5 -17.0 Weighted Average +3.6 +15.5 -2.7 +17.1 -8.5 +7.3 -10.5 -0.4 All +12.7 +29.6 +2.0 +28.8 -5.1 +13.0 -7.9 +2.2 Table 6: Relative evaluation of the discourse-based transfer module with respect to the figures in Table 2. with respect to the rhetorical relation assignment according to the Position-Independent evaluation. More sophisticated discourse features, such as those discussed by Maynard (1998), for example, and a tighter integration with the lexicogrammar of the two languages may yield better cues for learning discourse-based translation models. 5 Conclusion We presented a systematic empirical study of the role of discourse structure in MT. Our study strongly supports the need for enriching MT systems with a discourse module, capable of re-ordering and re- packaging the information in a source text in a way that is consistent with the discourse rendering of a target language. We presented an extended shift- reduce parsing model that can be used to map dis- course trees specific to a source language into dis- course trees specific to a target language. Our model outperforms a baseline with respect to its ability to predict the discourse structure of sentences. Our model also outperforms the baseline with respect to its ability to derive discourse structures that are closer to the natural, rhetorical rendering in a tar- get language than the original discourse structures in the source language. Our model is still unable to determine correctly how to re-package sentences into paragraphs; a better understanding of the notion of \"paragraph\" is required in order to improve this. References William A. Gale and Kenneth W. Church. 1993. A program for aligning sentences in bilingual cor- pora. Computational Linguistics, 19(1):75-102. Ulf Hermjakob and Raymond J. Mooney. 1997. Learning parse and translation decisions from ex- amples with rich context. In Proc. of ACL '97, pages 482-489, Madrid, Spain.. Sadao Kurohashi and Makoto Nagao. 1994. Auto- matic detection of discourse structure by check- ing surface information in sentences. In Proc. of COLING'94, volume 2, pages 1123--1127, Kyoto, Japan. David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proc. of ACL '95, pages 276-283, Cambridge, Massachusetts. William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243-281. Daniel Marcu. 1999. A decision-based approach to rhetorical parsing. In Proc. of ACL '99, pages 365- 372, Maryland. Daniel Marcu, Estibaliz Amorrortu, and Magdalena Romera. 1999. Experiments in constructing a cor- pus of discourse trees. In Proc. of the ACL '99 Workshop on Standards and Tools for Discourse Tagging, pages 48-57, Maryland. Senko K. Maynard. 1998. Principles of Japanese Discourse: A Handbook. Cambridge Univ. Press. J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers. Sidney Siegel and N.J. Castellan. 1988. Non- parametric Statistics for the Behavioral Sciences. McGraw-Hill, Second edition. Kazuo Sumita, Kenji Ono, T. Chino, Teruhiko Ukita, and Shin'ya Amano. 1992. A discourse structure analyzer for Japanese text. In Proceed- ings of the International Conference on Fifth Gen- eration Computer Systems, v 2, pages 1133-1140. J. White and T. O'Connell. 1994. Evaluation in the ARPA machine-translation program: 1993 methodology. In Proceedings of the ARPA Human Language Technology Workshop, pages 135-140, Washington, D.C."
  },
  {
    "title": "Evaluating Automatic Dialogue Strategy Adaptation for a Spoken Dialogue System",
    "abstract": "In this paper, we describe an empirical evaluation of an adaptive mixed initiative spoken dialogue system. We conducted two sets of experiments to evaluate the mixed initiative and automatic adaptation aspects of the system, and analyzed the resulting dialogues along three dimensions: performance factors, discourse features, and initiative distribution. Our results show that 1) both the mixed initiative and automatic adaptation aspects led to better system performance in terms of user satisfaction and dialogue efficiency, and 2) the system's adaptation behavior better matched user expectations, more efficiently resolved dialogue anomalies, and resulted in higher overall dialogue quality.",
    "content": "1 Introduction Recent advances in speech technologies have enabled spoken dialogue systems to employ mixed initiative di- alogue strategies (e.g. (Allen et al., 1996; Sadek et al., 1996; Meng et al., 1996)). Although these systems inter- act with users in a manner more similar to human-human interactions than earlier systems employing system ini- tiative strategies, their response strategies are typically selected using only local dialogue context, disregarding dialogue history. Therefore, their gain in naturalness and performance under optimal conditions is often overshad- owed by their inability to cope with anomalies in dia- logues by automatically adapting dialogue strategies. In contrast, Figure 1 shows a dialogue in which the sys- tem automatically adapts dialogue strategies based on the current user utterance and dialogue history.¹ Af- ter failing to obtain a valid response to an information- seeking query in utterance (4), the system adapted dia- logue strategies to provide additional information in (6) that assisted the user in responding to the query. Further- more, after the user responded to a limited system prompt in (10) with a fully-specified query in (11), implicitly indicating her intention to take charge of the problem- ¹S and U indicate system and user utterances, respectively. The words appearing in square brackets are the output from the Lucent Automatic Speech Recognizer (Reichl and Chou, 1998; Ortmanns et al., 1999), configured to use class-based probabilistic n-gram language models. The task and dialogue initiative annotations are explained in Section 2.1. solving process, the system again adapted strategies, hence providing an open-ended prompt in (13). Previous work has shown that dialogue systems in which users can explicitly change the system's dia- logue strategies result in better performance than non- adaptable systems (Litman and Pan, 1999). However, no earlier system allowed for initiative-oriented auto- matic strategy adaptation based on information dynam- ically extracted from the user's spoken input. In this paper, we briefly introduce MIMIC, a mixed initiative spoken dialogue system that automatically adapts dia- logue strategies. We then describe two experiments that evaluated the effectiveness of MIMIC's mixed initiative and automatic adaptation capabilities. Our results show that, when analyzed along the performance dimension, MIMIC's mixed initiative and automatic adaptation fea- tures lead to more efficient dialogues and higher user sat- isfaction. Moreover, when analyzed along the discourse and initiative dimensions, MIMIC's adaptation capabil- ities result in dialogues in which system behavior better matches user expectations and dialogue anomalies are re- solved more efficiently. 2 MIMIC: An Adaptive Mixed Initiative Dialogue System MIMIC is a voice-enabled, telephone-based dialogue system that provides movie showtime information. Two main features distinguish MIMIC from other existing spoken dialogue systems. First, MIMIC automatically adapts dialogue strategies based on dialogue character- istics and thus tailors its behavior to the dialogue con- text. Second, MIMIC's initiative module is decoupled from the dialogue manager's goal and strategy selection processes, thereby enabling the system's adaptation be- havior to be easily modified. We outline characteristics of MIMIC relevant to our experiments and analyses. For further details about the system, see (Chu-Carroll, 2000). 2.1 Adaptation of Dialogue Strategies Figure 1 showed an actual interaction between MIMIC and a user. The task and dialogue initiative holders annotated for each system turn are the factors that af- fect MIMIC's dialogue strategy adaptation. Following (Chu-Carroll and Brown, 1998), task initiative tracks the (1) (2) (3) S: Utterance Hello, this is MIMIC, the movie information system. How can I help you? U: What time is Analyze This playing in Montclair? [what time is analyze this playing in montclair] Task Initiative User Dialogue Initiative System (4) S: What theater would you like? User System (5) U: Montclair. [montclair] (6) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone. System System (7) What theater would you like? (8) U: Clearviews Screening Zone. [um movie screening zone] (9) S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pm, System System and 6:30pm. (10) Please say the name of the movie or theater or town you would like information about. (11) U: What time is Analyze This playing at the Wellmont Theater? (12) S: [what time is analyze this playing at the wellmont theater] Analyze this is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm, and 10:10pm. User System (13) Can I help you with anything else? Figure 1: An Adaptive Mixed Initiative Dialogue with MIMIC lead in the process toward achieving the dialogue partici- pants' domain goal, while dialogue initiative models the lead in determining the current discourse focus. In our information query application domain, MIMIC has task (and thus dialogue) initiative when its utterances are in- tended to provide helpful guidance toward achieving the user's domain goal, while it has dialogue but not task initiative if its utterances only specify the current dis- course goal.2 For example, as a result of MIMIC taking over task initiative in (6), helpful guidance, in the form of valid response choices, was provided in its attempt to obtain a theater name after the user failed to answer an earlier question intended to solicit this information. In (4), MIMIC specified the current discourse goal (re- questing information about a missing theater) but did not suggest valid response choices since it only had dialogue initiative. MIMIC's ability to automatically adapt dialogue strategies is achieved by employing an initiative mod- ule that determines initiative distribution based on par- ticipant roles, cues detected during the current user ut- terance, and dialogue history (Chu-Carroll and Brown, 1998). This initiative framework utilizes the Dempster- Shafer theory (Shafer, 1976; Gordon and Shortliffe, 1984), and represents the current initiative distribution as two basic probability assignments (bpas) that signify the overall amount of evidence supporting each agent hav- ing task and dialogue initiatives. The effects that a cue has on changing the current task and dialogue initiative distribution are also represented as bpas, obtained using an iterative training procedure on a corpus of transcribed 2In the dialogues collected in our experiments, which are described in Section 3, there are system turns in which MIMIC had neither task nor dialogue initiative. However, such cases are rare in this domain and will not be discussed in this paper. and annotated human-human dialogues. At the end of each user turn, the bpas representing the effects of cues detected during that turn are combined with the bpas rep- resenting the current initiative distribution to obtain the initiative distribution for the system's next turn. In Figure 1, utterance (3) triggered the cue Ambigu- ousAction since the town of Montclair has multiple the- aters. Although AmbiguousAction contributed to the sys- tem having both initiatives in the next turn, the effect of this cue, when combined with the initiative distribution of (3), resulted in MIMIC having dialogue but not task initiative in (4). However, in addition to triggering Am- biguousAction, utterance (5) also triggered NoNewInfo, since it did not contribute any new information. The ef- fect of these two cues, combined with the initiative dis- tribution in (5), was enough to lead MIMIC to take over task initiative in (6). 2.2 Modification of Adaptation Behavior MIMIC employs an initiative module which is decou- pled from its goal selection process, while allowing the outcome of both processes to guide its response strat- egy selection. A major advantage of this design is that changes in the system's strategy selection behavior, and thus its adaptation behavior, can be achieved by modi- fying parameters in the initiative module, while leaving the goal and strategy selection processes intact. The ini- tiative module determines initiative distribution based on participant roles, cues observed in the current user ut- terance, and dialogue history. Although all three factors may play a role in modifying MIMIC's adaptation be- havior, we discuss how the effects of participant roles and dialogue history are modified to create systems used as bases of comparison in our experiments. Participant roles affect the default initiative distribu- Utterance Task Dialogue Initiative Initiative (14) S: Hello, this is MIMIC, the movie information system. System System (15) Please say the name of the movie or theater or town you would like information about. (16) U: Analyze This. [analyze this] (17) S: Choices for task are when and where. System System (18) What would you like to know about Analyze This? (19) U: When. [when is] (20) S: Choices for theater are Middlebrook Galeria, Loews East Hanover, Montgomery Cinemas, System System Atlantic Triplex Cinema, and others. (21) What theater would you like? (22) U: Montclair. [montclair] (23) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone. System System (24) (25) U: Wellmont. [wellmont] (26) S: Analyze This is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm, and 10:10pm. System System (27) Please say the name of the movie or theater or town you would like information about. (28) U: Analyze This in Montclair. [analyze this in montclair] (29) S: Choices for task are location, what, where, and when. System System (30) What would you like to know about Montclair? (31) U: When. (32) [when] (33) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone System System (34) What theater would you like? (35) U: Clearviews Screening Zone. [screening zone] (36) S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pm, and 6:30pm. System System (37) Please say the name of the movie or theater or town you would like information about. Figure 2: A System Initiative Dialogue with MIMIC-SI tion for MIMIC's initial utterance. For naive users, it may be more appropriate for MIMIC to employ system- initiative strategies. This system, MIMIC-SI, can be achieved by setting the default initiative distribution to provide definite evidence for the system having both task and dialogue initiatives.3 Figure 2 shows a dialogue in which a user interacts with MIMIC-SI to obtain the same information as in Figure 1. MIMIC-SI prompts the user for one piece of information at a time and provides (sometimes unnecessary) assistance, in the form of valid dialogue moves, during each turn. Taking into account dialogue history allows MIMIC to determine the initiative distribution based on the cu- mulative effect of previously observed cues. This ef- fect can be disabled so that only local dialogue con- text affects the initiative distribution for the next turn. Based on MIMIC's parameter settings, this adjustment results in MIMIC-MI, a system which always has dia- 3 Based on the Dempster-Shafer theory, if the bpas of the default initiative distribution or those of a detected cue provide definite evi- dence for drawing a certain conclusion, then no subsequent cues have any effect on changing that conclusion. logue initiative but never has task initiative. MIMIC-MI behaves similarly to many existing non-adaptive mixed initiative dialogue systems (e.g., (Bennacef et al., 1996; Papineni et al., 1999)). Figure 3 shows a dialogue in- volving MIMIC-MI in which the user again attempts to acquire the same information as in the previous two dia- logues. Notice that (41)-(49) constitute a failed attempt to solicit a theater name from the user. Instead of provid- ing helpful information as in (6) in Figure 1, MIMIC-MI relied on the user to change her problem-solving strategy in (50) to obtain the information needed to answer the system's question. 3 Experimental Design Our main goal in evaluating MIMIC is to determine whether users find the mixed initiative and automatic adaptation aspects of its dialogue strategies useful. We compared MIMIC to two control systems: MIMIC-SI and MIMIC-MI, since they employ dialogue manage- ment strategies similar to those in many existing sys- tems. The comparison between MIMIC and MIMIC-SI --- (38) S: (39) Utterance Hello, this is MIMIC, the movie information system. How can I help you? (40) U: When is Analyze This playing in Montclair? Task Dialogue Initiative Initiative [when analyze this playing in montclair] (41) S: What theater would you like? User System (42) U: [] (43) S: What theater would you like? User System (44) U: Um. [um] (45) S: What theater would you like? User System (46) U: Montclair. [montclair] (47) S: What theater would you like? User System (48) U: Any. [any] (49) S: What theater would you like? User System (50) U: Where in Montclair is Analyze This playing? [where in montclair is analyze this playing] (51) S: Analyze This is playing at Wellmont Theater and Clearviews Screening Zone in Montclair. User System (52) U: When is Analyze This playing at Wellmont Theater? [when is analyze this playing at wellmont theater in] (53) S: Analyze This is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm, User System and 10:10pm. (54) Can I help you with anything else? (55) U: When is it playing at Clearviews Screening Zone? [when is it playing at movie screening zone] (56) S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm, 4:10pm, User System and 6:30pm. (57) Can I help you with anything else? Figure 3: A Non-Adaptive Mixed Initiative Dialogue with MIMIC-MI focused on the contribution of mixed-initiative dialogue management, while the comparison between MIMIC and MIMIC-MI emphasized the contribution of automatic strategy adaptation. The following three factors were controlled in our experiments: Town Theater (if playing) Movie Times after 5:10pm (if playing) Hoboken Antz (a) Easy Task 1. System version: For each experiment, two systems were used: MIMIC and a control system. In the first experiment MIMIC was compared with MIMIC-SI, and in the second experiment, with MIMIC-MI. 2. Order: For each experiment, all subjects were ran- domly divided into two groups. One group per- formed tasks using MIMIC first, and the other group used the control system first. 3. Task difficulty: 3-4 tasks which highlighted differ- ences between systems were used for each experi- ment. Based on the amount of information to be ac- quired, we divided the tasks into two groups: easy and difficult; an example of each is shown in Fig- ure 4. Town Millburn Berkeley Hgts Mountainside Madison Hoboken Theater (if playing) Movie Analyze This Analyze This Analyze This True Crime True Crime (b) Difficult Task Two Times (if playing) Figure 4: Sample Tasks for Evaluation Experiments Eight subjects participated in each experiment. Each of the subjects interacted with both systems to perform The subjects were Bell Labs researchers, summer students, and their friends Most of them are computer scientists electrical engi- --- all tasks. The subjects completed one task per call so that the dialogue history for one task did not affect the next task. Once they had completed all tasks in sequence using one system, they filled out a questionnaire to as- sess user satisfaction by rating 8-9 statements, similar to those in (Walker et al., 1997), on a scale of 1-5, where 5 indicated highest satisfaction. Approximately two days later, they attempted the same tasks using the other sys- tem. These experiments resulted in 112 dialogues with approximately 2,800 dialogue turns. In addition to user satisfaction ratings, we automat- ically logged, derived, and manually annotated a num- ber of features (shown in boldface below). For each task/subject/system triplet, we computed the task suc- cess rate based on the percentage of slots correctly filled in on the task worksheet, and counted the # of calls needed to complete each task. For each call, the user- side of the dialogue was recorded, and the elapsed time of the call was automatically computed. All user ut- terances were logged as recognized by our automatic speech recognizer (ASR) and manually transcribed from the recordings. We computed the ASR word error rate, ASR rejection rate, and ASR timeout rate, as well as # of user turns and average sentence length for each task/subject/system triplet. Additionally, we recorded the cues that the system automatically detected from each user utterance. All system utterances were also logged, along with the initiative distribution for each system turn and the dialogue acts selected to generate each system response. 4 Results and Discussion Based on the features described above, we com- pared MIMIC and the control systems, MIMIC-SI and MIMIC-MI, along three dimensions: performance fea- tures, in which comparisons were made using previously proposed features relevant to system performance (e.g., (Price et al., 1992; Simpson and Fraser, 1993; Danieli and Gerbino, 1995; Walker et al., 1997)); discourse fea- tures, in which comparisons were made using character- istics of the resulting dialogues; and initiative distribu- tion, where initiative characteristics of all dialogues in- volving MIMIC from both experiments were examined. 4.1 Performance Features For our performance evaluation, we first applied a three- way analysis of variance (ANOVA) (Cohen, 1995) to each feature using three factors: system version, order, neers, or linguists, and none had prior knowledge of MIMIC. 5We used the exact same set of tasks rather than designing tasks of similar difficulty levels because we intended to compare all available features between the two system versions, including ASR word error rate, which would have been affected by the choice of movie/theater names in the tasks. Although the vast majority of tasks were completed in one call, some subjects, when unable to make progress, did not change strategies as in (41)-(49) in Figure 3; instead, they hung up and started the task over. Performance Feature MIMIC SI P # of user turns 10.3 13.6 0.0075 Elapsed time (sec.) 229.5 277.5 0.0162 ASR timeout (%) 12.5 6.9 0.0239 User satisfaction (n=8) 21.9 19.8 0.0447 ASR rejection (%) 5.4 8.1 0.1911 Task success (%) 100 98.8 0.3251 # of calls 1.0 1.1 0.572 ASR word error (%) 28.1 31.1 0.8475 (a) MIMIC vs. MIMIC-SI (n=32) Performance Feature MIMIC MI p ASR timeout (%) # of user turns 5.7 15.6 0.001 10.3 14.3 0.0199 User satisfaction (n=8) 29.5 24.4 0.0364 Elapsed time (sec.) ASR word error (%) Task success (%) 200.6 246.4 0.0457 23.0 30.6 0.0588 100 98.4 0.1639 1.21 1.21 0.5 ASR rejection (%) 8.4 7.7 0.8271 # of calls (b) MIMIC vs. MIMIC-MI (n=24) Table 1: Comparison of Performance Features and task difficulty. If no interaction effects emerged, we compared system versions using paired sample t-tests.8 Following the PARADISE evaluation scheme (Walker et al., 1997), we divided performance features into four groups: • Task success: task success rate, # of calls. • Dialogue quality: ASR rejection rate, ASR timeout rate, ASR word error rate. • Dialogue efficiency: # of user turns, elapsed time. • System usability: user satisfaction. For both experiments, the ANOVAs showed no inter- action effects among the controlled factors. Tables 1(a) and 1(b) summarize the results of the paired sample t- tests based on performance features, where features that differed significantly between systems are shown in ital- ics. These results show that, when compared with either User satisfaction was a per subject as opposed to a per task per- formance feature; thus, we performed a two-way ANOVA using the factors system version and order. This paper focuses on evaluating the effect of MIMIC's mixed ini- tiative and automatic adaptation capabilities. We assess these effects based on comparisons between system version when no interaction ef- fects emerged from the ANOVA tests using the factors system version, order, and task difficulty. Effects based on system order and task diffi- culty alone are beyond the scope of this paper. Typically p<0.05 is considered statistically significant (Cohen, 1995). control system, users were more satisfied with MIMIC10 and that MIMIC helped users complete tasks more effi- ciently. Users were able to complete tasks in fewer turns and in a more timely manner using MIMIC. When comparing MIMIC and MIMIC-MI, dialogues involving MIMIC had a lower timeout rate. When MIMIC detected cues signaling anomalies in the dia- logue, it adapted strategies to provide assistance, which in addition to leading to fewer timeouts, saved users time and effort when they did not know what to say. In con- trast, users interacting with MIMIC-MI had to iteratively reformulate questions until they obtained the desired in- formation from the system, leading to more timeouts (see (41)-(49) in Figure 3). However, when comparing MIMIC and MIMIC-SI, even though users accomplished tasks more efficiently with MIMIC, the resulting dia- logues contained more timeouts. As opposed to MIMIC- SI, which always prompted users for one piece of infor- mation at a time, MIMIC typically provided more open- ended prompts when the user had task initiative. Even though this required more effort on the user's part in for- mulating utterances and led to more timeouts, MIMIC quickly adapted strategies to assist users when recog- nized cues indicated that they were having trouble. To sum up, our experiments show that both MIMIC's mixed initiative and automatic adaptation aspects re- sulted in better performance along the dialogue efficiency and system usability dimensions. Moreover, its adap- tation capabilities contributed to better performance in terms of dialogue quality. MIMIC, however, did not con- tribute to higher performance in the task success dimen- sion. In our movie information domain, the tasks were sufficiently simple; thus, all but one user in each experi- ment achieved a 100% task success rate. 4.2 Discourse Features Our second evaluation dimension concerns characteris- tics of resulting dialogues. We analyzed features of user utterances in terms of utterance length and cues observed and features of system utterances in terms of dialogue acts. For each feature, we again applied a three-way ANOVA test, and if no interaction effects emerged, we performed a paired sample t-test to compare system ver- sions. The cues detected in user utterances provide insight into both user intentions and system capabilities. The cues that MIMIC automatically detects are a subset of those discussed in (Chu-Carroll and Brown, 1998):11 • TakeOverTask: triggered when the user provides more information than expected; an implicit indi- cation that the user wants to take control of the 10 The range of user satisfaction scores was 8-40 for experiment one and 9-45 for experiment two. 11 A subset of these cues corresponds loosely to previously proposed evaluation metrics (e.g., (Danieli and Gerbino, 1995)). However, our system automatically detects these features instead of requiring manual annotation by experts. Discourse Feature Cue: Take OverTask MIMIC SI P 1.84 5 0 Cue: AmbiguousActResolved 1.69 Cue: AmbiguousAction Avg sentence length (words) Cue: InvalidAction Cue: NoNewInfo 4.59 0 3 6.59 0.0008 6.82 5.45 0.0016 1.16 0.94 0.1738 1.28 1.38 0.766 (a) MIMIC vs. MIMIC-SI (n=32) Discourse Feature MIMIC MI P Cue: TakeOverTask 2.33 0 0 Cue: InvalidAction 2.04 3.75 0.0011 Cue: NoNewInfo 2.25 4.79 0.0161 Cue: AmbiguousActResolved 2.08 Avg sentence length (words) 5.26 1.13 0.0297 5.63 0.1771 4.13 4.38 0.8767 (b) MIMIC vs. MIMIC-MI (n=24) Cue: AmbiguousAction Table 2: Comparison of User Utterance Features problem-solving process. • NoNewInfo: triggered when the user is unable to make progress toward task completion, either when the user does not know what to say or the ASR en- gine fails to recognize the user's utterance. • InvalidAction/InvalidActionResolved: triggered when the user utterance makes an invalid as- sumption about the domain and when the invalid assumption is corrected, respectively. • AmbiguousAction/AmbiguousActionResolved: trig- gered when the user query is ambiguous and when the ambiguity is resolved, respectively. Tables 2(a) and (b) summarize the results of the paired sample t-tests based on user utterance features where fea- tures whose numbers of occurrences were significantly different according to system version used are shown in italics. 12 Table 2(a) shows that users expected the system to adapt its strategies when they attempted to take control of the dialogue. Even though MIMIC-SI did not behave as expected, the users continued their attempts, resulting in significantly more occurrences of TakeOverTask in di- alogues with MIMIC-SI than with MIMIC. Furthermore, the average sentence length in dialogues with MIMIC was only 1.5 words per turn longer than in dialogues with MIMIC-SI, providing further evidence that users 12 Since system dialogue acts are often selected based on cues de- tected in user utterances, we only discuss results of our user utterance feature analysis, using dialogue act analysis results as additional sup- port for our conclusions. preferred to provide free-formed queries, regardless of system version used. Table 2(b) shows that MIMIC was more effec- tive at resolving dialogue anomalies than MIMIC-MI. More specifically, there were significantly fewer oc- currences of NoNewInfo in dialogues with MIMIC than with MIMIC-MI. In addition, while the number of occurrences of AmbiguousAction was not signifi- cantly different for the two systems, the number that were resolved (AmbiguousActionResolved) was signif- icantly higher in interactions with MIMIC than with MIMIC-MI. Since NoNewInfo and AmbiguousAction both prompted MIMIC to adapt strategies and, as a re- sult, provide additional useful information, the user was able to quickly resolve the problem at hand. This is fur- ther supported by the higher frequency of the system dia- logue act GiveOptions in MIMIC (p=0), which provides helpful information based on dialogue context. In sum, the results of our discourse feature analysis further confirm the usefulness of MIMIC's adaptation capabilities. Comparisons with MIMIC-SI provide ev- idence that MIMIC's ability to give up initiative better matched user expectations. Moreover, comparisons with MIMIC-MI show that MIMIC's ability to opportunisti- cally take over initiative resulted in dialogues in which anomalies were more efficiently resolved and progress toward task completion was more consistently made. 4.3 Initiative Analysis Our final analysis concerns the task initiative distri- bution in our adaptive system in relation to the fea- tures previously discussed. For each dialogue involving MIMIC, we computed the percentage of turns in which MIMIC had task initiative and the correlation coefficient (r) between the initiative percentage and each perfor- mance/discourse feature. To determine if this correlation was significant, we performed Fisher's r to z transform, upon which a conventional Z test was performed (Cohen, 1995). Tables 3(a) and (b) summarize the correlation between the performance and discourse features and the percent- age of turns in which MIMIC has task initiative, respec- tively. 13 Again, those correlations which are statistically significant are shown in italics. Table 3(a) shows a strong positive correlation between task initiative distribution and the number of user turns as well as the elapsed time of the dialogues. Although earlier results (Table 1(a)) show that dialogues in which the system always had task initiative tended to be longer, we believe that this corre- lation also suggests that MIMIC took over task initiative more often in longer dialogues, those in which the user was more likely to be having difficulty. Table 3(a) fur- ther shows moderate correlation between task initiative distribution and ASR rejection rate as well as ASR word error rate. It is possible that such a correlation exists 13 This test was not performed for user satisfaction, since user satis- faction was a per subject and not a per dialogue feature. Performance Feature # of user turns ASR rejection r P 0.71 0 0.55 0 Elapsed time 0.51 0.00002 ASR word error 0.46 0.00012 # of calls 0.15 0.1352 ASR timeout -0.003 0.4911 Task success rate 0 0.5 (a) Performance Features Discourse Feature Cue: AmbiguousActionResolved Cue: NoNewInfo Cue: TakeOverTask Cue: InvalidAction Average sentence length Cue: AmbiguousAction (b) Discourse Features r P 0.61 0 0.59 0 0.44 0.00028 0.42 0.00057 -0.40 0.00099 0.38 0.00169 Table 3: Correlation Between Task Initiative Distribution and Features (n=56) because ASR performance worsens when MIMIC takes over task initiative. However, in that case, we would have expected the results in Section 4.1 to show that the ASR rejection and word error rates for MIMIC-SI are signif- icantly greater than those for MIMIC, which are in turn significantly greater than those for MIMIC-MI, since in MIMIC-SI the system always had task initiative and in MIMIC-MI the system never took over task initiative. To the contrary, Tables 1(a) and 1(b) showed that the differences in ASR rejection rate and ASR word error rate were not significant between system versions, and Table 1(b) showed that ASR word error rate for MIMIC- MI was in fact quite substantially higher than that for MIMIC. This suggests that the causal relationship is the other way around, i.e., MIMIC's adaptation capabilities allowed it to opportunistically take over task initiative when ASR performance was poor. Table 3(b) shows that all cues are positively correlated with task initiative distribution. For AmbiguousAction, InvalidAction, and NoNewInfo, this correlation exists be- cause observation of these cues contributed to MIMIC having task initiative. However, note that AmbiguousAc- tionResolved has a stronger positive correlation with task initiative distribution than does AmbiguousAction, again indicating that MIMIC's adaptive strategies contributed to more efficient resolution of ambiguous actions. In brief, our initiative analysis lends additional sup- port to the conclusions drawn in our performance and discourse feature analyses and provides new evidence for the advantages of MIMIC's adaptation capabilities. In addition to taking over task initiative when previously identified dialogue anomalies were encountered (e.g., de- tection of ambiguous or invalid actions), our analysis shows that MIMIC took over task initiative when ASR performance was poor, allowing the system to better con- strain user utterances.<sup>14</sup> 5 Conclusions This paper described an empirical evaluation of MIMIC, an adaptive mixed initiative spoken dialogue system. We conducted two experiments that focused on evaluating the mixed initiative and automatic adaptation aspects of MIMIC and analyzed the results along three dimensions: performance features, discourse features, and initiative distribution. Our results showed that both the mixed initiative and automatic adaptation aspects of the sys- tem led to better performance in terms of user satisfac- tion and dialogue efficiency. In addition, we found that MIMIC's adaptation behavior better matched user expec- tations, more efficiently resolved anomalies in dialogues, and led to higher overall dialogue quality. Acknowledgments We would like to thank Bob Carpenter and Christine Nakatani for their help on experimental design, Jan van Santen for discussion on statistical analysis, and Bob Carpenter for his comments on an earlier draft of this pa- per. Support for the second author is provided by an NSF graduate fellowship and a Lucent Technologies GRPW grant. References James F. Allen, Bradford W. Miller, Eric K. Ringger, and Teresa Sikorski. 1996. A robust system for nat- ural spoken dialogue. In Proceedings of the 34th An- nual Meeting of the Association for Computational Linguistics, pages 62-70. S. Bennacef, L. Devillers, S. Rosset, and L. Lamel. 1996. Dialog in the RAILTEL telephone-based sys- tem. In Proceedings of the 4th International Confer- ence on Spoken Language Processing. Jennifer Chu-Carroll and Michael K. Brown. 1998. An evidential model for tracking initiative in collabora- tive dialogue interactions. User Modeling and User- Adapted Interaction, 8(3-4):215-253. Jennifer Chu-Carroll. 2000. MIMIC: An adaptive mixed initiative spoken dialogue system for information queries. In Proceedings of the 6th ACL Conference on Applied Natural Language Processing. To appear. Paul R. Cohen. 1995. Empirical Methods for Artificial Intelligence. MIT Press. Morena Danieli and Elisabetta Gerbino. 1995. Metrics for evaluating dialogue strategies in a spoken language <sup>14</sup>Although not currently utilized, the ability to adapt dialogue strate- gies when ASR performance is poor enables the system to employ dia- logue strategy specific language models for ASR. system. In Proceedings of the AAAI Spring Sympo- sium on Empirical Methods in Discourse Interpreta- tion and Generation, pages 34-39. Jean Gordon and Edward H. Shortliffe. 1984. The Dempster-Shafer theory of evidence. In Bruce Buchanan and Edward Shortliffe, editors, Rule-Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project, chapter 13, pages 272-292. Addison-Wesley. Diane J. Litman and Shimei Pan. 1999. Empirically evaluating an adaptable spoken dialogue system. In Proceedings of the 7th International Conference on User Modeling, pages 55-64. H. Meng, S. Busayaponchai, J. Glass, D. Goddeau, L. Hetherington, E. Hurley, C. Pao, J. Polifroni, S. Seneff, and V. Zue. 1996. WHEELS: A conversa- tional system in the automobile classifieds domain. In Proceedings of the International Conference on Spo- ken Language Processing, pages 542-545. Stefan Ortmanns, Wolfgang Reichl, and Wu Chou. 1999. An efficient decoding method for real time speech recognition. In Proceedings of the 5th European Con- ference on Speech Communication and Technology. K.A. Papineni, S. Roukos, and R.T. Ward. 1999. Free- flow dialog management using forms. In Proceedings of the 6th European Conference on Speech Communi- cation and Technology, pages 1411-1414. Patti Price, Lynette Hirschman, Elizabeth Shriberg, and Elizabeth Wade. 1992. Subject-based evaluation mea- sures for interactive spoken language systems. In Pro- ceedings of the DARPA Speech and Natural Language Workshop, pages 34-39. Wolfgang Reichl and Wu Chou. 1998. Decision tree state tying based on segmental clustering for acoustic modeling. In Proceedings of the International Confer- ence on Acoustics, Speech, and Signal Processing. M.D. Sadek, A. Ferrieux, A. Cozannet, P. Bretier, F. Panaget, and J. Simonin. 1996. Effective human- computer cooperative spoken dialogue: The AGS demonstrator. In Proceedings of the International Conference on Spoken Language Processing. Glenn Shafer. 1976. A Mathematical Theory of Evi- dence. Princeton University Press. Andrew Simpson and Norman M. Fraser. 1993. Black box and glass box evaluation of the SUNDIAL system. In Proceedings of the 3rd European Conference on Speech Communication and Technology, pages 1423- 1426. Gert Veldhuijzen van Zanten. 1999. User modelling in adaptive dialogue management. In Proceedings of the 6th European Conference on Speech Communication and Technology, pages 1183-1186. Marilyn A. Walker, Diane J. Litman, Candance A. Kamm, and Alicia Abella. 1997. PARADISE: A framework for evaluating spoken dialogue agents. In Proceedings of the 35th Annual Meeting of the Associ- ation for Computational Linguistics, pages 271-280."
  },
  {
    "title": "Insights into the Dialogue Processing of VERBMOBIL",
    "abstract": "We present the dialogue module of the speech-to-speech translation system VERBMOBIL. We follow the approach that the solution to dialogue processing in a mediating scenario cannot depend on a single constrained processing tool, but on a combination of several simple, efficient, and robust components. We show how our solution to dialogue processing works when applied to real data, and give some examples where our module contributes to the correct translation from German to English.",
    "content": "1 Introduction The implemented research prototype of the speech- to-speech translation system VERBMOBIL (Wahlster, 1993; Bub and Schwinn, 1996) consists of more than 40 modules for both speech and linguistic processing. The central storage for dialogue information within the overall system is the dialogue module that ex- changes data with 15 of the other modules. Basic notions within VERBMOBIL are turns and utterances. A turn is defined as one contribution of a dialogue participant. Each turn divides into utter- ances that sometimes resemble clauses as defined in a traditional grammar. However, since we deal ex- clusively with spoken, unconstrained contributions, utterances are sometimes just pieces of linguistic ma- terial. For the dialogue module, the most important di- alogue related information extracted for each utter- ance is the so called dialogue act (Jekat et al., 1995). Some dialogue acts describe solely the illocutionary force, while other more domain specific ones describe additionally aspects of the propositional content of an utterance. Prior to the selection of the dialogue acts, we ana- lyzed dialogues from VERBMOBIL's corpus of spoken and transliterated scheduling dialogues. More than 500 of them have been annotated with dialogue re- lated information and serve as the empirical founda- tion of our work. Throughout this paper we will refer to the exam- ple dialogue partly shown in figure 1. The transla- tions are as the deep processing line of VERBMOBIL provides them. We also annotated the utterances with the dialogue acts as determined by the semantic evaluation module. ''//'' shows where utterance boundaries were determined. We start with a brief introduction to dialogue pro- cessing in the VERBMOBIL setting. Section 3 intro- duces the basic data structures followed by two sec- tions describing some of the tasks which are carried out within the dialogue module. Before the con- cluding remarks in section 8, we discuss aspects of robustness and compare our approach to other sys- tems. 2 Introduction to Dialogue Processing in VERBMOBIL In contrast to many other NL-systems, the VERB- MOBIL system is mediating a dialogue between two persons. No restrictions are put on the locutors, ex- cept for the limitation to stick to the approx. 2500 words VERBMOBIL recognizes. Therefore, VERBMO- BIL and especially its dialogue component has to fol- low the dialogue in any direction. In addition, the dialogue module is faced with incomplete and incor- rect input, and sometimes even gaps. When designing a component for such a scenario, we have chosen not to use one big constrained pro- cessing tool. Instead, we have selected a combina- tion of several simple and efficient approaches, which together form a robust and efficient processing plat- form. As an effect of the mediating scenario, our mod- ule cannot serve as a \"dialogue controller\" like in man-machine dialogues. The only exception is when A01: Tag // Herr Scheytt. (GREET, INTRODUCE_NAME) (Hello, Mr Scheytt) B02: Guten Tag // Frau Klein // Wir müssen noch einen Termin ausmachen // für die Mitarbeiterbesprechung. (GREET, INTRODUCE_NAME, INIT_DATE, SUGGEST_SUPPORT_DATE) (Hello, Mrs. Klein, we should arrange an appointment, for the team meeting) A03: Ja,// ich würde Ihnen vorschlagen im Januar,// zwischen dem fünfzehnten und neunzehnten. (UPTAKE, SUGGEST_SUPPORT_DATE, REQUEST_COMMENT_DATE) (Well, I would suggest in January, between the fifteenth and the nineteenth) B04: Oh // das ist ganz schlecht. // zwischen dem elften und achtzehnten Januar bin ich in Hamburg. (UPTAKE, REJECT_DATE, SUGGEST_SUPPORT_DATE) (Oh, that is really inconvenient, I'm in Hamburg between the eighteenth of January and the eleventh,) ... A09: Doch ich habe Zeit von sechsten Februar bis neunten Februar (SUGGEST_SUPPORT_DATE) (I have time afterall from the 6th of February to the 9th of February) B10: Sehr gut // das paßt bei mir auch // Dann machen wir's gleich aus // für Donnerstag // den achten // Wie wäre es denn um acht Uhr dreißig // (FEEDBACK_ACKNOWLEDGEMENT, ACCEPT_DATE, INIT_DATE, SUGGEST_SUPPORT_DATE, SUGGEST_SUPPORT_DATE, SUGGEST_SUPPORT_DATE) (Very good, that too suits me, we will arrange for it, for thursday, the eighth, how about half past eighth) A11: Am achten // ginge es bei mir leider nur bis zehn Uhr // Bei mir geht es besser nachmittags (SUGGEST_SUPPORT_DATE, SUGGEST_SUPPORT_DATE, ACCEPT_DATE) (on the eighth, Is it only unfortunately possible for me until 10 o'clock, It suits me better in the afternoon) B12: gut // um wieviel Uhr sollen wir uns dann treffen ? (FEEDBACK_ACKNOWLEDGEMENT, SUGGEST_SUPPORT_DATE) (good, when should we meet) A13: ich würde \"ahm vierzehn Uhr vorschlagen // geht es bei Ihnen. (SUGGEST_SUPPORT_DATE, REQUEST_COMMENT_DATE) (I would suggest 2 o'clock, is that possible for you?) B14: sehr gut // das paßt bei mir auch // das können wir festhalten (ACCEPT_DATE, ACCEPT_DATE, ACCEPT_DATE) (very good, that suits me too, we can make a note of that) ... Figure 1: An example dialogue clarification dialogues are necessary between VERB- MOBIL and a user. Due to its role as information server in the overall VERBMOBIL system, we started early in the project to collect requirements from other components in the system. The result can be divided into three subtasks: • we allow for other components to store and re- trieve context information. • we draw inferences on the basis of our input. • we predict what is going to happen next. Moreover, within VERBMOBIL there are different processing tracks: parallel to the deep, linguistic- based processing, different shallow processing mod- ules also enter information into, and retrieve it from, the dialogue module. The data from these parallel tracks must be consistently stored and made acces- sible in a uniform manner. Figure 2 shows a screen dump of the graphical user interface of our component while processing the example dialogue. In the upper left corner we see the structures of the dialogue sequence memory, where the middle right row represents turns, and the left and right rows represent utterances as segmented by different analysis components. The upper right part shows the intentional structure built by the plan recognizer. Our module contains two instances of a finite state automaton. The one in the lower left corner is used for performing clarification dialogues, and the other for visualization purposes (see section 7). The thematic structure representing temporal expressions is displayed in the lower right corner. 3 Maintaining Context As basis for storing context information we devel- oped the dialogue sequence memory. It is a generic structure which mirrors the sequential order of turns and utterances. A wide range of operation has been defined on this structure. For each turn, we store e.g. the speaker identification, the language of the contribution, the processing track finally selected for translation, and the number of translated utter- DRAUL Figure 2: Overview of the dialogue module 10: T171RIUI Dialogs-act-ris GREET score 1 source:SHALLOW Predictions A: INIT DATE 20 A: SUGGEST SUPPORT DATE 18 B: INIT DATE 277 B: FEEDBACK ACKNOWLEDGEMENT 19 10:171 Speskar: A Language: GERMAN Vld Source: DEEP Veld Utterance: 4 ID: TI7IRIUI Phase: OPENING Dialogue-act-units GREET score source DEEP Predictione: A: INTRODUCE NAME B A: INIT DATE 1 8: GREET 283 B: REQUEST SUGGEST_DURATION O 10: 11718102 Dialogen-act-rits INIT DATE acore 1 source:SHALLOW Predictions: A: SUGGEST SUPPORT DATE 250 A: REQUEST SUGGEST DATE 128 8: SUGGEST SUPPORT DATE 50 8: FEEDBACK ACKNOWLEDGEMENT ST 10: T1718102 Phaas: OPENING Dialogue-act-site: INTRODUCE NAME soor source DEEP Predictione A: SUGGEST SUPPORT DATE 50 A: INIT DATE 47 B: GREET 31 B: INIT DATE ID: TI7IRIUS Phase: OPENING Dialogue-act-us INIT DATE score source:DEEP Predictions: A: SUGGEST SUPPORT DATE 403 A: MOTIMATE APPOINTMENT 171 SUGGEST SUPPORT DATE 77 B: INTRODUCE NAMETT ID: TIZIRIU Phase: NEGOTIATION Dialogue-act-wits SUGGEST SUPPORT_DATE Borse DEEP Predictione A: REQUEST COMMENT DATE 164 A: SUGGEST SUPPORT DATE 130 8: DIGRESS SCENARIO 130 8: GREET 130 Figure 3: A part of the sequence memory ances. For the utterances we store e.g. the dialogue act, dialogue phase, and predictions. These data are partly provided by other modules of VERBMOBIL or computed within the dialogue module itself (see be- low). Figure 3 shows the dialogue sequence memory af- ter the processing of turn B02. For the deep anal- ysis side (to the right), the turn is segmented into four utterances: Guten Tag // Frau Klein // Wir müssen noch einen Termin ausmachen // für die Mitarbeiterbesprechung, for which the semantic eval- uation component has assigned the dialogue acts GREET, INTRODUCE_NAME, INIT_DATE, and SUG- GEST_SUPPORT_DATE respectively. To the left we see the results of one of the shallow analysis com- ponents. It splits up the input into two utterances Guten Tag Frau Klein // Wir müssen ... die Mi- tarbeiterbesprechung and assigns the dialogue acts GREET and INIT_DATE. The need for and use of this structure is high- lighted by the following example. In the domain of appointment scheduling the German phrase Geht es bei Ihnen? is ambiguous: bei Ihnen can either re- fer to a location, in which case the translation is Would it be okay at your place? or, to a certain time. In the latter case the correct translation is Is that possible for you?. A simple way of disambiguat- ing this is to look at the preceding dialogue act(s). In our example dialogue, turn A13, the utterance ich würde ähm vierzehn Uhr vorschlagen (I would hmm fourteen o'clock suggest) contains the proposal of a time, which is characterized by the dialogue act SUGGEST_SUPPORT_DATE. With this dialogue act in the immediately preceding context the ambiguity is resolved as referring to a time and the correct trans- lation is determined. In our domain, in addition to the dialogue act the most important propositional information are the dates as proposed, rejected, and finally accepted by the users of VERBMOBIL. While it is the task of the semantic evaluation module to extract time informa- tion from the actual utterances, the dialogue module integrates those information in its thematic mem- ory. This includes resolving relative time expres- sions, e.g. two weeks ago, into precise time descrip- tions, like \"23rd week of 1996\". The information about the dates is split in a specialization hierarchy. Each date to be negotiated serves as a root, while the nodes represent the information about years, months, weeks, days, days of week, period of day and finally time. Each node contains also informa- tion about the attitude of the dialogue participants concerning this certain item: proposed, rejected, or accepted by one of the participants. Figure 4 shows parts of the thematic structure after the processing of turn B10. The black boxes stand for the date currently under consideration. Thursday, 8., is the current date agreed upon. We also see the previously proposed interval from 6.-9. of the same month in the box above (FROM_TO (6,9)). 4 Inferences Besides the mere storage of dialogue related data, there are also inference mechanisms integrating the data in representations of different aspects of the dialogue. These data are again stored in the context memories shown above and are accessed by the other VERBMOBIL modules. Plan Based Inferences Inspecting our corpus, we can distinguish three phases in most of the dialogues. In the first, the opening phase, the locutors greet each other and the topic of the dialogue is introduced. The dialogue then proceeds into the negotiation phase, where the actual negotiation takes place. It concludes in the closing phase where the negotiated topic is confirmed and the locutors say goodbye. This phase informa- tion contributes to the correct transfer of an utter- ance. For example, the German utterance Guten Tag is translated to \"Hello\" in the greeting phase, and to \"Good day\" in the closing phase. The task of determining the phase of the dialogue has been given to the plan recognizer (Alexander- sson, 1995). It builds a tree like structure which we call the intentional structure. The current ver- sion makes use of plan operators both hand coded and automatically derived from the VERBMOBIL cor- pus. The method used is transferred from the field of grammar extraction (Stolcke, 1994). To contribute to the robustness of the system, the processing of the recognizer is divided into several processing lev- els like the \"turn level\" and the \"domain dependent level\". The concepts of turn levels and the automatic acquisition of operators are described in (Alexander- sson, 1996). In figure 5 we see the structure after processing turns B02 and A03. The leaves of the tree are the dialogue acts. The root node of the left subtree for B02 is a GREET(T)-INIT-... operator which belongs to the greeting phase, while the partly visible one to the right belongs to the negotiation phase. In the example used in this paper we are process- ing a \"well formed\" dialogue, so the turn structure can be linked into a structure spanning over the whole dialogue. We also see in figure 3 how the phase information has been written into the boxes Varbrnobil's Dialogue Component Actions Show Configurations Planner Displaying: Thematic Structure DAY Debug DAY-OF-WEEK frame 5: 2 frame 6: FRI proposed-by: A proposed-by: A stale: OPEN stale: OPEN PERI frame_7: FROM_TO(6,9) frame 8: UNDEFINED frame proposed-by: A proposed-by: A propo stale: OPEN state: OPEN state: frame 14 8 proposed-by A state OPEN frame 12 THU frarde proposed-by A atate OPEN state Figure 4: Day/Day-of-Week detail of the thematic structure representing the utterances of turn B02 as segmented by the deep analysis. Thematic Inferences In scheduling dialogues, referring expressions like the German word nächste occur frequently. Depend- ing on the thematic structure it can be translated as next if the date referred to is immediately after the speaking time, or following in the other cases. The thematic structure is mainly used to resolve this type of anaphoric expressions if requested by the semantic evaluation or the transfer module. The information about the relation between the date under consid- eration and the speaking time can be immediately computed from the thematic structure. The thematic structure is also used to check whether the time expressions are correctly recog- nized. If some implausible dates are recognized, e.g. April, 31., a clarification can be invoked. The sys- tem proposes the speaker a more plausible date, and waits for an acceptance or rejection of the proposal. In the first case, the correct date will be translated, in the latter, the user is asked to repeat the whole turn. Using the current state of the thematic structure and the dialogue act in combination with the time information of an utterance, multiple readings can be inferred (Maier, 1996). For example, if both lo- cutors propose different dates, an implicit rejection of the former date can be assumed. 5 Predictions A different type of inference is used to generate pre- dictions about what comes next. While the plan- based component uses declarative knowledge, albeit acquired automatically, dialogue act predictions are based solely on the annotated VERBMOBIL corpus. The computation uses the conditional frequencies of dialogue act sequences to compute probabilities of the most likely follow-up dialogue acts (Reithinger et al., 1996), a method adapted from language model- ing (Jelinek, 1990). As described above, the dialogue sequence memory serves as the central repository for this information. The sequence memory in figure 3 shows in addi- Figure 5: Intentional structure for two turns tion to the actual recognized dialogue act also the predictions for the following utterance. In (Rei- thinger et al., 1996) it is demonstrated that ex- ploiting the speaker direction significantly enhances the prediction reliability. Therefore, predictions are computed for both speakers. The numbers after the predicted dialogue acts show the prediction proba- bilities times 1000. As can be seen in the figure, the actually recog- nized dialogue acts are, for this turn, among the two most probable predicted acts. Overall, approx. 74% of all recognized dialogue acts are within the first three predicted ones. Major consumers of the predictions are the seman- tic evaluation module, and the shallow translation module. The former module that uses mainly knowl- edge based methods to determine the dialogue act of an utterance exploits the predictions to narrow down the number of possible acts to consider. The shallow translation module integrates the predictions within a Bayesian classifier to compute dialogue acts di- rectly from the word string. 6 Robustness For the dialogue module there are two major points of insecurity during operation. On the one hand, the user's dialogue behaviour cannot be controlled. On the other hand, the segmentation as computed by the syntactic-semantic construction module, and the dialogue acts as computed by the semantic evalu- ation module, are very often not the ones a linguistic analysis on the paper will produce. Our example di- alogue is a very good example for the latter problem. Since no module in VERBMOBIL must ever crash, we had to apply various methods to get a high degree of robustness. The most knowledge intensive module is the plan recognizer. The robustness of this sub- component is ensured by dividing the construction of the intentional structure into several processing lev- els. Additionally, at the turn level the operators are learned from the annotated corpus. If the construc- tion of parts of the structure fails, some functionality has been developed to recover. An important ingre- dience of the processing is the notion of repair - if the plan construction is faced with something unex- pected, it uses a set of specialized repair operators to recover. If parts of the structure could not be built, we can estimate on the basis of predictions what the gap consisted of. The statistical knowledge base for the prediction algorithm is trained on the VERBMOBIL corpus that in its major parts contains well-behaved dialogues. Although prediction quality gets worse if a sequence of dialogue acts has never been seen, the interpola- tion approach to compute the predictions still deliv- ers useful data. As mentioned above, to contribute to the correct- ness of the overall system we perform different kinds of clarification dialogues with the user. In addi- tion to the inconsistent dates, we also e.g. recognize similar words in the input that will be most likely exchanged by the speech recognizer. Examples are the German words for thirteenth (dreizehnter) and thirtieth (dreißigster). Within a uniform computer-- human interaction, we resolve these problems. 7 Related Work In the speech-to-speech translation system JANUS (Lavie et al., 1996), two different approaches, a plan based and an automaton based, to model dialogues have been implemented. Currently, only one is used at a time. For VERBMOBIL, (Alexandersson and Re- ithinger, 1995) showed that the descriptive power of the plan recognizer and the predictive power of the statistical component makes the automaton ob- solete. The automatic acquisition of a dialogue model from a corpus is reported in (Kita et al., 1996). They extract a probabilistic automaton using an an- notated corpus of up to 60 dialogues. The transitions correspond to dialogue acts. This method captures only local discourse structures, whereas the plan based approach of VERBMOBIL also allows for the description of global structures. Comparable struc- tures are also defined in the dialogue processing of TRAINS (Traum and Allen, 1992). However, they are defined manually and have not been tested on larger data sets. 8 Conclusion and Future Work Dialogue processing in a speech-to-speech transla- tion system like VERBMOBIL requires innovative and robust methods. In this paper we presented differ- ent aspects of the dialogue module while processing one example dialog. The combination of knowledge based and statistical methods resulted in a reliable system. Using the VERBMOBIL corpus as empirical basis for training and test purposes significantly im- proved the functionality and robustness of our mod- ule, and allowed for focusing our efforts on real prob- lems. The system is fully integrated in the VERBMO- BIL system and has been tested on several thousands of utterances. Nevertheless, processing in the real system cre- ates still new challenges. One problem that has to be tackled in the future is the segmentation of turns into utterances. Currently, turns are very often split up into too many and too small utterances. In the future, we will have to focus on the problem of \"glue- ing\" fragments together. When given back to the transfer and generation modules, this will enhance translation quality. Future work includes also more training and the ability to handle sparse data. Although we use one of the largest annotated corpora available, for purposes like training we still need more data. Acknowledgements This work was funded by the German Federal Min- istry of Education, Science, Research and Technol- ogy (BMBF) in the framework of the VERBMOBIL Project under Grant 01IV101K/1. The responsibil- ity for the contents of this study lies with the au- thors. We thank our students Ralf Engel, Michael Kipp, Martin Klesen, and Paula Sevastre for their valuable contributions. Special thanks to Reinhard for Karger's Machine. References Alexandersson, Jan. 1995. Plan recognition in VERBMOBIL. In Mathias Bauer, Sandra Carberry, and Diane Litman, editors, Proceedings of the IJCAI-95 Workshop The Next Generation of Plan Recognition Systems: Challenges for and Insight from Related Areas of Al, pages 2-7, Montreal, August. Alexandersson, Jan. 1996. Some Ideas for the Auto- matic Acquisition of Dialogue Structure. In Anton Nijholt, Harry Bunt, Susann LuperFoy, Gert Veld- huijzen van Zanten, and Jan Schaake, editors, Proceedings of the Eleventh Twente Workshop on Language Technology, TWLT, Dialogue Manage- ment in Natural Language Systems, pages 149- 158, Enschede, Netherlands, June 19-21. Alexandersson, Jan and Norbert Reithinger. 1995. Designing the Dialogue Component in a Speech Translation System a Corpus Based Approach. In Proceedings of the 9th Twente Workshop on Language Technology (Corpus Based Approaches to Dialogue Modeling), Twente, Holland. Bub, Thomas and Johannes Schwinn. 1996. Verb- mobil: The evolution of a complex large speech- to-speech translation system. In Proceedings of ICSLP-96, pages 2371-2374, Philadelphia, PA. Jekat, Susanne, Alexandra Klein, Elisabeth Maier, Ilona Maleck, Marion Mast, and J. Joachim Quantz. 1995. Dialogue Acts in VERB- MOBIL. Verbmobil Report 65, Universität Ham- burg, DFKI Saarbrücken, Universität Erlangen, TU Berlin. Jelinek, Fred. 1990. Self-Organized Language Mod- eling for Speech Recognition. In A. Waibel and K.-F. Lee, editors, Readings in Speech Recogni- tion. Morgan Kaufmann, pages 450-506. Kita, Kenji, Yoshikazu Fukui, Masaki Nagata, and Tsuyoshi Morimoto. 1996. Automatic acquisition of probabilistic dialogue models. In Proceedings of ISSD-96, pages 109-112, Philadelphia, PA. Lavie, Alon, Lori Levin, Yan Qu, Alex Waibel, Donna Gates, Marsal Gavalda, Laura Mayfield, and Maite Taboada. 1996. Dialogue process- ing in a conversational speech translation sys- tem. In Proceedings of ICSLP-96, pages 554-557, Philadelphia, PA. Maier, Elisabeth. 1996. Context Construction as Subtask of Dialogue Processing - the VERB- MOBIL Case. In Anton Nijholt, Harry Bunt, Susann LuperFoy, Gert Veldhuijzen van Zan- ten, and Jan Schaake, editors, Proceedings of the Eleventh Twente Workshop on Language Tech- nology, TWLT, Dialogue Management in Natu- ral Language Systems, pages 113-122, Enschede, Netherlands, June 19-21. Reithinger, Norbert, Ralf Engel, Michael Kipp, and Martin Klesen. 1996. Predicting Dialogue Acts for a Speech-To-Speech Translation System. In Proceedings of International Conference on Spo- ken Language Processing (ICSLP-96). Stolcke, Andreas. 1994. Bayesian Learning of Prob- abilistic Language Models. Ph.D. thesis, Univer- sity of California at Berkeley. Traum, David R. and James F. Allen. 1992. A \"Speech Acts\" Approach to Grounding in Conver- sation. In Proceedings of International Conference on Spoken Language Processing (ICSLP'92), vol- ume 1, pages 137-140. Wahlster, Wolfgang. 1993. Verbmobil-Translation of Face-to-Face Dialogs. Technical report, Ger- man Research Centre for Artificial Intelligence (DFKI). In Proceedings of MT Summit IV, Kobe, Japan, July 1993."
  },
  {
    "title": "Automatic Selection of Class Labels from a Thesaurus for an Effective Semantic Tagging of Corpora",
    "abstract": "It is widely accepted that tagging text with semantic information would improve the quality of lexical learning in corpus-based NLP methods. However, available on-line taxonomies are rather entangled and introduce an unnecessary level of ambiguity. The noise produced by the redundant number of tags often overrides the advantage of semantic tagging. In this paper, we propose an automatic method to select from WordNet a subset of domain-appropriate categories that effectively reduce the overambiguity of WordNet, and help at identifying and categorize relevant language patterns in a more compact way. The method is evaluated against a manually tagged corpus, SEMCOR.",
    "content": "1 Introduction It is well known that statistically-based approaches to lexical knowledge acquisition are faced with the problem of low counts. Many language patterns (from simple co- occurrences to more complex syntactic associations among words) occur very rarely, or are never encoun- tered, in the learning corpus. Since rare patterns are the majority, the quality and coverage of lexical learning may result severely affected. The obvious strategy to reduce this problem is to gener- alise word patterns according to some clustering tech- niques. In the literature, two generalisation strategies have been adopted: Distributional approaches: Several papers adopt distribu- tional techniques to identify clusters of words according to some defined measure of similarity. Among these, in (Grishman and Sterling, 1994) a method is proposed to cluster syntactic triples, while in (Pereira and Tishby 1992, 1993), (Dagan et al., 1994) pure bigrams are anal- ysed. The most intuitive evaluation of the effectiveness of dis- tributional approaches to the problem of word general- ization is presented in (Grishman and Sterling, 1994). In this paper it is argued that distributional (called also smoothing) techniques introduce a certain degree of addi- tional error, because co-occurrences may be erroneously conflated in a cluster, and some of the co-occurrences be- ing generalized are themselves incorrect. In general, the effect is a higher recall at the price of a lower precision. Another drawback of these methods is that, since clusters have only a numeric description, they are often hard to evaluate on a linguistic ground. Semantic tagging: Another adopted solution is to gener- alise the observed word patterns by grouping patterns in which words have the same semantic tag. Semantic tags are assigned from on-line thesaura like WordNet (Basili et al, 1996) (Resnik, 1995), Roget's categories (Yarowsky 1992) (Chen and Chen, 1996), the Japanese BGH (Utsuro et al, 1993), or assigned manually (Basili et al, 1992)$^1$. The obvious advantage of semantic tags is that words are clustered according to an intuitive principle (they belong to the same concept) rather than to some probabilistic measure. Semantic tagging has been proven useful for learning and categorising interesting relations among words, and for systematic lexical learning in sublan- guages, as shown in (Basili et al, 1996) and (Basili et al, 1996b). On the other hand, semantic tagging has a serious draw- back, which is not solely due to the limited availability of on-line resources, but rather to the entangled structure of thesaura. Wordnet and Roget's thesaura have not been conceived, despite their success among researchers in lex- ical statistics, as tools for automatic language processing. The purpose was rather to provide the linguists with a very refined, general purpose, linguistically motivated source of taxonomic knowledge. As a consequence, in most on-line thesaura words are ex- tremely ambiguous, with very subtle distinctions among senses. (Dolan, 1994) and (Krovetz and Croft, 1992) claim that fine-grained semantic distinctions are unlikely to be of practical value for many applications. Our experience supports this claim: often, what matters is to be able to distinguish among contrastive (Pustejowsky, 1995) ambi- guities of the bank_river bank_organisation flavour. High ambiguity, entangled nodes, and asymmetry have al- ready been emphasised in (Hearst and Shutze, 1993) as being an obstacle to the effective use of on-line thesaura in corpus linguistics. In most cases, the noise introduced by overambiguity almost overrides the positive effect of semantic clustering. For example, in (Brill and Resnik, 1994) clustering PP heads according to WordNet synsets produced only a 1% improvement in a PP disambiguation task, with respect to the non-clustered method. There are reported cases in which the use of WordNet worsened the performance of an automatic indexing method. Even context-based sense disambiguation becomes a pro- hibitive task on a wide-scale basis, because when words in the context of an ambiguous word are replaced by $^1$ Manually assigning semantic tags if of course rather time-consuming, however on-line thesaura are not avail- able in many languages, like Italian. their synsets, there is a multiplication of possible con- texts, rather than a generalization. In (Agirre and Rigau, 1996) a method called Conceptual Distance is proposed to reduce this problem, but the reported performance in disambiguation still do not reach 50%. A possible alternative is to manually select a set of high-. level tags from the thesaurus. This approach is adopted in (Chen and Chen, 1996) and in (Basili et al, 1996) where only a dozen categories are used. As discussed in the latter paper, high-level tags reduce the problem of overambiguity and allow the detection of more regular behaviours in the analysis of lexical patterns. On the other hand, high-level tags may be overgeneral, and the acquired lexical rules, while usually perform well in the task of selecting the correct word associations (for ex- ample in PP disambiguation, or sense interpretation), are less capable of filtering out the noise. Overgeneral cate- gories may even fail to capture contrastive ambiguities of words. So far the manual selection of an appropriate set of se- mantic tags has been a matter of personal intuitions, but we believe that this task should be performed in a more principled, and automatic, way. In this paper, we present a method for the selection of the \"best-set\" of WordNet categories for an effective, domain- tailored, semantic tagging of a corpus. The purpose of the method is to automatically select: • A domain-appropriate set of categories, that well represent the semantics of the domain. • A \"right\" level of abstraction, so as to mediate at best between overambiguity and overgenerality. • A balanced (for the domain) set of categories, i.e. words should be evenly distributed among cate- gories. The second feature is the most important, since as we re- marked so far, assigning semantic characteristics to words is very useful in lexical learning tasks, but over- ambiguity is the major obstacle to an effective use of the- saura in semantic tagging. In the following sections, we define a method for the au- tomatic selection of the \"best-set\" of WordNet categories, for nouns given an application corpus. First, an iterative method is used to create alternative sets of balanced categories. Sets have an increasing level of generality. Second, a scoring function is applied to al- ternative sets to identify the \"best\" set. The best set is modelled as the linear function of four performance fac- tors: generality, coverage of the domain, average ambigu- ity, and discrimination power. An interpolation method is adopted to estimate the parameters of the model against a reference, correctly tagged, corpus (SEMCOR). The per- formance of the selected set of categories is evaluated in terms of effective reduction of overambiguity. The described method only requires a medium-range (stemmed) application corpus and a thesaurus. The model parameters are tuned against a reference correctly tagged corpus, but this is not strictly necessary if correctly tagged corpora are not available. 2 Selection of Alternative Sets of Se- mantic Categories from WordNet The first step of the method is generating alternative sets of WordNet categories. Alternative sets are selected ac- cording to the following principles: • Balanced categories: words must be uniformly dis- tributed among categories of a set; • Increasing level of generality: alternative sets are se- lected by uniformly increasing the level of generality of the categories belonging to a set; • Domain-appropriateness: selected categories in a set are those pointed by (an increasingly large number of) words of the application domain, weighted by their frequency in the corpus. The set-generation algorithm is an iterative application of the algorithm proposed in (Hearst and Shutze, 1993) for creating WordNet categories of a fixed average size. In its modified version, the algorithm is as follows2: Let S be a set of WordNet synsets s, W the set of different words (nouns) in the corpus, P(s) the number of words in W that are instances of s, weighted by their frequency, UB and LB the upper and lower bound for P(s), N, h and k constant values. i=1 UB=N LB=UB*h; do { initialise S with the set of WordNet topmost; initialise the set of categories C₁ with the empty set; new_cat(S); if i=1 or Ci≠Ci-1 then add C₁ to the set of Cat. i=i+1; UB=UB+k; LB=UB*h; } until Cᵢ is not an empty set; where: new_cat(S): for any category s of S { if s does not belong to Cᵢ then { if P(s) <= UB and P(s) >= LB then put s in the set Cᵢ else if P(s) > UB then { let S' be the set of direct descendents of s new_cat(S') } else add s to SCT(Cᵢ) } } 2The procedure new_cat(S) is almost the same as in (Hearst and Shutze, 1993). For sake of brevity, the algo- rithm is not further explained here. N, h and k are the initial parameters of the algorithm. We experimentally observed that only h (the ratio between lower and upper bound) significantly modifies the result- ing sets of categories (Ci): we established that a good compromise is h=0.4. SCT(Ci) is the set of \"smaller\" WordNet categories with P(s)<LB that do not belong to the Ci set (see next section). 3 Scoring Alternative Sets of Cate- gories The algorithm of section 2 creates alternative sets of bal- anced and increasingly general categories C₁. We now need a scoring function to evaluate these alternatives. The following performance factors have been selected to express the scoring function: Generality: In principle, we would like to represent the semantics of the domain using the highest possible level of generalisation. We can express the generality G'(C₁) as 1/DM(Ci), being DM(Ci) the average distance between the categories of Ci and the WordNet topmost synsets. Due to the graph structure of WordNet, different paths may connect each element cij of Ci with different topmosts, therefore we compute DM(Ci) as: n DM(Ci) = 1* Σ dm(cij) n j=1 where dm(cij) is the average distance of each cij from the topmosts. Figure 1 illustrates a possible sysnsets hierar- chical in which, for Ci={Ci1 Ci2), being dm(ci1)=(4+3)/2 =3.5 and dm(ci2)=3, DM(Ci)=(3+3.5)/2=3.25 that several words are not assigned to any category, be- cause when branching from an overpopulated category to its descendants, some of the descendants may be under- populated. Each iterative step that creates a Ci also cre- ates a set of underpopulated categories SCT(C₁). To en- sure full coverage, these categories may be added to Ci, or alternatively, they can be replaced by their direct ances- tors, but clearly a \"good\" selection of C₁ is one that mini- mizes this problem. The coverage CO(C₁) is therefore de- fined as the ratio Nc(C₁)/W, where Nc(C₁) is the number of words that reach at least one category of Ci Discrimination Power: a certain selection of categories may not allow a full discrimination of the lowest-level senses for a word (leaves-synsets hereafter). Figure 2 il- lustrates an example. If C₁ = {ci1 C12 C13 C14), W2 cannot be fully disambiguated by any sense selection algorithm, be- cause two of its leaves-synsets belong to the same cate- gory ci2. With respect to w2, Ci2 is overgeneral (though nothing can be said about the actual importance of dis- criminating between such two synsets). We measure the discrimination power DP(C₁) as the ratio (Nc(C₁)-Npc(Ci))/Nc(C₁), where Nc(C₁) is the number of words that reach at least one category of Ci, and Npc(Ci) is the number of words that have at least two leaves- synsets that reach the same category cij of Ci. For the ex- ample of figure 2 DP1, DP(C₁)=(3-1)/3=0.66. Cil Ci2 C13 Ci4 General Synset O Leaf Synset Corpus Word W1 W2 W3 • Topmost Synset General Synset Cin Ci2 Figure 1 - An example of synsets hierarchy As defined, G'(C₁) is a linear function (low values for low generality, high value for high generality), whilst our goal is to mediate at best between overspecificity and overgenerality. Therefore, we model the generality as G(C₁)=G'(C₁)*Gauss(G'(C₁)), where Gauss (G'(C₁)) is a Gauss distribution function computed by using the aver- age and the variancy of G'(C₁) values over the set of all categories Ci, selected by the algorithm in section 2, nor- malised in the [0,1] interval. Coverage: the algorithm of section 2, for any set Ci, does not allow a full coverage of the nouns in the domain. Given a selected pair <UB,LB>, it may well be the case Figure 2 - An example of distribution of leaves- synsets among categories Average Ambiguity: each choice of C₁ in general re- duces the initial ambiguity of the corpus. In part, because there are leaves-synsets that converge into a single cate- gory of the set, in part because there are leaves-synsets of a word that do not reach any of these categories. This phenomenon is accounted for by the inverse of the aver- age ambiguity A(C₁). The A(C₁) is measured as: 1 A (Ci) = Nc(Ci) * Nc(Ci) ΣCwj(Ci) j=1 where Nc(C₁) is the number of words that reach at least one category of Ci and, for each word wj in this set, Cwj(C₁) is the number of categories of Ci reached. In figure 2, the average ambiguity is 2 for the set C₁ = {Ci1 C12 C13 C14), and is 5/3=1.66 for Ci = {Ci1 C12 Ci3}. 1 0,9 DP 0,8 0,7 -`-`-.1/A 0,6 0,5 0,4 0,3 0,2 0,1 0 2000 5000 8000 11000 14000 17000 20000 23000 26000 31000 36000 41000 45000 49000 55000 00009 64000 72000 81000 101000 117000 134000 160000 264000 Figure 3 - Performance factors G, CO, DP and 1/A, for each generated set of categories The scoring function for a set of categories C₁ is defined as the linear combination of the performance parameters described above: set Ci. (1) Score (C₁) = AG(C₁ )+ BCO(C )+ XDP(C; )+ 8(1/A(C₁)) Figure 3 plots the values of G, CO, DP and 1/A for the different sets of categories generated by the algorithm of Section 2. Alternative sets of categories are identified by their upperbound³. The figure shows that DP(C₁) has a regular decreasing behaviour, while 1/A(C₁) is less regu- lar. The coverage CO(C₁) has a rather unstable behaviour due to the entangled structure of WordNet. We attempted slight changes in the definitions and computation of CO, DP and 1/A (for example, weighting words with their frequency), but globally, the behaviour remain as those in figure 3. Notice that we assigned a positive effect on the score (modelled by 1/A) to the ability of eliminating certain leaves-synsets and a negative effect (modelled by DP) to the inability of discriminating among certain other leaves-synsets. This is reasonable in general, because our aim is to control overgenerality while reducing overam- biguity. However, nothing can be said on the appropri- ateness of a specific sense aggregation and/or sense elim- ination for a word. It may well be the case that merging two senses in a single category is a reasonable thing to do, if the senses do not draw interesting (for the domain) distinctions. Therefore eliminating a priori a sense of a word may be inappropriate in the domain. The (1) is computed for all the generated sets of categories C₁, and then normalised in the [0,1] interval. The effec- tiveness of this model is estimated in the following sec- tion. 4 Evaluation Experiments and Discus- sion of the Dafa The algorithm was applied to the 10,235 different nouns of the Wall Street Journal (hereafter WSJ) corpus that are classified in WordNet. Categories are generated with h= 0.4 and k=1,000. The cardinality of each set varies, but not uniformly, from 456 categories for UB=2000 (remember that words are frequency-weighted), to 1 cate- gory (i.e. the topmost entity) for UB=264,000. Medium- high level categories (those between 50,000 and 100,000 maximum words) range between 10-20 members for each To compute the score of each set C₁., the parameters α,β,χ and 8 in (1) must be estimated. To perform this task, we adopted a linear interpolation method, using SEMCOR (the semantically tagged Brown Corpus) as a reference corpus. In SEMCOR every word is unambiguously tagged with its leaf-synset. To build a reference scoring function against which to evaluate our model parameters, we proceeded as follows: • Since our categories are generated for an economic domain (WSJ) while SEMCOR is a tagged balanced corpus (the Brown Corpus), we extracted only the fragment of the corpus dealing with economic and fi- nancial texts. We obtained a reference corpus in- cluding 475 of the 1,235 nouns of the WSJ corpus. • For each set of categories C₁ generated by the algo- rithm in section 2, we computed on the reference cor- pus the following two performance figures: Precision: For each C₁, let W(C₁) be the set of words in the reference corpus covered by the set C₁. For each w₁ in 3Remember that words are weighted by their frequency in the corpus. This seems reasonable, but in any case we ob- served that our results do not vary when counting each word only once. [ERROR: Failed to process this page - Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.] 1 0,9 0,8 0,7 0,6 Perf 0,5 Score/ 0,4 α = 1.029 0,3 β = 0.412 0,2 x = 1.075 δ = 1.430 0,1 0 2000 5000 8000 11000 14000 17000 20000 23000 26000 31000 36000 41000 45000 49000 55000 60000 64000 72000 81000 101000 117000 134000 160000 264000 derlining • • • Figure 4b - Reference function and best-fitting Score function, with estimated parameters. The test corpus includes only 475 words of the over 10,000 in our learning corpus. This may well cause a shift of the reference scoring function, as compared with the \"real\" scoring function. In any case, figure 4a shows that the sets C; have peak performances in the range 50,000-100,000. In this range, the precision is around 73-76%, and the reduction of ambiguity is around 35%, which are both valuable results. We also experimented that, by changing slightly the model parameters and/or the definitions of the four performance figures in the (1), in any case the peak performance of the obtained scoring function falls in the 50,000-100,000 interval, and the function stays high around the peak, with local maxima. In other domains (see a brief summary in the conclud- ing remarks) for which we did not have a reference tagged corpus, we used (α=1 β=0,5 x=1 8=1) as model parameters in the (1), and still observed a scoring function similar in shape to that of figure 4b. Se- lected categories vary according to the domains, but the size of the best set stays around the 10-20 cate- gories. Evaluation is of course more problematic due to the absence of a tagged reference corpus. Therefore, we may conclude that the method is \"robust\", in the sense that it correctly identifies a range of reason- able choices for the set of categories to be used, eventually leaving the final choice to a linguist. As for the WSJ corpus, a short analysis of the linguistic data may be useful. In figure 5 the 14 \"best\" selected cate- gories for nouns are listed. Figure 6 shows four very frequent and very ambiguous words in the domain: bank, business, market and stock, with attached list of synsets as generated by WordNet, ordered from left to right by the increasing level of generality (leaf-sysnset leftmost). The senses marked with *\" are those that reach some of the categories (marked in bold in the figure) of the best- performing set, selected by the scoring function (1). For bank and market, we observed that the less plausible (for the domain) senses ridge and market_grocery_store are pruned out. The word stock retains only 5 out of 16 senses. Of these, the gunstock and progenitor senses should have been further dropped out, but there are 11 senses that are correctly pruned, like liquid, caudex, plant, etc. The word business still keeps its ambiguity, but the 9 subtle distinctions of WordNet are reduced to 7 senses. 1. person, individual, someone, 2. 3. 4. mortal, human, soul instrumentality, instrumentation attribute written_communication, written_language message, content, subject_matter, substance measure, quantity, amount, quantum action 5. 6. 7. 8. activity 9. group_action 10. organization 11. 12. 13. state 14. location psychological_feature possession Figure 5 List of best-performing categories Word: bank Sense n.1: bank,side -> slope,incline,side -> ... Sense n.2 (*): depository_financial_institution,bank,banking_concern,banking_company -> finan- cial_institution,financial_organization -> institution,establishment -> organization -> ... Sense n.3: bank -> ridge -> ... Sense n.4: bank -> array -> ... Sense n.5 (*): bank -> reserve,backlog,stockpile -> accumulation -> asset -> possession Sense n.6 (*): bank -> funds,finances,monetary_resource,cash_in_hand,pecuniary_resource -> asset -> possession Sense n.7: bank,camber -> slope,incline,side -> ... Sense n.8 (*): savings_bank,coin_bank,money_box,bank -> container -> instrumentality,instrumentation -> ... Sense n.9: bank,bank_building -> depository,deposit,repository -> ... Word: business Sense n.1 (*): business,concern,business_concern,business_organization -> enterprise -> organization -> ... Sense n.2 (*): commercial_enterprise,business_enterprise,business -> commerce,commercialism,mercantilism -> ... group_action -> ... Sense n.3 (*): occupation,business,line_of_work,line -> activity -> ... Sense n.4 (*): business -> concern,worry,headache,vexation -> negative_stimulus -> stimula- tion,stimulus,stimulant,input -> information -> cognition,knowledge -> psychological_feature Sense n.5 (*): business -> aim,object,objective,target -> goal,end -> content,cognitive_content,mental_object -> cogni- tion,knowledge -> psychological_feature Sense n.6: business,business_sector -> sector -> ... Sense n.7 (*): business -> business_activity,commercial_activity -> activity -> ... Sense n.8: clientele,patronage,business -> people -> ... Sense n.9 (*): business,stage_business,byplay -> acting,playing,playacting,performing -> activity -> ... Word: market Sense n.1: market -> class,social_class,socio-economic_class -> ... Sense n.2: grocery_store,grocery,market -> marketplace,mart -> ... Sense n.3 (*): market,marketplace -> activity -> ... Sense n.4 (*): market,securities_industry -> industry -> commercial_enterprise -> enterprise -> organization -> ... Word: stock Sense n.1 (*): stock -> capital,working_capital -> asset -> possession Sense n.2 (*): stock,gunstock -> support -> device -> instrumentality,instrumentation -> ... Sense n.3: stock,inventory -> merchandise,wares,product -> ... Sense n.4 (*): stock_certificate,stock -> security,certificate -> le- gal_document,legal_instrument,official_document,instrument -> document,written_document,papers -> writ- ing,written_material -> written_communication,written_language -> ... Sense n.5 (*): store,stock,fund -> accumulation -> asset -> possession Sense n.6 (*): stock -> progenitor,primogenitor -> ancestor,ascendant,ascendent,antecedent -> relative,relation -> person,individual,someone,mortal,human,soul -> ... Sense n.7: broth,stock -> soup -> ... Sense n.8: stock,caudex -> stalk,stem -> ... Sense n.9: stock -> plant_part -> ... Sense n.10: stock,gillyflower -> flower -> ... Sense n.11: Malcolm_line,stock,stock -> flower -> ... Sense n.12: lineage,line,line_of_descent,descent,bloodline,blood_line,blood,pedigree,ancestry,origin,parent- age,stock -> genealogy,family_tree -> ... Sense n.13: breed,strain,stock,variety -> animal_group -> ... Sense n.14: stock -> lumber,timber -> ... Sense n.15: stock -> handle,grip,hold -> ... Sense n.16: neckcloth,stock -> cravat -> ... Figure 6. Selected synsets for the words bank, business, market and stock. 5 Concluding Remarks It has already been demonstrated in (Basili et al, 1996) that tagging a corpus with semantic categories triggers a more effective lexical learning. However, overambiguity of on-line thesaura is known as the major obstacle to au- tomatic semantic tagging of corpora. The method pre- sented in this paper allows an efficient and simple selec- tion of a flat set of domain-tuned categories, that dramati- cally reduce the initial overambiguity of the thesaurus. We measured a 73% precision in reducing the initial am- biguity, and a 37% global reduction of ambiguity. Significantly, our method selects a limited number of categories (10-20, depending upon the learning corpus and the model parameters), out of the initial 47,110 leaf- synsets of WordNet4. We remark that our experiment is on large, meaning that we automatically evaluated the performance of the model on a large set of nouns taken from the Wall Street Journal. Most sense disambiguation or semantic tagging methods evaluate their performances manually, against few very ambiguous cases, with clear distinctions among senses. Instead, WordNet draws very subtle and fine- grained distinctions among words. We believe that our results are very encouraging. The model parameters for category selection has been tuned on SEMCOR, but a correctly tagged corpus is not strictly necessary. In our experiments, we applied a scor- ing function similar to that obtained for the Wall Street Journal to two other domains, a corpus of Airline reser- vations and the Unix handbook. We do not discuss the data here for the sake of space. The method constantly selects a set of categories at the medium-high level of generality, different for each domain. The selection \"seems\" good according to our linguistic intuition of the domains, but the absence of a correctly tagged corpus does not allow a large-scale evaluation. In the future, we plan to demonstrate that the method proposed in this paper, besides reducing the overambigu- ity of on-line thesaura, improves the performance of lexi- cal learning methods that are based on semantic tagging, such as PP disambiguation, case frame acquisition and and sense selection, with respect to a non-optimal choice of semantic categories. 6 Acknowledgements The method presented in this paper has been developed within the context of the ECRAN project LE 2110, funded by the European Community. One of the main research objectives of ECRAN is lexical tuning, being semantic tagging and sense disambiguation two important and preliminary objectives. This paper approached the prob- lem of domain-appropriate semantic tagging. We thank Christian Pavoni who developed much of the 4We used WordNet in this experiment, because it is now very popular among scholars in lexical statistics, but clearly our method could be applied to any on-line tax- onomy or lattice. software used in this experiment, as well as all our part- ners in the ECRAN project. References (Agirre and Rigau, 1996) E. Agirre and G. Rigau, Word Sense Disambiguation using Conceptual Density, proc. of COLING 1996 (Basili et al, 1992) Basili, R., Pazienza, M.T., Velardi, P., \"Computational Lexicons: the Neat Examples and the Odd Exemplars\", Proc. of Third Int. Conf. on Applied Natural Language Processing, Trento, Italy, 1-3 April, 1992. (Basili et al, 1996) Basili, R., M.T. Pazienza, P. Velardi, An Empyrical Symbolic Approach to Natural Lan- guage Processing, Artificial Intelligence, August 1996 (Basili et al, 1996b) R. Basili, R., M.T. Pazienza, P. Ve- lardi, Integrating general purpose and corpus-based verb classification, Computational Linguistics, 1996 (Brill and Resnik, 1994) E. Brill and P. Resnik, A trans- formation-based approach to prepositional phrase attachment disambiguation, proc. of COLING 1994 (Chen and Chen, 1996) K. Chen and C. Chen, A rule-based and MT-oriented Approach to Prepositional Phrase Attachment, proc. of COLING 1996 (Dagan et al, 1994)Dagan I., Pereira F., Lee L., Similarly- based Estimation of Word Co-occurrences Probabili- ties, Proc. of ACL, Las Cruces, New Mexico, USA, 1994. (Dolan, 1994) W. Dolan, Word Sense Ambiguation: Clus- tering Related Senses, Proc. of Coling 1994 (Hearst and Schuetze, 1993) M. Hearst and H. Schuetze, Customizing a Lexicon to Better Suite a Computa- tional Task, ACL SIGLEX, Workshop on Lexical Ac- quisition from Text, Columbus, Ohio, USA, 1993. (Krovetz and Croft, 1992) R. Krovetz and B. Croft, Lexi- cal Ambiguity and Information Retrieval, in ACM trans. on Information Systems, 10:2, 1992 (Grishman and Sterling, 1994) R. Grishman, J. Sterling, Generalizing Automatically Generated Selectional Patterns, Proc. of COLING '94, Kyoto, August 1994. (Pereira et al., 1993) Pereira F., N. Tishby, L. Lee, Distri- butional Clustering of English Verbs, Proc. of ACL, Columbus, Ohio, USA, 1993. (Pustejovsky, 1995) J. Pustejovsky, The generative Lexi- con, MIT Press, 1995 (Resnik, 1995) P. Resnik, Disambiguating Noun Group- ings with respect to Wordnet Senses, proc. of 3rd Workshop on Very Large Corpora, 1995 (Yarowsky, 1992) Yarowsky D., Word-Sense disam- biguation using statistical models of Roget's cate- gories trained on large corpora, Proc. of COLING 92, Nantes, July 1992. (Utsuro et al, 1993) T. Utsuro, Y. Matsumoto and M. Na- gao, \"Verbal case frame acquisition from Bilingual Corpora\" Proc. of IJCAI, 1993"
  },
  {
    "title": "Assigning Function Tags to Parsed Text",
    "abstract": "It is generally recognized that the common non-terminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and semantic information one would like about parts of a syntactic tree. For example, the Penn Treebank gives each constituent zero or more 'function tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already parsed to a simple-label level, achieves an F-measure of 87%, which rises to 99% when considering 'no tag' as a valid choice.",
    "content": "1 Introduction Parsing sentences using statistical information gathered from a treebank was first examined a decade ago in (Chitrao and Grishman, 1990) and is by now a fairly well-studied problem ((Charniak, 1997), (Collins, 1997), (Ratna- parkhi, 1997)). But to date, the end product of the parsing process has for the most part been a bracketing with simple constituent labels like NP, VP, or SBAR. The Penn treebank contains a great deal of additional syntactic and seman- tic information from which to gather statistics; reproducing more of this information automat- ically is a goal which has so far been mostly ignored. This paper details a process by which some of this information-the function tags- may be recovered automatically. In the Penn treebank, there are 20 tags (fig- ure 1) that can be appended to constituent la- bels in order to indicate additional information about the syntactic or semantic role of the con- * This research was funded in part by NSF grants LIS- SBR-9720368 and IGERT-9870676. stituent. We have divided them into four cate- gories (given in figure 2) based on those in the bracketing guidelines (Bies et al., 1995). A con- stituent can be tagged with multiple tags, but never with two tags from the same category.¹ In actuality, the case where a constituent has tags from all four categories never happens, but constituents with three tags do occur (rarely). At a high level, we can simply say that hav- ing the function tag information for a given text is useful just because any further information would help. But specifically, there are distinct advantages for each of the various categories. Grammatical tags are useful for any application trying to follow the thread of the text-they find the 'who does what' of each clause, which can be useful to gain information about the situa- tion or to learn more about the behaviour of the words in the sentence. The form/function tags help to find those constituents behaving in ways not conforming to their labelled type, as well as further clarifying the behaviour of ad- verbial phrases. Information retrieval applica- tions specialising in describing events, as with a number of the MUC applications, could greatly benefit from some of these in determining the where-when-why of things. Noting a topicalised constituent could also prove useful to these ap- plications, and it might also help in discourse analysis, or pronoun resolution. Finally, the 'miscellaneous' tags are convenient at various times; particularly the CLR 'closely related' tag, which among other things marks phrasal verbs and prepositional ditransitives. To our knowledge, there has been no attempt so far to recover the function tags in pars- ing treebank text. In fact, we know of only ¹There is a single exception in the corpus: one con- stituent is tagged with -LOC-MNR. This appears to be an error. ADV Non-specific adverbial HLN Headline BNF Benefactive CLF It-cleft PUT Locative complement of 'put' LGS Logical subject SBJ Subject LOC Location TMP Temporal CLR 'Closely related' MNR Manner NOM Nominal PRD Predicate PRP Purpose TPC Topic TTL Title VOC Vocative DIR Direction DTV Dative EXT Extent Figure 1: Penn treebank function tags Grammatical 53.% Form/Function 37.% Topicalisation 2.2% Miscellaneous 9.5% DTV 0.48% 0.25% NOM 6.8% 2.5% TPC 100% 2.2% CLR 94.% 8.8% LGS 3.0% 1.5% ADV 11.% 4.2% CLF 0.34% 0.03% PRD 18.% 9.3% BNF 0.072% 0.026% HLN 2.6% 0.25% PUT 0.26% 0.13% DIR 8.3% 3.0% TTL 3.1% 0.29% SBJ 78.% 41.% EXT 3.2% 1.2% VOC 0.025% 0.013% LOC 25.% 9.2% MNR 6.2% 2.3% PRP 5.2% 1.9% TMP 33.% 12.% Figure 2: Categories of function tags and their relative frequencies one project that used them at all: (Collins, 1997) defines certain constituents as comple- ments based on a combination of label and func- tion tag information. This boolean condition is then used to train an improved parser. 2 Features We have found it useful to define our statisti- cal model in terms of features. A 'feature', in this context, is a boolean-valued function, gen- erally over parse tree nodes and either node la- bels or lexical items. Features can be fairly sim- ple and easily read off the tree (e.g. 'this node's label is X', 'this node's parent's label is Y'), or slightly more complex ('this node's head's part- of-speech is Z'). This is concordant with the us- age in the maximum entropy literature (Berger et al., 1996). When using a number of known features to guess an unknown one, the usual procedure is to calculate the value of each feature, and then essentially look up the empirically most proba- ble value for the feature to be guessed based on those known values. Due to sparse data, some of the features later in the list may need to be ignored; thus the probability of an unknown fea- ture value would be estimated as P(f|f1, f2,..., fn) ≈ P(f|f1, f2,..., fj), j≤n, (1) where P refers to an empirically observed prob- ability. Of course, if features 1 through i only co-occur a few times in the training, this value may not be reliable, so the empirical probability is usually smoothed: P(f|f1, f2,..., fi) ≈ λiP(f|f1, f2,..., fi) + (1-λi)P(f|f1, f2,..., fi-1). (2) The values for λi can then be determined ac- cording to the number of occurrences of features 1 through i together in the training. One way to think about equation 1 (and specifically, the notion that j will depend on the values of f1... fn) is as follows: We begin with the prior probability of f. If we have data indicating P(f|f1), we multiply in that likeli- hood, while dividing out the original prior. If we have data for P(f|f1, f2), we multiply that in while dividing out the P(f|f1) term. This is repeated for each piece of feature data we have; at each point, we are adjusting the probability P(f|f1, f2,..., fn) ≈ P(f) P(f|f1) P(f|f1, f2) P(f) P(f|f1) ... P(f|f1, f2,..., fj) P(f|f1, f2,..., fj-1) , j ≤ n (3) j ≈ II P(f|f1,..., fi-1, fi) i=0 P(f|f1,..., fi-1) we already have estimated. If knowledge about feature fi makes f more likely than with just f1... fi-1, the term where fi is added will be greater than one and the running probability will be adjusted upward. This gives us the new probability shown in equation 3, which is ex- actly equivalent to equation 1 since everything except the last numerator cancels out of the equation. The value of j is chosen such that features f1... f; are sufficiently represented in the training data; sometimes all n features are used, but often that would cause sparse data problems. Smoothing is performed on this equa- tion exactly as before: each term is interpolated between the empirical value and the prior esti- mated probability, according to a value of λi that estimates confidence. But aside from per- haps providing a new way to think about the problem, equation 3 is not particularly useful as it is it is exactly the same as what we had before. Its real usefulness comes, as shown in (Charniak, 1999), when we move from the no- tion of a feature chain to a feature tree. These feature chains don't capture everything we'd like them to. If there are two independent features that are each relatively sparse but occa- sionally carry a lot of information, then putting one before the other in a chain will effectively block the second from having any effect, since its information is (uselessly) conditioned on the first one, whose sparseness will completely di- lute any gain. What we'd really like is to be able to have a feature tree, whereby we can condition those two sparse features independently on one common predecessor feature. As we said be- fore, equation 3 represents, for each feature fi, the probability of f based on fi and all its pre- decessors, divided by the probability of f based only on the predecessors. In the chain case, this means that the denominator is conditioned on every feature from 1 to i-1, but if we use a feature tree, it is conditioned only on those fea- tures along the path to the root of the tree. A notable issue with feature trees as opposed to feature chains is that the terms do not all cancel out. Every leaf on the tree will be repre- target feature a b c Figure 3: A small example feature tree sented in the numerator, and every fork in the tree (from which multiple nodes depend) will be represented at least once in the denomina- tor. For example: in figure 3 we have a small feature tree that has one target feature and four conditioning features. Features b and d are in- dependent of each other, but each depends on a; c depends directly only on b. The unsmoothed version of the corresponding equation would be P(fla, b, c, d) ≈ P(f) P(fa) P(fla, b) P(fla, b, c) P(fla, d) P(f) P(fa) P(fla, b) P(fla) , which, after cancelling of terms and smoothing, results in P(fla, b, c, d) ≈ P(fla, b, c)P(f|a, d) P(f|a) (4) Note that strictly speaking the result is not a probability distribution. It could be made into one with an appropriate normalisation-the so-called partition function in the maximum- entropy literature. However, if the indepen- dence assumptions made in the derivation of equation 4 are good ones, the partition func- tion will be close to 1.0. We assume this to be the case for our feature trees. Now we return the discussion to function tag- ging. There are a number of features that seem succeeding preceding tabel label function tag label parent's labet grandparent's parent's label head's POS head's POS grandparent's head's POS parent's head head alt-head's POS alt-head Figure 4: The feature tree used to guess function tags to condition strongly for one function tag or an- other; we have assembled them into the feature tree shown in figure 4.2 This figure should be relatively self-explanatory, except for the notion of an 'alternate head'; currently, an alternate head is only defined for prepositional phrases, and is the head of the object of the preposi- tional phrase. This data is very important in distinguishing, for example, 'by John' (where John might be a logical subject) from 'by next year' (a temporal modifier) and 'by selling it' (an adverbial indicating manner). 3 Experiment In the training phase of our experiment, we gathered statistics on the occurrence of func- tion tags in sections 2-21 of the Penn treebank. Specifically, for every constituent in the tree- bank, we recorded the presence of its function tags (or lack thereof) along with its condition- ing information. From this we calculated the empirical probabilities of each function tag ref- erenced in section 2 of this paper. Values of A were determined using EM on the development corpus (treebank section 24). To test, then, we simply took the output of our parser on the test corpus (treebank section 23), and applied a postprocessing step to add function tags. For each constituent in the tree, we calculated the likelihood of each function tag according to the feature tree in figure 4, and for each category (see figure 2) we assigned the most likely function tag (which might be the null tag). 2The reader will note that the 'features' listed in the tree are in fact not boolean-valued; each node in the given tree can be assumed to stand for a chain of boolean features, one per potential value at that node, exactly one of which will be true. 4 Evaluation To evaluate our results, we first need to deter- mine what is 'correct'. The definition we chose is to call a constituent correct if there exists in the correct parse a constituent with the same start and end points, label, and function tag (or lack thereof). Since we treated each of the four function tag categories as a separate fea- ture for the purpose of tagging, evaluation was also done on a per-category basis. The denominator of the accuracy measure should be the maximum possible number we could get correct. In this case, that means excluding those constituents that were already wrong in the parser output; the parser we used attains 89% labelled precision-recall, so roughly 11% of the constituents are excluded from the function tag accuracy evaluation. (For refer- ence, we have also included the performance of our function tagger directly on treebank parses; the slight gain that resulted is discussed below.) Another consideration is whether to count non-tagged constituents in our evaluation. On the one hand, we could count as correct any constituent with the correct tag as well as any correctly non-tagged constituent, and use as our denominator the number of all correctly- labelled constituents. (We will henceforth refer to this as the 'with-null' measure.) On the other hand, we could just count constituents with the correct tag, and use as our denominators the total number of tagged, correctly-labelled con- stituents. We believe the latter number ('no- null') to be a better performance metric, as it is not overwhelmed by the large number of un- tagged constituents. Both are reported below. Table 1: Baseline performance Category Baseline 1 Baseline 2 (always choose most likely tag) (never tag) Tag Precision Recall F-measure Grammatical 86.935% SBJ 10.534% 80.626% 18.633% Form/Function 91.786% TMP 3.105% 37.795% 5.738% Topicalisation 99.406% TPC 0.594% 100.00% 1.181% Miscellaneous 98.436% CLR 1.317% 84.211% 2.594% Overall 94.141% 3.887% 66.345% 7.344% Table 2: Performance within each category With-null -No-null- Category Accuracy Precision Recall F-measure Grammatical 98.909% 95.472% 95.837% 95.654% Form/Function 97.104% 80.415% 77.595% 78.980% Topicalisation 99.915% 92.195% 93.564% 92.875% Miscellaneous 98.645% 55.644% 65.789% 60.293% 5 Results 5.1 Baselines There are, it seems, two reasonable baselines for this and future work. First of all, most con- stituents in the corpus have no tags at all, so obviously one baseline is to simply guess no tag for any constituent. Even for the most com- mon type of function tag (grammatical), this method performs with 87% accuracy. Thus the with-null accuracy of a function tagger needs to be very high to be significant here. The second baseline might be useful in ex- amining the no-null accuracy values (particu- larly the recall): always guess the most common tag in a category. This means that every con- stituent gets labelled with '-SBJ-TMP-TPC-CLR' (meaning that it is a topicalised temporal sub- ject that is 'closely related' to its verb). This combination of tags is in fact entirely illegal by the treebank guidelines, but performs ad- equately for a baseline. The precision is, of course, abysmal, for the same reasons the first baseline did so well; but the recall is (as one might expect) substantial. The performances of the two baseline measures are given in Table 1. 5.2 Performance in individual categories In table 2, we give the results for each category. The first column is the with-null accuracy, and the precision and recall values given are the no- null accuracy, as noted in section 4. Grammatical tagging performs the best of the four categories. Even using the more difficult no-null accuracy measure, it has a 96% accu- racy. This seems to reflect the fact that gram- matical relations can often be guessed based on constituent labels, parts of speech, and high- frequency lexical items, largely avoiding sparse- data problems. Topicalisation can similarly be guessed largely on high-frequency information, and performed almost as well (93%). On the other hand, we have the form/function tags and the 'miscellaneous' tags. These are characterised by much more semantic information, and the relationships between lexical items are very important, making sparse data a real problem. All the same, it should be noted that the performance is still far better than the baselines. 5.3 Performance with other feature trees The feature tree given in figure 4 is by no means the only feature tree we could have used. In- Table 3: Overall performance on different inputs With-null -No-null- Category Accuracy Precision Recall F-measure Parsed 98.643% 87.173% 87.381% 87.277% Treebank 98.805% 88.450% 88.493% 88.472% deed, we tried a number of different trees on the development corpus; this tree gave among the best overall results, with no category perform- ing too badly. However, there is no reason to use only one feature tree for all four categories; the best results can be got by using a separate tree for each one. One can thus achieve slight (one to three point) gains in each category. 5.4 Overall performance The overall performance, given in table 3, ap- pears promising. With a tagging accuracy of about 87%, various information retrieval and knowledge base applications can reasonably ex- pect to extract useful information. The performance given in the first row is (like all previously given performance values) the function-tagger's performance on the correctly- labelled constituents output by our parser. For comparison, we also give its performance when run directly on the original treebank parse; since the parser's accuracy is about 89%, working di- rectly with the treebank means our statistics are over roughly 12% more constituents. This second version does slightly better. The main reason that tagging does worse on the parsed version is that although the con- stituent itself may be correctly bracketed and la- belled, its exterior conditioning information can still be incorrect. An example of this that ac- tually occurred in the development corpus (sec- tion 24 of the treebank) is the 'that' clause in the phrase 'can swallow the premise that the re- wards for such ineptitude are six-figure salaries', correctly diagrammed in figure 5. The function tagger gave this SBAR an ADV tag, indicating an unspecified adverbial function. This seems ex- tremely odd, given that its conditioning infor- mation (nodes circled in the figure) clearly show that it is part of an NP, and hence probably mod- ifies the preceding NN. Indeed, the statistics give the probability of an ADV tag in this condition- ing environment as vanishingly small. VP MD VP can VB NP swallow DT NN SBAR the premise IN S that MD Figure 5: SBAR and conditioning info can VB SBAR VP VP NP DT NN IN S the premise that swallow Figure 6: SBAR and conditioning info, as parsed However, this was not the conditioning infor- mation that the tagger received. The parser had instead decided on the (incorrect) parse in figure 6. As such, the tagger's decision makes much more sense, since an SBAR under two VPS whose heads are VB and MD is rather likely to be an ADV. (For instance, the 'although' clause of the sentence 'he can help, although he doesn't want to.' has exactly the conditioning environ- ment given in figure 6, except that its prede- cessor is a comma; and this SBAR would be cor- rectly tagged ADV.) The SBAR itself is correctly bracketed and labelled, so it still gets counted in the statistics. Happily, this sort of case seems to be relatively rare. Another thing that lowers the overall perfor- mance somewhat is the existence of error and in- consistency in the treebank tagging. Some tags seem to have been relatively easy for the human treebank taggers, and have few errors. Other tags have explicit caveats that, however well- justified, proved difficult to remember for the taggers-for instance, there are 37 instances of a PP being tagged with LGS (logical subject) in spite of the guidelines specifically saying, '[LGS] attaches to the NP object of by and not to the PP node itself.' (Bies et al., 1995) Each mistag- ging in the test corpus can cause up to two spu- rious errors, one in precision and one in recall. Still another source of difficulty comes when the guidelines are vague or silent on a specific issue. To return to logical subjects, it is clear that 'the loss' is a logical subject in 'The company was hurt by the loss', but what about in 'The com- pany was unperturbed by the loss'? In addition, a number of the function tags are authorised for 'metaphorical use', but what exactly constitutes such a use is somewhat inconsistently marked. It is as yet unclear just to what degree these tagging errors in the corpus are affecting our results. 6 Conclusion This work presents a method for assigning func- tion tags to text that has been parsed to the simple label level. Because of the lack of prior research on this task, we are unable to com- pare our results to those of other researchers; but the results do seem promising. However, a great deal of future work immediately suggests itself: • Although we tested twenty or so feature trees besides the one given in figure 4, the space of possible trees is still rather un- explored. A more systematic investiga- tion into the advantages of different feature trees would be useful. • We could add to the feature tree the val- ues of other categories of function tag, or the function tags of various tree-relatives (parent, sibling). • One of the weaknesses of the lexical fea- tures is sparse data; whereas the part of speech is too coarse to distinguish 'by John' (LGS) from 'by Monday' (TMP), the lexi- cal information may be too sparse. This could be assisted by clustering the lexical items into useful categories (names, dates, etc.), and adding those categories as an ad- ditional feature type. • There is no reason to think that this work could not be integrated directly into the parsing process, particularly if one's parser is already geared partially or entirely to- wards feature-based statistics; the func- tion tag information could prove quite use- ful within the parse itself, to rank several parses to find the most plausible. References Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural lan- guage processing. Computational Linguistics, 22(1):39-71. Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre, 1995. Bracketing Guide- lines for Treebank II Style Penn Treebank Project, January. Eugene Charniak. 1997. Statistical pars- ing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelli- gence, pages 598-603, Menlo Park. AAAI Press/MIT Press. Eugene Charniak. 1999. A maximum-entropy- inspired parser. Technical Report CS-99-12, Brown University, August. Mahesh V. Chitrao and Ralph Grishman. 1990. Statistical parsing of messages. In DARPA Speech and Language Workshop, pages 263- 266. Michael Collins. 1997. Three generative, lexi- calised models for statistical parsing. In Pro- ceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16-23. Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum en- tropy models. In Proceedings of the Second Annual Conference on Empirical Methods in Natural Language Processing, pages 1-10."
  },
  {
    "title": "An Information Extraction Core System for Real World German Text Processing",
    "abstract": "This paper describes SMES, an information extraction core system for real world German text processing. The basic design criterion of the system is to provide a set of basic powerful, robust, and efficient natural language components and generic linguistic knowledge sources which can easily be customized for processing different tasks in a flexible manner.",
    "content": "1 Introduction There is no doubt that the amount of textual infor- mation electronically available today has passed its critical mass leading to the emerging problem that the more electronic text data is available the more difficult it is to find or extract relevant information. In order to overcome this problem new technologies for future information management systems are ex- plored by various researchers. One new line of such research is the investigation and development of in- formation extraction (IE) systems. The goal of IE is to build systems that find and link relevant infor- mation from text data while ignoring extraneous and irrelevant information (Cowie and Lehnert, 1996). Current IE systems are to be quite successfully in automatically processing large text collections with high speed and robustness (see (Sundheim, 1995), (Chinchor et al., 1993), and (Grishman and Sund- heim, 1996)). This is due to the fact that they can provide a partial understanding of specific types of text with a certain degree of partial accuracy using fast and robust shallow processing strategies (basi- cally finite state technology). They have been \"made sensitive\" to certain key pieces of information and *DFKI GmbH. Stuhlsatzenhausweg 3, 66123 Saarbrücken, Germany, neumann@dfki.uni-sb.de +LMU, Oettingenstrasse 67, 80538 München, Ger- many, backofen@informatik.uni-muenchen.de ‡DFKI GmbH, baur@dfki.uni-sb.de §DFKI GmbH, mbecker@dfki.uni-sb.de ∥DFKI GmbH, chbraun@dfki.uni-sb.de thereby provide an easy means to skip text without deep analysis. The majority of existing information systems are applied to English text. A major drawback of previ- ous systems was their restrictive degree of portabil- ity towards new domains and tasks which was also caused by a restricted degree of re-usability of the knowledge sources. Consequently, the major goals which were identified during the sixth message un- derstanding conference (MUC-6) were, on the one hand, to demonstrate task-independent component technologies of information extraction, and, on the other hand, to encourage work on increasing porta- bility and \"deeper understanding\" (cf. (Grishman and Sundheim, 1996)). In this paper we report on SMES an information extraction core system for real world German text processing. The main research topics we are con- cerned with include easy portability and adaptabil- ity of the core system to extraction tasks of differ- ent complexity and domains. In this paper we will concentrate on the technical and implementational aspects of the IE core technology used for achieving the desired portability. We will only briefly describe some of the current applications built on top of this core machinery (see section 7). 2 The overall architecture of SMES The basic design criterion of the SMES system is to provide a set of basic powerful, robust, and efficient natural language components and generic linguistic knowledge sources which can easily be customized for processing different tasks in a flexible manner. Hence, we view SMES as a core information extrac- tion system. Customization is achieved in the fol- lowing directions: • defining the flow of control between modules (e.g., cascaded and/or interleaved) • selection of the linguistic knowledge sources • specifying domain specific knowledge • defining task-specific additional functionality K-Base ASCII Text Tokenizer Morph./Lex. Processing Fragment Processing Fragment comb. Template gen. Templates Marked-up Text HTML Interface Figure 1: A blueprint of the core system Netscape Browser Figure 1 shows a blueprint of the core system (which roughly follows the design criteria of the generic information extraction system described in (Hobbs, 1992)). The main components are: A tokenizer based on regular expressions: it scans an ASCII text file for recognizing text structure, spe- cial tokens like date and time expressions, abbrevi- ations and words. A very efficient and robust German morphological component which performs morphological inflection and compound processing. For each analyzed word it returns a (set of) triple containing the stem (or a list of stems in case of a compound), the part of speech, and inflectional information. Disambigua- tion of the morphological output is performed by a set of word-case sensitive rules, and a Brill-based unsupervised tagger. A declarative specification tool for expressing fi- nite state grammars for handling word groups and phrasal entities (e.g., general NPs, PPs, or verb groups, complex time and date expressions, proper name expressions). A finite state grammar con- sists of a set of fragment extraction patterns defined as finite state transducers (FST), where modular- ity is achieved through a generic input/output de- vice. FST are compiled to Lisp functions using an extended version of the compiler defined in (Krieger, 1987). A bidirectional lexical-driven shallow parser for the combination of extracted fragments. Shal- low parsing is basically directed through frag- ment combination patterns FCP of the form (FSTleft, anchor, FSTright), where anchor is a lex- ical entry (e.g., a verb like \"to meet\") or a name of a class of lexical entries (e.g., \"transitive-verb\"). FCPs are attached to lexical entries (e.g., verbs), and are selected right after a corresponding lexical entry has been identified. They are applied to their left and right stream of tokens of recognized fragments. The fragment combiner is used for recognizing and extracting clause level expressions, as well as for the instantiation of templates. An interface to TDL, a type description language for constraint-based grammars (Krieger and Schäfer, 1994). TDL is used in SMES for performing type- driven lexical retrieval, e.g., for concept-driven fil- tering, and for the evaluation of syntactic agreement tests during fragment processing and combination. The knowledge base is the collection of differ- ent knowledge sources, viz. lexicon, subgram- mars, clause-level expressions, and template pat- terns. Currently it includes 120.000 lexical root en- tries, subgrammars for simple and complex date and time expressions, person names, company names, currency expressions, as well as shallow grammars for general nominal phrases, prepositional phrases, and general verb-modifier expressions. Additionally to the above mentioned components there also exists a generic graphical editor for text items and an HTML interface to the Netscape browser which performs marking of the relevant text parts by providing typed parentheses which also serve as links to the internal representation of the extracted information. There are two important properties of the system for supporting portability: • Each component outputs the resulting struc- tures uniformly as feature value structures, to- gether with its type and the corresponding start and end positions of the spanned input expres- sions. We call these output structures text items. • All (un-filtered) resulting structures of each component are cached so that a component can take into account results of all previous compo- nents. This allows for the definition of cascaded as well as interleaved flow of control. The for- mer case means that it is possible to apply a cascade of finite state expressions (comparable to that proposed in (Appelt et al., 1993)), and the latter supports the definition of finite state expressions which incrementally perform a mix of keyword spotting, fragment processing, and template instantiation.¹ Of course, it is also possible—and usually the case in our current applications—to combine both sorts of The system has already successfully been ap- plied to classifying event announcements made via email, scheduling of meetings also sent via email, and extraction of company information from on-line newswires (see 7 for more details). In the next sec- tion, we are describing some of the components' properties in more detail. 3 Word level processing Text scanning Each file is firstly preprocessed by the text scanner. Applying regular expressions (the text scanner is implemented in lex, the well-known Unix tool), the text scanner identifies some text structure (e.g., paragraphs, indentations), word, number, date and time tokens (e.g, \"1.3.96\", \"12:00 h\"), and expands abbreviations. The output of the text scanner is a stream of tokens, where each word is simply represented as a string of alphabetic char- acters (including delimiters, e.g. \"Daimler-Benz\"). Number, date and time expressions are normalized and represented as attribute values structures. For example the character stream \"1.3.96\" is represented as (:date ((:day 1)(:mon 3) (:year 96)), and \"13:15 h\" as (:time ((:hour 13)(:min 15))). Morphological processing follows text scanning and performs inflection, and processing of com- pounds. The capability of efficiently processing com- pounds is crucial since compounding is a very pro- ductive process of the German language. The morphological component called MONA is a descendant of MORPHIX, a fast classification-based morphology component for German (Finkler and Neumann, 1988). MONA improves MORPHIX in that the classification-based approach has been combined with the well-known two-level approach, originally developed by (Koskenniemi, 1983). Actually, the extensions concern • the use of tries (see (Aho et al., 1983)) as the sole storage device for all sorts of lexical infor- mation in MONA (e.g., for lexical entries, prefix, inflectional endings), and • the analysis of compound expressions which is realized by means of a recursive trie traversal. During traversal two-level rules are applied for recognizing linguistically well-formed decompo- sitions of the word form in question. The output of MONA is the word form together with all its readings. A reading is a triple of the form (stem, in flection, pos), where stem is a string or a list of strings (in the case of compounds), in flection is the inflectional information, and pos is the part of speech. Currently, MONA is used for the German and Ital- ian language. The German version has a very broad control flow. coverage (a lexicon of more then 120.000 stem en- tries), and an excellent speed (5000 words/sec with- out compound handling, 2800 words/sec with com- pound processing (where for each compound all lex- ically possible decompositions are computed).2 Part-of-speech disambiguation Morphological ambiguous readings are disambiguated wrt. part- of-speech using case-sensitive rules and filtering rules which have been determined using Brill's un- supervised tagger (Brill, 1995). The filtering rules are also used for tagging unknown words. The filtering rules are determined on the basis of unannotated corpora. Starting from untagged corpora, MONA is used for initial tagging, where unknown words are ambiguously tagged as noun, verb, and adjective. Then, using contextual informa- tion from unambiguously analysed word forms, filter rules are determined which are of the form change tag of word form from noun or verb to noun if the previous word is a determiner. First experiments using a training set of 100.000 words and a set of about 280 learned filter rules yields a tagging accuracy (including tagging of un- known words) of 91.4%.4 Note that the un-supervised tagger required no hand-tagged corpora and considered unknown words. We expect to increase the accuracy by im- proving the un-supervised tagger through the use of more linguistic information determined by MONA especially for the case of unknowns words. 4 Fragment processing Word group recognition and extraction is performed through fragment extraction patterns which are ex- pressed as finite state transducers (FST) and which are compiled to Lisp functions using a compiler based on (Krieger, 1987). An FST consists of a unique name, the recognition part, the output de- scription, and a set of compiler parameters. The recognition part An FST operates on a stream of tokens. The recognition part of an FST is used for describing regular patterns over such token 2Measurement has been performed on a Sun 20 using an on-line lexicon of 120.000 entries. 3Generally, only nouns (and proper names) are writ- ten in standard German with an capitalized initial letter (e.g., \"der Wagen\" the car vs. \"wir wagen\" we venture). Since typing errors are relatively rare in press releases (or similar documents) the application of case-sensitive rules are a reliable and straightforward tagging means for the German language. 4Brill reports a 96% accuracy using a training set of 350.000 words and 1729 rules. However, he does not handle unknown words. In (Aone and Hausman, 1996), an extended version of Brill's tagger is used for tagging Spanish texts, which includes unknown words. They re- port an accuracy of 92.1% streams. For supporting modularity the different possible kind of tokens are handled via basic edges, where a basic edge can be viewed as a predicate for a specific class of tokens. More precisely a basic edge is a tuple of the form (name, test, variable), where name is the name of the edge, test is a predicate, and variable holds the current token Te, if test applied on To holds. For example the following basic edge (:mona-cat \"partikel\" pre) tests whether To produced by MONA is a particle, and if so binds the token to the variable pre (more precisely, each variable of a basic edge denotes a stack, so that the current token is actually pushed onto the stack). We assume that for each component of the sys- tem for which fragment extraction patterns are to be defined, a set of basic edges exists. Furthermore, we assume that such a set of basic edges remains fix at some point in the development of the system and thus can be re-used as pre-specified basic building blocks to a grammar writer Using basic edges the recognition part of an FST is then defined as a regular expression using a func- tional notation. For example the recognition part for simple nominal phrases might be defined as follows: (:conc (:starn (:mona-cat \"det\" det) 1) (:star (:mona-cat \"adj\" adj)) (:mona-cat \"n\" noun)) Thus defined, a nominal phrase is the concatena- tion of one optional determiner (expressed by the loop operator :star≤n, where n starts from 0 and ends by 1), followed by zero or more adjectives fol- lowed by a noun. Output description part The output structure of an FST is constructed by collecting together the variables of the recognition part's basic edges fol- lowed by some specific construction handlers. In or- der to support re-usability of FST to other applica- tions, it is important to separate the construction handlers from the FST definition. Therefore, the output description part is realized through a func- tion called BUILD-ITEM which receives as input the edge variables and a symbol denoting the class of the FST. For example, if np is used as a type name for nominal phrases then the output description of the above NP-recognition part is (build-item :type :np :out (list det adj noun)). The function BUILD-ITEM then discriminates ac- cording to the specified type and constructs the de- sired output to some pre-defined requests (note, that in the above case the variables DET and ADJ might have received no token. In that case their default value NIL is used as an indication of this fact). Using this mechanism it is possible to define or re-define the output structure without changing the whole FST. Special edges There exist some special ba- sic edges namely (:var var), (:current-pos pos) and (:seek name var). The edge (:var var) is used for simply skipping or consuming a token without any checks. The edge :current-pos is used for storing the position of the current token in the variable pos, and the edge :seek is used for calling the FST named name, where var is used as a storage for the output of name. This is similar to the seek edge known from Augmented Transition Networks with the no- tably distinction that in our system recursive calls are disallowed. Thus :seek can also be seen as a macro expanding operator. The :seek mechanism is very useful in defining modular grammars, since it allows for a hierarchical definition of finite state grammars, from general to specific constructions (or vice versa). The following example demonstrates the use of these special edges: (compile-regexp (:conc (:current-pos start) (:alt (:seek time-phase time) (:conc (:star≤n (:seek time-expr-vorfield vorfield) 1) (:seek mona-time time))) (:current-pos end)) :name time-expr :output-desc (build-item :type time-expr :start start :end end :out (list vorfield time)))) This FST recognizes expressions like \"spätestens um 14:00 h\" (by two o'clock at the latest) with the output description ((:out (:time-rel \"spaet\") (:time- prep \"um\") (:minute 0) (:hour 14)) (:end 4) (:start 0) (:type time-expr)) Interface to TDL The interface to TDL, a typed feature-based language and inference system is also realized through basic edges. TDL allows the user to define hierarchically-ordered types consisting of type constraints and feature constraints, and has been originally developed for supporting high-level competence grammar development. In SMES we are using TDL for two purposes: 1. defining domain-specific type lattices 2. expressing syntactic agreement constraints The first knowledge is used for performing concept-based lexical retrieval (e.g., for extracting word forms which are compatible to a given super- type, or for filtering out lexical readings which are incompatible wrt. a given type), and the second knowledge is used for directing fragment process- ing and combination, e.g., for filtering out certain un-grammatical phrases or for extracting phrases of certain syntactic type. The integration of TDL and finite state expres- sions is easily achieved through the definition of ba- sic edges. For example the edge (:mona-cat-type (:and \"n\" \"device\") var) will accept a word form which has been analyzed as a noun and whose lexical entry type identifier is subsumed by \"device\". As an example of defining agreement test consider the basic edge (:mona-cat-unify \"det\" \"[(num %1) (case %2 = gen-val) (gender %3)]\" agr det) which checks whether the current token is a deter- miner and whether its inflection information (com- puted by MONA) unifies with the specified con- straints (here, it is checked whether the determiner has a genitive reading, where structure sharing is expressed through variables like %1). If so, agr is bound to the result of the unifier and token is bound to det. If in the same FST a similar edge for noun tokens follows which also makes reference to the vari- able agr, the new value for agr is checked with its old value. In this way, agreement information is propa- gated through the whole FST. An important advantage of using TDL in this way is that it supports the specification of very compact and modular finite expressions. However, one might argue that using TDL in this way could have dra- matic effects on the efficiency of the whole system, if the whole power of TDL would be used. In some sense this is true. However, in our current system we only allow the use of type subsumption which is performed by TDL very efficiently, and constraints used very carefully and restrictively. Furthermore, the TDL interface opens up the possibility of inte- grating deeper processing components very straight- forwardly. Control parameters In order to obtain flexible control mechanisms for the matching phase it is pos- sible to specify whether an exact match is requested or whether an FST should already succeed when the recognition part matches a prefix of the input string (or suffix, respectively). The prefix matching mech- anism is used in conjunction with the Kleene :star and the identity edge :var, to allow for searching the whole input stream for extracting all matching expressions of an FST (e.g., extracting all NP's, or time expressions). For example the following FST extracts all genitive NPs found in the input stream and collects them in a list: (compile-regexp (:star (:alt (:seek gen-phrase x) (:var dummy))) :output-desc (build-item :type list :out x ) :prefix T :suffix NIL :name gen-star) Additionally, a boolean parameter can be used to specify whether longest or shortest matches should be prefered (the default is longest match, see also (Appelt et al., 1993) where also longest subsuming phrases are prefered). 5 Fragment combination and template generation Bidirectional shallow parsing The combina- tion of extracted fragments is performed by a lexical- driven bidirectional shallow parser which operates on fragment combination patterns FCP which are attached to lexical entries (mainly verbs). We call these lexical entries anchors. The input stream for the shallow parser consists of a double-linked list of all extracted fragments found in some input text, all punctuation tokens and text tokens (like newline or paragraph) and all found an- chors (i.e., all other tokens of the input text are ig- nored). The shallow parser then applies for each anchor its associated FCP. An anchor can be viewed as splitting the input stream into a left and right in- put part. Application of an FCP then starts directly from the input position of the anchor and searches the left and right input parts for candidate frag- ments. Searching stops either if the beginning or the end of a text has been reached or if some punc- tuation, text tokens or other anchors defined as stop markers have been recognized. General form of fragment combination pat- terns A FCP consists of a unique name, an recog- nition part applied on the left input part and one for the right input part, an output description part and a set of constraints on the type and number of col- lected fragments. As an prototypical case, consider the following FCP defined for intransitive verbs like to come or to begin: (compile-anchored-regexp ((:set (cdr (assoc :start ?*)) anchor-pos) (:set ((:np (1 1) (nom-val (1 1)))) nec) (:set ((:tmp (0 2))) opt)) ((:dl-list-left (:star (:alt (:ignore-token (\",\" \";\")) (:ignore-fragment :type (:time-phase :pp)) (:add-nec (:np :name-np) :np nec Icompl) (:add-opt (:time-expr:date-expr) (:dl-list-right (:star (:alt :tmp opt Icompl)))) (:ignore-token (\",\" \";\")) (:add-nec (:np) :np nec rcompl) (:add-opt (:time-expr:date-expr) :name intrans :tmp opt rcompl))))) :output-desc (build-item :type :intrans :out (list anchor-pos Icompl rcompl))) The first list remembers the position of the active anchor and introduces two sets of constraints, which are used to define restrictions on the type and num- ber of necessary and optional fragments, e.g., the first constraint says that exactly one :np fragment (expressed by the lower and upper bound in (1 1)) in nominative case must be collected, where the sec- ond constraint says that at most two optional frag- ments of type :tmp can be collected. The two con- straints are maintained by the basic edges :add-nec and :add-opt. :add-nec performs as follows. If the current token is a fragment of type :np or :name-np then inspect the set named nec and select the con- straint set typed :np. If the current token agrees in case (which is tested by type subsumption) then push it to lcompl and reduce the upper bound by 1. Since next time the upper bound is 0 no more fragments will be considered for the set nec. In a similar manner :add-opt is processed. The edges :ignore-token and :ignore-fragment are used to explicitly specify what sort of tokens will not be considered by :add-nec or :add-opt. In other words this means, that each token which is not men- tioned in the FCP will stop the application of the FCP on the current input part (left or right). Complex verb constructions In our current system, FCPs are attached to main verb entries. Expressions which contain modal, auxiliary verbs or separated verb prefixes are handled by lexical rules which are applied after fragment processing and before shallow processing. Although this mech- anism turned out to be practical enough for our current applications, we have defined also complex verb group fragments, VGF. A VGF is applied after fragment processing took place. It collects all verb forms used in a sentence, and returns the underlying dependency-based structure. Such an VGF is then used as a complex anchor for the selection of appro- priate fragment combination patterns as described above. The advantage of verb group fragments is that they help to handle more complex construc- tions (e.g., time or speech act) in a more systematic (but still shallow) way. Template generation An FCP expresses restric- tions on the set of candidate fragments to be col- lected by the anchor. If successful the set of found fragments together with the anchor builds up an in- stantiated template or frame. In general a template is a record-like structure consisting of features and their values, where each collected fragment and the $^5$In some sense this mechanism behaves like the sub- categorization principle employed in constraint-based lexical grammars. anchor builds up a feature/value pair. An FCP also defines which sort of fragments are necessary or op- tional for building up the whole template. FCPs are used for defining linguistically oriented general head- modifier construction (linguistically based on depen- dency theory) and application-specific database en- tries. The \"shallowness\" of the template construc- tion/instantiation process depends on the weakness of the defined FST of an FCP. A major drawback of our current approach is that necessary and optional constraints are defined to- gether in one FCP. For example, if an FCP is used for defining generic clause expressions, where com- plements are defined through necessary constraints and adjuncts through optional constraints then it has been shown that the constraints on the adjuncts can change for different applications. Thus we ac- tually lack some modularity concerning this issue. A better solution would be to attach optional con- straints directly with lexical entries and to \"splice\" them into an FCP after its selection. 6 Coverage of knowledge sources The lexicon in use contains more than 120.000 stem entries (concerning morpho-syntactic information). The time and date subgrammar covers a wide range of expressions including nominal, preposi- tional, and coordinated expressions, as well as com- bined date-time expressions (e.g., \"vom 19. (8.00 h) bis einschl. 21. Oktober (18.00 h)\" yields: (:pp (from :np (day. 19) (hour. 8) (minute. 0)) (to :np (day. 21) (month. 10) (hour. 18) (minute. 0)))) The NP/PP subgrammars cover e.g., coordinate NPs, different forms of adjective constructions, gen- itive expressions, pronouns. The output struc- tures reflects the underlying head-modifier relations (e.g., \" Die neuartige und vielfältige Gesellschaft \" yields: (((:sem (:head \"gesellschaft\") (:mods \"neuar- tig\" \"vielfaeltig\") (:quantifier \"d-det\")) (:agr nom-acc- val) (:end. 6) (:start. 1) (:type. :np))) 30 generic syntactic verb subcategorization frames are defined by fragment combination patterns (e.g, for transitive verb frame). Currently, these verb frames are handled by the shallow parser with no or- dering restriction, which is reasonably because Ger- man is a language with relative free word order. However, in future work we will investigate the in- tegration of shallow linear precedence constraints. The specification of the current data has been performed on a tagged corpora of about 250 texts (ranging in size from a third to one page) which are about event announcement, appointment scheduling and business news following a bottom-up grammar development approach. 7 Current applications On top of SMES three application systems have been implemented: 1. appointment scheduling via email: extraction of co-operate act, duration, range, appointment, sender, receiver, topic 2. classification of event announcements sent via email: extraction of speaker, title, time, and location 3. extraction of company information from news- paper articles: company name, date, turnover, revenue, quality, difference For these applications the main architecture (as described above), the scanner, morphology, the set of basic edges, the subgrammars for time/date and phrasal expressions could be used basically un- changed. In (1) SMES is embedded in the COSMA system, a German language server for existing appointment scheduling agent systems (see (Busemann et al., 1997), this volume, for more information). In case (2) additional FST for the text structure have been added, since the text structure is an important source for the location of relevant information. How- ever, since the form of event announcements is usu- ally not standardized, shallow NLP mechanisms are necessary. Hence, the main strategy realized is a mix of text structure recognition and restricted shallow analysis. For application (3), new subgrammars for company names and currency expressions have to be defined, as well as a task-specific reference resolution method. Processing is very robust and fast (between 1 and 10 CPU seconds (Sun UltraSparc) depending on the size of the text which ranges from very short texts (a few sentences) upto short texts (one page)). In all of the three applications we obtained high coverage and good results. Because of the lack of compara- ble existing IE systems defined for handling German texts in similar domains and the lack of evaluation standards for the German language (comparable to that of MUC), we cannot claim that these results are comparable. However, we have now started the implementa- tion of a new application together with a commer- cial partner, where a more systematic evaluation of the system is carried out. Here, SMES is applied on a quite different domain, namely news items concern- ing the German IFOR mission in former Yugoslavia. Our task is to identify those messages which are about violations of the peace treaty and to extract the information about location, aggressor, defender and victims. The corpus consists of a set of monthly reports (Jan. 1996 to Aug. 1996) each consisting of about 25 messages from which 2 to 8 messages are about fighting actions. These messages have been hand- tagged with respect to the relevant information. Al- though we are still in the development phase we will briefly describe our experience of adapting SMES to this new domain. Starting from the assumption that the core machinery can be used un-changed we first measured the coverage of the existing linguistic knowledge sources. Concerning the above mentioned corpus the lexicon covers about 90%. However, from the 10% of unrecognized words about 70% are proper names (which we will handle without a lexicon) and 1.5% are spelling errors, so that the lexicon actually covers more then 95% of this unseen text corpus. The same \"blind\" test was also carried out for the date, time, and location subgrammar, i.e., they have been run on the new corpus without any adaption to the specific domain knowledge. For the date-/time expressions we obtained a recall of 77% and a pre- cision of 88%, and for the location expressions we obtained 66% and 87%, respectively. In the latter case, most of the unrecognized expressions concern expressions like \"nach Taszar/Ungarn\", \"im serbis- chen bzw. kroatischen Teil Bosniens\", or \"in der Moslemisch-kroatischen Föderation\". For the gen- eral NP and PP subgrammars we obtained a recall of 55% and a precision of 60% (concerning correct head-modifier structure). The small recall is due to some lexical gap (including proper names) and un- foreseen complex expressions like \"die Mehrzahl der auf 140.000 geschätzten moslemischen Flüchtlinge\". But note that these grammars have been written on the basis of different corpora. In order to measure the coverage of the fragment combination patterns FCP, the relevant main verbs of the tagged corpora have been associated with the corresponding FCP (e.g., the FCP for transi- tive verbs), without changing the original definition of the FCPs. The only major change to be done concerned the extension of the output description function BUILD-ITEM for building up the new tem- plate structure. After a first trial run we obtained an unsatisfactory recognition rate of about 25%. One major problem we identified was the frequent use of passive constructions which the shallow parser was not able to process. Consequently, as a first actual extension of SMES to the new domain we extended the shallow parser to cope with passive construc- tions. Using this extension we obtained an recogni- tion of about 40% after a new trial run. After the analysis of the (partially) unrecognized messages (including the misclassified ones), we iden- tified the following major bottlenecks of our current system. First, many of the partially recognized tem- plates are part of coordinations (including enumera- tions), in which case several (local) templates share the same slot, however this slot is only mentioned one time. Resolving this kind of \"slot sharing\" re- quires processing of elliptic expressions of different kinds as well as the need of domain-specific inference rules which we have not yet foreseen as part of the core system. Second, the wrong recognition of mes- sages is often due to the lack of semantic constraints which would be applied during shallow parsing in a similar way as the subcategorization constraints. Although these current results should and can be improved we are convinced that the idea of develop- ing a core IE-engine is a worthwhile venture. 8 Related work In Germany, IE based on innovative language tech- nology is still a novelty. The only groups which we are aware of which also consider NLP-based IE are (Hahn, 1992; Bayer et al., 1994). None of them make use of such sophisticated components, as we do in SMES. Our work is mostly influence by the work of (Hobbs, 1992; Appelt et al., 1993; Grishman, 1995) as well as by the work described in (Anderson et al., 1992; Dowding et al., 1993). 9 Conclusion We have described an information extraction core system for real world German text processing. The basic design criterion of the system is of providing a set of basic powerful, robust, and efficient natural language components and generic linguistic knowl- edge sources which can easily be customized for pro- cessing different tasks in a flexible manner. The main features are: a very efficient and robust mor- phological component, a powerful tool for expressing finite state expressions, a flexible bidirectional shal- low parser, as well as a flexible interface to an ad- vanced formalism for typed feature formalisms. The system has been fully implemented in Common Lisp and C. Future research will focus towards automatic adaption and acquisition methods, e.g., automatic extraction of subgrammars from a competence base and learning methods for domain-specific extraction patterns. 10 Acknowledgement The research underlying this paper was supported by research grants from the German Bundesmin- isterium für Bildung, Wissenschaft, Forschung und Technologie (BMBF) to the DFKI projects PARADICE, FKZ ITW 9403 and PARADIME, FKZ ITW 9704. We would like to thank the follow- ing people for fruitful discussions: Hans Uszkoreit, Gregor Erbach, and Luca Dini. References A. Aho, J. Hopcraft, and J. Ullmann. 1983. Data struc- tures and algorithms. Addison Wesley, Reading, Mass. P. Anderson, P. Hays, A. Huettner, L. Schmandt, I. Nirenburg, and S. Weinstein. 1992. Automatic ex- traction of facts from press releases to generate news stories. In 3rd ANLP, pages 170-177, Trento, Italy. C. Aone and K. Hausman. 1996. Unsupervised learning of a rule-based Spanish part of speech tagger. In Pro- ceedings of COLING-96, pages 53-58, Kopenhagen, Denmark, Europe. D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson. 1993. Fastus: A finite state processor for information extraction from real world text. In Proceedings of the 13th IJCAI, Chambery, France, August. T. Bayer, U. Bohnacker, and H. Mogg-Schneider. 1994. Infoportlab an experimental document understand- ing system. In Proceedings of the 1st DAS. E. Brill. 1995. Unsupervised learning of disambiguation rules for part of speech tagging. In Very Large Corpora Workshop. S. Busemann, T. Declereck, A. Diagne, L. Dini, J. Klein, and S. Schmeier. 1997. Natural language dialogue ser- vice for appointment scheduling agents. This volume. N. Chinchor, L. Hirschman, and D. Lewis. 1993. Evalu- ating message understanding systems: An analysis of the third message understanding conference (muc-3). Computational linguistics, 19(3). J. Cowie and W. Lehnert. 1996. Information extraction. Communications of the ACM, 39(1):51-87. J. Dowding, J. Gawron, D. Appelt, J. Bear, L. Cherny, R. Moore, and D. Moran. 1993. Gemini: A natural language system for spoken-language understanding. In 31th ACL, Ohio. W. Finkler and G. Neumann. 1988. Morphix: A fast realization of a classification-based approach to mor- phology. In H. Trost, editor, Proceedings of 4th ÖFAI, Berlin, August. Springer. R. Grishman and B. Sundheim. 1996. Message Under- standing Conference 6: A Brief History. In Pro- ceedings of COLING-96, pages 466-471, Kopenhagen, Denmark, Europe. R. Grishman. 1995. The NYU MUC-6 System or Where's the Syntax? In Sixth Message Understanding Conference (MUC-6). Morgan Kaufmann, November. U. Hahn. 1992. On text coherence parsing. In Pro- ceedings of COLING-92, pages 25-31, Nantes, France, Europe. J. Hobbs. 1992. The generic information extraction sys- tem. In B. Sundheim, editor, Fourth Message Un- derstanding Conference (MUC-4), McLean, Virginia, June. Distributed by Morgan Kaufmann Publishers, Inc., San Mateo, California. K. Koskenniemi. 1983. Two-level model for morphologi- cal analysis. In 8th IJCAI, pages 683-685, Karlsruhe. Hans-Ulrich Krieger and Ulrich Schäfer. 1994. TDL—a type description language for constraint-based gram- mars. In Proceedings of COLING-94, pages 893-899. Hans-Ulrich Krieger. 1987. Nil—eine Lisp-basierte natürlichsprachliche Schnittstelle zu Ramses. Un- terstützung der NMR-Diagnostik von Hirn- und Mam- matumoren. Master's thesis, RWTH Aachen. B. Sundheim, editor. 1995. Sixth Message Understand- ing Conference (MUC-6), Washington. Distributed by Morgan Kaufmann Publishers, Inc., San Mateo, Cali- fornia."
  },
  {
    "title": "Natural Language in Four Spatial Interfaces",
    "abstract": "We describe our experiences building spoken language interfaces to four demonstration applications all involving 2- or 3-D spatial displays or gestural interactions: an air combat command and control simulation, an immersive VR tactical scenario viewer, a map-based air strike simulation tool with cartographic database, and a speech/gesture controller for mobile robots.",
    "content": "1 Introduction The NAUTILUS natural language processor has been under development at our facility since about 1988. During those years it has been used and tested in five different demonstration projects, four of which we describe in some detail in this report: an air combat command and control simulation, an im- mersive VR tactical scenario viewer, a map-based air strike simulation tool with cartographic database, and a speech/gesture controller for mobile robots. All four applications involve spatial displays or in- teractions, including 2D map-based graphical dis- plays (radar screen, geographic map), 3D perspec- tive scenes, and hand gesture input. 2 NAUTILUS NAUTILUS is built around an early version of the PROTEUS chart parser from New York University's Courant Institute (Grishman, 1986). The three sub- sequent system components were developed at our own facility. 2.1 PROTEUS PROTEUS syntactic grammars consist primarily of two rule types: context-free rules written in BNF no- tation, and restriction rules written in a high-level algorithmic language. Each restriction rule is at- tached to a particular nonterminal in the right-hand side of one or more context-free rules. During pars- ing the restriction fires immediately after that non- terminal has been constructed, testing the subtree at that point for well-formedness, or attaching an attribute value for use later on. Nodes rejected by a restriction are not added to the active chart and so do not contribute to the remainder of the parse. 2.2 TINSEL One of the attributes composed by PROTEUS dur- ing parsing is an operator-operand regularized form intended to serve as the representation to which semantic selection and interpretation rules can be applied. The TINSEL semantic interpreter (Wau- chope, 1990) applies case-frame rules and selection restrictions to the PROTEUS regularized output. The interpreter can either be invoked post-parse (ap- plied top-down to each candidate sentential regu- larization) or interleaved with PROTEUS, testing each individual clausal or noun phrase immediately upon construction. In interleaved mode, if a node's regularization does not pass the case-frame or se- lection criteria then the node is not added to the chart, which can prune the search space and reduce parsing time considerably. If the node does pass se- lection, its regularization is augmented with the rel- evant semantic class and role information, becoming an intermediate semantic representation suitable for further processing such as reference resolution and quantifier scoping. The TINSEL interpreter is primarily model- driven, which is to say that the case frame behavior of each predicate in the domain must be explicitly encoded in a declarative semantic representation. As a result we have not attempted to incorporate any generalized case-frame rules into the interpreter it- self, so TINSEL is not bound to any particular the- ory of thematic relations, giving the system devel- oper maximum flexibility in devising useful semantic representations. TINSEL does contain some general rules for handling noun phrases, however, such as automatically attempting to interpret certain prepo- sitional phrases as implicit BE-verb relative clauses (the hammer on the table ⇒ the hammer which is on the table), etc. 2.3 FOCAL The FOCAL (FOCus Algorithm) reference resolu- tion module was developed by visiting MIT grad- uate student Gina-Anne Levow. It resolves defi- nite, indefinite, and pronominal references as sub- sets of objects from a closed-world model developed for each application. Model objects have a TIN- SEL semantic class attribute, permissable identify- ing specifiers (S.S. Loveboat, waypoint No. 2, NTDS icons), and a marker indicating if the object repre- sents a collection of unindividualized entities (map rings, aircraft trails). FOCAL uses semantic class, number, recency, and constituent order within the sentence when choosing antecedents for anaphoric references. It assumes demonstratives (that fighter) to be anaphoric and attempts to resolve other def- inite references (the fighters) first as anaphoric and then as universal. Since none of our interfaces to date has involved declarative or hypothetical utter- ances, indefinite expressions (a fighter) are inter- preted strictly as closed-world references, i.e., one of the known fighters. 2.4 FUNTRAN The FUNTRAN (FUNctional TRANslator) module takes TINSEL and FOCAL output and constructs a quantified logical form suitable for evaluation in the runtime environment to issue a command or query to the target application. The logical quan- tifiers and connectives (FORALL, EXISTS, NOT, AND, etc.) have generic procedural definitions as Lisp macros, so the system developer just needs to develop a so-called Translation Function (TF) for each of the TINSEL predicates in the domain. TFs are Lisp functions (defuns) of the same name as the predicate, taking keyword arguments corresponding to each of the predicate's semantic slots, and ex- changing appropriately coded information with the target application via so-called Interface Functions, described next. FUNTRAN also composes simple fragmentary English responses to database queries based on the results of the TF predicate evaluation. 2.5 Back End Translator At this point the generic NAUTILUS code ends and the system developer must hand-craft an application-specific interface layer between the Translation Functions and the target. The nature of that interface depends on whether NAUTILUS and the target are running in the same Lisp process or as separate Unix processes, possibly on different machines. In one of our projects (InterLACE), the target application is just a Lisp program running in the same process space as the NLP system, so the primitive Interface Functions (IFs) for communicat- ing between the two are just Lisp function calls. In two others (InterVR and InterROB), the target ap- plication runs on another Unix machine on the local net, so the IFs on the NAUTILUS side must encode and transmit message strings over an IPC socket to a corresponding decoder layer linked into the appli- cation. In the fourth project (Eucalyptus) we de- veloped versions for both approaches, one where the application object code (compiled from C) is loaded into Lisp and the IFs are foreign function calls, and the other doing IPC message passing. 2.6 Speech I/O For speech input we use the Phonetic Engine (PE200) from Speech Systems Inc. with the speech recognition software running on a Sun workstation to which the PE200 hardware is connected by a serial line. Under various circumstances we have linked the software in with either NAUTILUS or the applica- tion, or have run it as a separate process commu- nicating with NAUTILUS via an IPC socket. For speech output, a DECtalk speech synthesizer is con- nected to the other Sun serial port and can be sent output from NAUTILUS either by Unix system calls or by writing data directly to the port. 3 Application Projects 3.1 Eucalyptus NAUTILUS was first used in the Eucalyptus (Wau- chope, 1994) spoken language interface to the KOALAS Airborne Early Warning C2 simulation (Barrett and Aldrich, 1990). The original KOALAS interface consisisted of a mouse-sensitive simulated radar screen with a conventional graphical user in- terface composed of command pushbuttons, dialog boxes and scrolling display windows. Our objec- tive in Eucalyptus was to make the same command and data access functionality available via natural language, integrated as much as possible with the graphical interface to allow multimodal interactions. For example, a NL command to the system might re- sult in the display of the same dialog box used in the corresponding GUI command, but with the dialog's data fields fully or partially filled from the NL input; the user can then fill in any remaining empty fields and issue final acceptance either graphically or ver- bally. To that end, the command-oriented Interface Functions in Eucalyptus consist largely of calls to the base functions underlying the KOALAS GUI. Eucalyptus also includes deictic reference, allow- ing the user to click on one or more radar blips or screen locations while speaking verbal references like this fighter or these CAP stations. When a mouse click occurs, NAUTILUS asks the application for the identities of all the objects located at or near the mouse event, and then takes the subset of those ob- jects that match the semantics of the verbal phrase (which can be determined from predicate context as well: for example the word here in Have fighter 1 re- fuel here necessarily refers to a tanker aircraft). To avoid the problems of time-correlating speech with graphical input and distinguishing anaphora from deixis, we reserve the words this, these and here for deictic reference and that, those and there for anaphoric reference, and allow no more than one plural deictic reference per utterance. Database query is used both in answering explicit interrogatives (Which fighters aren't holding CAP station?) as well as dereferencing qualified NPs (moving aircraft). The system can also interpret NP sentence fragments as followup commands or queries by substituting the NP into the semantically rele- vant slot of the prior utterance's logical form. As originally designed, FOCAL expected a closed- world model of all domain objects to be available at startup time. This had to be modified somewhat in Eucalyptus since the KOALAS world includes hy- pothetical objects (suspected threat aircraft) which the user and system can create and destroy at will. References to hypothetical entities are resolved by having FOCAL dynamically consult the application database for the current object population at the time the phrase was uttered. The core syntactic grammar of about 150 context- free rules and 50 restriction rules developed for Eu- calyptus has been re-used in all the other NAU- TILUS projects. Each one has augmented it with a few dozen additional rules for handling application- specific constructs like station two sector one (Euca- lyptus), latitude forty degrees north longitude ninety five degrees west (InterVR), the town of Leipzig (In- terLACE), and thirty degrees left (InterROB). The Eucalyptus lexicon totals about 425 words, many of them unused morphological variants gen- erated automatically by the PROTEUS lexical macros; by comparison, the vocabulary for the speech recognition front end is only 260 words. To- tal input coverage is on the order of 100 million ut- terances, deliberately high to test the speech sys- tem's ability to detect a wide variety of noun phrase determiners. Unlike the NAUTILUS grammar, the speech grammar excludes iteration and recursion (such as compounding) to maintain a reasonable level of recognition accuracy. An experimental ad- dition of relative clauses to the speech grammar was \"productive\" only in the linguistic sense, since the resulting exponential increase in grammar size caused recognition rates to drop to unacceptable lev- els. 3.2 InterVR We next used NAUTILUS in InterVR (Everett et al., 1994), a spoken language controller for an immer- sive 3D or \"Virtual Reality\" tactical combat simula- tion viewer. Here the emphasis was on the utility of speech I/O in an eyes- and hands-busy virtual envi- ronment. Like Eucalyptus, InterVR supports com- mands, queries, complex reference and anaphora, and NP followups. A non-immersive desktop version of the viewer allowed mouse selection of a platform and thus singular deictic reference (this helicopter), but the immersive display version did not include a dataglove or other pointing device. We did not have time or resources to tackle the problem of resolving referents based on visual context (for example hav- ing that helicopter refer to the one nearest the center of the user's field of view), but we are currently in- vestigating the interaction of vision and language in the InterROB project, to be discussed shortly. The InterVR speech component has a vocabu- lary comparable in size to that of Eucalyptus but a more constrained input range (about 1 million ut- terances) mainly due to a less liberal variety of NP determiners. The IPC code developed for Eucalyp- tus ported immediately to the new application, and NAUTILUS's modular architecture allowed speech modeling, NLP knowledge base development and IF coding to be pursued independently by different team members with a minimum of coordination. 3.3 InterLACE InterLACE (Wauchope, 1996) is an integrated nat- ural language interface and graphical map display for the Air Force's LACE land/air combat simu- lation system (Anken, 1989). LACE includes a large object-oriented cartographic database of most of central Germany, containing a total of over 12,000 objects such as towns, lakes, rivers, and railroads. Since the application is written in Common LISP and so can run in the same process environment as NAUTILUS, we dispensed with independently mod- eling FOCAL entities for the domain and just let the LACE database objects serve as the extensions of referring expressions. To avoid having to enter hun- dreds of foreign proper names into the PROTEUS lexicon, we modified the PROTEUS lexical tagger to assume that any input word might be a proper name (applying that assumption only to non-English words failed the first time we encountered the river Main and the Czech towns of Most and As). Deictic reference operates similarly to Eucalyptus: a mouse click can select a number of overlapping map objects at once, to be resolved by an accompanying verbal reference; for example What's the population here? would resolve to a town object whereas Does this cross the Elbe? might resolve to a road. The InterLACE domain necessitated extending FUNTRAN to generate proper logical forms for spa- tial comparatives and superlatives (Is Wurzen closer than Grimma?, the closest lake to Eilenburg) and implicit and explicit reflexives (Do these roads cross [each other]?), and also introduced direct address (Tank 1, head north) and iterative arguments to nav- igation commands (Head north on road E2 for 2 km to Wurzen). An experimental study of NL inputs from novice InterLACE users showed that of 822 in- puts, 14 contained typos or misspellings and 30 con- tained ungrammaticalities, for an illformedness rate of 5%. Of the remaining 778 utterances NAUTILUS failed to understand 23 (3%) due to incomplete cov- erage. Since the PE200's phonetic rules are for American English and (unlike PROTEUS) the module cannot be tricked into recognizing unknown inputs as pos- sible proper names, a complete speech input com- ponent for InterLACE was impractical. For demo purposes we opted to implement a 160-word speech interface containing just fifty German proper names, few enough that the recognizer doesn't have too much trouble distinguishing them, for an input cov- erage of about 10 million utterances. Similarly for speech output we provided the same vocabulary to DECtalk along with phonetic transcriptions to pro- duce acceptable German pronunciations. 3.4 InterROB InterROB is a new project exploring the integra- tion of spoken and gestural inputs to a pair of mo- bile robots with rangefinder vision capability. To date the system accepts commands only, using a 66-word speech vocabulary with an input range of about 11,000 utterances. The robots currently rec- ognize two types of gesture: distance (hands held apart) and direction (wave left/right). Since the sys- tem does not yet query information from the robots, deictic reference must currently be resolved on the robot side rather than (as in the systems described earlier) by having NAUTILUS choose from a set of candidates provided by the application. This means that phrases like that waypoint or the waypoint over there must be assigned a special FOCAL extension (a pseudo-object called a gesture-waypoint) which is not one of the four actual waypoint objects in the closed world, whereas with query capability NAU- TILUS might be able to obtain enough information from the robot to determine which of the four actual waypoints is being gestured toward. Another goal in InterROB is to go beyond the usual restriction of deictic reference to demonstra- tive or indexical references (that, here, there) and allow gestures to accompany any sort of definite or indefinite NP. This could then be extended to in- clude extralinguistic context in general, such as in- terpreting the waypoint to mean the one the robot is currently facing, or my right to mean 90 degrees per- pendicular to the way the robot perceives the opera- tor to be facing. We also plan to extend the robotic vision capabilities with additional hard/software to allow visual object recognition for lexical acquisition. References Ralph Grishman. 1986. PROTEUS parser reference manual. PROTEUS Project Memorandum #4, Department of Computer Science, Courant Insti- tute of Mathematical Sciences, New York Univer- sity, July. Kenneth Wauchope. 1994. Eucalyptus: integrat- ing natural language input with a graphical user interface. NRL technical report NRL/FR/5510- 94-9711, February. Chris Barrett and Charles Aldrich. 1990. Final re- port: KOALAS test planning tool concept demon- stration: users manual. Los Alamos National Lab- oratory, Los Alamos, New Mexico. Craig S. Anken. 1989. LACE: land air combat in ERIC. Rome Air Development Center RADC- TR-89-219, October. Kenneth Wauchope. 1990. A tandem semantic in- terpreter for incremental parse selection. NRL technical report 9288, September. Stephanie S. Everett, Kenneth Wauchope, and Manuel A. Pérez. 1994. A natural language inter- face for virtual reality systems. NCARAI techni- cal report AIC-94-046. Kenneth Wauchope. 1996. Multimodal interaction with a map-based simulation system. NCARAI technical report AIC-96-019."
  },
  {
    "title": "Responding to Semantically Ill-Formed Input",
    "abstract": "One cause of failure in natural language interfaces is semantic overshoot; this is reflected in input sentences which do not correspond to any semantic pattern in the system. We describe a system which provides helpful feedback in such cases by identifying the \"semantically closest\" inputs which the system would be able to understand.",
    "content": "1. Introduction Natural language interfaces have achieved a lim- ited success in small, well circumscribed domains, such as query systems for simple data bases. One task in constructing such an interface is identi- fying the relationships which exist in a domain, and the possible linguistic expressions of these re- lationships. As we set our sights on more complex domains, it will become much harder to develop a complete or nearly complete catalog of the rele- vant relationships and linguistic expressions; sub- stantial gaps will be inevitable. In consequence, many inputs will be rejected because they fail to match the semantic/linguistic model we have con- structed for the domain. We are concerned with the following question: what response should we give a user when his in- put cannot be analyzed for the reasons just de- scribed? The response \"please rephrase\" gives the user no clue as to how to rephrase. This leads to the well-known \"stonewalling\" phenomenon, where a user tries repeatedly, without success, to rephrase his request in a form the system will un- derstand. This may seem amusing to the outside observer, but it can be terribly frustrating to the user, and to the system designer watching his sys- tem being used. We propose instead to provide the user with sentences which are semantically close to the orig- inal input (in a sense to be defined below) and are acceptable inputs to the system. Such feed- back may occasionally be confusing, but we ex- pect that more often it will be helpful in showing the system's capabilities and suggesting possible rephrasings. In the remainder of this paper we briefly review the prior work on responding to ill- formedness, describe our proposal and its imple- mentation as part of a small question-answering system, and relate our initial experiences with this system. 2. Background A. Relative and Absolute Ill-form- edness Weischedel and Sondheimer (Weischedel, 1983) have distinguished two types of ill-formedness: ab- solute ill-formedness and relative ill-formedness. Roughly speaking, an absolutely ill-formed input is one which does not conform to the syntactic and semantic constraints of the natural language or the sublanguage; a relatively ill-formed input is one which is outside the coverage of a particular natural language interface. Our concern is pri- marily with relative ill-formedness. For complex domains, we believe that it will be difficult to cre- ate complete semantic models, and therefore that relatively ill-formed input will be a serious prob- lem — a problem that it will be hard for users to remedy without suitable feedback. B. Syntactic and Semantic Ill- formedness Earlier studies have examined both syntactically and semantically ill-formed input. Among the work on syntactically ill-formed input has been EPISTLE (Miller 1981), the work of Weischedel and Sondheimer (Weischedel 1980, Kwasney 1981, and Weischedel 1983), and Carbonell and Hayes (Carbonell 1983). Some of this work has involved the relaxation of syntactic constraints; other (such as Carbonell and Hayes) a reliance primarily on semantic structures when syntactic analysis fails. Our system has been primarily motivated by our concern about the possiblity of constructing com- plete semantic models, so we have focussed to date on semantic ill-formedness, but we believe that our system will have to be extended in the future to handle syntactic ill-formedness as well. C. Error Identification and Cor- rection For some applications, it is sufficient that the point of ill-formedness be identified, and the constraint be relaxed so that an analysis can be obtained. This was the case in Wilks' early work on \"Prefer- ence Semantics\" (Wilks 1975), which was used for machine translation applications. In other appli- cations it is necessary to obtain an analysis con- forming to the system's semantic model in order for further processing of the input to take place, in effect \"correcting\" the user's input. This is the case for data base query (our current appli- cation), for command systems (such as MURPHY (Selfridge 1986)), and for message entry systems (such as NOMAD (Granger 1983) and VOX (Mey- ers 1985)). D. System Organization Error correction can be provided either by making pervasive changes to a set of rules, or by providing uniform correction procedures which work with a standard (non-correcting) set of rules. In the syn- tactic domain, EPISTLE is an example of the for- mer, the metarule approach (Weischedel 1983) an example of the latter. We feel that, particularly for semantic correction, it is important to take the \"uniform procedure\" approach, since a semantic model for a large domain will be difficult enough to build and maintain without having to take the needs of a correction mechanism into account. It is equally important to have a procedure which will operate on a system with separate syntactic and semantic components, so that we may reap the advantages of such an organization (concise- ness, modularity). The NOMAD system used pro- cedures associated with individual words and so was very hard to extend (Granger 1983, p. 195); the VOX system remedied some of these defects but used a \"conceptual grammar\" mixing syntac- tic and semantic constraints (Meyers 1985). The MURPHY system (Selfridge 1986) is most simi- lar to our own work in terms of the approach to semantic constraint relaxation and user feedback; however, it used a syntactic representation which would be difficult to extend, and required weights in the semantic model for the correction proce- dure. The ill-formedness we are considering may also be viewed as one type of violation of the in- tensional constraints of the data base (constraints in this case on the classes of objects which may participate in particular relations). Intensional constraints have been studied in connection with natural language query systems by several re- searchers, including Mays (1980) and Gal (1985). In particular, the technique that we have adopted is similar in general terms to that suggested by Mays to handle non-existent relationships. In addition to the shortcomings of some of the systems just described, we felt it important to de- velop and test a system in order to gain experience in the effectiveness of these correction techniques. Although (as just noted) many techniques have been described, the published reports contain vir- tually no evaluation of the different approaches. 3. System Overview Our feedback mechanism is being evaluated in the context of a small question-answering system with a relatively standard structure. Processing of a question begins with two stages of syntax analysis: parsing, using an augmented context-free gram- mar, and syntactic regularization, which converts the various types of clauses (active and passive; interrogative, imperative, and declarative; rela- tive and reduced relative; etc.) into a canonical form. In this canonical form, each clause is rep- resented as a list consisting of: tense, aspect, and voice markers; the verb (root form); and a list of operands, each marked by \"subject\", \"object\", or the governing preposition. For example, \"John re- ceived an A in calculus.\" would be translated to (past receive (subject John) (object A) (in calculus)) Processing continues with semantic analysis, which translates the regularized parse into an extended-predicate-calculus formula. One aspect of this translation is the determination of quanti- fier scope. Another aspect is the mapping of each verb and its operands (subject, objects, and mod- ifiers) into a predicate-argument structure. The predicate calculus formula is then interpreted as a data base retrieval command. Finally, the re- trieved data is formatted for the user. The translation from verb plus operands to predicate plus arguments is controlled by the model for the domain. The domain vocabulary is organized into a set of verb, noun, adjective, and adverb semantic classes. The model is a set of patterns stated in terms of these semantic classes. Each pattern represents one combination of verb and operands which is valid (meaningful) in this domain. For example, the pattern which would match the sentence given just above is (v-receive (subject nstudent) (object ngrade) (in ncourse)) where v-receive is the class of verbs including re- ceive, get, etc.; nstudent the class of students; ngrade the class of grades; and ncourse the class of course names. Associated with each pattern is a rule for creating the corresponding predicate- argument structure. 4. The Diagnostic Process In terms of the system just described, the analy- sis failures we are concerned with correspond to the presence in the input of clauses which do not match any pattern in the model. The essence of our approach is quite simple: find the patterns in the model which come closest to matching the input clause, and create sentences using these pat- terns. Implementation of this basic idea, however, has required the development of several processing steps, which we now describe. Our first task is to identify the clauses to which we should apply our diagnostic procedure. Our first impulse might be to trigger the proce- dure as soon as we parse a clause which doesn't match the model. However, the process of match- ing clause against model serves in our system to check selectional constraints. These constraints are needed to filter out, from syntactically valid analyses, those which are semantically ill-formed. In a typical query we may have several seman- tically ill-formed analyses (along with one well- formed one), and thus several occasions of failure in the matching process before we obtain the cor- rect analysis. We must therefore wait until syntax analy- sis is complete and see if there is any syntactic analysis satisfying all selectional constraints. If there is no such analysis, we look for an analysis in which all but one clause satisfies the selectional constraints; if there is such an analysis, we mark the offending clause as our candidate for diagnos- tic processing. Next we look for patterns in the model which “roughly match” this clause. As we explained above, the regularized clause contains a verb and a set of syntactic cases with case labels and fillers; each model pattern specifies a verb class and a set of cases, with each case slot specifying a la- bel and the semantic class of its filler. We define a distance measure between a clause and a pat- tern by assigning a score to each type of mismatch (clause and pattern have the same syntactic case with different semantic classes; clause and pattern include the same semantic class but in different cases; clause has case not present in pattern; etc.) and adding the scores. We then select the pat- tern or patterns which, according to this distance measure, are closest to the offending clause. We now must take each of these patterns and build from it a sentence or phrase the user can un- derstand. Each pattern is in effect a syntactic case frame, with slots whose values have to be filled in. If the case corresponds to one present in the clause, we copy the value from the clause; if the case is optional, we delete it. Othewise we create a slot filler consisting of an indefinite article and a noun describing the semantic class allowed in that slot (for example, if the pattern allows members of the class of students in a slot, we would generate the filler “a student”). When all the slots have been filled, we have a structure comparable to the regularized clause structure produced by syntactic analysis. Finally each filled-in pattern must be trans- formed to a syntactic form parallel to that of the original offending clause. (If we don’t do this — if, for example, the input is a yes-no question and the feedback is a declarative sentence — the system output can be quite confusing.) We isolate the tense, voice, aspect, and other syntactic features of the original clause (this is part of the syntactic regularization process) and transfer these features to the generated structure. If the offending clause is an embedded clause in the original sentence, we save the context of the offending clause (the matrix sentence) and insert the “corrected” clause into this context. We take the resulting structure and apply a sentence generation procedure. The gen- eration procedure, guided by the syntactic feature markers, applies “forward” transformations which eventually generate a sentence string. These sen- tences are presented as the system’s suggestions to the user. 5. Examples The system has been implemented as described above, and has been tested as part of a question- answering system for a small “student transcript.” data base. The syntactic model currently has pat- terns for 30 combinations of verbs and arguments. While the model has been gradually growing, it still has sufficient \"gaps\" to give adequate oppor- tunity for applying the diagnostics. A few examples will serve to clarify the oper- ation of the system. The system has models (take (subject student) (object course)) and (offer (subject school) (object course)) but no model of the form (offer (subject student) (object course)) Accordingly, if a user types Did any students offer V11? (where V11 is the name of a course), the system will respond Sorry, I don't understand the pattern (students offer courses) and will offer the \"suggestions\" Did any students take V11? and Did some school offer V11? Prepositional phrase modifiers are analyzed by inserting a \"be\" and treating the result as a relative clause. For example, \"students in V11\" would be expanded to \"students [such that] [stu- dents] be in V11\". If the resulting clause is not in the semantic model, the usual correction proce- dures are applied. As part of our policy of limiting the model for testing purposes, we did not include a pattern of the form (be (subject student) (in course)) but there is a pattern of the form (enroll (subject student) (in course)) (for sentences such as \"Tom enrolled in V11.\"). Therefore if the user types List the students in V11. the system will generate the suggestions List the students who enroll in V11. and List the students. (the second suggestion arising by deleting the modifier). 6. Current Status The system has been operational since the summer of 1986. Since that time we have been regularly testing the system on various volunteers and revis- ing the system to improve its design and feedback. We instructed the volunteers to try to use the sys- tem to get various pieces of information, rather than setting them a fixed task, so the queries tried have varied widely among users. The experimental results indicate both the strength and weakness of the technique we have described. On the one hand, semantic pattern mismatch is not the primary cause of failure; vo- cabulary overshoot (using words not in the dictio- nary) is much more common. In a series of tests involving 375 queries (by 8 users), 199 (53%) were successful, 95 (25%) failed due to missing vocabu- lary, 22 (6%) failed due to semantic pattern mis- match, and 59 (16%) failed for other reasons. On the other hand, in cases of semantic pattern mis- match, the suggestions made by the system usu- ally include an appropriate rephrasing of the query (as well as some extraneous suggestions). Of the 22 failures due to semantic pattern mismatch (in both series of tests), we judge that in 14 cases the suggestions included an appropriate rephrasing. 7. Assessment These results, while not definitive, suggest that the technique described above is a useful one, but will have to be combined with other tech- niques to forge a general strategy for dealing with problems encountered in interpreting the input. Extending the syntactic coverage of our system, which at present is quite limited, should reduce the frequency of some types of failure. To ob- tain further improvement, we will have to extend our technique to deal with input containing un- known words. It should be possible to do this in a straightforward way by adding dictionary en- tries for the closed syntactic classes, guessing from morphological clues the syntactic class(es) of new words not in the dictionary, obtaining a parse, and then applying the techniques just described (with a new word treated as a semantic unknown, not belonging to any class). Our system only offers suggestions; it does not aspire to correct the user's input. That would be an unreasonable expectation for our simple sys- tem, which does not maintain any user or dis- course model. Our current system typically gen- erates several equally-rated suggestions for an ill- formed input. For a more sophisticated system which does maintain a richer model, correction may be a feasible goal. Specifically, we might gen- erate the suggested questions as we do now and then see if any question corresponds to a plausible goal. 8. Acknowledgements This report is based upon work supported by the National Science Foundation under Grant No. DCR-8501843 and the Defense Advanced Research Projects Agency under Contract N00014-85-K- 0163 from the Office of Naval Research. References [1] J. G. Carbonell and P. J. Hayes, 1983, Re- covery Strategies for Parsing Extragrammati- cal Language. Am. J. Computational Linguis- tics 9(3-4), pp. 123-146. [2] A. Gal and J. Minker, 1985, A Natural Lan- guage Data Base Interface that Provides Coop- erative Answers. Proc. Second Conf. Artificial Intelligence Applications, IEEE Computer So- ciety, pp. 352-357. [3] R. H. Granger 1983 The NOMAD System: Expectation-Based Detection and Correction of Errors during Understanding of Syntactically and Semantically Ill-Formed Text. Am. J. Com- `putational Linguistics, 9(3-4), pp. 188-196. [4] S. C. Kwasney and N. K. Sondheimer, 1981, Relaxation Techniques for Parsing Ill-Formed Input. Am. J. Computational Linguistics, 7, pp. 99-108. [5] E. Mays, 1980, Failures in Natural Language Systems: Applications to Data Base Query Sys- tems. Proc. First Nat'l Conf. Artificial Intelli- gence (AAAI-80), pp. 327-330. [6] A. Meyers, 1985, VOX - An Extensible Nat- ural Language Processor. Proc. IJCAI-85, Los Angeles, CA, pp. 821-825. [7] L. A. Miller, G. E. Heidorn, and K. Jensen, 1981, Text-critiquing with the EPISTLE Sys- tem: An Author's Aid to Better Syntax. In Proc. Nat'l Comp. Conf., AFIPS Press, Arling- ton, VA, pp. 649-655. [8] M. Selfridge, 1986, Integrated Processing Pro- duces Robust Understanding, Computational Linguistics, 12(2), pp. 89-106. [9] R. M. Weischedel and J. E. Black, 1980, Re- sponding Intelligently to Unparsable Inputs. Am. J. Computational Linguistics, 6(2), pp. 97- 109. [10] R. M. Weischedel and N. K. Sondheimer, 1983, Meta-rules as a Basis for Processing Ill- Formed Input. Am. J. Computational Linguis- tics, 9(3-4), pp. 161-177. [11] Y. Wilks, 1975, An Intelligent Analyser and Understander of English. Comm. ACM 18, pp. 264-274."
  },
  {
    "title": "Structure from Anarchy: Meta Level Representation of Expert System Propositions for Natural Language Interfaces",
    "abstract": "In this paper we describe a meta level representation used for mapping natural language input into propositions of an expert system. This representation is based on verb classes that are structured hierarchically, with more general information encoded in the top level nodes and more specific information in the lower level nodes. Because of its structure, the representation is able to provide a detailed classification of the propositions, supplying a basis for defining semantics. It allows the system to answer questions about relationships between propositions without inferencing, as well as to answer questions the expert system could not previously handle.",
    "content": "1 Introduction A great deal of work has been done in constructing natural language interfaces to well structured underlying systems, such as data base systems. These natural language interfaces generally make use of an assumed system structure, such as a schema, to define semantics [Martin, Appelt and Pereira 83; Grosz et. al. 85] [Woods et. al. 72; Woods 73] [Kaplan 79]. On the other hand, almost no effort has been made in constructing natural language interfaces to systems that do not have such an extensive description, e.g. expert systems². The lack of such a schema means that there is no easy way to obtain information about propositions³ of the underlying system. Thus, in order to build a natural language interface to expert systems the semantic interpreter must be able to provide the necessary structure. In an earlier paper [Datskovsky Moerdler etal. 87] we briefly described a semantic interpreter that maps user statements into facts of an expert system, as well an inference engine for expert systems that can efficiently utilize this input. In this paper we discuss the meta level description of the expert system propositions, similar to a schema of a data base, utilized by the semantic interpreter and show how this structure is used in processing of user questions. Our structure consists of a group of hierarchies which are formed from verb categories. The hierarchies provide a grouping of the propositions of an expert system by topic. For example, all propositions that deal with interpersonal relationships are grouped under one hierarchy, while those dealing with transfer of possession are grouped under another. The meaning of a proposition is specified step by step, as the hierarchy is traversed, thus allowing for mapping of various sentences, or parts of sentences into the propositions. To test our theories, the approach is currently being implemented as a front end to a small expert system that deals with personal income tax matters. 2 Expert Systems vs. Data Base Systems Many techniques used in building natural language interfaces for data base systems can not carry over into the expert system domain because of the differences between the two underlying systems. ⁴Throughout this paper we are only concerned with expert systems that must communicate with their users in order to gather data before giving advice, such as Mycin [Shortliffe 76] ⁵In the rule of the form IF A and B then C, A, B and C are propositions. The terms proposition and fact are used interchangeably throughout this paper. ¹This research was partially supported by Office of Naval Research grant N00014-82-K-0256. In particular, we are implementing our ideas on one module of Taxpert (Ensor et. al. 85], an expert system designed in conjunction with AT&T Bell Laboratories. This module helps users determine whether they can or can not claim an individual as a dependent [Datskovsky Moerdler et.al. 87] A semantic interpreter for a data base system usually relies on the regular structure of the data base as encoded in the schema describing it. The schema usually describes the fields and tables of a given data base and provides such information as the key field of a table, the type of data found in each field, relationships between the fields (e.g. all the fields of a given table describe its key field), etc.. The relationships between tables are indicated by similarities and differences of their fields. A typical natural language interface associates semantics of nouns, adjectives and verb phrases of a natural language with fields of a data base. Verbs of the natural language are also associated with actions that can be performed on the tables of a given data base, such as Find and Join. In contrast, no schema or description is available for expert systems. The propositions of an expert system may have arbitrary meanings. No relationship between the propositions is clearly defined. Although meta level structures have been built by systems such as Theresias [Davis 78], these structures are inadequate for defining semantics. Theresias provides such information as the relationships between antecedents and consequents of rules, groupings of rules by their left hand sides, etc.. Only one type of representation, schemata, actually gives a shallow (3 levels) description of propositions (as opposed to rules). However, this information is not sufficient for complete semantic definition and a more complex structure is required. Another major difference is in the function of the two systems. A data base system is not expected to know or solve a user's problem, but only supply the information that the user requests. Consequently, an interface to a data base system must be able to retrieve information requested by the user. In figure 1 we present a typical interaction between Lifer-Ladder [Hendrix et. al. 78] and a user. The questions here involve retrieval of information from a naval data base. user: What is the length and hull number of the Constellation? system: user: the home port? system: Figure 1: Interaction between a user and LIFER-LADDER (Taken from [Tennant 81]) On the other hand, an expert system is designed to be a problem solver. A user consults it about an issue and it must gather information in order to advise him. In figure 2 we present a typical interaction between a user and the Mycin [Shortliffe 76] system (taken from transcripts generated by the author). First, in questions 1-6 the system gathers information about the patient, such as age, sex, lab analysis, etc., and then, after many more questions not shown in the figure, makes a recommendation based on the gathered data. Note that the menu interface predefines the order in which information is entered into the system, whereas with a natural language interface, information is entered in no particular order, i.e it may be imbedded in every user input. The addition of new information with every user statement means that the expert system has to pose fewer questions and that the natural language interface must be responsible for managing all the new information. Further, the interface may have to derive information not only from user statements, but also from questions. This means that it has to derive the problem to solve, as well as facts that can be used for its solution from any given question, and add these facts to the data base (or working memory). The action of extracting a goal$^1$ and adding facts at the same time has no analogy in a data base system, but would be similar to allowing the user to query and update the data base at the same time. 1) Patient's name: ** PT244 2) Age: ** 80 YEARS 3) Sex: ** MALE 6) From what site was the specimen for CULTURE-1 taken? ** SPUTUM More similar Questions Follow and finally a recommendation is made (Determining which drugs are desirable for use against the (Klebsiella- pneumoniae...] [REC-1] My preferred therapy recommendation is as follows: Figure 2: Mycin Transcript Throughout this paper goals refer to the goals the expert system must prove, not long term user goals. 3 The Structure in More Detail To translate user input into facts and goals of an underlying expert system, a structure that is able to provide a foundation for the translation is necessary. This structure must provide the meaning of the expert system propositions, relationships between them and supply a means of mapping semantics of words and phrases into those propositions. It is also desirable that such a structure be general, and hence to some extent transportable from one system to another. <Transfer of possession> [hmn/org,,\"\"] Non Phys.obj[-.abstract,\".\"] Phys. obj [-concrete,.\"] Money [-monetary.\",\" ] Donation [-,-,org, \"] Income [-.-,hmn.\"] Tax [-.-.-,payment/earned] Non tax [-.-.-,payment/given] (?dep is gross_income is ?income) (?dap is amount_of_support?support) Figure 3: Partial Tree formed for the Transfer of possession category<sup>6</sup> Our structure consists of a group of hierarchies formed from classes of verbs. We have analyzed over 90 verbs most common to our domain and classified them into 13 categories<sup>7</sup>. These categories can be used in any domain that requires the verbs belonging to them, because they are derived from general properties of the verbs, thus allowing for a degree of transportability. Each verb category is organized hierarchically where each node of a hierarchy is derived from the meanings of one or more verbs. A number of selectional restrictions is attached to each node indicating constraints on the <sup>6</sup>In the figure, * stands for wild card, and - means that the feature is inherited from the parent node. <sup>7</sup>This is not an absolute number. More categories may be needed in other domains where a greater number of verbs is necessary. <sup>8</sup>These categories are based on works in linguistics, e.g. [Osgood 79] and on Roget's Thesaurus. For a more detailed description of the categories see [Datskovsky Moerdler et.al. 87] agent, patient, object and modifier of an input sentence (not all four restrictions are specified for every category). The hierarchies group propositions of an expert system by topic. The leaves of the hierarchies contain either expert system facts or pointers to other hierarchies, thus forming a connected forest. The top level nodes of the hierarchies provide general classes into which a group of propositions of an expert system might fall. At the lower levels of the hierarchies the propositions are separated into more specific subclasses of a given parent node, thus further specifying their meanings. At the lowest level, each node points to only one proposition thus uniquely defining it within its class. For example, figure 3 shows the partial hierarchy for the Transfer of Possession category. The top level node of the hierarchy is derived from the properties of the verbs of the general class of Transfer of Possession. Verbs from that class have pointers to this node and all the propositions that deal with transfer of possession can be accommodated by this node and the nodes below it. The selectional restrictions on this node indicate that the transfer is initiated by either a human or an organization and that the beneficiary of the transfer, the object being transferred, as well as any modifiers can be unspecified until some lower level. The two nodes at the next level further divide the class of transfer of possession verbs and predicates into those dealing with physical object transfers and non physical object transfers. The [-] in the selectional restrictions indicate that the feature is inherited from the parent node. The restrictions on the two nodes also further specify that the object being transferred must be concrete in order to take the Phys Obj link and abstract in order to take the Non Phys Obj link. At the next level, the concept of physical object transfers (as embodied by the Phys Obj node) is further specified. In this example only one of its children, the Money node is shown<sup>9</sup>. Again, verbs dealing specifically with money transfers may point directly to this node. The restriction on the object of the transfer must be monetary in order for this node to be chosen during parsing. This node is further subdivided into Donation and Income, where the distinction is made based on the recipient of the transfer, since donations are normally given to organizations, and income to people. Next, Income can come in two forms, Taxable and Non Taxable, as indicated by the selectional restrictions of the objects <sup>9</sup>The node has 2 other children in the complete tree. of the transfer, and finally, the bottom level of the hierarchy contains expert system propositions. The propositions (?dependent is gross_income ?income) and (?dependent is amount_of_support ?support) belong to a general class of Transfer of Possession, and a more specific class Income, indicating that both propositions describe a type of income that is generally transferred from one party to another. However, because one deals with taxable income and the other with non taxable income, these propositions are further subdivided into subclasses at the next level. This kind of gradual division of propositions into subclasses not only provides a means for mapping user input into facts and goals of an expert system, but also allows the system to answer questions about relationships between the propositions, often without any inferencing. In addition, it allows the system to make meta level inferences it could not make without the structure. In the next section we present a brief description of the parsing algorithm and illustrate it with an example. 3.1 Parsing Algorithm: Overview and Example. During parsing, an appropriate hierarchy is selected according to the definition of the verb in the system's dictionary, where each verb can point to any level in a hierarchy, and a selectional restriction based algorithm is used to traverse the hierarchy with the nouns of the sentence guiding the parser down the hierarchy, until an expert system proposition is reached. The information for this algorithm is encoded into each hierarchy, with the restrictions on the arguments of the verbs based on noun features derived from Roget's thesaurus. The system is currently being implemented in Common lisp on a Symbolics Lisp Machine. It uses an ATN parser which has been modified to call the semantics at various points before deciding which arc to take next. Syntax and semantics run in parallel, with syntax providing a deep structure of a sentence, and semantics supplying information for modifier attachment. Although the verb hierarchies are the primary source of facts, some facts are derived directly from the noun features. As an example of how the natural language interface derives both propositions and goals from Yes/No questions posed by the user consider the question Can I claim my son who earns a salary of $2000?. A trace of the system execution of this sentence is shown in appendix I. The trace shows the nodes of the different hierarchies considered by the algorithm and where the interaction between syntax and semantics occurs. It also shows all the predicates derived by the system and a complete syntactic parse. In yes/no questions the goal is generally indicated by the main verb. The syntactic parser identifies claim as the main verb of the sentence. The verb claim is defined in the system's dictionary as Classification <+> Dependency10, indicating that the verb belongs to the general category of Classification and a more specific subnode of that category, Dependency. The <+> indicates that the syntactic subject of the sentence is the semantic agent. Based on the definition of the verb the algorithm enters the Classification hierarchy at the Dependency node, as demonstrated in statements 1 and 2 of the system trace, thus limiting the choice of propositions that this input can map into to the general category of Classification and the subclass Dependency (see figure 4). Since only one proposition, (?user can_claim ?dependent), falls into this classification, it is derived as the goal, indicating that the user wants to know whether he can or can not claim a dependent (the variables of the proposition will later be instantiated with the appropriate values). The additional information in the relative clause states that the dependent earns a salary of $2000, or (?dependent is gross_income ?income). To derive this additional information, the system selects a hierarchy based on the meaning of the verb of the relative clause. The verb to earn is defined in the dictionary as Transfer of possession <+>, so the algorithm enters the Transfer of Possession hierarchy (shown in figure 3). The choice of propositions that this input can map into is now limited to those in the general class of Transfer of Possession. Next, because of the feature concrete of the object (two thousand dollars) of the sentence the algorithm selects Phys Obj as the next node to consider. Based on the feature monetary of the word dollars the Money node is selected next. The Income node is chosen because the recipient of the money has the feature human, and finally, because salary is defined as payment/earned, the node Tax is selected, since earned payments are generally taxable. Finally (?dependent is gross_income ?income) is added to the working memory. The variables ?dependent and 10 Although there are other meanings of the verb, this is the most frequently used meaning in the tax domain, so the system tries this category first. ?income are later instantiated with son and $2000 respectively. The derivation of this predicate can be seen in statement 5-13 of the system trace in appendix I. Propositions can also be derived from certain noun phrases. In this example, the phrase my son indicates the existence of a child-parent relationship. The system then checks for agreement between the head pronoun I and the possessive my and once this agreement is verifies maps the representation of this relationship into the proposition (?dependent is son_of ?user), as shown in statement 4 of the trace. <Classification> [hmn/org.\",\" Secrecy(-,,secret,\"] Categorization(-.\". \"\" People [-,hmn,\".\"] Dependency(-.-,subjection,] (?user can_claim ?dependent) Obj [-.obj..\"] system with a way of dealing with input sentences like My son earns $2000, that do not completely specify a particular proposition. The sentence indicates that the desired proposition is in the class Income, and the system can proceed to specify the appropriate subclass by posing questions to the user without any additional inferencing on the part of the expert system. This particular capability of the algorithm will be discussed in greater detail in future work. 3.2 Other Questions that can be Answered from the Hierarchies The hierarchies allow the system to handle a number of questions that could not be previously handled by the expert system, and answer other questions without invoking the inference process. In particular, these include questions that deal with relationships between facts and comparisons between sessions, as well as questions requiring general information. User: My daughter receives a stipend of $5000, while my son gets a salary of $2000. WHY is my daughter's tax situation different from my son's? Figure 4: Partial Tree formed for the Classification category. The mapping of natural language into propositions of the expert system as demonstrated above is possible because of the classification of propositions and descriptions of their meanings provided by the hierarchies. Note that the hierarchies are used to define semantics of words of the natural language e.g. the verb to earn is directly related to the meta level structure, or the Transfer of Possession hierarchy. The structure given by the hierarchies also provides a description of the propositions and gives similarities and differences between them. For example, both propositions (?dependent is gross_income ?income) and (?dependent is amount_of_support ? support) would have the general properties of the class Income, with unique features of their particular subclasses Tax and Non Tax. This unique classification allows for the mapping of the input in the above example into the appropriate proposition. It also allows the system to answer questions about the differences between the two propositions, as shown in the next section. Another benefit of this representation is that it provides the System: Your daughter's stipend is non taxable income. [Answered by looking at the Income node of the Transfer of Possession hierarchy, where the two paths diverged.] Figure 5: A Question Answered from the Transfer of Possession Hierarchy As an example of questions that can be answered without invoking the inference process, consider the hypothetical example in figure 4 where the user tells the expert that his daughter receives a stipend of $5000, which translates into the proposition (daughter is amount_of_support 5000), since stipend is defined in the dictionary as payment-given. The fact that his son has a salary of $2000 translates into the proposition (son is gross_income 2000). To answer the WHY question the system could check where the derivation paths for the two sets of inputs diverged, and the difference between the two subclasses would constitutes the answer. In this example the paths diverge at the Income node of the Transfer of possession hierarchy, thus the answer can be supplied by simply examining the hierarchy. The question in the first example required both a comparison between two derivation paths as well as the knowledge of the differences between two propositions. As a second example consider the question What kinds of family relationships are recognized by the tax code? This question is about general properties of the tax code and could not be handled by the expert system without the natural language interface, even though all the necessary information was already available in the system. To answer this question it is enough to search the hierarchies for a Relationship node with a child node that describes family relationships. Such a parent- child pair is found in the Possession hierarchy (see figure 6). The answer returned would consist of all the children found under this pair. <Possession> [hmn/org, \",\") Object(-.phys.obj..*] Relationship{-,hmn/org.\"] Business(-.-*] Legal (-.-,leg.rel] Family [hmn,rel,\"] Business(-.-*] Spouse Parent... Child Figure 6: Partial Tree formed for the Possession category. The question handling algorithm is currently under design. To process WH questions the system must first be able to determine whether it can be answered from the hierarchies, or whether the inference engine of the expert system should be invoked. Many of the necessary clues that indicate the question type have been identified, however there is still some more work to be done on this, as well as on the implementation of the module. It is clear, however, that the hierarchies give the system the ability to handle many more types of questions than the expert system alone could handle, and in many instances allow questions to be answered without invoking the inference process of the expert system. 4 Comparison with Previous Work: NLIs to Expert Systems and Other Work in Semantics There has been some effort to construct natural language interfaces to expert systems, namely Prospector [Duda et. al. 79] and Xcalibur [Carbonell et.al. 83; Carbonell and Hayes 84]. Prospector is one of the first expert systems to communicate with its users in natural language. During the consultation the user simply describes what has been discovered at a given site by using patterns, built with the help of the Lifer (Hendrix et. al. 78] system, of the form \"There is <deposit>\", \"There may be <deposit>\", etc. There is not much published information that describes Prospector's natural language module. We can only hypothesize that a very simple and limited set of sentences is accepted by the system based on sample system sessions. Xcalibur's interaction with the user greatly resembles that of a natural language interface to a data base system. Unlike systems such as Mycin, Xcalibur does not do most of the asking. It is not responsible for solving the user's problem, but rather the user has to know what he wants and query accordingly. Most expert systems are designed to solve a user's problem, and this property must be reflected in the interface. Xcalibur does not seem to be suitable as an interface for such systems because it is designed to retrieve information rather than solve a problem. 4.1 Other work in Semantics Our work draws on Palmer's [Palmer 85], but is different from it in several ways. Palmer's inference-driven semantic analysis is specifically designed for a finite, well-defined, i.e. limited domain. The main element of her approach is a set of partially instantiated logical terms, or semantic propositions, which capture the different relationships that can occur in a given domain. Unlike Palmer's work, our interpreter deals with a complex real world domain. It also makes a greater separation between domain specific and domain independent knowledge to allow for a degree of transportability. Also, while our semantics provides a hierarchical organization, Palmer's does not. Other work that has influenced our own also includes that of Graeme Hirst [Hirst 83] and Steve Lytinen [Lytinen 84]. One of the main differences between our work and the work mentioned above (including Palmer's) is that our semantics imposes a structure on top of an unstructured underlying system, which is not the goal of the work mentioned above. 5 Possible Automation of Hierarchy Design The lack of automatic construction of the hierarchies and automatic classification of propositions in them is currently a limitation in our system. If, for a given domain, a certain tree has to be extended, such extension will have to be done by hand. Also, propositions have to be hand encoded in the hierarchies. This makes transportability to other domains more difficult. After the top level categories are selected, the rest of the nodes of the hierarchies and the propositions, as well as the selectional restrictions can not be done interactively. However, we feel that the hierarchies lend themselves to automation construction by an Expert System Expert, because they are based on the linguistic properties of the verbs in the domain, as well as on the knowledge of the meanings of propositions. In the future, we would like to design a customization phase similar to that of Team [Martin, Appelt and Pereira 83; Grosz et. al. 85] and Teli [Ballard 86]. With such a customization phase, a given expert, such as an Expert Systems Expert, can spend several hours automatically building up the necessary parse trees for a given domain. We feel that such a module would greatly enhance the system and make it much more usable. 6 Conclusions and Future Research In this paper we presented a structure for expert systems, similar to a data base schema, that facilitates construction of natural language interfaces. This structure is based on verb classification and hierarchical structuring within each category. The hierarchies provide a grouping of expert system propositions into classes, thus capturing the similarities and differences between the propositions. This grouping provides a mapping between user input and the propositions of the expert system, as well as a mechanism for dealing with several types of questions without additional expert system inferencing. The structure provides a mechanism for answering questions that could not be previously handled by the expert system. It also provides a flexible and somewhat general mapping allowing for a degree of transportability. One of our primary goals is to complete the implementation of our ideas. Processing of statements and yes/no questions has been fully implemented and the work on paragraph parsing and handling of semantically incomplete input is our current focus. In the future we plan to add such features as complete WH question processing and an automatic hierarchy construction algorithm. 7 Acknowledgments I would like to thank my advisor, Kathleen McKeown for all her help and guidance in this work and Robert Ensor of AT&T for his helpful comments. Appendix I (process '((can I claim my son who earns a salary of twothousand dollars))) 1. In Tree: CLASSIFY 2. Considering the children of DEPENDENCY 3. the proposition that was derived is ((?USER ICAN_CLAIMI?DEPENDENT)) back to syntax... 4. the proposition derived from the noun phrase (MY SON) is (?DEPENDENT IS ISON_OFI?USER) 5. In Tree: TRANS_OF_POS 6. Considering the children of TRANS_OF_POS back to syntax... 7. Considering the children of TRANS_OF_POS back to syntax... 8. Considering the children of TRANS_OF_POS 9. Considering the children of IPHYS_OBJI 10. Considering the children of MONEY 11. Considering the children of INCOME 12. Considering the children of TAX 13. the proposition that was derived is ((?DEPENDENT IS IGROSS_INCOME ?INCOME)) back to syntax... ((S (SUBJ (NP (DET NIL) (DESCRIBERS NIL) (HEAD ((PRON 1))) (NUMBER SING)(CONJ NIL) (SEM (HUMAN) NIL)) (QUALIFIERS NIL) (QUESTION NO)(CASE OBJECTIVE))) (AUXS (CAN)) (TENSE PRES) (MAINVERB CLAIM) (SEM-MVERB ((CLASSIFY DEPENDENCY))) (ADVERB NIL) (IND-OBJ NIL) (SUBCONJ NIL) (D-OBJ (NP (DET MY) (DESCRIBERS NIL)(HEAD ((NOUN SON))) (NUMBER SING)(CONJ NIL) (SEM ((HUMAN MALE RELATIVE + CHILD) NIL)) (QUALIFIERS ((S (SUBJ (NP (DET MY) (DESCRIBERS NIL) (HEAD ((NOUN SON))) (SEM ((HUMAN MALE RELATIVE CHILD) NIL)) (QUESTION NO))X(AUXS NIL) (TENSE PRES) (MAINVERB EARN) (SEM-MVERB ((TRANS_OF_POS +))) (ADVERB NIL) (IND-OBJ NIL) (SUBCONJ NIL) (D-OBJ (NP (DET A) (DESCRIBERS NIL) (HEAD (NOUN SALARY))) (NUMBER SING)(CONJ NIL) (SEM ((CONCRETE MONETARY PAYMENT- EARNED) NIL)) (QUALIFIERS ((PP (PREP OF) (PREP-OBJ (NP (DET NIL) (DESCRIBERS (TWOTHOUSAND)) (HEAD ((NOUN DOLLAR))) (NUMBER PLURAL) (CONJ NIL) (SEM ((MONETARY) (CONCRETE NUMBER))) (QUALIFIERS NIL) (QUESTION NO) OBJECTIVE)))))) (QUESTION NO) (CASE OBJECTIVE))(ADICOMP NIL) (MODIF NIL) (TYPE WH-RELATIVE) (QUESTION-ELEMENT NIL) (VOICE ACTIVE) (CONJ NIL) (CSENT NIL)))) (QUESTION NO) (CASE POSSESSIVE))) (ADICOMP NIL) (MODIF NIL) (TYPE INTERROGATIVE) (QUESTION-ELEMENT (YES-NO)) (VOICE ACTIVE)(CONJ NIL)(CSENT NIL))) THE GOAL IS: (?USER ICAN_CLAIM?DEPENDENT) References [Ballard 86] Ballard B. Semantic Acquisition in TELI: A Transportable, User-Customized Natural Language Processor. In Proceedings of twenty first ACL conference. 1986. [Carbonell and Hayes 84] Carbonell J., Hayes, P. Recovery Strategies Parsing Extragrammatical Language. Technical Report CMU-CS-84-107, Carngie-Mellon University, 1984. [Carbonell et.al. 83] Carbonell J., Boggs W.M., Mauldin M., Anick, P. Xcalibur Project Report 1. Technical Report CMU-CS-83-143, Carngie-Mellon University, Digital Equipment Corp., 1983. [Clancey 83] Clancey, W. The Epistimology of a Rule-Based Expert System - a Framework for Explanation. Artificial Intelligence 20, 1983. [Datskovsky Moerdler et.al. 87] G. Datskovsky Moerdler, K. McKeown, J.R. Ensor. Building Natural Language Interface to Expert Systems. In Proceedings of the IJCAI. 1987. [Davis 78] Davis, R. Knowledge Acquisition in Rule-Based Systems-Knowledge About Representation as a Basis for System Construction and Maintanance. Pattern Directed Inference Systems. Academic Press, 1978. [Duda et. al. 79] Duda, R., Gasching, J., Hart, P. Model Design in the Prospector Consultant System for Mineral Exploration. In Michie, D. (editor), Expert Systems in the micro-electronic age. Edinburgh University Press, 1979. [Ensor et. al. 85] Ensor, Gabbe and Blumenthal. Taxpert -- A Framework for Exploring Interactions Among Experts. 1985.in preparation. [Grosz et. al. 85] Grosz, B., Martin, P., Appelt, D., Pereira, F., Team: An Experiment in the Design of Transportable Natural Language Interfaces. Technical Report, SRI International, 1985. [Hendrix et. al. 78] Hendrix, G., Sacerdoti, E., Sagalowicz D., Slocum J.Developing a Natural Language Interface to Complex Data. ACM Transactions on Database Systems, 1978. [Hirst 83] Hirst, G. Semantic Interpretation Against Ambiguity. PhD thesis, Brown University, 1983. [Kaplan 79] Kaplan, S.J. Cooperative Responses From a Portable Natural Language Data Base Query System. PhD thesis, University of Pennsylvania, 1979. [Levin 85] Levin, B. Lexical Semantics in Review: An Introduction. In Levin, B. (editor), Lexical Semantics in Review. MIT, 1985. [Lytinen 84] Lytinen S.L. The Organization of Knowledge in a Multi-lingual Integrated Parser. PhD thesis, Yale University, 1984. [Martin, Appelt and Pereira 83] Martin, P., Appelt, D., Pereira, F. Transportability and Generality in a Natural Language Interface System. In Proceedings of IJCAI. 1983. [Osgood 79] Osgood, Charles, E. Focus on Meaning Volume I: Explorations in Semantic Space. Mouton Publishers, 1979. [Palmer 83] Palmer, M. Inference-Driven Semantic Analysis. In Proceedings of the AAAI. 1983. [Palmer 85] Stone Palmer, M. Driving Semantics for a Limited Domain. PhD thesis, University of Edinburg, 1985. [Pollack 83] Pollack, M.E. Generating Expert Answers Through Goal Inference. Technical Report, SRI International, 1983. [Shortliffe 76] Shortliffe, E.H. Mycin: A rule-based computer program for advising physicians regarding anitimicrobial therapy selection. PhD thesis, Stanford University, 1976. [Tennant 81] Tennant, H. Natural Language Processing. Petocelli Books, USA, 1981. [Webber 71] Nash-Webber, B., Verbs of Composition. 1971.Harvard University, 1971. [Woods 73] Woods, W.A. An Experimental parsing System for Transition Network Grammars. In Rustin (editor), Natural Language Processing. Algorithmic Press, 1973. [Woods et. al. 72] Woods W., Kaplan R., Nash-Webber B. The Lunar Sciences Natural Lnaguage Information System: Final Report. Technical Report 2378, BBN, Cambridge, Mass, 1972."
  },
  {
    "title": "Forest-Based Statistical Sentence Generation",
    "abstract": "This paper presents a new approach to statistical sentence generation in which alternative phrases are represented as packed sets of trees, or forests, and then ranked statistically to choose the best one. This representation offers advantages in compactness and in the ability to represent syntactic information. It also facilitates more efficient statistical ranking than a previous approach to statistical generation. An efficient ranking algorithm is described, together with experimental results showing significant improvements over simple enumeration or a lattice-based approach.",
    "content": "Introduction Large textual corpora offer the possibility of a statistical approach to the task of sentence generation. Like any large-scale NLP or AI task, the task of sentence generation requires immense amounts of knowledge. The knowledge needed includes lexicons, grammars, ontologies, collocation lists, and morphological tables. Ac- quiring and applying accurate, detailed knowl- edge of this breadth poses difficult problems. Knight and Hatzivassiloglou (1995) suggested overcoming the knowledge acquisition bottle- neck in generation by tapping the information inherent in textual corpora. They performed ex- periments showing that automatically-acquired, corpus-based knowledge greatly reduced the need for deep, hand-crafted knowledge. At the same time, this approach to generation im- proved scalability and robustness, offering the potential in the future for higher quality out- put. In their approach, K & H adapted techniques used in speech recognition. Corpus-based sta- tistical knowledge was applied to the generation process after encoding many alternative phras- ings into a structure called a lattice (see Fig- ure 1). A lattice was able to represent large numbers alternative phrases without requiring the large amount of space that an explicitly enu- merated list of individual alternatives would re- quire. The alternative sentences in the lattice were then ranked according to a statistical lan- guage model, and the most likely sentence was chosen as output. Since the number of phrases that needed be considered typically grew ex- ponentially with the length of the phrase, the lattice was usually too large for an exhaustive search, and instead an n-best algorithm was used to heuristically narrow the search. The lattice-based method, though promising, had several drawbacks that will be discussed shortly. This paper presents a different method of statistical generation based on a forest struc- ture (a packed set of trees). A forest is more compact than a lattice, and it offers a hierar- chical organization that is conducive to repre- senting syntactic information. Furthermore, it facilitates dramatically more efficient statistical ranking, since constraints can be localized, and the combinatorial explosion of possibilities that need be considered can be reduced. In addition to describing the forest data structure we use, this paper presents a forest-based ranking algo- rithm, and reports experimental results on its efficiency in both time and space. It also favor- ably compares these results to the performance of a lattice-based approach. 2 Representing Alternative Phrases 2.1 Enumerated lists and lattices The task of sentence generation involves map- ping from an abstract representation of mean- ing or syntax to a linear ordering of words. Subtasks of generation usually include choosing content words, determining word order, \"The chicken may have to be eaten by you\", etc. tences, including \"You may have to eat chicken\", Figure 1: A lattice representing 576 different sen- chickens required the chicken an poulets obliged to b be having 28 a poulet could have might may poulets poulet chickens chicken you required obliged to having b 28 be could have might may eaten by eat eating the poulets poulet you chickens chicken poulets an poulet a chickens chicken deciding when to insert function words, per- forming morphological inflections, and satis- fying agreement constraints, as well as other tasks. One way of leveraging corpus-based knowl- edge is to explicitly enumerate many alternate possibilities and select the most likely according to a corpus-based statistical model. Since many subphrases and decisions will be common across proposed sentences, a lattice is a more efficient way than one-by-one enumeration to represent them. A lattice is a graph where each arc is la- beled with a word. A complete path from the left-most node to right-most node through the lattice represents a possible sentence. Multiple arcs leaving a particular node represent alter- nate paths. A lattice thus allows structure to be shared between sentences. An example of a lattice is shown in Figure 1. This lattice encodes 576 unique sentences. In practice, a lattice may represent many trillions of sentences. Without a compact representation for so many sentences, statistical generation would be much less feasi- ble. The lattice in Figure 1 illustrates several types of decisions that need to be made in gen- eration. For example, there is a choice be- tween the root words \"chicken\" and \"poulet\", the choice of whether to use singular or plural forms of these words, the decision whether to use an article or not, and if so, which one— definite or indefinite. There are also other word choice decisions such as whether to use the aux- iliary verb \"could\", \"might\", or \"may\", and whether to express the mode of eating with the predicate \"have to\", \"be obliged to\", or \"be re- quired to\". Finally, there is a choice between active voice (bottom half of lattice), and pas- sive voice (top half). Inspection of the lattice reveals some un- avoidable duplication, however. For example, the word \"chicken\" occurs four times, while the sublattice for the noun phrase contain- ing \"chicken\" is repeated twice. So is the verb phrase headed by the auxiliaries \"could\", \"might\", and \"may\". Such repetition is com- mon in a lattice representation for text genera- tion, and has a negative impact on the efficiency of the ranking algorithm because the same set of score calculations end up being made several times. Another drawback of the duplication is that the representation consumes more storage space than necessary. Yet another drawback of the lattice represen- tation is that the independence between many choices cannot be fully exploited. Stolcke et al. (1997) noted that 55% of all word dependencies occur between adjacent words. This means that most choices that must be made in non-adjacent parts of a sentence are independent. For ex- ample, in Figure 1, the choice between \"may\", \"might\", or \"could” is independent of the choice between \"a\", \"an\" or \"the\" to precede \"chicken\" or \"poulet\". Independence reduces the combi- nation of possibilities that must be considered, and allows some decisions to be made with- out taking into account the rest of the context. Even adjacent words are sometimes indepen- dent of each other, such as the words \"tail\" and \"ate\" in the sentence \"The dog with the short tail ate the bone\". A lattice does not offer any way of representing which parts of a sentence are independent of each other, and thus can- not take advantage of this independence. This negatively impacts both the amount of process- ing needed and the quality of the results. In contrast, a forest representation, which we will discuss shortly, does allow the independence to be explicitly annotated. A final difficulty with using lattices is that the search space grows exponentially with the length of the sentence(s), making an exhaustive search for the most likely sentence impractical for long sentences. Heuristic-based searches of- fer only a poor approximation. Any pruning that is done renders the solution theoretically inadmissable, and in practice, frequently ends up pruning the mathematically optimal solu- tion. 2.2 Forests These weaknesses of the lattice representation can be overcome with a forest representation. If we assign a label to each unique arc and to each group of arcs that occurs more than once in a lattice, a lattice becomes a forest, and the prob- lems with duplication in a lattice are eliminated. The resulting structure can be represented as a set of context-free rewrite rules. Such a forest need not necessarily comply with a particular theory of syntactic structure, but it can if one wishes. It also need not be derived specifically from a lattice, but can be generated directly from a semantic input. With a forest representation, it is quite nat- ural to incorporate syntactic information. Syn- tactic information offers some potentially signif- icant advantages for statistical language model- ing. However, this paper will not discuss statis- tical modeling of syntax beyond making men- tion of it, leaving it instead for future work. In- stead we focus on the nature of the forest repre- sentation itself and describe a general algorithm for ranking alternative trees that can be used with any language model. A forest representation corresponding to the lattice in Figure 1 is shown in Figure 3. This forest structure is an AND-OR graph, where the AND nodes represent sequences of phrases, and the OR nodes represent mutually exclusive al- ternate phrasings for a particular relative po- sition in the sentence. For example, at the top level of the forest, node S.469 encodes the choice between active and passive voice versions of the sentence. The active voice version is the left child node, labelled S.328, and the passive voice version is the right child node, S.358. There are eight OR-nodes in the forest, corresponding to the eight distinct decisions mentioned earlier that need to be made in deciding the best sen- tence to output. The nodes are uniquely numbered, so that re- peated references to the same node can be iden- tified as such. In the forest diagram, only the first (left-most) reference to a node is drawn completely. Subsequent references only show the node name written in italics. This eases readability and clarifies which portions of the forest actually need to have scores computed during the ranking process. Nodes N.275, NP.318, VP.225 and PRP.3 are repeated in the forest of Figure 3. S.469 ⇒ S.328 S.469 ⇒ S.358 S.328 ⇒ PRP.3 PRP.3 ⇒ PRP.3 VP.327 VP.327 ⇒ \"you\" VP.327 ⇒ VP.248 NP.318 S.358 ⇒ NP.318 VP.357 NP.318 ⇒ NP.317 NP.318 ⇒ N.275 Figure 2: Internal representation of top nodes in forest Figure 2 illustrates how the forest is repre- sented internally, showing context-free rewrite rules for some of the top nodes in the forest. OR-nodes are indicated by the same label oc- curing more than once on the left-hand side of a rule. This sample of rules includes an example of multiple references to a node, namely node NP.318, which occurs on the right-hand side of two different rules. A generation forest differs from a parse forest in that a parse forest represents different pos- sible hierarchical structures that cover a single phrase. Meanwhile a generation forest gener- ally represents one (or only a few) heirarchi- cal structures for a given phrase, but represents many different phrases that generally express the same meaning. 2.3 Previous work on packed generation trees There has been previous work on developing a representation for a packed generation forest structure. Shemtov (1996) describes extensions to a chart structure for generation originally presented in (Kay, 1996) that is used to gen- erate multiple paraphrases from a semantic in- put. A prominent aspect of the representation is the use of boolean vector expressions to asso- ciate each sub-forest with the portions of the in- put that it covers and to control the unification- based generation process. A primary goal of the representation is to guarantee that each part of the semantic input is expressed once and only once in each possible output phrase. In contrast, the packed forest in this paper keeps the association between the semantic in- put and nodes in the forest separate from the forest representation itself. (In our system, these mappings are maintained via an external cache mechanism as described in (Langkilde and Knight, 1998)). Once-and-only-once coverage of the semantic input is implicit, and is achieved by the process that maps from the input to a forest. 3 Forest ranking algorithm The algorithm proposed here for ranking sen- tences in a forest is a bottom-up dynamic pro- gramming algorithm. It is analogous to a chart parser, but performs an inverse compari- son. Rather than comparing alternate syntactic structures indexed to the same positions of an Figure 3: A generation forest PRP.3 S.328 VP.225 AUX.55 VP.190 1 OR OR MD.52 MD.53 MD.54 VB.84 VP.223 may might could have VB.200 VP.224 1 be OR VP.248 TO.243 VP.246 VBG.142 JJ.88 JJ.86 1 1 having obliged required Comments: italicized node numbers indicate duplicate nodes VP.327 S.469 OR NP.318 1 OR 1 to OR VB.154 VP.146 NP.317 Z.316 OR N.275 1 eat be VB.245 VBG. 195 1 eating OR DT.314 DT.315 DT.319 NΝΝ.273 ΝΝ.274 NNS.270 NNS.271 1 1 1 a an the poulet chicken chickens poulets S.358 NP.318 VP.357 VP.344 IN.354 PRP.3 Ν.275 VP.225 TO.341 VB.342 VBN.330 by to be eaten input sentence, it compares alternate phrases corresponding to the same semantic input. As in a probabilistic chart parser, the key insight of this algorithm is that the score for each of the phrases represented by a particu- lar node in the forest can be decomposed into a context-independent (internal) score, and a context-dependent (external) score. The inter- nal score, once computed, is stored with the phrase, while the external score is computed in combination with other sibling nodes. In general, the internal score for a phrase as- sociated with a node p can be defined recur- sively as: J I(p) = Π=1 I(C5) * E(cj|context(c1..cj-1)) where I stands for the internal score, E the ex- ternal score, and c; for a child node of p. The specific formulation of I and E, and the pre- cise definition of the context depends on the lan- guage model being used. As an example, in a bigram model,¹ I=1 for leaf nodes, and E can be expressed as: E = P(FirstWord(c;)|LastWord(cj-1)) Depending on the language model being used, a phrase will have a set of externally-relevant features. These features are the aspects of the phrase that contribute to the context-dependent scores of sibling phrases. In the case of the bi- gram model, the features are the first and last words of the phrase. In a trigram model it is the first and last two words. In more elaborate lan- guage models, features might include elements such as head word, part-of-speech tag, constitu- tent category, etc. A crucial advantage of the forest-based method is that at each node only the best in- ternally scoring phrase for each unique combi- nation of externally relevant features needs to be maintained. The rest can be pruned with- out sacrificing the guarantee of obtaining the overall optimal solution. This pruning reduces exponentially the total number of phrases that need to be considered. In effect, the ranking 1 A bigram model is based on conditional probabil- ities, where the likelihood of each word in a phrase is assumed to depend on only the immediately previous word. The likelihood of a whole phrase is the product of the conditional probabilities of each of the words in the phrase. VP.344 ⇒ VP.225 TO.341 VB.342 VBN.330 225: 341: 342: 330: might have to be eaten may have could have might be required may be required could be required might be having may be having could be having might be obliged may be obliged could be obliged 344: might ... eaten may ... eaten could ... eaten Figure 4: Pruning phrases from a forest node, assuming a bigram model algorithm exploits the independence that exists between most disjunctions in the forest. To illustrate this, Figure 4 shows an exam- ple of how phrases in a node are pruned, as- suming a bigram model. The rule for node VP.344 in the forest of Figure 3 is shown, to- gether with the phrases corresponding to each of the child nodes. If every possible com- bination of phrases is considered for the se- quence of nodes on the right-hand side, there are three unique first words, namely \"might\", \"may\" and \"could\", and only one unique final word, \"eaten\". Given that only the first and last words of a phrase are externally relevant features in a bigram model, only the three best scoring phrases (out of the 12 total) need to be maintained for node VP.344-one for each unique first-word and last-word pair. The other nine phrases can never be ranked higher, no matter what constituents VP.344 later combines with. Pseudocode for the ranking algorithm is shown below. \"Node\" is assumed to be a record composed at least of an array of child nodes, \"Node->c[1..N],\" and best-ranked phrases, \"Node->p[1..M].\" The function Con- catAndScore concatenates two strings together, and computes a new score for it based on the formula given above. The function Prune guar- antees that only the best phrase for each unique set of features values is maintained. The core loop in the algorithm considers the children of the node one-by-one, concatenating and scoring the phrases of the first two children and prun- ing the results, before considering the phrases of the third child, and concatenating them with the intermediate results, and so on. From the pseudocode, it can be seen that the complex- ity of the algorithm is dominated by the num- ber of phrases associated with a node (not the number of rules used to represent the forest, nor the number of children in a an AND node). More specifically, because of the pruning, it de- pends on the number of features associated with the language model, and the average number of unique combinations of feature values that are seen. If f is the number of features, v the av- erage number of unique values seen in a node for each feature, and N the number of N best being maintained for each unique set of fea- ture values (but not a cap on the number of phrases), then the algorithm has the complex- ity O((vN)2f) (assuming that children of AND nodes are concatenated in pairs). Note that f=2 for the bigram model, and f=4 for the trigram model. In comparison, the complexity of an exhaus- RankForest(Node) { if (Leafp(Node)) LeafScore( Node); for j=1 to J { if (not(ranked?(Node->c[j]))) RankForest(Node->c[j]); } for m=1 to NumberOfPhrasesIn(Node->c[1]) Node->p[m] = (Node->c[1])->p[m]; k=fpr j=2 to J { for m=1 to NumberOfPhrasesIn( Node) for n=1 to NumberOfPhrasesIn( Node->c[j]) temp[k++] = ConcatAndScore( Node->p[m], (Node->c[j])->p[n]); Prune(temp); for m=1 to NumberOfPhrasesIn(temp) Node->p[m] = (temp[m]); } } tive search algorithm on a lattice is O((vN)'), where I is approximately the length of the longest sentence in the lattice. The forest-based algorithm thus offers an exponential reduction in complexity while still guaranteeing an opti- mal solution. A capped N-best heuristic search algorithm on the other hand has complexity O(vNl). However, as mentioned earlier, it typ- ically fails to find the optimal solution with longer sentences. In conclusion, the tables in Figure 5 and Fig- ure 6 show experimental results comparing a forest representation to a lattice in terms of the time and space used to rank sentences. These results were generated from 15 test set inputs, whose average sentence length ranged from 14 to 36 words. They were ranked using a bigram model. The experiments were run on a Sparc Ultra 2 machine. Note that the time results for the lattice are not quite directly comparable to those for a forest because they include overhead costs for loading portions of a hash table. It was not possible to obtain timing measurements for the search algorithm alone. We estimate that roughly 80% of the time used in processing the lattice was used for search alone. Instead, the results in Figure 5 should be interpreted as a comparison between different kinds of systems. In that respect, it can be observed from Ta- ble 5 that the forest ranking program performs at least 3 or 4 seconds faster, and that the time needed does not grow linearly with the num- ber of paths being considered as it does with the lattice program. Instead it remains fairly constant. This is consistent with the theoreti- cal result that the forest-based algorithm does not depend on sentence length, but only on the number of different alternatives being consid- ered at each position in the sentence. From Table 6 it can be observed that when there are a relatively moderate number of sen- tences being ranked, the forest and the lattice are fairly comparable in their space consump- tion. The forest has a little extra overhead in representing hierarchical structure. However, the space requirements of a forest do not grow linearly with the number of paths, as do those of the lattice. Thus, with very large numbers of paths, the forest offers significant savings in space. The spike in the graphs deserves particular comment. Our current system for producing forests from semantic inputs generally produces OR-nodes with about two branches. The par- ticular input that triggered the spike produced a forest where some high-level OR-nodes had a much larger number of branches. In a lattice, any increase in the number of branches expo- nentially increases the processing time and stor- age space requirements. However, in the forest representation, the increase is only polynomial with the number of branches, and thus did not produce a spike. 100 90 80 70 60 50 40 30 20 10 0 10000 1e+06 1e+08 1e+10 1e+12 1e+14 1e+16 1e+18 1e+20 \"lattice\" \"forest\" ---x Figure 5: Time required for the ranking pro- cess using a lattice versus a forest representa- tion. The X-axis is the number of paths (log10 scale), and the Y-axis is the time in seconds. 4 Future Work The forest representation and ranking algorithm have been implemented as part of the Nitro- gen generator system. The results shown in the previous section illustrate the time and space advantages of the forest representation which make calculating the mathematically op- timal sentence in the forest feasible (particularly for longer sentences). However, obtaining the mathematically optimal sentence is only valu- able if the mathematical model itself provides a good fit. Since a forest representation makes it possible to add syntactic information to the mathematical model, the next question to ask is whether such a model can provide a better fit for natural English than the ngram models we have used previously. In future work, we plan to modify the forests our system produces 250000 200000 150000 100000 50000 0 10000 1e+06 1e+08 1e+10 1e+12 1e+14 1e+16 1e+18 1e+20 \"lattice\" \"forest\" ---x Figure 6: Size of the data structure for a lattice versus a forest representation. The X-axis is the number of paths (log10 scale), and the Y-axis is the size in bytes. so they conform to the Penn Treebank corpus (Marcus et al., 1993) annotation style, and then do experiments using models built with Tree- bank data. 5 Acknowledgments Special thanks go to Kevin Knight, Daniel Marcu, and the anonymous reviewers for their comments. This research was supported in part by NSF Award 9820291. References M. Kay. 1996. Chart generation. In Proc. ACL. K. Knight and V. Hatzivassiloglou. 1995. Two- level, many-paths generation. In Proc. ACL. I. Langkilde and K. Knight. 1998. Generation that exploits corpus-based statistical knowl- edge. In Proc. COLING-ACL. M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of english: the Penn treebank. Computational Linguistics, 19(2). H. Shemtov. 1996. Generation of paraphrases from ambiguous logical forms. In Coling'96. A. Stolcke. 1997. Linguistic knowledge and empirical methods in speech recognition. AI Magazine, 18(4):25-31."
  },
  {
    "title": "Detecting Errors within a Corpus using Anomaly Detection",
    "abstract": "We present a method for automatically detecting errors in a manually marked corpus using anomaly detection. Anomaly detection is a method for determining which elements of a large data set do not conform to the whole. This method fits a probability distribution over the data and applies a statistical test to detect anomalous elements. In the corpus error detection problem, anomalous elements are typically marking errors. We present the results of applying this method to the tagged portion of the Penn Treebank corpus.",
    "content": "1 Introduction Manually marking corpora is a time consuming and expensive process. The process is subject to human error by the experts doing the marking. Unfortunately, many natural language process- ing methods are sensitive to these errors. In order to ensure accuracy in a corpus, typically several experts pass over the corpus to ensure consistency. For large corpora this can be a tremendous expense. In this paper, we propose a method for au- tomatically detecting errors in a marked cor- pus using an anomaly detection technique. This technique detects anomalies or elements which do not fit in with the rest of the corpus. When applied to marked corpora, the anomalies tend to be errors in the markings of the corpus. To detect the anomalies, we first compute a probability distribution over the entire corpus. Then we apply a statistical test which identi- fies which elements are anomalies. In this case the anomalies are the elements with very low likelihood. These elements are marked as errors and are thrown out of the corpus. The model is recomputed on the remaining elements. At con- clusion, we are left with two data sets: one the normal elements and the second the detected anomalous elements. We evaluate this method over the part of speech tagged portion of the Penn Treebank cor- pus (Marcus et al., 1993). In one experiment, our method detected 1000 anomalies within a data set of 1.25 million tagged elements. Human judges evaluated the results of the application of this method and verified that 69% of iden- tified anomalies are in fact tagging errors. In another experiment, our method detected 4000 anomalies of which 44% are tagging errors. 1.1 Related Work The tagged portion of the Penn Treebank has been extensively utilized for construction and evaluation of taggers. This includes transformation-based tagging (Brill, 1994; Brill and Wu, 1998). Weischedel et al. (1993) applied Markov Models to tagging. Abney et al. (1999) applied boosting to part of speech tagging. Ad- wait Ratnaparkhi (1996) estimates a probabil- ity distribution for tagging using a maximum entropy approach. Regarding error detection in corpora, Rat- naparkhi (1996) discusses inconsistencies in the Penn Treebank and relates them to inter- annotator differences in tagging style. Abney, Schapire and Singer (1999) discuss how to use boosting for cleaning data. Much related work to the anomaly detection problem stems from the field of statistics in the study of outliers. This work examines de- tecting and dealing with outliers in univariate data, multivariate data, and structured data where the probability distribution over the data is given a priori. Statistics gives a set of discor- dancy tests which can be applied to any given element in the dataset to determine whether it is an outlier. A survey of outliers in statistics is given in Barnett and Lewis (1994). Anomaly detection is extensively used within the field of computer security specifically in in- trusion detection (Denning, 1987). Typically anomaly detection methods are applied to de- tect attacks by comparing the activity during an attack to the activity under normal use (Lane and Brodley, 1997; Warrender et al., 1999). The method used in this paper is based on a method for anomaly detection which detects anomalies in noisy data (Eskin, 2000). The sparse Markov transducer probability modeling method is an extension of adaptive mixtures of probabilistic transducers (Singer, 1997; Pereira and Singer, 1999). Naive Bayes learning, which is used to estimate probabilities in this paper, is described in (Mitchell, 1997). 2 Anomaly Detection More formally, anomaly detection is the process of determining when an element of data is an outlier. Given a set of training data without a probability distribution, we want to construct an automatic method for detecting anomalies. We are interested in detecting anomalies for two main reasons. One, we are interested in model- ing the data and the anomalies can contaminate the model. And two, the anomalies themselves can be of interest as they may show rarely oc- curring events. For the purposes of this work, we are most interested in identifying mistagged elements, i.e. the second case. In order to motivate a method for detect- ing anomalies, we must first make assumptions about how the anomalies occur in the data. We use a \"mixture model\" for explaining the pres- ence of anomalies, one of several popular models in statistics for explaining outliers (Barnett and Lewis, 1994). In the mixture model, there are two probability distributions which generate the data. An element $x_i$ is either generated from the majority distribution or with (small) probabil- ity λ from an alternate (anomalous) distribu- tion. Our distribution for the data, D, is then: D = (1 – λ)M + λΑ (1) where M is the majority distribution, and A is the anomalous distribution. The mixture framework for explaining anomalies is indepen- dent of the properties of the distributions M and A. In other words, no assumptions about the nature of the probability distributions are necessary. The specific probability distribu- tions, M and A, are chosen based on prior knowledge of the problem. Typically M is a structured distribution which is estimated over the data using a machine learning technique, while A is a uniform (random) distribution rep- resenting elements which do not fit into M. In the corpus error detection problem, we are assuming that for each tag in the corpus with probability (1 – λ) the human annotator markes the corpus with the correct tag and with prob- ability λ the human annotator makes an error. In the case of an error, we assume that the tag is chosen at random. 2.1 Detection of Anomalies Detecting anomalies, in this framework, is equivalent to determining which elements were generated by the distribution A and which ele- ments were generated by distribution M. Ele- ments generated by A are anomalies, while ele- ments generated by M are not. In our case, we have probability distributions associated with the distributions M and A, $P_M$ and $P_A$ respec- tively. The algorithm partitions the data into two sets, the normal elements M and the anomalies A. For each element, we make a determination of whether it is an anomaly and should be in- cluded in A or a majority element in which it should be included in M. We measure the like- lihood of the distribution under both cases to make this determination. The likelihood, L, of distribution D with probability function P over elements $x_1,..., x_N$ is defined as follows: $L(D) = \\prod_{i=1}^N P_D(x_i) = $ (2) $(1 - \\lambda)^{|M|} \\prod_{x_i \\in M} P_M(x_i) (\\lambda)^{|A|} \\prod_{x_j \\in A} P_A(x_j)$ Since the product of small numbers is difficult to compute, we instead compute the log likeli- hood, LL. The log likelihood for our case is: $LL(D) = |M|log(1 - \\lambda) + \\sum_{x_i \\in M} log(P_M(x_i))$ (3) $+ |A|log \\lambda + \\sum_{x_j \\in A} log(P_A(x_j))$ In order to determine which elements are anomalies, we use a general principal for deter- mining outliers in multivariate data (Barnett, 1979). We measure how likely each element $z_i$ is an outlier by comparing the difference between the log likelihood of the distribution if the ele- ment is removed from the majority distribution and included in the anomalous distribution. If this difference is sufficiently large, we declare the element an anomaly. Specifically what this difference should be de- pends on the probability distributions and prior knowledge of the problem such as the rate of the anomalies, λ. 3 Methodology 3.1 Corpus The corpus we use is the Penn Treebank tagged corpus. The corpus contains approximately 1.25 million manually tagged words from Wall Street Journal articles. For each word, a record is gen- erated containing the following elements: 1. The tag of the current word $T_i$. 2. The current word $W_i$. 3. The previous tag $T_{i-1}$. 4. The next tag $T_{i+1}$. Over records containing these 4 elements, we compute our probability distributions. 3.2 Probability Modeling Methods The anomaly detection framework is indepen- dent of specific probability distributions. Dif- ferent probability distributions have different properties. Since the anomaly detection frame- work does not depend on a specific probability distribution, we can choose the probability dis- tribution to best model the data based on our intuitions about the problem. To illustrate this, we perform two sets of ex- periments, each using a different probability dis- tribution modeling method. The first set of experiments uses sparse Markov transducers as the probability modeling method, while the sec- ond uses a simple naive Bayes method. 3.3 Sparse Markov Transducers Sparse Markov transducers compute probabilis- tic mappings over sparse data. A Markov trans- ducer is defined to be a probability distribution conditional on a finite set of inputs. A Markov transducer of order L is the conditional proba- bility distribution of the form: $P(Y_t|X_t,X_{t-1},X_{t-2},X_{t-3}...X_{t-(L-1)})$ (4) where $X_k$ are random variables over the in- put alphabet $\\Sigma_{in}$ and $Y_k$ is a random variable over the output alphabet $\\Sigma_{out}$. This probability distribution stochastically defines a mapping of strings over the input alphabet into the output alphabet. The mapping is conditional on the L previous input symbols. In the case of sparse data, the probability distribution is conditioned on only some of the inputs. We use sparse Markov transducers to model these type of distributions. A sparse Markov transducer is a conditional probability of the form: $P(Y_t|\\phi^1 X_{t_1} \\phi^2 X_{t_2}...\\phi^k X_{t_k})$ (5) where $\\phi$ represents a wild card symbol and $t_i = t - \\sum_{j=1}^{i-1} n_j - (i - 1)$. The goal of the sparse Markov transducer estimation algorithm is to estimate a conditional probability of this form based upon a set of inputs and their cor- responding outputs. However, the task is com- plicated due to the lack of knowledge a priori of which inputs the probability distribution is conditional on. Intuitively, a fixed order Markov Chain of or- der L is equivalent to a n-gram with n = L. In a variable order Markov Chain, the value of n changes depending on the context. For ex- ample, some elements in the data may use a bigram, while others may use a trigram. The sparse Markov transducer uses a weighted sum of n-grams for different values of n and these weights depend on the context. In addition the weighted sum is over not only n-grams, but also n-grams with wild cards such as a trigram where only the first and last element is conditioned on. In this case we are looking at the input se- quence of the current word, $W_t$, the previous tag, $T_{t-1}$, and the next tag, $T_{t+1}$. The out- put is the set of all possible tags. The models that are in the weighted sum are the trigram, $W_tT_{t-1}T_{t+1}$; the bigrams $W_tT_{t-1}$, $W_tT_{t+1}$ and $T_{t-1}T_{t+1}$; and the unigrams $W_t$, $T_{t-1}$ and $T_{t+1}$. The specific weights of each model depends on the context or the actual values of $W_t$, $T_{t-1}$, and $T_{t+1}$. Sparse Markov transducers depend on a set of prior probabilities that incorporate prior knowl- edge about the importance of various elements in the input sequence. These prior probabilities are set based on the problem. For this problem, we use the priors to encode the information that the current word, Wt, is very important in de- termining the part of speech. Each model in the weighted sum uses a pseudo-count predictor. This predictor com- putes the probability of an output (tag) by the number of times that a specific output was seen in a given context. In order to avoid probabil- ities of 0, we assume that we have seen each output at least once in every context. In fact, these predictors can be any probability distri- bution which can also depend on what works best for the task. 3.4 Naive Bayes The probability distribution for the tags was also estimated using a straight forward naive Bayes approach. We are interested in the probability of a tag, given the current word, the previous tag, and the next tag, or the probability distribution P(Ti|Wi, Ti-1, Ti+1) which using Bayes Rule is equivalent to: P(Ti|Wi, Ti-1, Ti+1) = P(Wi, Ti-1, Ti+1|Ti) * P(Ti) P(Wi, Ti-1, Ti+1) (6) If we make the Naive Bayes independence as- sumption and we assume that the denominator is constant for all values this reduces to: P(Ti|Wi, Ti-1, Ti+1) = P(Wi|Ti) * P(Ti−1|Ti) * P(Ti+1|Ti) * P(Ti) C (7) where C is a normalization constant in order to have the probabilities sum to 1. Each of the val- ues on the right side of the equation can easily be computed over the data estimating a proba- bility distribution. 3.5 Computing Probability Distributions Each probability distribution was trained over each record giving a model over the entire data. The probability model is then used to deter- ming whether or not an element is an anomaly by applying the test in equation (3). Typi- cally this can be done in an efficient manner because the approach does not require reesti- mating the model over the entire data set. If an element is designated as an anomaly, we remove it from the set of normal elements and efficiently reestimate the probability distribution to obtain more anomalous elements. 4 Results/Evaluation The method was applied to the Penn Tree- bank corpus and a set of anomalies were gen- erated. These anomalies were evaluated by hu- man judges to determine if they are in fact tag- ging errors in the corpus. The human judges were natural language processing researchers (not the author) familiar with the Penn Tree- bank markings. In the experiments involving the sparse Markov transducers, after applying the method, 7055 anomalies were detected. In the ex- periments involving the naive Bayes learning method, 6213 anomalies were detected. Sample output from the system is shown in figure 1. The error is shown in the context marked with !!!. The likelihood of the tag is also given which is extremely low for the errors. The system also outputs a suggested tag and its likelihood which is the tag with the highest likelihood for that context. As we can see, these errors are clearly annotation errors. Since the anomalies detected from the two probability modeling methods differed only slightly, we performed human judge verification of the errors over only the results of the sparse Markov transducer experiments. The anomalies were ordered based on their likelihood. Using this ranking, the set of anoma- lies were broken up into sets of 1000 records. We examined the first 4000 elements by randomly selecting 100 elements out of each 1000. Human judges were presented with the sys- tem output for four sets of 100 anomalies. The judges were asked to choose among three op- tions for each example: 1. Corpus Error - The tag in the corpus sen- tence is incorrect. 2. Unsure - The judge is unsure whether or not the corpus tag is correct Error 0.000035: Its/PRP$ fast-food/NN restaurants/NNS -/: including/VBG Denny/NNP 's/POS,/, Hardee/NNP 's/POS,/, Quincy/NNP 's/POS and/CC EI/NNP Pollo/NNP Loco/NNP (/( \"/\" !!!the/NN!!! only/JJ significant/JJ fast-food/NN chain/NN to/TO specialize/VB in/IN char-broiled/JJ chicken/NN \"/\" )/) -/: are/VBP stable/JJ,/, recession-resistant/JJ and/CC growing/VBG./. Suggested Tag: DT (0.998262) Error 0.019231: Not/RB even/RB Jack/NNP Lemmon/NNP 's/POS expert/JJ doddering/JJ !!!makes/NNS!!! this/DT trip/NN worth/NN taking/VBG./. Suggested Tag: VBZ (0.724359) Error 0.014286: It/PRP also/RB underscores/VBZ the/DT difficult/JJ task/NN ahead/RB as/IN !!!Coors/NNS!!! attempts/VBZ to/TO purchase/VB Stroh/NNP Brew- ery/NNP Co./NNP and/CC fight/VB off/RP increasingly/RB tough/JJ competition/NN from/IN Anheuser-Busch/NNP Cos/NNP ./. Suggested Tag: NNP (0.414286) Figure 1: Sample output of anomalies in Penn Treebank corpus. The errors are marked with !!!. 3. System Error - The tag in the corpus sen- tence is correct and the system incorrectly marked it as an error. The \"unsure\" choice was allowed because of the inherent subtleties in differentiating between types of tags such as \"VB vs. VBP\" or \"VBD vs. VBN\". Over the 400 examples evaluated, 158 were corpus errors, 202 were system errors and the judges were unsure in 40 of the cases. The cor- pus error rate was computed by throwing out the unsure cases and computing: Corpus error rate = Corpus Errors System Errors + Corpus Errors (8) The total corpus error rate over the 400 manu- ally checked examples was was 44%. As can be seen, many of the anomalies are in fact errors in the corpus. For each error, we asked the human judge to determine if the correct tag is the systems sug- gested tag. Out of the total 158 corpus errors, the systems correct tag would have corrected the error in 145 cases. Since the verified examples were random, we can assume that 91% of corpus errors would be automatically corrected if the system would re- place the suspect tag with the suggested tag. Ig- noring the \"unsure\" elements for the purposes of this analysis, if we attempted to automati- cally correct the first 1000 examples where the error rate was 69%, this method would have led to a reduction of the total number of errors in the corpus by 245. 5 Conclusion This paper presents a fully automatic method for detecting errors in corpora using anomaly detection techniques. As shown, the anomalies detected in the Penn Treebank corpus tend to be tagging errors. This method has some inherent limitations because not all errors in the corpus would mani- fest themselves as anomalies. In infrequent con- texts or ambiguous situations, the method may not have enough information to detect an error. In addition, if there are inconsistencies between annotators, the method would not detect the errors because the errors would be manifested over a significant portion of the corpus. Although this paper presents a fully au- tomatic method for error detection in cor- pora, this method can also be used as a semi- automatic method for correcting errors. The method can guide an annotator to the elements which are most likely errors. The method can greatly reduce the number of elements that an annotator needs to examine. Future work in this area involves modeling the corpora with other probability distributions. --- Anomaly Rank Corpus Errors System Error Unsure Corpus Error Rate 1-1000 63 28 9 69% 1001-2000 36 54 10 40% 2001-3000 18 70 12 20% 3001-4000 41 50 9 45% Totals 158 202 40 44% Table 1: Results of error detection experiments on the tagged portion of the Penn Treebank The method is very sensitive to the effective- ness of the probability model in modeling the normal elements. Extensions to the probabil- ity distributions presented here such as adding information about endings of words or using more features could increase the accuracy of the probability distribution and the overall perfor- mance of the anomaly detection system. Other future work involves applying this method to other marked corpora. References Steve Abney, Robert E. Schapire, and Yoram Singer. 1999. Boosting applied to tag- ging and PP attachment. In Proceedings of the Joint SIGDAT Conference on Empiri- cal Methods in Natural Language Processing Conference and Very Large Corpora. V. Barnett and T. Lewis. 1994. Outliers in Sta- tistical Data. John Wiley and Sons. V. Barnett. 1979. Some outlier tests for multi- variate samples. South African Statist, 13:29- 52. Eric Brill and Jun Wu. 1998. Classifier com- bination for improved lexical disambiguation. In Proceedings of COLING-ACL. Eric Brill. 1994. Some advances in transformation-based part of speech tagging. In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 722-727. D.E. Denning. 1987. An intrusion detection model. IEEE Transactions on Software En- gineering, SE-13:222-232. Eleazar Eskin. 2000. Anomaly detection over noisy data using learned probability distribu- tions. In Proceedings of the Seventeenth In- ternational Conference on Machine Learning (ICML-2000) (to appear). T. Lane and C. E. Brodley. 1997. Sequence matching and learning in anomaly detection for computer security. In AAAI Workshop: AI Approaches to Fraud Detection and Risk Management, pages 43-49. AAAI Press. Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics, 19(2):313-330. Tom Mitchell. 1997. Machine Learning. Mc- Graw Hill. Fernando Pereira and Yoram Singer. 1999. An efficient extension to mixture techniques for prediction and decision trees. Machine Learning, 36(3):183-199. Adwait Ratnaparkhi. 1996. A maximum en- tropy model part-of-speech tagger. In Pro- ceedings of the Empirical Methods in Natural Language Processing Conference. Yoram Singer. 1997. Adaptive mixtures of probalistic transducers. Neural Computation, 9(8):1711-1733. Christina Warrender, Stephanie Forrest, and Barak Pearlmutter. 1999. Detecting intru- sions using system calls: alternative data models. In 1999 IEEE Symposium on Secu- rity and Privacy, pages 133–145. IEEE Com- puter Society. Ralph Weischedel, Marie Meteer, Richard Schwartz, Lance Ramshaw, and Jeff Pal- mucci. 1993. Coping with ambiguity and un- known words through probabilistic models. Computational Linguistics, 19(2):359-382. ---"
  },
  {
    "title": "A New Algorithm for the Alignment of Phonetic Sequences",
    "abstract": "Alignment of phonetic sequences is a necessary step in many applications in computational phonology. After discussing various approaches to phonetic alignment, I present a new algorithm that combines a number of techniques developed for sequence comparison with a scoring scheme for computing phonetic similarity on the basis of multivalued features. The algorithm performs better on cognate alignment, in terms of accuracy and efficiency, than other algorithms reported in the literature.",
    "content": "1 Introduction Identification of the corresponding segments in se- quences of phones is a necessary step in many appli- cations in both diachronic and synchronic phonol- ogy. Usually we are interested in aligning sequences that represent forms that are related in some way: a pair of cognates, or the underlying and the sur- face forms of a word, or the intended and the ac- tual pronunciations of a word. Alignment of pho- netic sequences presupposes transcription of sounds into discrete phonetic segments, and so differs from matching of utterances in speech recognition. On the other hand, it has much in common with the alignment of proteins and DNA sequences. Many methods developed for molecular biology can be adapted to perform accurate phonetic alignment. Alignment algorithms usually contain two main components: a metric for measuring distance be- tween phones, and a procedure for finding the op- timal alignment. The former is often calculated on the basis of phonological features that encode cer- tain properties of phones. An obvious candidate for the latter is a well-known dynamic programming (DP) algorithm for string alignment (Wagner and Fischer, 1974), although other algorithms can used as well. The task of finding the optimal alignment is closely linked to the task of calculating the distance between two sequences. The basic DP algorithm accomplishes both tasks. Depending on the appli- cation, either of the results, or both, can be used. Within the last few years, several different ap- proaches to phonetic alignment have been reported. Covington (1996) used depth-first search and a spe- cial distance function to align words for histori- cal comparison. In a follow-up paper (Covington, 1998), he extended the algorithm to align words from more than two languages. Somers (1998) pro- posed a special algorithm for aligning children's ar- ticulation data with the adult model. Gildea and Ju- rafsky (1996) applied the DP algorithm to pre-align input and output phonetic strings in order to im- prove the performance of their transducer induction system. Nerbonne and Heeringa (1997) employed a similar procedure to compute relative distance be- tween words from various Dutch dialects. Some characteristics of these implementations are juxta- posed in Table 1. In this paper, I present a new algorithm for the alignment of cognates. It combines various tech- niques developed for sequence comparison with an appropriate scoring scheme for computing phonetic similarity on the basis of multivalued features. The new algorithm performs better, in terms of accuracy and efficiency, than comparable algorithms reported by Covington (1996) and Somers (1999). Although the main focus of this paper is diachronic phonol- ogy, the techniques proposed here can also be ap- plied in other contexts where it is necessary to align phonetic sequences. 2 Comparing Phones To align phonetic sequences, we first need a func- tion for calculating the distance between individual phones. The numerical value assigned by the func- tion to a pair of segments is referred to as the cost, or penalty, of substitution. The function is often ex- tended to cover pairs consisting of a segment and the null character, which correspond to the opera- Algorithm Calculation Calculation Dynamic Phonological of alignment of distance programming features Covington (1996) explicit implicit no no Somers (1998) explicit no no Nerbonne and Heeringa (1997) implicit explicit yes multivalued binary Gildea and Jurafsky (1996) explicit implicit yes binary Table 1: Comparison of alignment algorithms. tions of insertion and deletion (also called indels). A distance function that satisfies the following ax- ioms is called a metric: 1. Va,b: d(a,b) ≥ 0 (nonnegative property) 2. Va,b: d(a,b) = 0 ⇔ a = b (zero property) 3. Va,b: d(a,b) = d(b,a) (symmetry) 4. Va,b,c:d(a,b)+d(b,c) ≥ d(a, c) (triangle in- equality) 2.1 Covington's Distance Function vs. Feature-Based Metrics Covington (1996), for his cognate alignment algo- rithm, constructed a special distance function. It was developed by trial and error on a test set of 82 cognate pairs from various related languages. The distance function is very simple; it uses no phono- logical features and distinguishes only three types of segments: consonants, vowels, and glides. Many important characteristics of sounds, such as place or manner of articulation, are ignored. For example, both yacht and will are treated identically as a glide- vowel-consonant sequence. The function's values for substitutions are listed in the \"penalty\" column in Table 2. The penalty for an indel is 40 if it is pre- ceded by another indel, and 50 otherwise. Coving- ton (1998) acknowledges that his distance function is \"just a stand-in for a more sophisticated, perhaps feature-based, system\". 1 Both Gildea and Jurafsky (1996) and Nerbonne and Heeringa (1997) use distance functions based on binary features. Such functions have the ability to distinguish a large number of different phones. The underlying assumption is that the number of bi- nary features by which two given sounds differ is Covington's distance function is not a metric. The zero property is not satisfied because the function's value for two identical vowels is greater than zero. Also, the triangle in- equality does not hold in all cases; for example: p(e,i) = 30 and p(i,y) = 10, but p(e,y) = 100, where p(x, y) is the penalty for aligning [x] with [y]. a good indication of their proximity. Phonetic seg- ments are represented by binary vectors in which every entry stands for a single articulatory feature. The penalty for a substitution is defined as the Ham- ming distance between two feature vectors. The penalty for indels is established more or less arbi- trarily.2 A distance function defined in such a way satisfies all metric axioms. It is interesting to compare the values of Cov- ington's distance function with the average Ham- ming distances produced by a feature-based met- ric. Since neither Gildea and Jurafsky (1996) nor Nerbonne and Heeringa (1997) present their fea- ture vectors in sufficient detail to perform the cal- culations, I adopted a fairly standard set of 17 bi- nary features from Hartman (1981).3 The average feature distances between pairs of segments corre- sponding to every clause in Covington's distance function are given in Table 2, next to Covington's \"penalties\". By definition, the Hamming distance between identical segments is zero. The distance between the segments covered by clause #3 is also constant and equal to one (the feature in question being [long] or [syllabic]). The remaining average feature distances were calculated using a set of most frequent phonemes represented by 25 letters of the Latin alphabet (all but q). In order to facilitate com- parison, the rightmost column of Table 2 contains the average distances interpolated between the min- imum and the maximum value of Covington's dis- tance function. The very high correlation (0.998) between Covington's penalties and the average dis- tances demonstrates that feature-based phonology provides a theoretical basis for Covington's manu- ally constructed distance function. 2Nerbonne and Heeringa (1997) fix the penalty for indels as half the average of the values of all substitutions. Gildea and Jurafsky (1996) set it at one fourth of the maximum substitution cost. 3 In order to handle all the phones in Covington's data set, two features were added: [tense] and [spread glottis]. Clause in Covington's distance function Covington's Average Interpolated penalty Hamming average distance distance 1 \"identical consonants or glides\" 0 0.0 0.0 2 \"identical vowels\" 5 0.0 0.0 3 \"vowel length difference only\" 10 1.0 12.4 4 \"non-identical vowels\" 30 2.2 27.3 5 \"non-identical consonants\" 60 4.81 58.1 6 \"no similarity\" 100 8.29 100.0 Table 2: The clause-by-clause comparison of Covington's distance function (column 3) and a feature-based distance function (columns 4 and 5). 2.2 Binary vs. Multivalued Features Although binary features are elegant and widely used, they might not be optimal for phonetic align- ment. Their primary motivation is to classify phonological oppositions rather than to reflect the phonetic characteristics of sounds. In a strictly bi- nary system, sounds that are similar often differ in a disproportionately large number of features. It can be argued that allowing features to have several pos- sible values results in a more natural and phoneti- cally adequate system. For example, there are many possible places of articulation, which form a near- continuum ranging from [labial] to [glottal]. Ladefoged (1995) devised a phonetically-based multivalued feature system. This system has been adapted by Connolly (1997) and implemented by Somers (1998). It contains about 20 features with values between 0 and 1. Some of them can take as many as ten different values (e.g. [place]), while others are basically binary oppositions (e.g. [nasal]). Table 3 contains examples of multivalued features. The main problem with both Somers's and Con- nolly's approaches is that they do not differenti- ate the weights, or saliences, that express the rel- ative importance of individual features. For ex- ample, they assign the same salience to the fea- ture [place] as to the feature [aspiration], which results in a smaller distance between [p] and [k] than between [p] and [ph]. I found that in order to avoid such incongruous outcomes, the salience values need to be carefully differentiated; specifi- cally, the features [place] and [manner] should be assigned significantly higher saliences than other features (the actual values used in my algorithm are given in Table 4). Nerbonne and Heeringa (1997) experimented with weighting each feature by infor- mation gain but found it had an adverse effect on the quality of the alignments. The question of how to derive salience values in a principled manner is still open. 2.3 Similarity vs. Distance Although all four algorithms listed in Table 1 mea- sure relatedness between phones by means of a dis- tance function, such an approach does not seem to be the best for dealing with phonetic units. The fact that Covington's distance function is not a metric is not an accidental oversight; rather, it reflects certain inherent characteristics of phones. Since vowels are in general more volatile than consonants, the pref- erence for matching identical consonants over iden- tical vowels is justified. This insight cannot be ex- pressed by a metric, which, by definition, assigns a zero distance to all identical pairs of segments. Nor is it certain that the triangle inequality should hold for phonetic segments. A phone that has two dif- ferent places of articulation, such as labio-velar [w], can be close to two phones that are distant from each other, such as labial [b] and velar [g]. In my algorithm, below, I employ an alternative approach to comparing segments, which is based on the notion of similarity. A similarity scoring scheme assigns large positive scores to pairs of related seg- ments; large negative scores to pairs of dissimilar segments; and small negative scores to indels. The optimal alignment is the one that maximizes the overall score. Under the similarity approach, the score obtained by two identical segments does not have to be constant. Another important advantage of the similarity approach is the possibility of perform- ing local alignment of phonetic sequences, which is discussed in the following section. 3 Tree Search vs. Dynamic Programming Once an appropriate function for measuring simi- larity between pairs of segments has been designed, Feature Phonological Numerical name term value Place [bilabial] 1.0 [labiodental] 0.95 [dental] 0.9 [alveolar] 0.85 [retroflex] 0.8 [palato-alveolar] 0.75 [palatal] 0.7 [velar] 0.6 [uvular] 0.5 [pharyngeal] 0.3 [glottal] 0.1 Manner [stop] 1.0 [affricate] 0.9 [fricative] 0.8 [approximant] 0.6 [high vowel] 0.4 [mid vowel] 0.2 [low vowel] 0.0 High [high] 1.0 [mid] 0.5 [low] 0.0 Back [front] 1.0 [central] 0.5 [back] 0.0 Table 3: Multivalued features and their values. we need an algorithm for finding the optimal align- ment of phonetic sequences. While the DP algo- rithm, which operates in quadratic time, seems to be optimal for the task, both Somers and Covington opt for exhaustive search strategies. In my opinion, this is unwarranted. Somers's algorithm is unusual because the se- lected alignment is not necessarily the one that minimizes the sum of distances between individ- ual segments. Instead, it recursively selects the most similar segments, or \"anchor points\", in the sequences being compared. Such an approach has a serious flaw. Suppose that the sequences to be aligned are tewos and divut. Even though the corre- sponding segments are slightly different, the align- ment is straightforward. However, an algorithm that looks for the best matching segments first, will er- roneously align the two t's. Because of its recursive nature, the algorithm has no chance of recovering from such an error.<sup>4</sup> <sup>4</sup>The criticism applies regardless of the method of choosing the best matching segments (see also Section 5). Syllabic 5 Place 40 Voice 10 Nasal 10 Lateral 10 Aspirated 5 High 5 Back 5 Manner 50 Retroflex 10 Long 1 Round 5 Table 4: Features used in ALINE and their salience settings. Covington, who uses a straightforward depth-first search to find the optimal alignment, provides the following arguments for eschewing the DP algo- rithm. First, the strings being aligned are rel- atively short, so the efficiency of dy- namic programming on long strings is not needed. Second, dynamic programming normally gives only one alignment for each pair of strings, but comparative re- construction may need the n best alter- natives, or all that meet some criterion. Third, the tree search algorithm lends it- self to modification for special handling of metathesis or assimilation.<sup>5</sup> (Coving- ton, 1996) The efficiency of the algorithm might not be rel- evant in the simple case of comparing two words, but if the algorithm is to be of practical use, it will have to operate on large bilingual wordlists. More- over, combining the alignment algorithm with some sort of strategy for identifying cognates on the basis of phonetic similarity is likely to require comparing thousands of words against one another. Having a polynomially bound algorithm in the core of such a system is crucial. In any case, since the DP algo- rithm involves neither significantly larger overhead nor greater programming effort, there is no reason to avoid using it even for relatively small data sets. The DP algorithm is also sufficiently flexible to accommodate most of the required extensions with- out compromising its polynomial complexity. A simple modification will produce all alignments that are within ε of the optimal distance (Myers, 1995). By applying methods from the operations research literature (Fox, 1973), the algorithm can be adapted to deliver the n best solutions. Moreover, the basic set of editing operations (substitutions and indels) <sup>5</sup>Covington does not elaborate on the nature of the modifi- cations. can be extended to include both transpositions of ad- jacent segments (metathesis) (Lowrance and Wag- ner, 1975) and compressions and expansions (Oom- men, 1995). Other extensions of the DP algorithm that are applicable to the problem of phonetic align- ment include affine gap scores and local compari- son. The motivation for generalized gap scores arises from the fact that in diachronic phonology not only individual segments but also entire morphemes and syllables are sometimes deleted. In order to take this fact into account, the penalty for a gap can be calculated as a function of its length, rather than as a simple sum of individual deletions. One solution is to use an affine function of the form gap(x) = r+sx, where r is the penalty for the introduction of a gap, and s is the penalty for each symbol in the gap. Gotoh (1982) describes a method for incorporating affine gap scores into the DP alignment algorithm. Incidentally, Covington's penalties for indels can be expressed by an affine gap function with r = 10 and s = 40. Local comparison (Smith and Waterman, 1981) is made possible by using both positive and neg- ative similarity scores. In local, as opposed to global, comparison, only similar subsequences are matched, rather than entire sequences. This often has the beneficial effect of separating inflectional and derivational affixes from the roots. Such affixes tend to make finding the proper alignment more dif- ficult. It would be unreasonable to expect affixes to be stripped before applying the algorithm to the data, because one of the very reasons to use an au- tomatic aligner is to avoid analyzing every word in- dividually. 4 The algorithm Many of the ideas discussed in previous sections have been incorporated into the new algorithm for the alignment of phonetic sequences (ALINE). Sim- ilarity rather than distance is used to determine a set of best local alignments that fall within ε of the optimal alignment.<sup>6</sup> The set of operations con- tains insertions/deletions, substitutions, and expan- sions/compressions. Multivalued features are em- ployed to calculate similarity of phonetic segments. Affine gaps were found to make little difference when local comparison is used and they were subse- <sup>6</sup>Global and semiglobal comparison can also be used. In a semiglobal comparison, the leading and trailing indels are assigned a score of zero. algorithm Alignment input: phonetic sequences x and y output: alignment of x and y define S(i, j) = -∞ when i < 0 or j < 0 for i ← 0 to |x| do S(i, 0) ← 0 for j ← 0 to |y| do S(0, j) ← 0 for i ← 1 to |x| do for j ← 1 to |y| do S(i, j) ← max( S(i-1,j) + G<sub>skip</sub>(x<sub>i</sub>), S(i, j-1) + G<sub>skip</sub>(y<sub>j</sub>), S(i-1,j-1) + G<sub>sub</sub>(x<sub>i</sub>,y<sub>j</sub>), S(i-1,j-2) + G<sub>exp</sub>(x<sub>i</sub>,y<sub>j-1</sub>y<sub>j</sub>), S(i-2,j-1) + G<sub>exp</sub>(x<sub>i-1</sub>x<sub>i</sub>,y<sub>j</sub>), 0) T ← (1-ε) × max<sub>i,j</sub> S(i,j) for i ← 1 to |x| do for j ← 1 to |y| do if S(i, j) > T then Retrieve(i, j, 0) Figure 1: The algorithm for computing the align- ment of two phonetic sequences. quently removed from ALINE.<sup>7</sup> The algorithm has been implemented in C++ and will be made avail- able in the near future. Figure 1 contains the main components of the algorithm. First, the DP approach is applied to compute the similarity matrix S using the σ scor- ing functions. The optimal score is the maximum entry in the whole matrix. A recursive procedure Retrieve (Figure 2) is called on every matrix en- try that exceeds the threshold score T. The align- ments are retrieved by traversing the matrix until a zero entry is encountered. The scoring functions for indels, substitutions and expansions are defined in Figure 3. C<sub>skip</sub>, C<sub>sub</sub>, and C<sub>exp</sub> are the maximum scores for indels, substitutions, and expansions, re- spectively. C<sub>vwl</sub> determines the relative weight of consonants and vowels. The default values are C<sub>skip</sub> =-10, C<sub>sub</sub> = 35, C<sub>exp</sub> = 45 and C<sub>vwl</sub> = 10. The <i>diff</i> function returns the difference between segments <i>p</i> and <i>q</i> for a given feature <i>f</i>. Set R<sub>v</sub> contains fea- tures relevant for comparing two vowels: Syllabic, Nasal, Retroflex, High, Back, Round, and Long. Set <sup>7</sup>They may be necessary, however, when dealing with lan- guages that are rich in infixes. procedure Retrieve(i, j, s) if S(i, j) = 0 then print(Out) print(\"alignment score is s\") else if S(i-1,j-1) + σsub(xi, yj) + s ≥ T then push(Out, \"align xi with yj\") Retrieve(i-1,j-1, s + σsub(xi, yj)) pop(Out) if S(i, j-1) + σskip(yj) + s ≥ T then push(Out, \"align null with yj\") Retrieve(i, j-1, s + σskip(yj)) pop(Out) if S(i-1,j-2) + σexp(xi, yj-1yj) + s ≥ T then push(Out, \"align xi with yj-1yj\") Retrieve(i-1, j-2, s + σexp(xi, yj-1yj)) pop(Out) if S(i-1,j) + σskip(xj) + s ≥ T then push(Out, \"align xj with null\") Retrieve(i-1, j, s + σskip(xj)) pop(Out) if S(i-2,j-1) + σexp(yj, xi-1xi) + s ≥ T then push(Out, \"align xi-1xi with yj\") Retrieve(i-2, j-1, s + σexp(yj, xi-1xi)) pop(Out) Figure 2: The procedure for retrieving alignments from the similarity matrix. Rc contains features for comparing other segments: Syllabic, Manner, Voice, Nasal, Retroflex, Lateral, Aspirated, and Place. When dealing with double- articulation consonantal segments, only the nearest places of articulation are used. For a more detailed description of the algorithm see (Kondrak, 1999). ALINE represents phonetic segments as vectors of feature values. Table 4 shows the features that are currently used by ALINE. Feature values are encoded as floating-point numbers in the range [0.0, 1.0]. The numerical values of four principal features are listed in Table 3. The numbers are based on the measurements performed by Lade- foged (1995). The remaining features have exactly two possible values, 0.0 and 1.0. A special fea- ture 'Double', which has the same values as 'Place', indicates the second place of articulation. Thanks to its continuous nature, the system of features and their values can easily be adjusted and augmented. 5 Evaluation The best alignments are obtained when local com- parison is used. For example, when aligning En- σskip(P) = Cskip σsub (p,q) = Csub - δ(p,q) - V(p) - V(q) σexp(P, q1q2) = Cexp - δ(p, q1) – δ(p,q2) - V(p) - max(V(q1), V(q2)) where V(p) = { 0 if p is a consonant Cvwl otherwise δ(p,q) = ∑ diff(p,q, f) × salience(f) f∈R where R = { Rc if p or q is a consonant Ry otherwise Figure 3: Scoring functions. glish grass with Latin grāmen, it is important to match only the first three segments in each word; the remaining segments are unrelated. ALINE obvi- ously does not know the particular etymologies, but it can make a guess because [s] and [m] are not very similar phonetically. Only local alignment is able to distinguish between the essential and non-essential correspondences in this case (Table 5). The operations of compression and expansion prove to be very useful in the case of complex cor- respondences. For example, in the alignment of Latin factum with Spanish hecho, the affricate [f] should be linked with both [k] and [t] rather than with just one of them, because it originates from the merger of the two consonants. Note that taking a se- g r - - æ s g r ā m e n || g r æ s || g r ā m en || g r ā s g r ā men Table 5: Three alignments of English grass and Latin grāmen obtained with global, semiglobal, and local comparison. The double bars delimit the aligned subsequences. Covington's alignments ALINE's alignments three : trēs θ r i y t r ē s 0 r iy t r ē s blow : flare b l o w b l o w f l ā r e f l ā re full : plēnus f u l f u l p l ē n u s p l ēnus fish : piscis f i s k i s f i š p i s k i s p i s kis I : ego - a y e g o ay e go tooth : dentis - t u w θ d e n t i s den t i s Table 6: Examples of alignments of English and Latin cognates. quence of substitution and deletion as compression is unsatisfactory because it cannot be distinguished from an actual sequence of substitution and dele- tion. ALINE posits this operation particularly fre- quently in cases of diphthongization of vowels (see the alignments in Table 6). Covington's data set of 82 cognates provides a convenient test for the algorithm. His English/Latin set is particularly interesting, because these two languages are not closely related. Some of the alignments produced by Covington's algorithm and ALINE are shown in Table 6. ALINE accurately discards inflectional affixes in piscis and flare. In fish/piscis, Covington's aligner produces four alter- native alignments, while ALINE selects the cor- rect one. Both algorithms are technically wrong on tooth/dentis, but this is hardly an error consid- ering that only the information contained in the phonetic string is available to the aligners. On Covington's Spanish/French data, ALINE does not make any mistakes. Unlike Covington's aligner, it properly aligns [l] in arbol with the second [r] in arbre. On his English/German data, it selects the correct alignment in those cases where Coving- ton's aligner produces two alternatives. In the fi- nal, mixed set, ALINE makes a single mistake in daughter/thugater, in which it posits a dropped pre- fix rather than a syncopated syllable; in all other cases, it is right on target. Overall, ALINE clearly performs better than Covington's aligner. Somers (1999) tests one version of his algo- rithm, CAT, on the same set of cognates. CAT em- ploys binary, rather than multivalued, features. An- other important characteristic is that it pre-aligns the stressed segments in both sequences. Since CAT distinguishes between individual consonants, in some cases it produces more accurate alignments than Covington's aligner. However, because of its pre-alignment strategy, it is guaranteed to produce wrong alignments in all cases when the stress has moved in one of the cognates. For example, in the Spanish/French pair cabeza/cap, it aligns [p] with [θ] rather than [b] and fails to align the two [a]'s. The problem is even more acute for closely related languages that have different stress rules.8 In contrast, ALINE does not even consider stress, which, in the context of diachronic phonology, is too volatile to depend on. Except for the single case of daughter/thugater, ALINE produces better align- ments than Somers's algorithm. 6 Future Directions The goal of my current research is to combine the new alignment algorithm with a cognate identifica- tion procedure. The alignment of cognates is possi- 8For example, stress regularly falls on the initial syllable in Czech and on the penultimate syllable in Polish, while in Russian it can fall anywhere in the word. ble only after the pairs of words that are suspected of being cognate have been identified. Identification of cognates is, however, an even more difficult task than the alignment itself. Moreover, it is hardly fea- sible without some kind of pre-alignment between candidate lexemes. A high alignment score of two words should indicate whether they are related. An integrated cognate identification algorithm would take as input unordered wordlists from two or more related languages, and produce a list of aligned cog- nate pairs as output. Such an algorithm would be a step towards developing a fully automated language reconstruction system. Acknowledgments I would like to thank Graeme Hirst, Elan Dresher, Steven Bird, and Carole Bracco for their comments. This research was supported by Natural Sciences and Engineering Research Council of Canada. References John H. Connolly. 1997. Quantifying target- realization differences. Clinical Linguistics & Phonetics, 11:267-298. Michael A. Covington. 1996. An algorithm to align words for historical comparison. Computational Linguistics, 22(4):481-496. Michael A. Covington. 1998. Alignment of mul- tiple languages for historical comparison. In Proceedings of COLING-ACL'98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 275-280. Bennett L. Fox. 1973. Calculating the Kth short- est paths. INFOR - Canadian Journal of Op- erational Research and Information Processing, 11(1):66-70. Daniel Gildea and Daniel Jurafsky. 1996. Learning bias and phonological-rule induction. Computa- tional Linguistics, 22(4):497-530. Osamu Gotoh. 1982. An improved algorithm for matching biological sequences. Journal of Molecular Biology, 162:705-708. Steven Lee Hartman. 1981. A universal alphabet for experiments in comparative phonology. Com- puters and the Humanities, 15:75-82. Grzegorz Kondrak. 1999. Alignment of pho- netic sequences. Technical Report CSRG- 402, University of Toronto. Available from ftp.cs.toronto.edu/csri-technical-reports. Joseph B. Kruskal. 1983. An overview of sequence comparison. In David Sankoff and Joseph B. Kruskal, editors, Time warps, string edits, and macromolecules: the theory and practice of se- quence comparison, pages 1-44. Reading, Mass.: Addison-Wesley. Peter Ladefoged. 1995. A Course in Phonetics. New York: Harcourt Brace Jovanovich. Roy Lowrance and Robert A. Wagner. 1975. An extension of the string-to-string correction prob- lem. Journal of the Association for Computing Machinery, 22:177-183. Eugene W. Myers. 1995. Seeing conserved signals. In Eric S. Lander and Michael S. Waterman, edi- tors, Calculating the Secrets of Life, pages 56-89. Washington, D.C.: National Academy Press. John Nerbonne and Wilbert Heeringa. 1997. Measuring dialect distance phonetically. In Proceedings of the Third Meeting of the ACL Special Interest Group in Computational Phonology (SIGPHON-97). Available at http://www.cogsci.ed.ac.uk/sigphon/. B. John Oommen. 1995. String alignment with substitution, insertion, deletion, squashing, and expansion operations. Information Sciences, 83:89-107. T. F. Smith and Michael S. Waterman. 1981. Iden- tification of common molecular sequences. Jour- nal of Molecular Biology, 147:195–197. Harold L. Somers. 1998. Similarity metrics for aligning children's articulation data. In Proceed- ings of COLING-ACL'98: 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computa- tional Linguistics, pages 1227-1231. Harold L. Somers. 1999. Aligning phonetic segments for children's articulation assessment. Computational Linguistics, 25(2):267-275. Robert A. Wagner and Michael J. Fischer. 1974. The string-to-string correction problem. Jour- nal of the Association for Computing Machinery, 21(1):168-173."
  },
  {
    "title": "Automatic Acquisition of Two-Level Morphological Rules",
    "abstract": "We describe and experimentally evaluate a complete method for the automatic acquisition of two-level rules for morphological analyzers/generators. The input to the system is sets of source-target word pairs, where the target is an inflected form of the source. There are two phases in the acquisition process: (1) segmentation of the target into morphemes and (2) determination of the optimal two-level rule set with minimal discerning contexts. In phase one, a minimal acyclic finite state automaton (AFSA) is constructed from string edit sequences of the input pairs. Segmentation of the words into morphemes is achieved through viewing the AFSA as a directed acyclic graph (DAG) and applying heuristics using properties of the DAG as well as the elementary edit operations. For phase two, the determination of the optimal rule set is made possible with a novel representation of rule contexts, with morpheme boundaries added, in a new DAG. We introduce the notion of a delimiter edge. Delimiter edges are used to select the correct two-level rule type as well as to extract minimal discerning rule contexts from the DAG. Results are presented for English adjectives, Xhosa noun locatives and Afrikaans noun plurals.",
    "content": "1 Introduction Computational systems based on the two-level model of morphology (Koskenniemi, 1983) have been remarkably successful for many languages (Sproat, 1992). The language specific information of such a system is stored as 1. a morphotactic description of the words to be processed as well as 2. a set of two-level morphonological (or spelling) rules. Up to now, these two components had to be coded largely by hand, since no automated method existed to acquire a set of two-level rules for input source- target word pairs. To hand-code a 100% correct rule set from word pairs becomes almost impossi- ble when a few hundred pairs are involved. Fur- thermore, there is no guarantee that such a hand coded lexicon does not contain redundant rules or rules with too large contexts. The usual approach is rather to construct general rules from small sub- sets of the input pairs. However, these general rules usually allow overrecognition and overgeneration even on the subsets from which they were inferred. Simons (Simons, 1988) describes methods for studying morphophonemic alternations (using anno- tated interlinear text) and Grimes (Grimes, 1983) presents a program for discovering affix posi- tions and cooccurrence restrictions. Koskenniemi (Koskenniemi, 1990) provides a sketch of a discovery procedure for phonological two-level rules. Golding and Thompson (Golding and Thompson, 1985) and Wothke (Wothke, 1986) present systems to automat- ically calculate a set of word-formation rules. These rules are, however, ordered one-level rewrite rules and not unordered two-level rules, as in our system. Kuusik (Kuusik, 1996) also acquires ordered one- level rewrite rules, for stem sound changes in Esto- nian. Daelemans et al. (Daelemans et al., 1996) use a general symbolic machine learning program to ac- quire a decision tree for matching Dutch nouns to their correct diminutive suffixes. The input to their process is the syllable structure of the nouns and a given set of five suffix allomorphs. They do not learn rules for possible sound changes. Our process au- tomatically acquires the necessary two-level sound changing rules for prefix and suffix allomorphs, as well as the rules for stem sound changes. Connec- tionist work on the acquisition of morphology has been more concerned with implementing psycholog- ically motivated models, than with acquisition of rules for a practical system ((Sproat, 1992, p.216) and (Gasser, 1994)). The contribution of this paper is to present a com- plete method for the automatic acquisition of an op- timal set of two-level rules (i.e. the second com- ponent above) for source-target word pairs. It is assumed that the target word is formed from the source through the addition of a prefix and/or a suffix¹. Furthermore, we show how a partial acqui- sition of the morphotactic description (component one) results as a by-product of the rule-acquisition process. For example, the morphotactic description Two-level rules have the following syntax (Sproat, 1992, p.145): [6] CP op LC - RC of the target word in the input pair [1] Source Target happy happier CP (correspondence part), LC (left context) and RC (right context) are regular expressions over the al- phabet of feasible pairs. In most, if not all, imple- mentations based on the two-level model, the corre- spondence part consists of a single special pair. We also consider only single pair CPs in this paper. The operator op is one of four types: is computed as [2] happier = happy + er The right-hand side of this morphotactic description is then mapped on the left-hand side, [3] happy + er happi 0 er For this example the two-level rule [4] y:i ↔ P:P can be derived. These processes are described in de- tail in the rest of the paper: Section 2 provides an overview of the two-level rule formalism, Section 3 describes the acquisition of morphotactics through segmentation and Section 4 presents the method for computing the optimal two-level rules. Section 5 evaluates the experimental results and Section 6 summarizes. 2 Two-level Rule Formalism Two-level rules view a word as having a lexical and a surface representation, with a correspondence be- tween them (Antworth, 1990), e.g.: [5] Lexical: happy + er Surface: happi 0 er Each pair of lexical and surface characters is called a feasible pair. A feasible pair can be written as lexical-character:surface-character. Such a pair is called a default pair when the lexical character and surface character are identical (e.g. h:h). When the lexical and surface character differ, it is called a spe- cial pair (e.g. y:i). The null character (0) may ap- pear as either a lexical character (as in +:0) or a surface character, but not as both. 1. Exclusion rule: a:b / LC - RC 2. Context restriction rule: a:b ⇒ LC - RC 3. Surface coercion rule: a:b ← LC - RC 4. Composite rule: a:b ↔ LC - RC The exclusion rule (/) is used to prohibit the ap- plication of another, too general rule, in a particular subcontext. Since our method does not overgener- alize, we will consider only the ⇒, ← and ↔ rule types. ¹Non-linear operations (such as infixation) are not considered here, since the basic two-level model deals with it in a round-about way. We can note that exten- sions to the basic two-level model have been proposed to handle non-linear morphology (Kiraz, 1996). 3 Acquisition of Morphotactics The morphotactics of the input words are acquired by (1) computing the string edit difference between each source-target pair and (2) merging the edit se- quences as a minimal acyclic finite state automa- ton. The automaton, viewed as a DAG, is used to segment the target word into its constituent mor- phemes. 3.1 Determining String Edit Sequences A string edit sequence is a sequence of elementary operations which change a source string into a tar- get string (Sankoff and Kruskal, 1983, Chapter 1). The elementary operations used in this paper are single character deletion (DELETE), insertion (IN- SERT) and replacement (REPLACE). We indicate the copying of a character by NOCHANGE. A cost is associated with each elementary operation. Typ- ically, INSERT and DELETE have the same (posi- tive) cost and NOCHANGE has a cost of zero. RE- PLACE could have the same or a higher cost than INSERT or DELETE. Edit sequences can be ranked by the sum of the costs of the elementary opera- tions that appear in them. The interesting edit se- quences are those with the lowest total cost. For most word pairs, there are more than one edit se- quence (or mapping) possible which have the same minimal total cost. To select a single edit sequence which will most likely result in a correct segmen- tation, we added a morphology-specific heuristic to a general string edit algorithm (Vidal et al., 1995). This heuristic always selects an edit sequence con- taining two subsequences which identify prefix-root and root-suffix boundaries. The heuristic depends on the elementary operations being limited only to INSERT, DELETE and NOCHANGE, i.e. no RE- PLACEs are allowed. We assume that the tar- get word contains more morphemes than the source word. It therefore follows that there are more IN- SERTs than DELETEs in an edit sequence. Fur- thermore, the letters forming the morphemes of the target word appear only as the right-hand compo- nents of INSERT operations. Consider the edit se- quence to change the string happy into the string unhappier: 0:u INSERT 0:n INSERT h:h NOCHANGE a:a NOCHANGE p:p NOCHANGE p:p NOCHANGE y:0 DELETE 0:i INSERT 0:e INSERT 0:r INSERT [7] Note that the prefix un- as well as the suffix - er consist only of INSERTs. Furthermore, the prefix-root morpheme boundary is associated with an INSERT followed by a NOCHANGE and the root-suffix boundary by a NOCHANGE-DELETE- INSERT sequence. In general, the prefix-root boundary is just the reverse of the root-suffix bound- ary, i.e. INSERT-DELETE-NOCHANGE, with the DELETE operation being optional. The heuristic resulting from this observation is a bias giving high- est precedence to INSERT operations, followed by DELETE and NOCHANGE, in the first half of the edit sequence. In the second half, the precedence is reversed. 3.2 Merging Edit Sequences A single source-target edit sequence may contain spurious INSERTs which are not considered to form part of a morpheme. For example, the 0:i insertion in Example 7 should not contribute to the suffix - er to form -ier, since -ier is an allomorph of -er. To combat these spurious INSERTs, all the edit se- quences for a set of source-target words are merged as follows: A minimal acyclic finite state automaton (AFSA) is constructed which accepts all and only the edit sequences as input strings. This AFSA is then viewed as a DAG, with the elementary edit op- erations as edge labels. For each edge a count is kept of the number of different edit sequences which pass through it. A path segment in the DAG consisting of one or more INSERT operations having a simi- lar count, is then considered to be associated with a morpheme in the target word. The 0:e 0:r INSERT sequence associated with the -er suffix appears more times than the 0:i 0:e 0:r INSERT sequence asso- ciated with the -ier suffix, even in a small set of adjectively-related source-target pairs. This means that there is a rise in the edge counts from 0:i to 0:e (indicating a root-suffix boundary), while 0:e and 0:r have similar frequency counts. For prefixes a fall in the edge frequency count of an INSERT sequence indicates a prefix-root boundary. To extract the morphemes of each target word, every path through the DAG is followed and only the target-side of the elementary operations serving as edge labels, are written out. The null characters (0) on the target-side of DELETEs are ignored while the target-side of INSERTs are only written if their frequency counts indicate that they are not sporadic allomorph INSERT operations. For Example 7, the following morphotactic description results: [8] Target Word = Prefix + Source + Suffix unhappier = un + happy + er Phase one can segment only one layer of affix ad- ditions at a time. However, once the morpheme boundary markers (+) have been inserted, phase two should be able to acquire the correct two-level rules for an arbitrary number of affix additions: prefix1+prefix2+...+root+suffix1+suffix2+... 4 Acquiring Optimal Rules To acquire the optimal rules, we first determine the full length lexical-surface representation of each word pair. This representation is required for writ- ing two-level rules (Section 2). The morphotactic de- scriptions from the previous section provide source- target input pairs from which new string edit se- quences are computed: The right-hand side of the morphotactic description is used as the source and the left-hand side as the target string. For instance, Example 8 is written as: Source: un+happy+er Target: unhappier The edit sequence [9] [10] u:u n:n +:0 h:h a:a p:p p:p y:i +:0 e:e r:r maps the source into the target and provides the lexical and surface representation required by the two-level rules: Lexical: un+happy+er Surface: un0 happi 0 er [11] The REPLACE elementary string edit operations (e.g. y:i) are now allowed since the morpheme boundary markers (+) are already present in the source string. REPLACEs allow shorter edit se- quences to be computed, since one REPLACE does the same work as an adjacent INSERT-DELETE pair. REPLACE, INSERT and DELETE have the same associated cost and NOCHANGE has a cost of zero. The morpheme boundary marker (+) is always mapped to the null character (0), which makes for linguistically more understandable map- pings. Under these conditions, the selection of any minimal cost string edit mapping provides an ac- ceptable lexical-surface representation². To formulate a two-level rule for the source-target pair happy-unhappier, we need a correspondence pair (CP) and a rule type (op), as well as a left con- text (LC) and a right context (RC) (see Section 2). Rules need only be coded for special pairs, i.e. IN- SERTS, DELETES or REPLACEs. The only special pair in the above example is y:i, which will be the CP of the rule. Now the question arises as to how large the context of this rule must be? It should be large enough to uniquely specify the positions in the lexical-surface input stream where the rule is applied. On the other hand, the context should not be too large, resulting in an overspecified con- text which prohibits the application of the rule to unseen, but similar, words. Thus to make a rule as general as possible, its context (LC and RC) should be as short as possible³. By inspecting the edit se- quence in Example 10, we see that y changes into i when y is preceded by a p:p, which serves as our first attempt at a (left) context for y:i. Two ques- tions must be asked to determine the correct rule type to be used (Antworth, 1990, p.53): Question 1 Is E the only environment in which L:S is allowed? Question 2 Must L always be realized as S in E? The term environment denotes the combined left and right contexts of a special pair. E in our ex- ample is \"p:p-\", L is y and S is i. Thus the answer to question one is true, since y:i only occurs after p:p in our example. The answer to question two is also true, since y is always realized as i after a p:p in the above edit sequence. Which rule type to use is gleaned from Table 1. Thus, to continue our ex- ample, we should use the composite rule type (): ² y:i ↔ p:p - [12] Our assumption is that such a minimal cost mapping will lead to an optimal rule set. In most (if not all) of the examples seen, a minimal mapping was also intuitively acceptable. ³ If abstractions (e.g. sets such as V denoting vow- els) over the regular pairs are introduced, it will not be so simple to determine what is \"a more general con- text\". However, current implementations require ab- stractions to be explicitly instantiated during the compi- lation process ((Karttunen and Beesley, 1992, pp.19-21) and (Antworth, 1990, pp.49-50)). Thus, with the cur- rent state of the art, abstractions serve only to make the rules more readable. Q1 Q2 op false false none true false ⇒ false true ⇐ true true ↔ Table 1: Truth table to select the correct rule type. This example shows how to go about coding the set of two-level rules for a single source-target pair. However, this soon becomes a tedious and error prone task when the number of source-target pairs increases, due to the complex interplay of rules and their contexts. 4.1 Minimal Discerning Rule Contexts It is important to acquire the minimal discerning context for each rule. This ensures that the rules are as general as possible (to work on unseen words as well) and prevents rule conflicts. Recall that one need only code rules for the special pairs. Thus it is necessary to determine a rule type with associ- ated minimal discerning context for each occurrence of a special pair in the final edit sequences. This is done by comparing all the possible contiguous con- texts of a special pair against all the possible con- texts of all the other feasible pairs. To enable the computational comparison of the growing left and right contexts around a feasible pair, we developed a \"mixed-context\" representation. We call the par- ticular feasible pair for which a mixed-context is to be constructed, a marker pair (MP), to distinguish it from the feasible pairs in its context. The mixed- context representation is created by writing the first feasible pair to the left of the marker pair, then the first right-context pair, then the second left-context pair and so forth: [13] LC1, RC1, LC2, RC2, LC3, RC3, ..., MP The marker pair at the end serves as a label. Special symbols indicate the start (SOS) and end (EOS) of an edit sequence. If, say, the right-context of a MP is shorter than the left-context, an out-of-bounds sym- bol (OOB) is used to maintain the mixed-context format. For example the mixed-context of y:i in the edit sequence in Example 10, is represented as: [14] p:p, +:0, p:p, e:e, a:a, r:r, h:h, EOS, +:0, OOB, n:n, OOB, u:u, SOS, OOB, y:i The common prefixes of the mixed-contexts are merged by constructing a minimal AFSA which ac- cepts all and only these mixed-context sequences. ⁴ A two-level rule requires a contiguous context. The transitions (or edges, when viewed as a DAG) of the AFSA are labeled with the feasible pairs and spe- cial symbols in the mixed-context sequence. There is only one final state for this minimal AFSA. Note that all and only the terminal edges leading to this final state will be labeled with the marker pairs, since they appear at the end of the mixed-context sequences. More than one terminal edge may be la- beled with the same marker pair. All the possible (mixed) contexts of a specific marker pair can be recovered by following every path from the root to the terminal edges labeled with that marker pair. If a path is traversed only up to an intermediate edge, a shortened context surrounding the marker pair can be extracted. We will call such an interme- diate edge a delimiter edge, since it delimits a short- ened context. For example, traversing the mixed context path of y:i in Example 14 up to ere would result in the (unmixed) shortened context: p:p p:p - +:0 e:e [15] From the shortened context we can write a two-level rule y:i op p:p p:p - +:0 e:e [16] which is more general than a rule using the full con- text: [17] y:i op SOS u:u n:n h:h a:a p:p p:p - +:0 e:e r:r EOS For each marker pair in the DAG which is also a special pair, we want to find those delimiter edges which produce the shortest contexts providing a true answer to at least one of the two rule type de- cision questions given above. The mixed-context prefix-merged AFSA, viewed as a DAG, allow us to rephrase the two questions in order to find answers in a procedural way: Question 1 Traverse all the paths from the root to the terminal edges labeled with the marker pair L:S. Is there an edge e₁ in the DAG which all these paths have in common? If so, then question one is true for the environment E con- structed from the shortened mixed-contexts as- sociated with the path prefixes delimited by e1. Question 2 Consider the terminal edges which have the same L-component as the marker pair L:S and which are reachable from a common edge e2 in the DAG. Do all of these terminal edges also have the same S-component as the marker pair? If so, then question two is true for the environment E constructed from the short- ened mixed-contexts associated with the path prefixes delimited by e2. For each marker pair, we traverse the DAG and mark the delimiter edges nearest to the root which allow a true answer to either question one, question two or both (i.e. e₁ = e2). This means that each path from the root to a terminal edge can have at most three marked delimiter edges: One delimiting a con- text for a ⇒ rule, one delimiting a context for a ⇔ rule and one delimiting a context for a ⇐ rule. The marker pair used to answer the two questions, serves as the correspondence part (Section 2) of the rule. To continue with Example 14, let us assume that the DAG edge labeled with e:e is the closest edge to the root which answers true only to question one. Then the ⇒ rule is indicated: y:i ⇒ p:p p:p - +:0 e:e [18] However, if the edge labeled with r:r answers true to both questions, we prefer the composite rule (⇔) associated with it although this results in a larger context: [19] y:i ⇔ a:a p:p p:p - +:0 e:e r:r The reasons for this preference are that the ⇔ rule • provides a more precise statement about the ap- plicable environment of the rule and it • seems to be preferred in systems designed by linguistic experts. Furthermore, from inspecting examples, a delimiter edge indicating a ⇒ rule generally delimits the short- est contexts, followed by the delimiter for ⇔ and the delimiter for ⇐. The shorter the selected con- text, the more generally applicable is the rule. We therefore select only one rule per path, in the fol- lowing preference order: (1) ⇒, (2) ⇔ and (3) ⇐. Note that any of the six possible precedence orders would provide an accurate analysis and generation of the pairs used for learning. However, our sug- gested precedence seems to strike the best balance between over- or underrecognition and over- or un- dergeneration when the rules would be applied to unseen pairs. The mixed-context representation has one obvious drawback: If an optimal rule has only a left or only a right context, it cannot be acquired. To solve this problem, two additional minimal AFSAs are con- structed: One containing only the left context in- formation for all the marker pairs and one contain- ing only the right context information. The same process is then followed as with the mixed contexts. The final set of rules is selected from the output of all three the AFSAs: For each special pair 1. we select any of the ⇔ rules with the shortest contexts of which the special pair is the left- hand side, or 2. if no ↔ rules were found, we select the shortest ↔ and ↔ rules for each occurrence of the special pair. They are then merged into a single ↔ rule with disjuncted contexts. The rule set learned is complete since all possible combinations of marker pairs, rule types and con- texts are considered by traversing all three DAGs. Furthermore, the rules in the set have the shortest possible contexts, since, for a given DAG, there is only one delimiter edge closest to the root for each path, marker pair and rule type combination. 5 Results and Evaluation Our process works correctly for examples given in (Antworth, 1990). There were two incorrect seg- mentations in the twenty one adjective pairs given on page 106. It resulted from an incorrect string edit mapping of (un)happy to (un)happily. For the suf- fix, the sequence... 0:1 0:1 y:y was generated instead of the sequence... y:0 0:1 0:1 0:y. The reason for this is that the root word and the inflected form end in the same letter (y) and one NOCHANGE (y:y) has a lower cost than a DELETE (y:0) plus an INSERT (0:y). The acquired segmentation for the 21 pairs, with the suffix segmentation of (un) happily manu- ally corrected, is: [20] Target = Prefix + Source + Suffix bigger = big + er biggest = big + est unclear = un + clear unclearly = un + clear + ly unhappy = un + happy unhappier = un + happy + er unhappiest = un + happy + est unhappily = un + happy + ly unreal = un + real cooler = cool + er coolest = cool + est coolly = cool + ly clearer = clear + er clearest = clear + est clearly = clear + ly redder = red + er reddest = red + est really = real + ly happier = happy + er happiest = happy + est happily = happy + ly From these segmentations, the morphotactic com- ponent (Section 1) required by the morphological analyzer/generator is generated with uncomplicated text-processing routines. Three correct rules, including two gemination rules, resulted for these twenty one pairs⁵: ⁵The results in this paper were verified on the two- level processor PC-KIMMO (Antworth, 1990). The two- [21] 0:d ↔ d:d - +:0 0:g ↔ g:g - +:0 y:i ↔ - +:0 To better illustrate the complexity of the rules that can be learned automatically by our process, consider the following set of fourteen Xhosa noun- locative pairs: [22] Source Word → Target Word inkosi → enkosini iinkosi → ezinkosini ihashe → ehasheni imbewu → embewini amanzi → emanzini ubuchopho → ebucotsheni ilizwe → elizweni ilanga → elangeni ingubo → engubeni ingubo → engutyeni indlu → endlini indlu → endlwini ikhaya → ekhayeni ikhaya → ekhaya Note that this set contains ambiguity: The locative of ingubo is either engubeni or engutyeni. Our pro- cess must learn the necessary two-level rules to map ingubo to engubeni and engutyeni, as well as to map both engubeni and engutyeni in the other direction, i.e. to ingubo. Similarly, indlu and ikhaya each have two different locative forms. Furthermore, the two source words inkosi and iinkosi (the plural of inkosi) differ only by a prefixed i, but they have dif- ferent locative forms. This small difference between source words provides an indication of the sensitiv- ity required of the acquisition process to provide the necessary discerning information to a two-level mor- phological processor. At the same time, our pro- cess needs to cope with possibly radical modifica- tions between source and target words. Consider the mapping between ubuchopho and its locative ebucotsheni. Here, the only segments which stay the same from the source to the target word, are the three letters -buc-, the letter -o- (the deletion of the first -h- is correct) and the second -h-. The target words are correctly segmented during phase one as: level rule compiler KGEN (developed by Nathan Miles) was used to compile the acquired rules into the state tables required by PC-KIMMO. Both PC-KIMMO and KGEN are available from the Summer Institute of Lin- guistics. [23] Target = Prefix + Source + Suffix enkosini = e + inkosi + ni ezinkosini = e + iinkosi + ni ehasheni = e + ihashe + ni embewini = e + imbewu + ni emanzini = e + amanzi + ni ebucotsheni = e + ubuchopho + ni elizweni = e + ilizwe + ni elangeni = e + ilanga + ni engubeni = e + ingubo + ni engutyeni = e + ingubo + ni endlini = e + indlu + ni endlwini = e + indlu + ni ekhayeni = e + ikhaya + ni ekhaya = e + ikhaya + ni Note that the prefix e+ is computed for all the in- put target words, while all but ekhaya (a correct alternative of ekhayeni) have +ni as a suffix. From this segmented data, phase two correctly computes 24 minimal context rules: [24] 0:e ⇔ o:y +:0 - n:n 0:i ⇔ u:w +:0 - n:n 0:s ⇔ p:t - h:h +:0 ⇒ e:e +:0 ⇒ o:y - +:0 ⇒ u:w - +:0 ⇒ n:n a:0 ⇔ - m:m a:e ⇔ - +:0 n:n b:t ⇔ - o:y h:0 ⇔ - o:o i:0 ⇐ +:0 - n:n i:0 ⇐ - h:h i:0 ⇐ - k:k i:0 ⇐ - l:l i:0 ⇐ - m:m i:0 ⇒ +:0 - i:z ⇔ - i:i o:e ⇔ - +:0 n:n o:y ⇔ b:t p:t ⇔ o:o u:0 ⇔ +:0 - b:b u:i ⇔ +:0 n:n u:w ⇔ l:l - +:0 0:i The ⇔ and ⇒ rules of a special pair can be merged into a single ⇔ rule. For example the four rules above for the special pair +:0 can be merged into [25] +:0 ⇔ e:e | o:y - | u:w - | - n:n because both the two questions becomes true for the disjuncted environment e:e - | o:y - | u:w - | - n:n. The vertical bar (\"/\") is the traditional two- level notation which indicate the disjunction of two (or more) contexts. The five ⇐ rules and the single ⇒ rule of the special pair i:0 in Example 24 can be merged in a similar way. In this instance, the context of the ⇒ rule (+:0-) needs to be added to some of the contexts of the ⇐ rules of i:0. The following ⇔ rule results: [26] i:0 ⇔ +:0 - n:n | +:0 - h:h | +:0 - k:k | +:0 - l:l | +:0 - m:m In this way the 24 rules are reduced to a set of 16 rules which contain only a single ⇔ rule for each special pair. This merged set of 16 two-level rules analyze and generate the input word pairs 100% cor- rectly. The next step was to show the feasibility of au- tomatically acquiring a minimal rule set for a wide coverage parser. To get hundreds or even thousands of input pairs, we implemented routines to extract the lemmas (\"head words\") and their inflected forms from a machine-readable dictionary. In this way we extracted 3935 Afrikaans noun-plural pairs which served as the input to our process. Afrikaans plu- rals are almost always derived with the addition of a suffix (mostly -e or -s) to the singular form. Dif- ferent sound changes may occur during this process. For example, gemination, which indicates the short- ening of a preceding vowel, occurs frequently (e.g. kat → katte), as well as consonant-insertion (e.g. kas → kaste) and elision (ampseed → ampsede). Several sound changes may occur in the same word. For example, elision, consonant replacement and gemination occurs in loof → lowwe. Afrikaans (a Germanic language) has borrowed a few words from Latin. Some of these words have two plural forms, which introduces ambiguity in the word mappings: One plural is formed with a Latin suffix (-a) (e.g. emetikum → emetika) and one with an indigenous suffix (-s) (emetikum → emetikums). Allomorphs occur as well, for example -ens is an allomorph of the suffix -s in bed + s → beddens. During phase one, all but eleven (0.3%) of the 3935 input word pairs were segmented correctly. To facilitate the evaluation of phase two, we define a simple rule as a rule which has an environment con- sisting of a single context. This is in contrast with an environment consisting of two or more contexts disjuncted together. Phase two acquired 531 simple rules for 44 special pairs. Of these 531 simple rules, 500 are ⇐ rules, nineteen are ⇒ rules and twelve are ⇔ rules. The average length of the simple rule contexts is 4.2 feasible pairs. Compare this with the All the examples comes from the 3935 input word pairs. average length of the 3935 final input edit sequences which is 12.6 feasible pairs. The 531 simple rules can be reduced to 44 rules (i.e. one rule per spe- cial pair) with environments consisting of disjuncted contexts. These 44 rules analyze and generate the 3935 word pairs 100% correctly. The total number of feasible pairs in the 3935 final input edit strings is 49657. In the worst case, all these feasible pairs should be present in the rule contexts to accurately model the sound changes which might occur in the input pairs. However, the actual result is much bet- ter: Our process acquires a two-level rule set which accurately models the sound changes with only 4.5% (2227) of the input feasible pairs. To obtain a prediction of the analysis and gener- ation accuracy over unseen words, we divided the 3935 input pairs into five equal sections. Each fifth was held out in turn as test data while a set of two-level rules was learned from the remaining four- fifths. The average recognition accuracy as well as the generation accuracy over the held out test data is 93.9%. 6 Summary We have described and experimentally evaluated, for the first time, a process which automatically ac- quires optimal two-level morphological rules from input word pairs. These can be used by a pub- licly available two-level morphological processor. We have demonstrated that our acquisition process is portable between at least three different languages and that an acquired rule set generalizes well to words not in the training corpus. Finally, we have shown the feasibility of automatically acquiring two- level rule sets for wide-coverage parsers, with word pairs extracted from a machine-readable dictionary. 7 Acknowledgements Part of this work was completed during the first au- thor's stay as visiting researcher at ISSCO (Univer- sity of Geneva). We gratefully acknowledge the sup- port of ISSCO, as well as the Swiss Federal Govern- ment for providing a bursary which made this visit possible. For helpful comments on an earlier draft of the paper, we wish to thank Susan Armstrong and Sabine Lehmann as well as the anonymous review- ers. References Evan L. Antworth. 1990. PC-KIMMO: A Two-level Processor for Morphological Analysis. Summer In- stitute of Linguistics, Dallas, Texas. Walter Daelemans, Peter Berck and Steven Gillis. 1996. Unsupervised Discovery of Phonological Categories through Supervised Learning of Mor- phological Rules. In COLING-96: 16th Interna- tional Conference on Computational Linguistics, pages 95-100, Copenhagen, Denmark. Michael Gasser. 1994. Acquiring Receptive Mor- phology: A Connectionist Model. In Proceedings of ACL-94. Association for Computational Lin- guistics, Morristown, New Jersey. Andrew R. Golding and Henry S. Thompson, 1985. A morphology component for language programs. Linguistics, 23:263-284. Joseph E. Grimes. 1983. Affix positions and cooc- currences: the PARADIGM program. Summer In- stitute of Linguistics Publications in Linguistics No. 69. Dallas: Summer Institute of Linguistics and University of Texas at Arlington. Lauri Karttunen and Kenneth R. Beesley. 1992. Two-level Rule Compiler. Technical Report ISTL- 92-2. Xerox Palo Alto Research Center. George Anton Kiraz. 1996. SEMHE: A general- ized two-level System. In Proceedings of ACL-96. Association for Computational Linguistics, pages 159-166, Santa Cruz, California. Kimmo Koskenniemi. 1983. Two-level Morphol- ogy: A General Computational Model for Word- Form Recognition and Production. PhD Disserta- tion. Department of General Linguistics, Univer- sity of Helsinki. Kimmo Koskenniemi. 1990. A discovery procedure for two-level phonology. Computational Lexicol- ogy and Lexicography: Special Issue dedicated to Bernard Quemada, Vol. I (Ed. L. Cignoni, C. Pe- ters). Linguistica Computazionale, Pisa, Volume VI, 1990, pages 451-465. Evelin Kuusik. 1996. Learning Morphology: Al- gorithms for the Identification of Stem Changes. In COLING-96: 16th International Conference on Computational Linguistics, pages 1102-1105, Copenhagen, Denmark. David Sankoff and Joseph B. Kruskal. 1983. Time warps, string edits, and macromolecules: the the- ory and practice of sequence comparison. Addison- Wesley, Massachusetts. Gary F. Simons. 1988. Studying morphophonemic alternation in annotated text, parts one and two. Notes on Linguistics, 41:41-46; 42:27-38. Richard Sproat. 1992. Morphology and Computa- tion. The MIT Press, Cambridge, England. Enrique Vidal, Andrés Marzal and Pablo Aibar. 1995. Fast Computation of Normalized Edit Dis- tances. IEEE Trans. Pattern Analysis and Ma- chine Intelligence, 17:899-902. Klaus Wothke. 1986. Machine learning of morpho- logical rules by generalization and analogy. In COLING-86: 11th International Conference on Computational Linguistics, pages 289-293, Bonn."
  },
  {
    "title": "A Maximum Entropy Approach to Identifying Sentence Boundaries",
    "abstract": "We present a trainable model for identifying sentence boundaries in raw text. Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of ., ?, and ! as either a valid or invalid sentence boundary. The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman-alphabet language. Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.",
    "content": "Introduction The task of identifying sentence boundaries in text has not received as much attention as it deserves. Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996)). Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992)). On first glance, it may appear that using a short list of sentence-final punctuation marks, such as ., ?, and !, is sufficient. However, these punctua- tion marks are not used exclusively to mark sen- tence breaks. For example, embedded quotations may contain any of the sentence-ending punctua- tion marks and . is used as a decimal point, in e- mail addresses, to indicate ellipsis and in abbrevia- tions. Both ! and ? are somewhat less ambiguous *The authors would like to acknowledge the support of ARPA grant N66001-94-C-6043, ARO grant DAAH04- 94-G-0426 and NSF grant SBR89-20230. but appear in proper names and may be used mul- tiple times for emphasis to mark a single sentence boundary. Lexically-based rules could be written and excep- tion lists used to disambiguate the difficult cases described above. However, the lists will never be exhaustive, and multiple rules may interact badly since punctuation marks exhibit absorption proper- ties. Sites which logically should be marked with multiple punctuation marks will often only have one ((Nunberg, 1990) as summarized in (White, 1995)). For example, a sentence-ending abbreviation will most likely not be followed by an additional period if the abbreviation already contains one (e.g. note that D.C is followed by only a single . in The presi- dent lives in Washington, D.C.). As a result, we believe that manually writing rules is not a good approach. Instead, we present a solu- tion based on a maximum entropy model which re- quires a few hints about what information to use and a corpus annotated with sentence boundaries. The model trains easily and performs comparably to sys- tems that require vastly more information. Training on 39441 sentences takes 18 minutes on a Sun Ultra Sparc and disambiguating the boundaries in a single Wall Street Journal article requires only 1.4 seconds. 2 Previous Work To our knowledge, there have been few papers about identifying sentence boundaries. The most recent work will be described in (Palmer and Hearst, To appear). There is also a less detailed description of Palmer and Hearst's system, SATZ, in (Palmer and Hearst, 1994).¹ The SATZ architecture uses either a decision tree or a neural network to disambiguate sentence boundaries. The neural network achieves 98.5% accuracy on a corpus of Wall Street Journal ¹We recommend these articles for a more compre- hensive review of sentence-boundary identification work than we will be able to provide here. articles using a lexicon which includes part-of-speech (POS) tag information. By increasing the quantity of training data and decreasing the size of their test corpus, Palmer and Hearst achieved performance of 98.9% with the neural network. They obtained simi- lar results using the decision tree. All the results we will present for our algorithms are on their initial, larger test corpus. In (Riley, 1989), Riley describes a decision-tree based approach to the problem. His performance on the Brown corpus is 99.8%, using a model learned from a corpus of 25 million words. Liberman and Church suggest in (Liberman and Church, 1992) that a system could be quickly built to divide newswire text into sentences with a nearly negligible error rate, but do not actually build such a system. 3 Our Approach We present two systems for identifying sentence boundaries. One is targeted at high performance and uses some knowledge about the structure of En- glish financial newspaper text which may not be ap- plicable to text from other genres or in other lan- guages. The other system uses no domain-specific knowledge and is aimed at being portable across En- glish text genres and Roman alphabet languages. Potential sentence boundaries are identified by scanning the text for sequences of characters sep- arated by whitespace (tokens) containing one of the symbols!, or?. We use information about the to- ken containing the potential sentence boundary, as well as contextual information about the tokens im- mediately to the left and to the right. We also con- ducted tests using wider contexts, but performance did not improve. We call the token containing the symbol which marks a putative sentence boundary the Candidate. The portion of the Candidate preceding the poten- tial sentence boundary is called the Prefix and the portion following it is called the Suffix. The system that focused on maximizing performance used the following hints, or contextual \"templates\": • The Prefix • The Suffix • The presence of particular characters in the Pre- fix or Suffix • Whether the Candidate is an honorific (e.g. Ms., Dr., Gen.) • Whether the Candidate is a corporate designa- tor (e.g. Corp., S.p.A., L.L.C.) • Features of the word left of the Candidate • Features of the word right of the Candidate The templates specify only the form of the in- formation. The exact information used by the maximum entropy model for the potential sentence boundary marked by. in Corp. in Example 1 would be: Previous WordIsCapitalized, Prefix=Corp, Suf- fix=NULL, PrefixFeature=CorporateDesignator. (1) ANLP Corp. chairman Dr. Smith resigned. The highly portable system uses only the identity of the Candidate and its neighboring words, and a list of abbreviations induced from the training data.2 Specifically, the \"templates\" used are: • The Prefix • The Suffix • Whether the Prefix or Suffix is on the list of induced abbreviations • The word left of the Candidate • The word right of the Candidate • Whether the word to the left or right of the Candidate is on the list of induced abbreviations The information this model would use for Exam- ple 1 would be: Previous Word=ANLP, Following- Word=chairman, Prefix=Corp, Suffix=NULL, Pre- fixFeature=InducedAbbreviation. The abbreviation list is automatically produced from the training data, and the contextual ques- tions are also automatically generated by scanning the training data with question templates. As a re- sult, no hand-crafted rules or lists are required by the highly portable system and it can be easily re- trained for other languages or text genres. 4 Maximum Entropy The model used here for sentence-boundary de- tection is based on the maximum entropy model used for POS tagging in (Ratnaparkhi, 1996). For each potential sentence boundary token (., ?, and !), we estimate a joint probability distribution p of the token and its surrounding context, both of which are denoted by c, occurring as an actual sentence boundary. The distribution is given by: p(b, c) = π Π=1 (b,c), where b ∈ {no, yes}, where j=1; 2 A token in the training data is considered an abbre- viation if it is preceded and followed by whitespace, and it contains a that is not a sentence boundary. the aj's are the unknown parameters of the model, and where each aj corresponds to a fj, or a feature. Thus the probability of seeing an actual sentence boundary in the context c is given by p(yes,c). The contextual information deemed useful for sentence-boundary detection, which we described earlier, must be encoded using features. For exam- ple, a useful feature might be: fj (b, c) = { 1 0 if Prefix(c) = Mr & b = no otherwise This feature will allow the model to discover that the period at the end of the word Mr. seldom occurs as a sentence boundary. Therefore the parameter cor- responding to this feature will hopefully boost the probability p(no, c) if the Prefix is Mr. The param- eters are chosen to maximize the likelihood of the training data using the Generalized Iterative Scaling (Darroch and Ratcliff, 1972) algorithm. The model also can be viewed under the Maxi- mum Entropy framework, in which we choose a dis- tribution p that maximizes the entropy H (p) H (p) = - Σp(b, c) log p(b, c) under the following constraints: Σp(b, c) f; (b, c) = ∑p(b, c) f; (b, c), 1 ≤ j ≤ k where p(b, c) is the observed distribution of sentence- boundaries and contexts in the training data. As a result, the model in practice tends not to commit towards a particular outcome (yes or no) unless it has seen sufficient evidence for that outcome; it is maximally uncertain beyond meeting the evidence. All experiments use a simple decision rule to clas- sify each potential sentence boundary: a potential sentence boundary is an actual sentence boundary if and only if p(yes c) >.5, where p(yes c) = p(yes. c) p(yes, c) + p(no, c) and where c is the context including the potential sentence boundary. 5 System Performance We trained our system on 39441 sentences (898737 words) of Wall Street Journal text from sections 00 through 24 of the second release of the Penn Treebank³ (Marcus, Santorini, and Marcinkiewicz, We did not train on files which overlapped with Palmer and Hearst's test data, namely sections 03, 04, 05 and 06. Sentences WSJ Brown 20478 51672 Candidate P. Marks 32173 61282 Accuracy False Positives False Negatives 98.8% 97.9% 201 750 171 506 Table 1: Our best performance on two corpora. 1993). We corrected punctuation mistakes and er- roneous sentence boundaries in the training data. Performance figures for our best performing system, which used a hand-crafted list of honorifics and cor- porate designators, are shown in Table 1. The first test set, WSJ, is Palmer and Hearst's initial test data and the second is the entire Brown corpus. We present the Brown corpus performance to show the importance of training on the genre of text on which testing will be performed. Table 1 also shows the number of sentences in each corpus, the number of candidate punctuation marks, the accuracy over po- tential sentence boundaries, the number of false posi- tives and the number of false negatives. Performance on the WSJ corpus was, as we expected, higher than performance on the Brown corpus since we trained the model on financial newspaper text. Possibly more significant than the system's per- formance is its portability to new domains and lan- guages. A trimmed down system which used no information except that derived from the training corpus performs nearly as well, and requires no re- sources other than a training corpus. Its perfor- mance on the same two corpora is shown in Table 2. Test False Corpus Accuracy Positives WSJ 98.0% 396 Brown 97.5% 1260 False Negatives 245 265 Table 2: Performance on the same two corpora using the highly portable system. Since 39441 training sentences is considerably more than might exist in a new domain or a lan- guage other than English, we experimented with the quantity of training data required to maintain per- formance. Table 3 shows performance on the WSJ corpus as a function of training set size using the best performing system and the more portable system. As can seen from the table, performance degrades as the quantity of training data decreases, but even --- Number of sentences in training corpus 500 1000 2000 4000 8000 16000 39441 Best performing 97.6% 98.4% 98.0% 98.4% 98.3% 98.3% 98.8% Highly portable 96.5% 97.3% 97.3% 97.6% 97.6% 97.8% 98.0% Table 3: Performance on Wall Street Journal test data as a function of training set size for both systems. with only 500 example sentences performance is bet- ter than the baselines of 64.0% if a sentence bound- ary is guessed at every potential site and 78.4% if only token-final instances of sentence-ending punc- tuation are assumed to be boundaries. 6 Conclusions We have described an approach to identifying sen- tence boundaries which performs comparably to other state-of-the-art systems that require vastly more resources. For example, Riley's performance on the Brown corpus is higher than ours, but his sys- tem is trained on the Brown corpus and uses thirty times as much data as our system. Also, Palmer & Hearst's system requires POS tag information, which limits its use to those genres or languages for which there are either POS tag lexica or POS tag annotated corpora that could be used to train auto- matic taggers. In comparison, our system does not require POS tags or any supporting resources be- yond the sentence-boundary annotated corpus. It is therefore easy and inexpensive to retrain this sys- tem for different genres of text in English and text in other Roman-alphabet languages. Furthermore, we showed that a small training corpus is sufficient for good performance, and we estimate that annotating enough data to achieve good performance would re- quire only several hours of work, in comparison to the many hours required to generate POS tag and lexical probabilities. 7 Acknowledgments We would like to thank David Palmer for giving us the test data he and Marti Hearst used for their sentence detection experiments. We would also like to thank the anonymous reviewers for their helpful insights. References Brill, Eric. 1994. Some advances in transformation- based part-of-speech tagging. In Proceedings of the Twelfth National Conference on Artificial In- telligence, volume 1, pages 722-727. Collins, Michael. 1996. A new statistical parser based on bigram lexical dependencies. In Proceed- ings of the 34th Annual Meeting of the Association for Computational Linguistics, June. Cutting, Doug, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, pages 133- 140, Trento, Italy, April. Darroch, J. N. and D. Ratcliff. 1972. Generalized Iterative Scaling for Log-Linear Models. The An- nals of Mathematical Statistics, 43(5):1470-1480. Liberman, Mark Y. and Kenneth W. Church. 1992. Text analysis and word pronunciation in text-to- speech synthesis. In Sadaoki Furui and M. Mohan Sondi, editors, Advances in Speech Signal Process- ing. Marcel Dekker, Incorporated, New York. Marcus, Mitchell, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computa- tional Linguistics, 19(2):313-330. Nunberg, Geoffrey. 1990. The Linguistics of Punc- tuation. Number 18 in CSLI Lecture Notes. Uni- versity of Chicago Press. Palmer, David D. and Marti A. Hearst. 1994. Adap- tive sentence boundary disambiguation. In Pro- ceedings of the 1994 conference on Applied Natu- ral Language Processing (ANLP), Stuttgart, Ger- many, October. Palmer, David D. and Marti A. Hearst. To appear. Adaptive multilingual sentence boundary disam- biguation. Computational Linguistics. Ratnaparkhi, Adwait. 1996. A maximum entropy model for part-of-speech tagging. In Conference on Empirical Methods in Natural Language Pro- cessing, pages 133-142, University of Pennsylva- nia, May 17-18. Riley, Michael D. 1989. Some applications of tree-based modelling to speech and language. In DARPA Speech and Language Technology Work- shop, pages 339-352, Cape Cod, Massachusetts. White, Michael. 1995. Presenting punctuation. In Proceedings of the Fifth European Workshop on Natural Language Generation, pages 107-125, Lei- den, The Netherlands. ---"
  },
  {
    "title": "The NLP Role in Animated Conversation for CALL",
    "abstract": "Language learning is a relatively new appli- cation for natural language processing (NLP) and for intelligent tutoring and learning environments (ITLEs). NLP has a crucial role to play in foreign language ITLEs, whether they are designed for explicit or implicit learning of the vocabulary and grammar. FLUENT is an implicit approach, in which NLP and shared- control animation support a two-medium conversation, designed to foster implicit learning of language. This report highlights specific ways that FLUENT already uses NLP, suggests potential benefits from additional use of NLP and grounds the method in widely used language learning pedagogy. It concludes by describing and evaluating the system's use in the classroom, with a particularly challenging kind of learner.",
    "content": "1 Introduction Language learning is a relatively new application for natural language processing (NLP), compared to translation and database interfaces. The 1988 Trieste conference on foreign language intelligent tutoring systems did not manage a proceedings, so few were the functioning systems using NLP and other techniques of artificial intelligence (AI). Indeed the field of Computer-Assisted Language Learning (CALL) has been dominated by the work of language teachers who, tired of waiting for us, have proceeded to learn some programming or hire computer science students to make their own systems, most of them just for fairly inflexible drill-and-practice. The 1990s have witnessed increasing interest in bringing AI to language learning systems, reflected in Chanier's (1994) special issue, Holland's anthology (1995) and Zock's (1996) panel. The AI is principally of two kinds: NLP and what we will call ITLE, the overlapping categories of intelligent tutoring systems and interactive learning environments. In this paper, we report on a foreign language ITLE that has moved from the lab into the classroom. After noting alternative strategies for using NLP in CALL (section 2), we describe our system and its NLP require- ments (section 3), as well as its pedagogical foundations (section 4). Finally, we describe its graduation into the classroom, for use with a particularly challenging kind of learner: the highly motivated but computer- unfamiliar immigrant. 2 A Crucial Role for NLP in CALL Developers of foreign language ITLEs have only recently begun to expand the use of NLP in language learning systems beyond relatively simple uses of syntax. More sophisticated and widespread use has awaited greater robustness and ease of use of NLP software as well as better speed and memory of the hardware. Recent improvements along these lines are playing key roles in meeting the special NLP requirements of foreign language ITLEs. Since the language is the subject matter to be learned, ITLEs must expect the input to be in error, and therefore require very robust NLP systems with error detection capabilities. In addition, conversational ITLEs must respond to student input in a time frame that parallels natural dialog. Advances in parsing technology and faster machines with more memory are making NLP attractive to the developers of ITLEs. The flexibility, increased coverage and other advantages of NLP are finding their way into the design of language learning systems. The issue is no longer whether language learning systems should use NLP, but how NLP can best be applied to language learning. One possible strategy for creating a language ITLE can be called the direct approach. A tenet for ITLEs in general is that they represent the knowledge to be learned in a domain expertise module. In the case of language learning, the knowledge to be learned already exists in computational form as the lexicon and grammar of a NLP system, so the idea would be to incorporate these NLP knowledge bases as the ITLE domain expertise. However, successful language use, in contrast to other ITLE domains, is not just a matter of overt stepwise reasoning. The goal of language learning is not (only) to conjugate verbs or passivize sentences, but to understand and produce meaningful language in context. Even for the limited role of teaching grammar explicitly, a direct approach encounters two obstacles. First, a performance-oriented representation of domain knowledge is typically not structured in a way that is cognitively suitable for learners (Clancey, 1987). Moreover, NLP knowledge bases cannot be used directly in explanation since their representation language is incomprehensible to students (Chanier et al., 1992). To avoid these pedagogical problems, we turn to an alternative strategy that deploys NLP principally in the ITLE's learning environment, rather than as explicit expertise. This approach is more concerned with the performance of the NLP rather than its competence. ITLES applying NLP in this manner immerse the student in the language by situating the student in a life-like conversational setting. Pedagogical motivation for this kind of foreign language learning environment comes from classroom methods like Total Physical Response or TPR (Asher, 1977) and the Natural Approach (Krashen and Terrell, 1983). These techniques require only comprehension at first, as students act out appropriate responses to the teacher's commands. As student actions indicate sufficient progress, constituents of the utterances are recombined to increase the skill at understanding. There are also questions, which TPR students answer by pointing. In the natural approach, production is included, beginning with questions that take very short answers. There is a gradual progression from yes-no questions to other single-word-answer questions and either-or questions, and then on to phrases and ultimately sentences. The flexibility, efficiency and error handling of NLP is crucial to implementing the above pedagogy in an ITLE. A system that puts NLP to crucial use in support of these pedagogical considerations is FLUENT, to which we now turn. 3 FLUENT and NLP FLUENT uses NLP to converse with the student in the context of a realistic microworld situation. Actions performed by both the student and the system tutor appear as context-dependent animation and cause updates in the underlying representation. The generativity of both the NLP and the animation is crucial to the flexibility of this conversation, allowing it to be steered in pedagogically useful directions, yet also permitting student exploration. The NLP module generates descriptions, commands and questions that may be related to actions that the student carries out graphically with the mouse or may be related to the states of objects in the current situation. The generated language appears as both text and speech. Figure 1 shows FLUENT's Kitchen Microworld. Tools Control: Tutor Quit Style Variety Activity Who Controls Conversational Style: Quizmaster 11:54:10 Activity: COOK POTATOES Kitchen World Pause Tutor: What did I turn on? Student: Figure 1. A FLUENT Microworld Lexicons Natural Drawing Tool Images Animations Graphics Processor Grammars Language Fluent Executive Processor View Tool Views Objects Situation Actions Reasoner Plans Pedagogical Processor Tutorial, Schemas Tutorial Schema Tool Figure 2. FLUENT Architecture This section describes first the overall architecture of FLUENT and then turns to the specific NLP components, noting their roles and requirements in the context of FLUENT's pedagogical goals. Flexibility, extendibility and teacher involvement are achieved in FLUENT through its layered architecture, shown in Figure 2. In this framework, independent processes work on declarative structures that have been acquired by the system through tools that do not demand a knowledge of programming or linguistic theory (Schoelles and Hamburger, 1996). This work shares the interface orientation of Caldwell and Korelsky (1994), but is more abstract and flexible in that user- specifications are independent of, and combinable with, domain plans. On the other hand we do not follow work like Traum and Allen (1994), in pushing toward computation about more and more discourse phenomena The outer ring of Figure 2 depicts the knowledge acquisition level. It identifies existing tools for building tutorial schemas, language usage structures and the graphics of objects. The tutorial schema tool lets a teacher express pedagogical expertise in the form of lessons. The view tool allows the teacher some degree of control over the language generated by the system. The teacher inputs the language specifications for a lesson by manipulating a graphical user interface. The tools process these specifications to produce instantiated objects for the declarative level. The knowledge base, in the second ring, consists of: • Tutorial Schemas - structures, created by a teacher using tools, that coordinate the activities, conversational style and conversational variety of FLUENT when the system is in tutoring mode. • Views - structured objects that provide language specifications and constraints to achieve conversational variety, control over the difficulty of input presented to the student, and situational flexibility. • Lexicon - currently 550 words entered in base form, with information for finding inflected forms and with irregular forms are subentries. Contains both subcategorization and theta grids. • Grammar - context-free productions plus reduction functions that restrict the structure, perform agreement and other checks, build the parse tree and reference the case frame interpreter. See Felshin (1995) for more on the lexicon and grammar. • Plans and actions - rules, expressed in terms of classes of objects, that specify how the tutor organizes activity in the microworld, how the student initiates actions and how the microworld state is altered. • Microworld - a hierarchy of objects with their situational and graphical properties, as well as the associated plans and actions The procedural level in the third ring shows the processes that operate on the knowledge base to provide language tutoring or a learning environment. The main processes that achieve these functions are: • Pedagogical Processing - to decide what the tutor will do and say next, by selecting a tutorial schema from the knowledge base and executing it. • NLP Processing - a generative system which interprets language specifications in declarative form. These specifications are derived from the tutorial schema (incorporating views and microworld information) to construct an abstract syntactic structure, which in turn undergoes grammatical and morphological processing to produce a sentence. • Situation Reasoning - a plan executor that instantiates the plan and action rules based on the current situation. • Microworld Processing - maintains object information and performs the graphical actions in the microworld. The innermost ring represents the system executive which controls the interaction between the student and the system and coordinates the main processes based on this interaction. The student's interaction with the system takes two forms, learning and control. In a learning interaction, the student causes linguistic and graphical events in the microworld. In a control interaction the student can take control of choices about the plans and actions, the conversational style and the degree of linguistic variety. Further details on the operation of FLUENT can be found in Hamburger (1995) and in Schoelles and Hamburger (1996). The remainder of this section describes the functions of NLP within the FLUENT architecture. We point out their importance in this framework to FLUENT's pedagogical goals. As seen in Figure 2, NLP is involved in three levels of the architecture: the knowledge acquisition level, the declarative level and the procedural level. FLUENT, like many other natural language generation systems distinguishes between an early or deep phase that determines content and organization (what to say) and a later or surface phase for lexical content, morphology, and syntactic structure (how to say it). The deep phase is driven by the current tutorial schema, which, as noted earlier, coordinates the activities, conversational style and conversational variety. A tutorial schema is a list of triples, each consisting of a plan or action, an interaction type and a list of views, as in the example in Figure 3. The resulting conversation depends in part on student responses and some randomized choices, but would be something like: I am picking up a cup. [Tutor puts cup on table.] The cup is on the table. There is another cup on the sink. What is on the table? [Student: The cup.] Good! Pick up a cup. [Student turns on the water(!)] You did not pick up a cup, but you turned on the faucet. Turn off the faucet. [Student does so.] Great! You turned off the faucet. Pick up a cup... Plan/Action Interaction View Pick-up Tourguide Present-Progresv Put-down Tourguide (Location Location-other) None Quizmaster Wh-on-top Pick-up Commander (Command-indef Present-action Command-fail) Figure 3. Part of a Tutorial Schema (Lesson) A plan groups together subplans and individual actions to achieve a goal. The plans and actions are flexible in that their arguments are variables which can be bound when the schema is created, or at the time of execution. The sequence of plans and actions determines what to talk about by specifying predicates and arguments. Another component of a tutorial schema is the interaction type. There are three interaction types that can be specified in a tutorial schema. In Tourguide mode, the system performs the actions as well as generating language about the action or state. Commander mode is similar to Tourguide mode, except that for each action in the plan the system tells the student what to do (i.e. gives the student a command), the student then uses the mouse to perform the action command. The system will respond based on whether the student successfully or unsuccessfully carried out the command. In Quizmaster mode, the system performs an action and then asks a question about the action performed. The student responds by typing in an answer (i.e. a noun phrase) which is checked by the system. How to talk about the predicates and arguments or what type of question to ask is specified in the view. This data structure, whether selected or created using the view tool, becomes part of the tutorial schema and serves as the input to the surface generator. As an example, a portion of the Command-fail view used in Figure 3 is spelled out in Figure 4. It gives rise to the final portion of the conversation example accompanying Figure 3. View-Type: Complex Relation: Contrast Subview-1: View-Type: Polarity: Subtype: Tense: Subview-2: View-Type: Polarity: Simple Negative Action Past Simple Positive Figure 4 - A Specific View The view is the interface to the syntactic/semantic component. In a tutorial schema, an interaction type and view can by specified for entire plans, actions within a plan, or individual actions. Thus, at each step in the schema, how to talk about the predicate/argument structure can be specified. Views are also specified to handle error conditions. Some of the parameters specified in a view are the tense, aspect, modality, type of question (Wh or Yes/No), minimum or maximum number of arguments, and definiteness of a noun phrase. Views also specify whether to talk about the action itself or the state of the world before or after the action. Views mediate between the language teaching functions of the system and the NLP component that performs the actual text generation. This component is the NLG system developed for the Athena Project at M.I.T. by Sue Felshin. It is responsible for morphology, syntax, and some semantic functions. It was chosen because it does provide some semantics and a well-defined language independent interface called an Interlingua Structure (ILS). The ILS is a specification of data structures containing syntactic and semantic information. These structures are built by the knowledge module and passed to the NLG. The specific semantic features that this framework provides is an extensive hierarchical system of theta roles. Generation of linguistic responses by the system translates from the interlingua structure to Case Frame(CF)-structure to D-structure and to S-structure in separate steps. First, focusing chooses how to refer to objects. CF-structure is then built. Next, D-structure is built by applying the parameters of the English to principles of universal grammar. S-structure is produced by applying code transformation to D- structure. Finally, the S-structure is flattened to produce printable text (Felshin 1995). A limitation of the Athena framework is the lack of an interface tool to the lexicon. Lexical entries can only be added or changed by editing the source files and recompiling the lexicon. A task that can not be easily performed by a language teacher or exercise designer. Another limitation is that in some instances when variation in surface structure is possible the choice is made randomly thereby diminishing the application's control over the language generated. 4 FLUENT Pedagogy and NLP Even a flexible, extendible and teacher-accessible system may be of little value unless it is also engaging and beneficial to students. Our initial results, presented in the next section, suggest that it is engaging to the students who have used it. Here we argue that one should expect it to be beneficial as well, in view of its relationship to several pedagogical issues. Language learning pedagogy is made complex by the great complexity and variety of both languages and learners. It is therefore out of the question to make a universally useful pedagogy-based requirements statement and proceed to the ultimate, correct system design. Our aim in designing FLUENT has thus never been to create a panacea, but we do claim to be developing an important form of language learning interaction. Besides supporting this claim, we also point out the crucial role of NLP in enabling our ambitious approach. We consider five pedagogical issues. The first is specific to the domain of language: the explicit teaching of language facts versus implicit learning from ordinary conversation about non-language topics. The next two issues - situatedness and error-handling - are of general concern but play out in unique ways in the domain of language. Finally, active learning and the choice of difficulty level are issues that relate to general cognitive considerations and so are relevant for the learning of other domains as well as for language. The explicitness issue arises from a curious observation: all children succeed at implicit learning of their first language, while many adults fail in attempts to learn a second one. Is it that children spend all day at it, or, lacking a language, are more motivated? Do adults know too much, including another language, or have they lost some language capacity in maturation? Or are they just not properly taught? If it's the teaching, can we adapt the child's immersion experience to adults, taking care to make the situation simple and clear at all times as we do with toddlers? Success with immersion is not confined to first- language learning. Many an adult who has experienced second-language immersion will testify to its benefits, and Krashen reports classroom benefits with his method. FLUENT provides a conversational setting where a linguistic fact is not presented explicitly but rather can be indirectly acquired by the student through exposure to it in a meaningful context. To learn to communicate, one must sooner or later practice doing so, and FLUENT permits practice in an ongoing and completely understandable situation. NLP plays the crucial role of providing the variability required by the conversation. Even fairly simple microworlds lead to a combinatorial explosion of possible events and situations, along with a combinatorial explosion of sentences that must occur in appropriate circumstances. Next, consider the pedagogical proposal that situating a realistic activity in an authentic environment can promote learning both cognitively and motivationally. Without pursuing this issue in general, we note that it has a special significance for language, because the choice of a linguistic construct often hinges on aspects of the current situation, including the conversation itself. These pragmatic and discourse phenomena can be made to arise in FLUENT, since it conducts conversation in an ongoing situation. The resulting practice on these aspects of language can lead to the kind of implicit learning described above. An example of a situation-based aspect of language that FLUENT presents to students is that the choice of a definite article may depend on there being only one object of some type. Another is that different verb forms are used in descriptions that occur during versus after the action described. An example that relates to the conversation itself, as opposed to the situation, is that the discourse center is a prime contender for pronominal referent. Another is the correlation of imperative, interrogative and declarative forms with the type of conversational interaction. Pragmatics and discourse are very difficult for NLP. Although FLUENT does not engage in sophisticated NLP-style discourse reasoning, its tutorial schema tool and view tool make it easy for us or a teacher to prescribe the right discourse results without losing computational flexibility at the semantic and syntactic levels. A third pedagogical issue is the handling of syntactic and semantic errors made by the student. This issue is also related to the explicitness issue. For people and systems that teach linguistic form explicitly, error explanations are paramount. In the communicative approach, errors are tolerated in order to achieve the primary goal of communication. It is argued that explanations are often not well understood and in any case disrupt communication. A fully communicative system needs robust NLP on the language input side to make some sense of what the student is trying to communicate even in the presence of errors. We have had very positive response from teachers to a different use of NLP, on the generation side. In FLUENT, a misconception on the student's part is often indicated by an incorrect action performed by the student in relation to a command given by the system. In such a case, the system can (if the teacher has so specified) comment on the contrast between the instruction and the actual student action. To do so requires a two-clause sentence, of the form \"I asked you to <do[present] action-x>, but you <do[past] action- y>.\" Generating such responses is clearly a job for NLP. Before getting back to a repeat of the original command, the NLP module generates an instruction to perform an action to undo the original erroneous action. A fourth pedagogical issue is active versus passive learning. Active involvement by the student has proven successful in many domains. For language learning in particular, TPR and the natural approach (see section 2) let students respond to a human teacher both via language, in the case of questions, and with actions, for commands. In the computational realm, ITLES use problem-solving environments to afford students the opportunity to act. In its tutorial mode, FLUENT is an ITLE for TPR. Beyond that, in its student mode (and student-driven aspects of tutorial mode too) the student can actively participate by initiating and controlling actions though the mouse. The NLP component can express these student actions as verbs. The benefit of a NLP approach over a direct mapping between mouse actions and stored text, or even templates with fillers, is that a wider coverage of language can be achieved in an efficient manner. For example, a click on a graphical object could represent the action of picking up that object. To express this in past tense versus present tense only requires a change of a single parameter to the NLP module, whereas with a direct mapping two separate sentences must be stored. Clicking on a different object for a NLP based system again requires only the changing of a single parameter. Moreover, the same object can be described in terms of different properties at various times, according to the situation and discourse. The combinatorial nature of this design clearly permits the system to expose the student to a greater variety of language than would be practical for a non-NLP system. Finally, a NLP component provides the flexibility to give the student material at the right level of difficulty. In FLUENT, simply by varying a parameter the system can generate a command, a declarative statement or a question. Commands require understanding, but questions are more demanding, in that they require language output from the student as well. Another way to vary difficulty involves verbs with optional arguments. Use of NLP lets us choose whether a sentence is to contain all the arguments that a verb can subcategorize for or only the required ones. Again this is done by changing a single parameter input to the NLP module. 5 FLUENT in the ESL Classroom FLUENT is currently being evaluated in an English as a Second Language (ESL) environment. The Arlington Education and Employment Program is using FLUENT in its language laboratory. The students in this program are non-English speaking adults. The evaluation results reported in this paper are from two groups of students at level 2 of the program, with limited English and little or no exposure to computer- based systems. What exposure they have had has been with text-based systems mainly for vocabulary. FLUENT is their first exposure to an interactive conversational language learning system. The first group of 14 students generally worked in groups of two or three in a cooperative effort. In the second group, students worked individually. This time, students were so engrossed that they were allowed to work longer, so there was time for only six of them. The most difficult aspect of the interface for the students to master was the use of the mouse. However, the students who used the system a second or third time showed increased proficiency with the mouse. The evaluation procedure required each student to complete three lessons that had been prepared using the tutorial schema tool, with the advice of their teacher. This was followed by approximately 5 minutes in Movecaster mode to explore other facets of the microworld. All three lessons used the same plan, but progressively harder interaction types: first Tourguide, then Commander, and finally Quizmaster. In Commander mode, a student who did y when told to do x was told, in effect, \"You did not do x. Undo y.\" In Quizmaster, each WH-question required a noun phrase. If the student's answer was not correct, the system displayed a dialog box with a list of possible answers. The student was then to click on the correct choice. If the student made another error at this point - something that happened only once - the system displayed the correct answer. The students were asked to complete the questionnaire shown in Figure 5. The results are shown in Figure 6. Ten of the 14 students in the first group and all of the second group responded. 1. How well did you like it? It was _ VERY GOOD _GOOD_POOR 2. How good was it for learning? It was _VERY GOOD_ GOOD _POOR 3. Was it too hard? Was it too easy? _TOO HARD_GOOD_TOO EASY 4. Was the voice OK? _YES_NO 5. Do you want to say more? _____________ Figure 5. The questionnaire 1. 5 VERY GOOD 5 GOOD 0 POOR 2. 5 VERY GOOD 5 GOOD 0 POOR 3. 1 TOO HARD 9 GOOD 0 TOO EASY 4. 8 YES 1 NO 1 <no response> Figure 6a. Questionnaire Results - Group 1 1. 6 VERY GOOD 0 GOOD 0 POOR 2. 5 VERY GOOD 4 GOOD 0 POOR 3. 1 TOO HARD 1 GOOD 1 TOO EASY 4. 6 YES 0 NO 0 <no response> Figure 6b. Questionnaire Results - Group 2 Observations made during the sessions point to some strong points. The verbal protocol shows that the students liked the positive responses in Commander mode. When the student successfully completes the action, the system responds with a positive comment such as \"Great, you picked up the cup.\" or \"Good, you turned on the faucet.\" The students also liked being able to pick the correct answer in Quizmaster mode. They often knew the answer but didn't know how to spell it. When they were able to choose the answer they often carefully noted the spelling and were able to spell the word correctly the next time. Observations made during the first session indicated some weak points. These led us to make changes which apparently led to an even better response in the second session. Several students were frustrated in the first session when the system was doing the action and generating language, because the sequence of actions was occurring too rapidly for them to understand the utterances. They would be concentrating on what was said and their train of thought would be broken when the sentence was overwritten with the text of the next action. This was corrected for the second group by providing a button that allowed the student to advance the system to the next action. Another annoyance for the student was when a response was given that was correct but not exactly what the lesson required. For example, one action is to \"turn on the faucet\". When asked \"What did I turn on?\", several students responded \"the water\". Although not what the system was expecting (i.e. \"the faucet\") this response should be accepted. For the second session the system was able to accept various correct answers. The speech generated is a female voice using the text- to-speech capabilities of the Macintosh. A point of concern to us was how the students would react to this somewhat automated sounding voice. This decision to use text-to-speech was made after consulting with teachers of ESL They feel the quality is good enough for the level of students we are targeting. The student responses show that in general they were not distracted by the quality of the voice. They seem to be concentrating on the syntax and meaning of the sentence more than the pronunciation. 6 Conclusions NLP has an important role to play in foreign language intelligent tutoring and learning environments. The generativity and variability that NLP brings to the conversational tutor FLUENT are crucial to its meeting its pedagogical goals. In particular, the layered design of the NLP, from teacher specification to actual text is key to allowing teacher control over the language input to the student, while at the same time producing language that is meaningful to the student in the evolving situation. 7 Acknowledgements Our thanks to the Arlington Education and Employment Program, and especially to Daniel Norton for his support and suggestions. References Asher, A. 1977. Learning Another Language Through Actions: The Complete Teacher's Guidebook. Sky Oaks Productions, Los Gatos, California. Caldwell, D. and Korelsky, T. 1994 Bilingual Generation of Job Descriptions from Quasi- Conceptual Forms. 4th Conference on Applied Natural Language Processing, Stuttgart, Germany. Chanier, T. 1994. Special Issue on Language Learning: Editor's Introduction Journal of Artificial Intelligence in Education. Association for the Advancement of Computing in Education, Charlottesville, Virginia. Chanier, T., Pengelly, M., Twidale, M. and Self, J. 1992 Conceptual Modelling in Error Analysis in Computer-Assisted Language Learning Systems. In M. L. Swartz and M. Yazdani, editors, Intelligent Tutoring Systems for Foreign Language Learning. Springer-Verlag, Berlin, Germany. Clancey, W. J. 1987. Knowledge-Based Tutoring The GUIDON Program. The MIT Press, Cambridge, Massachusetts. Felshin, S. 1995. The Athena Language Learning Project NLP System: A Multilingual System for Conversation-Based Language Learning. In V. M. Holland, J. D. Kaplan, M. R. Sams, editors, Intelligent Language Tutors: Theory Shaping Technology. Lawrence Erlbaum Associates, Mahwah, New Jersey. Hamburger, H. 1995. Tutorial Tools for Language Learning by Two-Medium Dialogue. In V. M. Holland, J. D. Kaplan, M. R. Sams, editors, Intelligent Language Tutors: Theory Shaping Technology. Lawrence Erlbaum Associates, Mahwah, New Jersey. Holland, V. M. 1995. Introduction: The Case for Intelligent CALL. In V. M. Holland, J. D. Kaplan, M. R. Sams, editors, Intelligent Language Tutors: Theory Shaping Technology. Lawrence Erlbaum Associates, Mahwah, New Jersey. Krashen, S. and Terrell, Τ. 1983. The Natural Approach: Language Acquisition in the Classroom. Pergamon, Oxford, England. Schoelles, M. and Hamburger, H. 1996 Teacher- Usable Exercise Design Tools. In C. Frasson, G. Gauthier and A. Lesgold, editors, Intelligent Tutoring Systems: Proceedings of Third International Conference, Springer-Verlag, Berlin, Germany. Traum, D. R. and Allen, J. 1994 Discourse Obligations in Dialogue Processing. Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Las Cruces, New Mexico. Zock, M. 1996. Computational Linguistics and its Use in Real World: the Case of Computer Assisted- Language Learning. In COLING-96 Proceedings. Center for Sprogteknologi, Copenhagen, Denmark."
  },
  {
    "title": "Automatic Extraction of Subcategorization from Corpora",
    "abstract": "We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount¹.",
    "content": "1 Motivation Predicate subcategorization is a key component of a lexical entry, because most, if not all, recent syn- tactic theories 'project' syntactic structure from the lexicon. Therefore, a wide-coverage parser utilizing such a lexicalist grammar must have access to an accurate and comprehensive dictionary encoding (at a minimum) the number and category of a predi- cate's arguments and ideally also information about control with predicative arguments, semantic selec- tion preferences on arguments, and so forth, to allow the recovery of the correct predicate-argument struc- ture. If the parser uses statistical techniques to rank analyses, it is also critical that the dictionary encode the relative frequency of distinct subcategorization classes for each predicate. ¹This work was supported by UK DTI/SALT project 41/5808 'Integrated Language Database', CEC Telematics Applications Programme project LE1-2111 'SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering', and by SERC/EPSRC Ad- vanced Fellowships to both authors. We would like to thank the COMLEX Syntax development team for al- lowing us access to pre-release data (for an early exper- iment), and for useful feedback. Several substantial machine-readable subcatego- rization dictionaries exist for English, either built largely automatically from machine-readable ver- sions of conventional learners' dictionaries, or manu- ally by (computational) linguists (e.g. the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987); the COMLEX Syntax dictionary, Grishman et al. (1994)). Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, there- fore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev & Briscoe, 1989; see also section 3.1 below for an example). Furthermore, manual encod- ing is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different sub- categorizations), or other (sub)languages. These problems are compounded by the fact that predi- cate subcategorization is closely associated to lexical sense and the senses of a word change between cor- pora, sublanguages and/or subject domains (Jensen, 1991). In a recent experiment with a wide-coverage pars- ing system utilizing a lexicalist grammatical frame- work, Briscoe & Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. The close connection between sense and subcategorization and between subject do- main and sense makes it likely that a fully accurate 'static' subcategorization dictionary of a language is unattainable in any case. Moreover, although Sch- abes (1992) and others have proposed 'lexicalized' probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them. These problems suggest that automatic construc- tion or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few verbal subcategorization classes have been reported by Brent (1991, 1993), Manning (1993), and Ush- ioda et al. (1993). In these experiments the max- imum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et al. at- tempt to derive relative subcategorization frequency for individual predicates. We describe a new system capable of distinguish- ing 160 verbal subcategorization classes-a superset of those found in the ANLT and COMLEX Syn- tax dictionaries. The classes also incorporate infor- mation about control of predicative arguments and alternations such as particle movement and extra- position. We report an initial experiment which demonstrates that this system is capable of acquir- ing the subcategorization classes of verbs and the relative frequencies of these classes with compara- ble accuracy to the less ambitious extant systems. We achieve this performance by exploiting a more sophisticated robust statistical parser which yields complete though 'shallow' parses, a more compre- hensive subcategorization class classifier, and a pri- ori estimates of the probability of membership of these classes. We also describe a small-scale ex- periment which demonstrates that subcategorization class frequency information for individual verbs can be used to improve parsing accuracy. 2 Description of the System 2.1 Overview The system consists of the following six components which are applied in sequence to sentences contain- ing a specific predicate in order to retrieve a set of subcategorization classes for that predicate: 1. A tagger, a first-order HMM part-of-speech (PoS) and punctuation tag disambiguator, is used to assign and rank tags for each word and punctuation token in sequences of sentences (El- worthy, 1994). 2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an en- hanced version of the GATE project stemmer (Cunningham et al., 1995). 3. A probabilistic LR parser, trained on a tree- bank, returns ranked analyses (Briscoe & Car- roll, 1993; Carroll, 1993, 1994), using a gram- mar written in a feature-based unification gram- mar formalism which assigns 'shallow' phrase structure analyses to tag networks (or 'lattices') returned by the tagger (Briscoe & Carroll, 1994, 1995; Carroll & Briscoe, 1996). 4. A patternset extractor which extracts sub- categorization patterns, including the syntac- tic categories and head lemmas of constituents, from sentence subanalyses which begin/end at the boundaries of (specified) predicates. 5. A pattern classifier which assigns patterns in patternsets to subcategorization classes or re- jects patterns as unclassifiable on the basis of the feature values of syntactic categories and the head lemmas in each pattern. 6. A patternsets evaluator which evaluates sets of patternsets gathered for a (single) predicate, constructing putative subcategorization entries and filtering the latter on the basis of their re- liability and likelihood. For example, building entries for attribute, and given that one of the sentences in our data was (la), the tagger and lemmatizer return (1b). (1) a He attributed his failure, he said, to no<blank>one buying his books. b he_PPHS1 attribute_VVD his_APP$ fail- ure_NN1 he_PPHS1 say_VVD ,, to_II no<blank>one_PN buy_VVG his_APP$ book_NN2 ,, ,, (1b) is parsed successfully by the probabilistic LR parser, and the ranked analyses are returned. Then the patternset extractor locates the subanalyses con- taining attribute and constructs a patternset. The highest ranked analysis and pattern for this example are shown in Figure 12. Patterns encode the value of the VSUBCAT feature from the VP rule and the head lemma(s) of each argument. In the case of PP (P2) arguments, the pattern also encodes the value of PSUBCAT from the PP rule and the head lemma(s) of its complement(s). In the next stage of process- ing, patterns are classified, in this case giving the subcategorization class corresponding to transitive plus PP with non-finite clausal complement. The system could be applied to corpus data by first sorting sentences into groups containing in- stances of a specified predicate, but we use a different strategy since it is more efficient to tag, lemmatize and parse a corpus just once, extracting patternsets for all predicates in each sentence; then to classify the patterns in all patternsets; and finally, to sort and recombine patternsets into sets of patternsets, one set for each distinct predicate containing pat- ternsets of just the patterns relevant to that predi- cate. The tagger, lemmatizer, grammar and parser have been described elsewhere (see previous refer- ences), so we provide only brief relevant details here, concentrating on the description of the components 2The analysis shows only category aliases rather than sets of feature-value pairs. Ta represents a text adjunct delimited by commas (Nunberg 1990; Briscoe & Carroll, 1994). Tokens in the patternset are indexed by sequen- tial position in the sentence so that two or more tokens of the same type can be kept distinct in patterns. (Tp (V2 (N2 he_PPHS1) (V1 (VO attribute_VVD) (N2 (DT his_APP$) (N1 (NO (NO failure_NN1) (Ta (Pu,,) (V2 (N2 he_PPHS1) (V1 (VO say_VVD))) (Pu,,))))) (P2 (P1 (PO to_II) (N2 no<blank>one_PN) (V1 (VO buy_VVG) (N2 (DT his_APP$) (N1 (NO book_NN2))))))))) (1 ((((he:1 PPHS1)) (VSUBCAT NP_PP) ((attribute: 6 VVD)) ((failure: 8 NN1)) ((PSUBCAT SING) ((to:9 II)) ((no<blank>one: 10 PN)) ((buy:11 VVG)))) 1)) Figure 1: Highest-ranked analysis and patternset for (1b) of the system that are new: the extractor, classifier and evaluator. The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators). It is 'shallow' in that no atof which thetempt is made to fully anal- yse unbounded dependencies. However, the distinc- tion between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977), by Chomsky-adjunction to maximal projections of ad- juncts (XP → XP Adjunct) as opposed to 'govern- ment' of arguments (i.e. arguments are sisters within X1 projections; X1 → X0 Argl... ArgN). Further- more, all analyses are rooted (in S) so the grammar assigns global, shallow and often 'spurious' analy- ses to many sentences. There are 29 distinct val- ues for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class head lemmas of arguments, such as it (dummy sub- jects), whether (wh-complements), and so forth, to classify patterns as evidence for one of the 160 sub- categorization classes. Each of these classes can be parameterized for specific predicates by, for exam- ple, different prepositions or particles. Currently, the coverage of this grammar-the proportion of sen- tences for which at least one analysis is found is 79% when applied to the Susanne corpus (Sampson, 1995), a 138K word treebanked and balanced subset of the Brown corpus. Wide coverage is important since information is acquired only from successful parses. The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second. 2.2 The Extractor, Classifier and Evaluator The extractor takes as input the ranked analyses from the probabilistic parser. It locates the subanal- yses around the predicate, finding the constituents identified as complements inside each subanalysis, and the subiect clause preceding it. Instances of passive constructions are recognized and treated spe- cially. The extractor returns the predicate, the VSUBCAT value, and just the heads of the comple- ments (except in the case of PPs, where it returns the PSUBCAT value, the preposition head, and the heads of the PP's complements). The subcategorization classes recognized by the classifier were obtained by manually merging the classes exemplified in the COMLEX Syntax and ANLT dictionaries and adding around 30 classes found by manual inspection of unclassifiable pat- terns for corpus examples during development of the system. These consisted of some extra patterns for phrasal verbs with complex complementation and with flexible ordering of the preposition/particle, some for non-passivizable patterns with a surface direct object, and some for rarer combinations of governed preposition and complementizer combina- tions. The classifier filters out as unclassifiable around 15% of patterns found by the extractor when run on all the patternsets extracted from the Su- sanne corpus. This demonstrates the value of the classifier as a filter of spurious analyses, as well as providing both translation between extracted pat- terns and two existing subcategorization dictionar- ies and a definition of the target subcategorization dictionary. The evaluator builds entries by taking the pat- terns for a given predicate built from successful parses and records the number of observations of each subcategorization class. Patterns provide sev- eral types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a pred- icate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern. Currently, we simply select the pattern supported by the highest ranked parse. However, we are experimenting with alterna- tive approaches. The resulting set of putative classes for a predicate are filtered, following Brent (1993), by hypothesis testing on binomial frequency data. Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern support- ing an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabili- ties experimentally on the basis of the behaviour of the extractor. We estimate this probability more di- rectly by first extracting the number of verbs which are members of each class in the ANLT dictionary (with intuitive estimates for the membership of the novel classes) and converting this to a probability of class membership by dividing by the total number of verbs in the dictionary; and secondly, by multiplying the complement of these probabilities by the proba- bility of a pattern for class i, defined as the number of patterns for i extracted from the Susanne corpus divided by the total number of patterns. So, p(v -i), the probability of verb v not of class i occurring with a pattern for class i is: p(v-i) = (1- |anlt_verbs_in_class_i||patterns-for-i |anlt_verbs| patterns The binomial distribution gives the probability of an event with probability p happening exactly m times out of n attempts: P(m,n,p) = n! m!(nm)! pm (1-p)n-m The probability of the event happening mor more times is: n P(m+,n,p) = ∑ P(i, n, p) i=m Thus P(m,n,p(v -i)) is the probability that mor more occurrences of patterns for i will occur with a verb which is not a member of i, given n occur- rences of that verb. Setting a threshold of less than or equal to 0.05 yields a 95% or better confidence that a high enough proportion of patterns for i have been observed for the verb to be in class 23. 2.3 Discussion Our approach to acquiring subcategorization classes is predicated on the following assumptions: • most sentences will not allow the application of all possible rules of English complementation; • some sentences will be unambiguous even given the indeterminacy of the grammar¹; 3 Brent (1993:249-253) provides a detailed explana- tion and justification for the use of this measure. In fact, 5% of sentences in Susanne are assigned only a single analysis by the grammar. • many incorrect analyses will yield patterns which are unclassifiable, and are thus filtered out; • arguments of a specific verb will occur with greater frequency than adjuncts (in potential argument positions); • the patternset generator will incorrectly output patterns for certain classes more often than oth- ers; and • even a highest ranked pattern for i is only a probabilistic cue for membership of i, so mem- bership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i. This simple automated, hybrid linguis- tic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicog- raphers (Meyers et al., 1994), who propose five cri- teria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature. Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often 'understood' or implied if missing. Others are syntactic tests involving diathesis alternation possi- bilities (e.g. passive, dative movement, Levin (1993)) which require recognition that the 'same' argument, defined usually by semantic class / thematic role, is occurring across argument positions. We hope to ex- ploit this information where possible at a later stage in the development of our approach. However, recog- nizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes. At the moment, we exploit linguistic information about the syntactic type, obli- gatoriness and position of arguments, as well as the set of possible subcategorization classes, and com- bine this with statistical inference based on the prob- ability of class membership and the frequency and reliability of patterns for classes. 3 Experimental Evaluation 3.1 Lexicon Evaluation Method In order to test the accuracy of our system (as de- veloped so far) and to provide empirical feedback for further development, we took the Susanne, SEC (Taylor & Knowles, 1988) and LOB corpora (Gar- side et al., 1987) -a total of 1.2 million words-and extracted all sentences containing an occurrence of one of fourteen verbs, up to a maximum of 1000 citations of each. These verbs, listed in Figure 2, were chosen at random, subject to the constraint that they exhibited multiple complementation pat- terns. The sentences containing these verbs were tagged and parsed automatically, and the extractor, classifier and evaluator were applied to the resulting successful analyses. The citations from which entries were derived totaled approximately 70K words. The results were evaluated against a merged entry for these verbs from the ANLT and COMLEX Syn- tax dictionaries, and also against a manual analysis of the corpus data for seven of the verbs. The process of evaluating the performance of the system relative to the dictionaries could, in principle, be reduced to an automated report of type precision (percentage of correct subcategorization classes to all classes found) and recall (percentage of correct classes found in the dictionary entry). However, since there are disagree- ments between the dictionaries and there are classes found in the corpus data that are not contained in either dictionary, we report results relative both to a manually merged entry from ANLT and COMLEX, and also, for seven of the verbs, to a manual anal- ysis of the actual corpus data. The latter analysis is necessary because precision and recall measures against the merged entry will still tend to yield in- accurate results as the system cannot acquire classes not exemplified in the data, and may acquire classes incorrectly absent from the dictionaries. We illustrate these problems with reference to seem, where there is overlap, but not agreement between the COMLEX and ANLT entries. Thus, both predict that seem will occur with a sentential complement and dummy subject, but only ANLT predicts the possibility of a 'wh' complement and only COMLEX predicts the (optional) presence of a PP[to] argument with the sentential complement. One ANLT entry covers two COMLEX entries given the different treatment of the relevant complements but the classifier keeps them distinct. The corpus data for seem contains examples of further classes which we judge valid, in which seem can take a PP[to] and infinitive complement, as in he seems to me to be insane, and a passive participle, as in he seemed depressed. This comparison illustrates the problem of errors of omission common to computa- tional lexicons constructed manually and also from machine-readable dictionaries. All classes for seem are exemplified in the corpus data, but for ask, for example, eight classes (out of a possible 27 in the merged entry) are not present, so comparison only to the merged entry would give an unreasonably low estimate of recall. 3.2 Lexicon Evaluation - Results Figure 2 gives the raw results for the merged en- tries and corpus analysis on each verb. It shows the number of true positives (TP), correct classes pro- posed by our system, false positives (FP), incorrect classes proposed by our system, and false negatives (FN), correct classes not proposed by our system, as judged against the merged entry, and, for seven of the verbs, against the corpus analysis. It also shows, in the final column, the number of sentences from which classes were extracted. Dictionary Corpus (14 verbs) (7 verbs) Precision 65.7% 76.6% Recall 35.5% 43.4% Figure 3: Type precision and recall Ranking Accuracy ask 75.0% begin believe 100.0% 66.7% 100.0% 70.0% 75.0% 83.3% 81.4% cause give seem swing Mean Figure 4: Ranking accuracy of classes Figure 3 gives the type precision and recall of our system's recognition of subcategorization classes as evaluated against the merged dictionary entries (14 verbs) and against the manually analysed cor- pus data (7 verbs). The frequency distribution of the classes is highly skewed: for example for believe, there are 107 instances of the most common class in the corpus data, but only 6 instances in total of the least common four classes. More generally, for the manually analysed verbs, almost 60% of the false negatives have only one or two exemplars each in the corpus citations. None of them are returned by the system because the binomial filter always rejects classes hypothesised on the basis of such little evi- dence. In Figure 4 we estimate the accuracy with which our system ranks true positive classes against the correct ranking for the seven verbs whose corpus in- put was manually analysed. We compute this mea- sure by calculating the percentage of pairs of classes at positions (n,m) s.t. n < m in the system rank- ing that are ordered the same in the correct ranking. This gives us an estimate of the accuracy of the rel- ative frequencies of classes output by the system. For each of the seven verbs for which we under- took a corpus analysis, we calculate the token recall of our system as the percentage (over all exemplars) of true positives in the corpus. This gives us an es- timate of the parsing performance that would result from providing a parser with entries built using the system, shown in Figure 5. Further evaluation of the results for these seven verbs reveals that the filtering phase is the weak link in the system. There are only 13 true negatives which the system failed to propose, each exemplified in the data by a mean of 4.5 examples. On the other hand, there are 67 false negatives supported by an estimated mean of 7.1 examples which should, ide- Merged Entry TP FP FN Corpus Data TP FP FN No. of Sentences ask 9 0 18 9 0 10 390 begin 4 1 7 4 1 7 311 believe 4 4 11 4 4 8 230 cause 2 3 6 2 3 5 95 expect 6 5 3 - - - 223 find 5 7 15 - - - 645 give 5 2 11 5 2 5 639 help 6 3 8 - - - 223 like 3 2 7 - - - 228 move 4 3 9 - - - 217 produce 2 1 3 - - - 152 provide 3 2 6 - - - 217 seem 8 1 4 8 1 4 534 swing 4 0 10 4 0 8 45 Totals 65 34 118 36 11 47 4149 Figure 2: Raw results for test of 14 verbs Token Recall ask 78.5% begin 73.8% believe 34.5% cause 92.1% give 92.2% seem 84.7% swing 39.2% Mean 80.9% Figure 5: Token recall \"Baseline\" Lexicalised Mean crossings Recall Precision 1.00 70.7% 72.3% 0.93 71.4% 72.9% Figure 6: GEIG evaluation metrics for parser against Susanne bracketings ally, have been accepted by the filter, and 11 false positives which should have been rejected. The per- formance of the filter for classes with less than 10 exemplars is around chance, and a simple heuris- tic of accepting all classes with more than 10 exem- plars would have produced broadly similar results for these verbs. The filter may well be performing poorly because the probability of generating a sub- categorization class for a given verb is often lower than the error probability for that class. 3.3 Parsing Evaluation In addition to evaluating the acquired subcategoriza- tion information against existing lexical resources, we have also evaluated the information in the con- text of an actual parsing system. In particular we wanted to establish whether the subcategorization frequency information for individual verbs could be used to improve the accuracy of a parser that uses statistical techniques to rank analyses. The experiment used the same probabilistic parser and tag sequence grammar as are present in the acquisition system (see references above) -although the experiment does not in any way rely on the parsers or grammars being the same. We ran- domly selected a test set of 250 in-coverage sen- tences (of lengths 3-56 tokens, mean 18.2) from the Susanne treebank, retagged with possibly multiple tags per word, and measured the 'baseline' accu- racy of the unlexicalized parser on the sentences us- ing the now standard PARSEVAL/GEIG evaluation metrics of mean crossing brackets per sentence and (unlabelled) bracket recall and precision (e.g. Gr- ishman et al., 1992); see figure 65. Next, we col- lected all words in the test corpus tagged as possi- bly being verbs (giving a total of 356 distinct lem- mas) and retrieved all citations of them in the LOB corpus, plus Susanne with the 250 test sentences excluded. We acquired subcategorization and as- sociated frequency information from the citations, in the process successfully parsing 380K words. We then parsed the test set, with each verb subcate- gorization possibility weighted by its raw frequency score, and using the naive add-one smoothing tech- nique to allow for omitted possibilities. The GEIG measures for the lexicalized parser show a 7% im- provement in the crossing bracket score (figure 6). Over the existing test corpus this is not statisti- $^5$Carroll & Briscoe (1996) use the same test set, al- though the baseline results reported here differ slightly due to differences in the mapping from parse trees to Susanne-compatible bracketings. cally significant at the 95% level (paired t-test, 1.21, 249 df, p = 0.11)—although if the pattern of differ- ences were maintained over a larger test set of 470 sentences it would be significant. We expect that a more sophisticated smoothing technique, a larger acquisition corpus, and extensions to the system to deal with nominal and adjectival predicates would improve accuracy still further. Nevertheless, this experiment demonstrates that lexicalizing a gram- mar/parser with subcategorization frequencies can appreciably improve the accuracy of parse ranking. 4 Related Work Brent's (1993) approach to acquiring subcategoriza- tion is based on a philosophy of only exploiting un- ambiguous and determinate information in unanal- ysed corpora. He defines a number of lexical pat- terns (mostly involving closed class items, such as pronouns) which reliably cue one of five subcatego- rization classes. Brent does not report comprehen- sive results, but for one class, sentential complement verbs, he achieves 96% precision and 76% recall at classifying individual tokens of 63 distinct verbs as exemplars or non-exemplars of this class. He does not attempt to rank different classes for a given verb. Ushioda et al. (1993) utilise a PoS tagged corpus and finite-state NP parser to recognize and calcu- late the relative frequency of six subcategorization classes. They report an accuracy rate of 83% (254 errors) at classifying 1565 classifiable tokens of 33 distinct verbs in running text and suggest that in- correct noun phrase boundary detection accounts for the majority of errors. They report that for 32 verbs their system correctly predicts the most frequent class, and for 30 verbs it correctly predicts the sec- ond most frequent class, if there was one. Our sys- tem rankings include all classes for each verb, from a total of 160 classes, and average 81.4% correct. Manning (1993) conducts a larger experiment, also using a PoS tagged corpus and a finite-state NP parser, attempting to recognize sixteen distinct complementation patterns. He reports that for a test sample of 200 tokens of 40 verbs in running text, the acquired subcategorization dictionary listed the ap- propriate entry for 163 cases, giving a token recall of 82% (as compared with 80.9% in our experiment). He also reports a comparison of acquired entries for the verbs to the entries given in the Oxford Advanced Learner's Dictionary of Current English (Hornby, 1989) on which his system achieves a precision of 90% and a recall of 43%. His system averages 3.48 subentries (maximum 10)—less then half the num- ber produced in our experiment. It is not clear what level of evidence the performance of Manning's sys- tem is based on, but the system was applied to 4.1 million words of text (c.f. our 1.2 million words) and the verbs are all common, so it is likely that consid- erably more exemplars of each verb were available. 5 Conclusions and Further Work The experiment and comparison reported above sug- gests that our more comprehensive subcategoriza- tion class extractor is able both to assign classes to individual verbal predicates and also to rank them according to relative frequency with compa- rable accuracy to extant systems. We have also demonstrated that a subcategorization dictionary built with the system can improve the accuracy of a probabilistic parser by an appreciable amount. The system we have developed is straightfor- wardly extensible to nominal and adjectival pred- icates; the existing grammar distinguishes nominal and adjectival arguments from adjuncts structurally, so all that is required is extension of the classi- fier. Developing an analogous system for another language would be harder but not infeasible; sim- ilar taggers and parsers have been developed for a number of languages, but no extant subcategoriza- tion dictionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filter- ing would need to be estimated, perhaps using the technique described by Brent (1993). However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system. Our system needs further refinement to nar- row some subcategorization classes, for example, to choose between differing control options with pred- icative complements. It also needs supplementing with information about diathesis alternation pos- sibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. Grishman & Ster- ling (1992), Poznanski & Sanfilippo (1993), Resnik (1993), Ribas (1994) and others have shown that it is possible to acquire selection preferences from (par- tially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcatego- rization classes for the same predicate share seman- tic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. Boguraev & Briscoe, 1987). References Boguraev, B. & Briscoe, E. 1987. Large lexicons for natural language processing: utilising the gram- mar coding system of the Longman Dictionary of Contemporary English. Computational Linguistics 13.4: 219-240. Boguraev, B. & Briscoe, E. 1989. Introduction. In Boguraev, B. & Briscoe, E. eds. Computational Lex- icography for Natural Language Processing. Long- man, London: 1-40. Boguraev, B., Briscoe, E., Carroll, J., Carter, D. & Grover, C. 1987. The derivation of a gram- matically-indexed lexicon from the Longman Dictio- nary of Contemporary English. In Proceedings of the 25th Annual Meeting of the Association for Compu- tational Linguistics, Stanford, CA. 193-200. Brent, M. 1991. Automatic acquisition of subcatego- rization frames from untagged text. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, Berkeley, CA. 209-214. Brent, M. 1993. From grammar to lexicon: unsu- pervised learning of lexical syntax. Computational Linguistics 19.3: 243-262. Briscoe, E. & Carroll, J. 1993. Generalised proba- bilistic LR parsing for unification-based grammars. Computational Linguistics 19.1: 25-60. Briscoe, E. & Carroll, J. 1994. Parsing (with) punc- tuation. Rank Xerox Research Centre, Grenoble, MLTT-TR-007. Briscoe, E. & Carroll, J. 1995. Developing and eval- uating a probabilistic LR parser of part-of-speech and punctuation labels. In Proceedings of the 4th ACL/SIGPARSE International Workshop on Pars- ing Technologies, Prague, Czech Republic. 48-58. Carroll, J. 1993. Practical unification-based parsing of natural language. Cambridge University Com- puter Laboratory, TR-224. Carroll, J. 1994. Relating complexity to practical performance in parsing with wide-coverage unifica- tion grammars. In Proceedings of the 32nd Annual Meeting of the Association for Computational Lin- guistics, NMSU, Las Cruces, NM. 287-294. Carroll, J. & Briscoe, E. 1996. Apportioning de- velopment effort in a probabilistic LR parsing sys- tem through evaluation. In Proceedings of the ACL SIGDAT Conference on Empirical Methods in Natu- ral Language Processing, University of Pensylvania, Philadelphia, PA. 92-100. Carroll, J. & Grover, C. 1989. The derivation of a large computational lexicon for English from LDOCE. In Boguraev, B. and Briscoe, E. eds. Com- putational Lexicography for Natural Language Pro- cessing. Longman, London: 117-134. Cunningham, H., Gaizauskas, R. & Wilks, Y. 1995. A general architecture for text engineering (GATE) - a new approach to language R&D. Research memo CS-95-21, Department of Computer Science, Univer- sity of Sheffield, UK. de Marcken, C. 1990. Parsing the LOB corpus. In Proceedings of the 28th Annual Meeting of the As- sociation for Computational Linguistics, Pittsburgh, PA. 243-251. Elworthy, D. 1994. Does Baum-Welch re-estimation help taggers?. In Proceedings of the 4th Conf. Ap- plied NLP, Stuttgart, Germany. Garside, R., Leech, G. & Sampson, G. 1987. The computational analysis of English: A corpus-based approach. Longman, London. Grishman, R., Macleod, C. & Meyers, A. 1994. Comlex syntax: building a computational lexi- con. In Proceedings of the International Conference on Computational Linguistics, COLING-94, Kyoto, Japan. 268-272. Grishman, R., Macleod, C. & Sterling, J. 1992. Evaluating parsing strategies using standardized parse files. In Proceedings of the 3rd ACL Conference on Applied Natural Language Process- ing, Trento, Italy. 156-161. Grishman, R. & Sterling, J. 1992. Acquisition of selectional patterns. In Proceedings of the Inter- national Conference on Computational Linguistics, COLING-92, Nantes, France. 658-664. Jackendoff, R. 1977. X-bar syntax. MIT Press; Cambridge, MA.. Jensen, K. 1991. A broad-coverage natural language analysis system. In M. Tomita eds. Current Issues in Parsing Technology. Kluwer, Dordrecht. Levin, B. 1993. Towards a lexical organization of English verbs. Chicago University Press, Chicago. Manning, C. 1993. Automatic acquisition of a large subcategorisation dictionary from corpora. In Pro- ceedings of the 31st Annual Meeting of the Asso- ciation for Computational Linguistics, Columbus, Ohio. 235-242. Meyers, A., Macleod, C. & Grishman, R. 1994. Standardization of the complement adjunct distinc- tion. New York University, Ms. Nunberg, G. 1990. The linguistics of punctuation. CSLI Lecture Notes 18, Stanford, CA. Poznanski, V. & Sanfilippo, A. 1993. Detecting de- pendencies between semantic verb subclasses and subcategorization frames in text corpora. In Pro- ceedings of the SIGLEX ACL Workshop on the Ac- quisition of Lexical Knowledge from Text, Boguraev, B. & Pustejovsky, J. eds. Resnik, P. 1993. Selection and information: a class- based approach to lexical relationships. University of Pennsylvania, CIS Dept, PhD thesis. Ribas, P. 1994. An experiment on learning ap- propriate selection restrictions from a parsed cor- pus. In Proceedings of the International Conference on Computational Linguistics, COLING-94, Kyoto, Japan. Sampson, G. 1995. English for the computer. Ox- ford, UK: Oxford University Press. Schabes, Y. 1992. Stochastic lexicalized tree ad- joining grammars. In Proceedings of the Inter- national Conference on Computational Linguistics, COLING-92, Nantes, France. 426-432. Taylor, L. & Knowles, G. 1988. Manual of informa- tion to accompany the SEC corpus: the machine- readable corpus of spoken English. University of Lancaster, UK, Ms. Ushioda, A., Evans, D., Gibson, T. & Waibel, A. 1993. The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora. In Boguraev, B. & Pustejovsky, J. eds. SIGLEX ACL Workshop on the Acquisition of Lexical Knowl- edge from Text. Columbus, Ohio: 95-106."
  },
  {
    "title": "Analyzing Dependencies of Japanese Subordinate Clauses based on Statistics of Scope Embedding Preference",
    "abstract": "This paper proposes a statistical method for learning dependency preference of Japanese subordinate clauses, in which scope embedding preference of subordinate clauses is exploited as a useful information source for disambiguating dependencies between subordinate clauses. Estimated dependencies of subordinate clauses successfully increase the precision of an existing statistical dependency analyzer.",
    "content": "Introduction In the Japanese language, since word order in a sentence is relatively free compared with Euro- pean languages, dependency analysis has been shown to be practical and effective in both rule- based and stochastic approaches to syntactic analysis. In dependency analysis of a Japanese sentence, among various source of ambiguities in a sentence, dependency ambiguities of sub- ordinate clauses are one of the most problem- atic ones, partly because word order in a sen- tence is relatively free. In general, dependency ambiguities of subordinate clauses cause scope ambiguities of subordinate clauses, which result in enormous number of syntactic ambiguities of other types of phrases such as noun phrases.¹ In the Japanese linguistics, a theory of Mi- nami (1974) regarding scope embedding pref- erence of subordinate clauses is well-known. Minami (1974) classifies Japanese subordinate clauses according to the breadths of their scopes and claim that subordinate clauses which inher- ently have narrower scopes are embedded within the scopes of subordinate clauses which inher- ently have broader scopes (details are in sec- tion 2). By manually analyzing several raw cor- pora, Minami (1974) classifies various types of Japanese subordinate clauses into three cate- gories, which are totally ordered by the embed- ding relation of their scopes. In the Japanese computational linguistics community, Shirai et al. (1995) employed Minami (1974)'s theory on scope embedding preference of Japanese sub- ordinate clauses and applied it to rule-based Japanese dependency analysis. However, in their approach, since categories of subordinate clauses are obtained by manually analyzing a small number of sentences, their coverage against a large corpus such as EDR bracketed corpus (EDR, 1995) is quite low.² In order to realize a broad coverage and high performance dependency analysis of Japanese sentences which exploits scope embedding pref- erence of subordinate clauses, we propose a corpus-based and statistical alternative to the rule-based manual approach (section 3).³ ¹In our preliminary corpus analysis using the stochas- tic dependency analyzer of Fujio and Matsumoto (1998), about 30% of the 210,000 sentences in EDR bracketed corpus (EDR, 1995) have dependency ambiguities of sub- ordinate clauses, for which the precision of chunk (bun- setsu) level dependencies is about 85.3% and that of sen- tence level is about 25.4% (for best one) ~ 35.8% (for best five), while for the rest 70% of EDR bracketed cor- pus, the precision of chunk (bunsetsu) level dependencies is about 86.7% and that of sentence level is about 47.5% (for best one) ~ 60.2% (for best five). In addition to that, when assuming that those ambiguities of subor- dinate clause dependencies are initially resolved in some way, the chunk level precision increases to 90.4%, and the sentence level precision to 40.6% (for best one) ~67.7% clearly shows that dependency ambiguities of subordi- nate clauses are among the most problematic source of syntactic ambiguities in a Japanese sentence. ²In our implementation, the coverage of the categories of Shirai et al. (1995) is only 30% for all the subordinate clauses included in the whole EDR corpus. ³Previous works on statistical dependency analysis in- clude Fujio and Matsumoto (1998) and Haruno et al. (1998) in Japanese analysis as well as Lafferty et al. (1992), Eisner (1996), and Collins (1996) in English anal- ysis. In later sections, we discuss the advantages of our Table 1: Word Segmentation, POS tagging, and Bunsetsu Segmentation of A Japanese Sentence Word Segmentation POS (+ conjugation form) Tagging Bunsetsu Segmentation (Chunking) English Translation Tenki noun ga case- particle yoi adjective (base) kara dekakeyou predicate- verb conjunctive-particle (volitional) Tenki-ga yoi-kara dekakeyou let's go out because weather subject fine (Because the weather is fine, let's go out.) First, we formalize the problem of decid- ing scope embedding preference as a classifi- cation problem, in which various types of lin- guistic information of each subordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other. As in the case of Shirai et al. (1995), we formalize the problem of deciding dependency preference of subordinate clauses by utilizing the correlation of scope em- bedding preference and dependency preference of Japanese subordinate clauses. Then, as a sta- tistical learning method, we employ the decision list learning method of Yarowsky (1994), where optimal combination of those features are se- lected and sorted in the form of decision rules, according to the strength of correlation between those features and the dependency preference of the two subordinate clauses. We evaluate the proposed method through the experiment on learning dependency preference of Japanese subordinate clauses from the EDR bracketed corpus (section 4). We show that the pro- posed method outperforms other related meth- ods/models. We also evaluate the estimated de- pendencies of subordinate clauses in Fujio and Matsumoto (1998)'s framework of the statisti- cal dependency analysis of a whole sentence, in which we successfully increase the precisions of both chunk level and sentence level dependen- cies thanks to the estimated dependencies of subordinate clauses. 2 Analyzing Dependencies between Japanese Subordinate Clauses based on Scope Embedding Preference 2.1 Dependency Analysis of A Japanese Sentence First, we overview dependency analysis of a Japanese sentence. Since words in a Japanese sentence are not segmented by explicit delim- iters input sentences are first word segmented Scope of Subordinate Clause (((Tenki-ga) Phrase Structure (yoi-kara) ) (dekakeyou) ) Dependency (modification) Relation Figure 1: An Example of Japanese Subordinate Clause (taken from the Sentence of Table 1) part-of-speech tagged, and then chunked into a sequence of segments called bunsetsus. Each chunk (bunsetsu) generally consists of a set of content words and function words. Then, de- pendency relations among those chunks are es- timated, where most practical dependency ana- lyzers for the Japanese language usually assume the following two constraints: 1. Every chunk (bunsetsu) except the last one modifies only one posterior chunk (bun- setsu). 2. No modification crosses to other modifica- tions in a sentence. Table 1 gives an example of word segmenta- tion, part-of-speech tagging, and bunsetsu seg- mentation (chunking) of a Japanese sentence, where the verb and the adjective are tagged with their parts-of-speech as well as conjuga- tion forms. Figure 1 shows the phrase structure, the bracketing, and the dependency (modifica- tion) relation of the chunks (bunsetsus) within the sentence. *Word segmentation and part-of-speech tagging are performed by the Japanese morphological analyzer Chasen (Matsumoto et al., 1997), and chunking is done by the preprocessor used in Fujio and Matsumoto (1998). The phrase structure and the bracketing are shown just for explanation, and we do not consider them but consider only dependency relations in the analysis throughout this paper A Japanese subordinate clause is a clause whose head chunk satisfies the following properties. 1. The content words part of the chunk (bunsetsu) is one of the following types: (a) A predicate (i.e., a verb or an adjective). (b) nouns and a copula like \"Noun₁ dearu\" (in English, \"be Noun₁\"). 2. The function words part of the chunk (bunsetsu) is one of the following types: (a) Null. (b) Adverb type such as \"Verb₁ ippou-de\" (in English, \"(subject) Verb₁ ..., on the other hand,\"). (c) Adverbial noun type such as \"Verb₁ tame\" (in English, \"in order to Verb₁\"). (d) Formal noun type such as \"Verb₁ koto\" (in English, gerund \"Verb₁-ing\"). (e) Temporal noun type such as \"Verb₁ mae\" (in English, \"before (subject) Verb₁ ...\"). (f) A predicate conjunctive particle such as \"Verb₁ ga\" (in English, \"although (subject) Verb₁ ...,”). (g) A quoting particle such as \"Verb₁ to (iu)\" (in English, \"(say) that (subject) Verb₁ ...\"). (h) (a)~(g) followed by topic marking particles and/or sentence-final particles. Figure 2: Definition of Japanese Subordinate Clause 2.2 Japanese Subordinate Clause The following gives the definition of what we call a \"Japanese subordinate clause\" throughout this paper. A clause in a sentence is represented as a sequence of chunks. Since the Japanese lan- guage is a head-final language, the clause head is the final chunk in the sequence. A grammati- cal definition of a Japanese subordinate clause is given in Figure 2.6 For example, the Japanese sentence in Table 1 has one subordinate clause, whose scope is indicated as the shaded rectangle in Figure 1. 2.3 Scope Embedding Preference of Subordinate Clauses We introduce the concept of Minami (1974)'s classification of Japanese subordinate clauses by describing the more specific classification by Shirai et al. (1995). From 972 newspaper summary sentences, Shirai et al. (1995) man- ually extracted 54 clause final function words of Japanese subordinate clauses and classified them into the following three categories accord- ing to the embedding relation of their scopes. Category A: Seven expressions representing simultaneous occurrences such as \"Verb₁ to-tomoni (Clause2)\" and \"Verb₁ nagara (Clause2)\". Category B: 46 expressions representing cause and discontinuity such as \"Verb₁ te (Clause2)\" (in English \"Verb₁ and (Clause2)\") and \"Verb₁ node\" (in English \"because (subject) Verb₁ ...,”). Category C: One expression representing in- dependence, \"Verb₁ ga\" (in English, \"al- though (subject) Verb₁ ...,\"). The category A has the narrowest scope, while the category C has the broadest scope, i.e., Category A < Category B < Category C where the relation '<' denotes the embedding relation of scopes of subordinate clauses. Then, scope embedding preference of Japanese subor- dinate clauses can be stated as below: Scope Embedding Preference of Japanese Subordinate Clauses 1. A subordinate clause can be embedded within the scope of another subordinate clause which inherently has a scope of the same or a broader breadth. 2. A subordinate clause can not be embedded within the scope of another subordinate clause which inherently has a narrower scope. For example, a subordinate clause of 'Category B', can be embedded within the scope of another subordinate clause of 'Category B' or 'Category C', but not within that of 'Category A'. Figure 2 (a) Category A ⊂ Category C Scopes of Subordinate Clauses Category C Category A (((kakimaze-nagara) (ni-mashita-ga-,) ) (kogete-shimai-mashita-.)) stir_up-with boil- polite/past- although- comma scorch- perfect-polite/past-period (Although I boiled it with stirring it up, it had got scorched.) (b) Category C ⊂ Category A Scopes of Subordinate Clauses Category C ((((kogeru) (osore-ga) ) (ari-masu-ga-,)) ( Category A (tsuyobi-de) (kakimaze-nagara) (ni-mashou-.) ) ) scorch fear- sbj exist- polite- although- comma hot_fire-over stir_up-with (Although there is some fear of its getting scorched, let's boil it with stirring it up over a hot fire.) boil- polite (volitional)-period Figure 3: Examples of Scope Embedding of Japanese Subordinate Clauses (a) gives an example of an anterior Japanese subordinate clause (“kakimaze-nagara”, Cate- gory A), which is embedded within the scope of a posterior one with a broader scope (“ni- mashita-ga-,”, Category C). Since the poste- rior subordinate clause inherently has a broader scope than the anterior, the anterior is embed- ded within the scope of the posterior. On the other hand, Figure 3(b) gives an example of an anterior Japanese subordinate clause (“ari- masu-ga-,”, Category C), which is not embed- ded within the scope of a posterior one with a narrower scope (“kakimaze-nagara”, Category A). Since the posterior subordinate clause in- herently has a narrower scope than the anterior, the anterior is not embedded within the scope of the posterior. 2.4 Preference of Dependencies between Subordinate Clauses based on Scope Embedding Preference Following the scope embedding preference of Japanese subordinate clauses proposed by Mi- nami (1974), Shirai et al. (1995) applied it to rule-based Japanese dependency analysis, and proposed the following preference of decid- ing dependencies between subordinate clauses. Suppose that a sentence has two subordinate clauses Clause₁ and Clause₂, where the head vp chunk of Clause₁ precedes that of Clause₂. Dependency Preference of Japanese Subordinate Clauses 1. The head vp chunk of Clause₁ can modify that of Clause₂ if Clause₂ inherently has a scope of the same or a broader breadth compared with that of Clause₁. 2. The head vp chunk of Clause₁ can not mod- ify that of Clause₂ if Clause₂ inherently has a narrower scope compared with that of Clause₁. 3 Learning Dependency Preference of Japanese Subordinate Clauses As we mentioned in section 1, the rule-based approach of Shirai et al. (1995) to analyz- ing dependencies of subordinate clauses using scope embedding preference has serious limi- tation in its coverage against corpora of large size for practical use. In order to overcome the limitation of the rule-based approach, in this section, we propose a method of learning dependency preference of Japanese subordinate clauses from a bracketed corpus. We formalize the problem of deciding scope embedding pref- erence as a classification problem, in which var- ious types of linguistic information of each sub- ordinate clause are encoded as features and used for deciding which one of given two subordinate clauses has a broader scope than the other. As a statistical learning method, we employ the de- cision list learning method of Yarowsky (1994) Table 2: Features of Japanese Subordinate Clauses Feature Type # of Features Each Binary Feature Punctuation 2 with-comma, without-comma Grammatical 17 adverb, adverbial-noun, formal-noun, temporal-noun, (some features have distinction of chunk-final/middle) quoting-particle, copula, predicate-conjunctive-particle, topic-marking-particle, sentence-final-particle Conjugation form of chunk-final conjugative word 12 stem, base, mizen, ren'you, rentai, conditional, imperative, ta, tari, te, conjecture, volitional Lexical (lexicalized forms of 'Grammatical' features, with more than adverb (e.g., ippou-de, irai), adverbial-noun (e.g., tame, baai) topic-marking-particle (e.g., ha, mo), quoting-particle (to), predicate-conjunctive-particle (e.g., ga, kara), temporal-noun (e.g., ima, shunkan), formal-noun (e.g., koto), copula (dearu), sentence-final-particle (e.g., ka, yo) 9 occurrences in EDR corpus) 235 3.1 The Task Definition Considering the dependency preference of Japanese subordinate clauses described in sec- tion 2.4, the following gives the definition of our task of deciding the dependency of Japanese subordinate clauses. Suppose that a sen- tence has two subordinate clauses Clause1 and Clauser, where the head vp chunk of Clause1 precedes that of Clause2. Then, our task of de- ciding the dependency of Japanese subordinate clauses is to distinguish the following two cases: 1. The head vp chunk of Clause1 modifies that of Clause2. 2. The head vp chunk of Clause1 does not modify that of Clause2, but modifies that of another subordinate clause or the matrix clause which follows Clause2. Roughly speaking, the first corresponds to the case where Clause2 inherently has a scope of the same or a broader breadth compared with that of Clause1, while the second corresponds to the case where Clause2 inherently has a narrower scope compared with that of Clause1.7 3.2 Decision List Learning A decision list (Yarowsky, 1994) is a sorted list of the decision rules each of which decides the value of a decision D given some evidence E. Each decision rule in a decision list is sorted \"Our modeling is slightly different from those of other standard approaches to statistical dependency analy- sis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) which simply distinguish the two cases: the case where dependency relation holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. In contrast to those standard ap- proaches, we ignore the case where the head vp chunk of Clause1 modifies that of another subordinate clause which precedes Clause2. This is because we assume that this case is more loosely related to the scope embedding in descending order with respect to some pref- erence value, and rules with higher preference values are applied first when applying the deci- sion list to some new test data. First, let the random variable D represent- ing a decision varies over several possible values, and the random variable E representing some evidence varies over '1' and '0' (where '1' de- notes the presence of the corresponding piece of evidence, '0' its absence). Then, given some training data in which the correct value of the decision D is annotated to each instance, the conditional probabilities P(D=x | E=1) of ob- serving the decision D =x under the condition of the presence of the evidence E (E = 1) are calculated and the decision list is constructed by the following procedure. 1. For each piece of evidence, calculate the likeli- hood ratio of the conditional probability of a de- cision D=x1 (given the presence of that piece of evidence) to the conditional probability of the rest of the decisions D=x1: log2 P(D=x1|E=1) P(D≠x1|E=1) Then, a decision list is constructed with pieces of evidence sorted in descending order with re- spect to their likelihood ratios.8 2. The final line of a decision list is defined as 'a default', where the likelihood ratio is calculated as the ratio of the largest marginal probability of the decision D = x1 to the marginal proba- Yarowsky (1994) discusses several techniques for avoiding the problems which arise when an observed count is 0. Among those techniques, we employ the sim- plest one, i.e., adding a small constant a (0.1 ≤ a ≤ 0.25) to the numerator and denominator. With this modification, more frequent evidence is preferred when there exist several evidences for each of which the con- ditional probability P(D=x|E=1) --- (a) An Example Sentence with Chunking, Bracketing, and Dependency Relations Subordinate Clauses Clause1 Clause2 Seg1 Seg2 ((10%-nara) (neage-suru-ga-,) (3%-na-node-,) ((((tsui) (gyousha-hutan-toiu) ) (keesu-ga) ) (dete-kuru-darou-.) ) ) 10%-if raise-price 3%- emphatic_auxiliay _verb (te-form) -comma involuntary dealer-charge-of comma case- sbj happen-will/may- period (If the tax rate is 10%, the dealers will raise price, but, because it is 3%, there will happen to be the cases that the dealers pay the tax.) (b) Feature Expression of Head VP Chunk of Subordinate Clauses Seg₁: \"neage-suru-ga-,\" Head VP Chunk of Subordinate Clause F₁ = { with-comma, predicate-conjunctive-particle(chunk-final), predicate-conjunctive-particle(chunk-final)-\"ga\" } Seg2: \"3%-na-node-,\" F2 = { with-comma, chunk-final-conjugative-word-te-form } Feature Set (c) Evidence-Decision Pairs for Decision List Learning Evidence E (E=1) (feature names are abbreviated) F1 F2 Decision D with-comma with-comma \"beyond\" with-comma te-form \"beyond\" with-comma with-comma, te-form \"beyond\" pred-conj-particle(final) with-comma \"beyond\" with-comma, pred-conj-particle(final) with-comma \"beyond\" pred-conj-particle(final)-\"ga\" with-comma \"beyond\" with-comma, pred-conj-particle(final)-\"ga\" with-comma \"beyond\" Figure 4: An Example of Evidence-Decision Pair of Japanese Subordinate Clauses bility of the rest of the decisions D=-x₁: P(D=x1) log2 P(D=-x1) The 'default' decision of this final line is D=x₁ with the largest marginal probability. 3.3 Feature of Subordinate Clauses Japanese subordinate clauses defined in sec- tion 2.2 are encoded using the following four types of features: i) Punctuation: represents whether the head vp chunk of the subordinate clause is marked with a comma or not, ii) Gram- matical: represents parts-of-speech of function words of the head vp chunk of the subordi- nate clause, iii) Conjugation form of chunk- ³Terms of parts-of-speech tags and conjugation forms are borrowed from those of the Japanese morphological analysis system Chasen (Matsumoto et al., 1997). final conjugative word: used when the chunk- final word is conjugative, iv) Lexical: lexicalized forms of 'Grammatical' features which appear more than 9 times in EDR corpus. Each fea- ture of these four types is binary and its value is '1' or '0' ('1' denotes the presence of the cor- responding feature, '0' its absence). The whole feature set shown in Table 2 is designed so as to cover the 210,000 sentences of EDR corpus. 3.4 Decision List Learning of Dependency Preference of Subordinate Clauses First, in the modeling of the evidence, we con- sider every possible correlation (i.e., depen- dency) of the features of the subordinate clauses listed in section 3.3. Furthermore, since it is necessary to consider the features for both of the given two subordinate clauses, we consider all --- the possible combination of features of the an- terior and posterior head vp chunks of the given two subordinate clauses. More specifically, let Seg₁ and Seg2 be the head vp chunks of the given two subordinate clauses (Seg₁ is the ante- rior and Seg2 is the posterior). Also let F₁ and F2 be the sets of features which Seg₁ and Seg2 have, respectively (i.e., the values of these fea- tures are '1'). We consider every possible subset F1 and F2 of F₁ and F2, respectively, and then model the evidence of the decision list learning method as any possible pair (F1, F2).¹⁰ Second, in the modeling of the decision, we distinguish the two cases of dependency rela- tions described in section 3.1. We name the first case as the decision \"modify\", while the second as the decision \"beyond\". 3.5 Example Figure 4 illustrates an example of transforming subordinate clauses into feature expression, and then obtaining training pairs of an evidence and a decision from a bracketed sentence. Figure 4 (a) shows an example sentence which contains two subordinate clauses Clause₁ and Clause₂, with chunking, bracketing, and dependency re- lations of chunks. Both of the head vp chunks Seg₁ and Seg₂ of Clause₁ and Clause₂ modify the sentence-final vp chunk. As shown in Fig- ure 4 (b), the head vp chunks Seg₁ and Seg₂ have feature sets F₁ and F₂, respectively. Then, every possible subsets F₁ and F₂ of F₁ and F₂ are considered,¹¹ respectively, and training pairs of an evidence and a decision are collected as in Figure 4 (c). In this case, the value of the decision D is \"beyond\", because Seg₁ modifies the sentence-final vp chunk, which follows Seg₂. ¹⁰Our formalization of the evidence of decision list learning has an advantage over the decision tree learn- ing (Quinlan, 1993) approach to feature selection of de- pendency analysis (Haruno et al., 1998). In the feature selection procedure of the decision tree learning method, the utility of each feature is evaluated independently, and thus the utility of the combination of more than one features is not evaluated directly. On the other hand, in our formalization of the evidence of decision list learn- ing, we consider every possible pair of the subsets F₁ and F₂, and thus the utility of the combination of more than one features is evaluated directly. ¹¹Since the feature 'predicate-conjunctive- particle(chunk-final)' subsumes 'predicate-conjunctive- particle(chunk-final)-\"ga\"\", they are not considered Coverage/Precision (%) 100 80 60 40 Coverage (Our Model) 20 Coverage (Model (a)) Coverage (Model (b)) Precision (Our Model) Precision (Model (a)) Precision (Model (b)) 0 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 Lower Bound of P(DIE) Figure 5: Precisions and Coverages of Deciding Dependency between Two Subordinate Clauses Precision (%) 100 95 90 85 80 75 0 20 40 Our Model Model (a) Model (b) 60 80 100 Coverage (%) Figure 6: Correlation of Coverages and Precisions 4 Experiments and Evaluation We divided the 210,000 sentences of the whole EDR bracketed Japanese corpus into 95% train- ing sentences and 5% test sentences. Then, we extracted 162,443 pairs of subordinate clauses from the 199,500 training sentences, and learned a decision list for dependency prefer- ence of subordinate clauses from those pairs. The default decision in the decision list is D =\"beyond\", where the marginal probability P(D = \"beyond\") = 0.5378, i.e., the baseline precision of deciding dependency between two subordinate clauses is 53.78%. We limit the fre- quency of each evidence-decision pair to be more than 9. The total number of obtained evidence- decision pairs is 7,812. We evaluate the learned decision list through several experiments.¹² First, we apply the learned decision list to deciding dependency between two subordinate clauses of the 5% test sentences. We change the threshold of the probability P(D | E)¹³ in ¹²Details of the experimental evaluation will be pre- sented in Utsuro (2000). ¹³P(D | E) can be used equivalently to the likelihood the decision list and plot the trade-off between coverage and precision.<sup>14</sup> As shown in the plot of “Our Model” in Figure 5, the precision varies from 78% to 100% according to the changes of the threshold of the probability P(D | E). Next, we compare our model with the other two models: (a) the model learned by apply- ing the decision tree learning method of Haruno et al. (1998) to our task of deciding depen- dency between two subordinate clauses, and (b) a decision list whose decisions are the following two cases, i.e., the case where dependency rela- tion holds between the given two vp chunks or clauses, and the case where dependency relation does not hold. The model (b) corresponds to a model in which standard approaches to statis- tical dependency analysis (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998) are applied to our task of deciding dependency be- tween two subordinate clauses. Their results are also in Figures 5 and 6. Figure 5 shows that “Our Model” outperforms the other two mod- els in coverage. Figure 6 shows that our model outperforms both of the models (a) and (b) in coverage and precision. Finally, we examine whether the estimated dependencies of subordinate clauses improve the precision of Fujio and Matsumoto (1998)’s statistical dependency analyzer.<sup>15</sup> Depending on the threshold of P(D | E), we achieve 0.8～1.8% improvement in chunk level precision, and 1.6～4.7% improvement in sentence level.<sup>16</sup> 5 Conclusion This paper proposed a statistical method for learning dependency preference of Japanese ratio. <sup>14</sup>Coverage: the rate of the pairs of subordinate clauses whose dependencies are decided by the decision list, against the total pairs of subordinate clauses, Precision: the rate of the pairs of subordinate clauses whose depen- dencies are correctly decided by the decision list, against those covered pairs of subordinate clauses. <sup>15</sup>Fujio and Matsumoto (1998)’s lexicalized depen- dency analyzer is similar to that of Collins (1996), where various features were evaluated through performance test and an optimal feature set was manually selected. <sup>16</sup>The upper bounds of the improvement in chunk level and sentence level precisions, which are estimated by providing Fujio and Matsumoto (1998)’s statistical de- pendency analyzer with correct dependencies of subor- dinate clauses extracted from the bracketing of the EDR corpus, are 5.1% and 15%, respectively. subordinate clauses, in which scope embed- ding preference of subordinate clauses is ex- ploited. We evaluated the estimated dependen- cies of subordinate clauses through several ex- periments and showed that our model outper- formed other related models. References M. Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of ACL, pages 184–191. EDR (Japan Electronic Dictionary Research Insti- tute, Ltd.). 1995. EDR Electronic Dictionary Technical Guide. J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceed- ings of the 16th COLING, pages 340–345. M. Fujio and Y. Matsumoto. 1998. Japanese de- pendency structure analysis based on lexicalized statistics. In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Process- ing, pages 88–96. M. Haruno, S. Shirai, and Y. Oyama. 1998. Us- ing decision trees to construct a practical parser. In Proceedings of the 17th COLING and the 36th Annual Meeting of ACL, pages 505–511. J. Lafferty, D. Sleator, and D. Temperley. 1992. Grammatical trigrams: A probabilistic model of link grammar. In Proceedings of the AAAI Fall Symposium: Probabilistic Approaches to Natural Language, pages 89–97. Y. Matsumoto, A. Kitauchi, T. Yamashita, O. Imaichi, and T. Imamura. 1997. Japanese morphological analyzer ChaSen 1.0 users manual. Information Science Technical Report NAIST-IS- TR97007, Nara Institute of Science and Technol- ogy. (in Japanese). F. Minami. 1974. Gendai Nihongo no Kouzou. Taishuukan Shoten. (in Japanese). J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann. S. Shirai, S. Ikehara, A. Yokoo, and J. Kimura. 1995. A new dependency analysis method based on semantically embedded sentence structures and its performance on Japanese subordinate clauses. Transactions of Information Processing Society of Japan, 36(10):2353–2361. (in Japanese). T. Utsuro. 2000. Learning preference of depen- dency between Japanese subordinate clauses and its evaluation in parsing. In Proceedings of the 2nd International Conference on Language Resources and Evaluation. (to appear). D. Yarowsky. 1994. Decision lists for lexical ambi- guity resolution: Application to accent restora- tion in Spanish and French. In Proceedings of the 32nd Annual Meeting of ACL, pages 88–95."
  },
  {
    "title": "Dutch Sublanguage Semantic Tagging combined with Mark-Up Technology",
    "abstract": "In this paper, we want to show how the morphological component of an existing NLP-system for Dutch (Dutch Medical Language Processor - DMLP) has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system (Linguistic String Project - Medical Language Processor) of the New York University. The former can take advantage of the language independent developments of the latter, while focusing on idiosyncrasies for Dutch. This general strategy will be illustrated by a practical application, namely the highlighting of relevant information in a patient discharge summary (PDS) by means of modern HyperText Mark-Up Language (HTML) technology. Such an application can be of use for medical administrative purposes in a hospital environment.",
    "content": "1 Introduction Medical patient reports consist mainly of free text. While numerical data can be stored and processed (relatively) easily, free text is rather difficult to pro- cess by a computer, although in many cases it con- tains the most relevant information. The use of natural language does not facilitate the automation of these tasks and hinders access to the wealth of medical information. However, natural lan- guage still is the most frequently used and easiest way to transmit complex messages (Scherrer et al., 1989). Hence, some authors consider the study and application of Natural Language Processing (NLP) in Medicine (Scherrer et al., 1989), (McCray et al., 1995), (Chute, 1997) as one of the most challenging issues in the field of medical information retrieval (Baud et al., 1992a), (Friedman and Johnson, 1992). Up till now, not many NLP-driven systems have ac- tually been implemented (Spyns, 1996b). A concise overview of NLP-based information retrieval tech- niques for clinical narrative can be found in (Hersh, 1996, chapter 11, pp. 211-323). A possible environment for (medical) information retrieval is the Medical Registration Department of a hospital, and more in particular the medical en- coding service. Clinical data in free text format are replaced by a set of numerical codes that summarise the content of the entire document. In general, the patient discharge summary (PDS), being a synthe- sis of the patient stay, is used for the encoding and abstracting task instead of the entire medical record (Duisterhout, 1996). An important aspect of med- ical encoding consists of a thorough review of the PDS in order to discover the relevant words (diag- noses, surgical deeds, interventional equipment etc.) (Bowman, 1996, p.216). The aim of the NLP-based HTML application presented below is to speed up the reviewing process by displaying a PDS and high- lighting the keywords. The following sections provide details about some aspects of NLP systems for medical English (section 2.1: LSP-MLP) and Dutch (section 2.2: DMLP), and how results can be exchanged between them (sec- tion 2.3). Only some parts of the DMLP and LSP- MLP systems will be presented, namely those that are of importance for the experiment described be- low. Next to the NLP back-end, the user interface is described as well (section 2.4). The limitations of the current test are described in section 3 and some future directions for research are provided in the fourth and final section. 2 Material and Methods 2.1 The Linguistic String Project - Medical Language Processor The Linguistic String Project - Medical Language Processor (LSP-MLP) of the New York University is the first (and up till now the longest lasting) large scale project about NLP in Medicine (Sager et al., 1987), (Sager et al., 1995a). The LSP-MLP has also been ported to French and German, which il- lustrates the general applicability of its methodology and approach (Nhàn et al., 1989), (Oliver, 1992). The reason of its generality lies in the use of a well defined underlying linguistic theory (distributional- ism) (Harris, 1962), (Sager et al., 1981) and a scien- tifically based sublanguage approach (Grishman and Kittredge, 1986). Important for the present discussion is the seman- tic selection level of the LSP-MLP. All the words in the LSP dictionary are characterised by labels that indicate to which sublanguage word class(es) the words belong (e.g., H-TTCHIR: \"contains general and specific surgical treatment or procedure words which imply or denote surgical intervention by the physician\" (Sager et al., 1987, p.268); H-TXPROC: \"contains medical test words designating procedures performed on the patient and not on a patient spec- iment. The patient must be present to undergo the test\" (Sager et al., 1987, p.264)). An overview of the actual set of labels and word classes can be found in (Sager et al., 1995a). The semantic se- lection module uses distributionally established co- occurrence patterns of medical word classes to im- prove the parse tree by resolving cases of structural ambiguity (Hirschman, 1986). Consider the sentence 63 \"operatieve procedure: vijfvoudige coronaire by- pass.\" 1 displayed in figure 4. The word \"proce- dure\" is semantically ambiguous because it has two semantic labels: H-TTCHIR & H-TXPROC. Thanks to the co-occurrence patterns for the medical sub- language, only the label that is valid in this context (H-TTCHIR) is ultimately selected. In another con- text (e.g.: test procedure: ...), another co-occurrence pattern will apply and select the H-TXPROC reading. Other examples of resolution of word sense ambigui- ties by means of co-occurrence patterns can be found in (Sager et al., 1987, pp.83, 95). The very latest work includes the use of Stan- dard Generalized Mark-up Language (SGML) and World Wide Web (WWW) Graphical User Interface (GUI) technology to access and visualise better the requested information in the text (Sager et al., 1996). It focused on the use of static SGML or HTML-code 2 for displaying the results of NLP-based checklist screening of clinical documents. 1English: surgical procedure: quintuple coronary bypass. 2 \"Static\" HTML code eliminates the need for an on the fly conversion of the HTML file (\"dynamic\" HTML code) as presented in section 2.4. 2.2 The Dutch Medical Language Processor For the Dutch medical language, an NLP system of a medium sized coverage has been designed and im- plemented: the Dutch Medical Language Processor (DMLP) (Spyns, 1996c). With respect to the mor- phological level, there is a full form dictionary stored in the relational database format (currently some 100.000 full forms that are mostly non-compound wordforms) (Dehaspe, 1993). If necessary, a recog- niser characterises the unknown word forms morpho- logically (Spyns, 1994). Subsequently, a contextual disambiguation component tries to reduce the num- ber of morphological readings (Spyns, 1995). As the syntactic level uses a \"logic variant\" of the LSP grammar formalism (Hirschman and Dowding, 1990), the Dutch morpho-syntactic module (Spyns and Adriaens, 1992) can replace the LSP parser. Many of the LSP-MLP medical co-occurrence pat- terns are practically identical for English, French and German, so that the application of these pat- terns to Dutch parse trees can lead to interesting results, namely the feasibility of reusing the non lan- guage specific parts of the LSP-MLP for Dutch med- ical NLP (Spyns, 1996a). 2.3 The DMLP/LSP-MLP connection The linguistic data are passed on from the DMLP to the LSP-MLP system via syntactic parse trees. This is due to the fact that the selection module takes syn- tactic relationships into account during the semantic disambiguating phase. The linguistic information of the DMLP and the LSP-MLP systems correspond in a high degree. Se- mantic word class labels, which were originally not foreseen in the Dutch lexicon, had to be added. A parse tree transducer delivers nearly genuine Dutch LSP-MLP trees (Spyns, 1996a). Although on the side of the LSP-MLP some new sublanguage seman- tic co-occurrence patterns had to be defined, the co-occurrence patterns are highly language indepen- dent. This was in line with results earlier achieved. An example (see figure 1) shows the output of the parse tree transducer that reshapes the DMLP tree into the required LSP-MLP format. The current state of the transducer allows to transform nearly all the parse trees. 2.4 The WWW interface The basic idea was that when treating a patient, it is considered to be helpful to reread the admission history, the discharge summary, or other important parts of the medical record. ((SENTENCE (TEXTLET (OLDSENT (INTRODUCER (LN (TPOS (NULL)) (QPOS (NULL)) (APOS (ADJADJ (LAR (LA (NULL)) (AVAR (ADJ='OPERATIEVE': ('OPERATIEF') ('OPERATIEF'))) (RA (NULL))))) (NPOS (NULL))) (N='PROCEDURE': (F SINGULAR)^ ('PROCEDURE')) (':'=':': (':') (':'))) (CENTER (FRAGMENT (SA (NULL)) (NSTGF (NSTG (LNR (QPOS (NULL)) (LN (TPOS (NULL)) (APOS (ADJADJ (LAR (LA (NULL)) (NPOS (NULL))) (AVAR (ADJ='VIJFVOUDIGE': ('VIJFVOUDIG') ^ ('VIJFVOUDIG'))) (RA (NULL))) (AVAR (ADJ='CORONAIRE':('CORONAIR') ^ ('CORONAIR'))) (RA (NULL)))))) (ADJADJ (LAR (LA (NULL)) (NVAR (N='BYPASS': (SINGULAR) ^ ('BYPASS'))) (RN (NULL))))) (SA (NULL)))) (ENDMARK ('.'='.': ('.') ('.')))) (MORESENT (NULL)))) [((16 [SELECT-ATT ] H-TTCHIR) (21 [SELECT-ATT] H-TTCHIR H-TXPROC) (41 [SELECT-ATT ] H-TMREP) (49 [ SELECT-ATT] H-PTPART) (55 [SELECT-ATT ] H-TTCHIR)) ]) Figure 1: LSP like parse tree generated by the DMLP transducer for \"operatieve procedure: vijfvoudige coronaire bypass.\" [surgical procedure: quintuple coronary bypass] The highlighting of medical concepts of interest makes it possible to scan a docu- ment quickly, focusing on a particular type of information, such as Symptoms and Di- agnoses, or Treatments resolved (?, p.26). Also for the medico-administrative activities, such a tool can also be helpful. Medical secretaries have to summarise patient discharge summaries by \"trans- lating\" them into a fixed set of numerical codes of a classification (ICD-9-CM (Commission of Profes- sional and Hospital Activities, 1978)). These codes (indirectly) serve for statistical and financial pur- poses. If the most important relevant terms for the encoding task (essentially the H-DIAG (diagno- sis) and the H-TTCHIR (surgical deed) words) are already highlighted, the human encoder is able to detect them more rapidly so that the encoding speed can be improved. The documents are morphologically and syntacti- cally analysed by the DMLP first, the resulting parse trees being made conform to the LSP-format, and subsequently passed 3 on to the LSP-MLP. The LSP subselection module generates a pseudo- HTML file consisting of semantic labels and the ter- minal elements of the parse trees. The file with the pseudo-HTML codes (see figure 3) could eas- ily have been generated by the morphological com- ponent of the DMLP as well. In some occasions, it would be better to do so as the DMLP-LSP tree converter sometimes changes the word order. On the other hand, no advantage can then be taken from the sublanguage co-occurrence patterns for seman- tic disambiguation. Semantically ambiguous words will thus be highlighted more than once, which is bad for the precision score (more non relevant words are flagged). Without full fledged linguistic analysis, some ambiguities will not be resolved (?, p.27). As can be seen in figure 2 (and thus also in figure 3), the ambiguity for the word \"procedure\" in sentence 63 is resolved. The node number 2 only has the label H-TTCHIR. 3 Currently, the files are transmitted by e-mail. SENTENCE | TEXTLET | OLDSENT --- --- ---MORESENT INTRODUCER--- | | LNR--- | LN---- | TPOS---QPOS---APOS----NPOS | ADJADJ | LAR | LA---AVAR-----RA | *1*ADJ | OPERATIEVE -NVAR---RN | *2*N | CENTER---ENDMARK | FRAGMENT | NSTGF | NSTG | PROCEDURE LNR | LN----- | TPOS---QPOS---APOS-----NPOS | ADJADJ | LAR--- | LA---AVAR---RA | *3*ADJ VIJFVOUDIGE NVAR---RN | *5*N | BYPASS | ADJADJ | LAR | LA---AVAR---RA | *4*ADJ CORONAIRE *** Node Attributes *** Node ID: 1:: SELECT-ATT = H-TTCHIR Node ID: 2:: SELECT-ATT = H-TTCHIR Node ID: 3:: SELECT-ATT = H-TMREP Node ID: 4:: SELECT-ATT = H-PTPART Node ID: 5:: SELECT-ATT = H-TTCHIR Figure 2: LSP-MLP parse tree generated after sublanguage processing for sentence of figure 1 No actual HTML-codes were furnished but the se- mantic labels are noted according to the HTML-style (see figure 3). The NLP processing of a load of PDSs can be done in batch during the night so that the throughput of the encoder is not affected in the neg- ative sense. 63 < H - TTCHIR> operatieve </H-TTCHIR> <H-TTCHIR> procedure </H - TTCHIR> : <H-TMREP> vijfvoudige </H-TMREP> < H – PTPART> coronaire </H – PTPART> <H-TTCHIR>bypass</H-TTCHIR> Figure3: pseudo-HTML code generated after joint DMLP/LSP-MLP processing for the sentence in fig- ure 1 The GUI consists of two WWW-pages. The first page is conceived as a menu window. Two selection boxes allow the medical encoder to choose a text and the semantic labels. Currently, the set of PDSs is limited to nine texts. In the future, HTML-files for an unrestricted and varying number of PDSs will have to be produced. Before the encoder can start to view the NLP-processed PDSs, the HTML-code of the menu-page needs to be updated to include all the (path) names of the files concerned. This can easily be achieved by activating before each encoding session a C-shell script that scans a subdirectory and creates an actualised HTML-file for the menu page. Only the \"<OPTION >< /OPTION >\" lines of the first choice box need to be adapted. Through the HTML SUBMIT command, the op- tions selected by the medical encoder are passed (via a FORM and CGI-SCRIPT) to an external C-program. The C-program takes the filename and the requested sublanguage label(s) as parameters and generates a new HTML-file by replacing the occurrences of the concerned label(s) by a genuine HTML-code (<STRONG> & < /STRONG>) around the rel- evant words). This temporary file is directly fed into the browser and displayed as a second WWW-page (\"PDS-page\"). The words marked (= belonging to the selected semantic sublanguage word class) are displayed in boldface. As the pseudo-HTML codes are ignored by the browser, the rest of the PDS is displayed in a \"neutral\" way. Figure 4 shows the menu-page and PDS-page in which words concerning the diagnosis (H-DIAG), the surgical procedure (H-TTCHIR) and the bodypart (H- PTPART) are marked. The PDS-page is the bottom right part of the figure and partly overlaps the menu- page, which shows the selected PDS and labels 4. 3 Evaluation & Results Before a large scale validation involving \"a gold standard\" and various statistical metrics (e.g. see (Hripcsak et al., 1995)) is set up and conducted, a modest formative evaluation (Hirschman and Thompson, 1995) allowed to rapidly assess the func- tionality of the application from the point of view of the actual user. A limited validation test has been set up. A sample of 100 Dutch sentences of varying length and syntactic complexity was selected. All the words in the dictionary covering the 100 sentences were manually tagged with LSP semantic word class labels. The medical doctor supervising the medi- cal registration activities was asked to provide some combinations of semantic labels relevant from the 4The translation of the document PDS6 is as follows: 61 On 21/1/87 your patient has been operated in our cardiovascular surgery unit. 62 Pre-operative diagnosis: coronary sclerosis. 63 Operative procedure: quintuple coronary bypass. 64 Reconstruction of the left arteria mammaria on the LAD. 65 Venal jump graft from the aorta to the diagonalis, further to the LAD. 66 Venal jump graft from the aorta to the first branch of the circumflexus, further to the second branch of the circumflexus, till the RDP. 67 Single venal bypass from the aorta to the AV- Sulcusbranch. 68 After the procedure, the patient has been admitted to the Intensive Care unit. 69 Enclosed you can find the operation report. viewpoint of a medical encoder (using ICD-9-CM), and to evaluate the system's responses. For all the 100 sentences, pseudo-HTML code was generated. The recall was 100% (all the labels con- cerned were flagged). The precision ranged from 66% to 100% depending on the label combination. Nevertheless, these figures are temporary as exami- nation of the sentences showed that very few words had more than one semantic label so that the medi- cal subselection stage did not have a big impact. A larger test set needs to be processed in order to pro- vide more conclusive results. Probably, recall will drop while precision could raise. Nevertheless, the experience did prove to be valuable as the collabo- rating doctor, who had never heard of NLP before, said he was \"positively surprised and impressed\" by the capabilities of the system. He also judged the tool to be an interesting utility and consented in set- ting up a larger experiment to measure exactly the impact of the tool on the daily routine of the medical encoders. The evaluation procedure of this large test will be organised to comply as much as possible with the evaluation criteria recently proposed by Fried- man and Hripcsak (Friedman and Hripcsak, 1997). 4 Future Research In order to demonstrate the full power of the LSP- MLP, the same sentences could be processed by the joint DMLP/LSP-MLP systems and stored in a RDB table - as is done in other experiments in- volving the LSP system (Hirschman et al., 1981). Specific SQL-queries can then return the ID-number of the sentence with the relevant information instead of the information itself. If the ID-number is added to the original document as a pseudo-HTML code, the same mechanism as mentioned above can be used to highlight the sentences containing the relevant in- formation. Several variants on this base scheme can be thought of. Following the line of research of Sager (Sager et al., 1995a) and Wingert (Wingert et al., 1989), clas- sification codes could already be generated automat- ically (see also (Lovis et al., 1995)) and presented on the screen next to the original text. But the human encoder would remain responsible for the ultimate selection of the exact codes. Another possibility is the creation of \"views\" or \"masks\". HTML files can be generated with \"hard coded\" instructions to emphasise fixed combinations of semantic labels. Buttons in the menu-page allow to display very rapidly the selected view on the PDS. Several experiments for English have already been successfully carried out (?) on the use of \"static WWW-technology\". Interesting as well is the cre- ation of Document Type Definitions (DTD) that as- sociate a particular layout with a specific semantic label (see also (Zweigenbaum et al., 1997)). The DTDs can act as a locally defined view (GUI aspect) on common SGML data (NLP aspect). Other potential applications in the medical domain for the DMLP/LSP-MLP combination are e.g., the determination of patient profiles (Borst et al., 1991), quality assurance (Lyman et al., 1991) and extrac- tion of sign/syptom information for medication (Ly- man et al., 1985). Overviews of the possible utili- sation in the healthcare area of NLP based systems, irrespective of their theoretical background, can be found in (Baud et al., 1992b) & (Sager et al., 1987, chapter2). But before any application of such an extent can be envisaged for Dutch, the words of the dictio- nary database all have to receive the appropriate semantic label(s). Luckily, this process can be au- tomated. The LSP-team has implemented such rou- tines (Hirschman et al., 1975) but other techniques could be applied as well (see (Habert et al., 1996)). From a technical point of view, it would be better to group all the involved software modules (NLP, RDBMS, WWW) on the same platform to opti- mally exploit the potentialities offered by the combi- nation of the components mentioned. Ultimately, a client/server architecture (separating language spe- cific from domain specific issues and the linguistic aspects from user interface aspects) will be the best architecture for a real life application. We can conclude that the application presented above shows the feasibility to integrate Electronic Medical Record (EMR) systems with NLP appli- cations. This is the kernel message of the DOME project (Bouaud et al., 1996) that advocates the use of SGML - and HTML-technology for EMR systems. The above presented WWW-application could thus be integrated in such a hypertextual EMR system. References R. Baud, A.-M. Rassinoux, and J.-R. Scherrer. 1992a. Natural language processing and medi- cal records. In K.C. Lun, editor, Proc. of MED- INFO 92, pages 1362–1367. North-Holland. R. Baud, A.-M. Rassinoux, and J.-R. Scherrer. 1992b. Natural language processing and Semanti- cal Representation of Medical Texts. Methods of Information in Medicine, (31): 117–125. F. Borst, M. Lyman, N.T. Nhàn, L. Tick, N. Sager, and J.-R. Scherrer. 1991. Textinfo: A Tool for Automatic Determination of Patient Clinical Rug Proc. of SCAMC 91, pages 63–67. McGraw-Hill. New York. J. Bouaud, B. Séroussi, and P. Zweigenbaum. 1996. An experiment towards a document centered hy- pertextual computerised patient record. In Proc. of MIE 96, pages 453–457, Amsterdam. IOS Press. E. Bowman. 1996. Coding and classification sys- tems. In M. Abdelhak, S. Grostick, M-A. Hanken, and E. Jacobs (eds.), editors, Health Information: Management of a Strategic Resource, pages 214– 235. W.B. Saunders Company, Philadelphia. C. Chute, editor. 1997. Preprints of the IMIA WG6 Conference on Natural Language and Med- ical Concept Representation. Jacksonville. Commission of Professional and Hospital Activi- ties. 1978. The International Classification of Diseases, Ninth Revision, Clinical Modifications (ICD-9-CM). Ann Arbor, Michigan. L. Dehaspe. 1993. Report on the building of the sc menelas lexical database. Technical Report 93- 002, K.U. Leuven. J. Duisterhout. 1996. Coding and Classifications. In J. van Bemmel, editor, Handbook of Medical Informatics, pages 83–94. Bohn, Stafleu, Van Loghum, Houten/Diegem, preliminary version. C. Friedman and S. Johnson. 1992. Medical text processing: Past achievements, future directions. In M.J. Ball and M.F. Collen, editors, Aspects of the Computer-based Patient Record, pages 212– 228. Springer-Verlag, Berlin. C. Friedman and G Hripcsak. 1997. Evaluating Nat- ural Language Processors in the Clinical Domain. In (Chute, 1997), pages 41–52. R. Grishman and R. Kittredge, editors. 1986. An- alyzing Language in Restricted Domains: Sublan- guage Description and Processing. Lawrence Erl- baum Associates, Hillsdale, New Jersey. B. Habert, E. Naulleau, and A Nazarenko. 1996. Symbolic word classification for medium-size cor- pora. In Proc. of COLING 96, pages 490–495. Z. Harris. 1962. String Analysis of Sentence Struc- tures. Mouton, The Hague. W. Hersh. 1996. Information Retrieval, A Health Care Perspective. Springer Verlag, New York. L. Hirschman and J. Dowding. 1990. Restriction grammar: a logic grammar. In P. Saint-Dizier and S. Szpakowicz, editors, Logic and Logic Grammars L. Hirschman and H. Thompson. 1995. Overview of evaluation in speech and natural language pro- cessing. In J. and Mariani, editor, State of the Art in Natural Language Processing, pages 475–518. L. Hirschman, R. Grishman, and N. Sager. 1975. Grammatically-based automatic word class forma- tion. Information Processing and Management, pages 39–57. L. Hirschman, G. Story, E. Marsh, M. Lyman, and N. Sager. 1981. An experiment in auto- mated health care evaluation from narrative medi- cal records. Computers and Biomedical Research, (14):447–463. L. Hirschman. 1986. Discovering sublanguage struc- tures. In (Grishman and Kittredge, 1986), pages 211–234. G. Hripcsak, C. Friedman, P. Alderson, W. Du- Mouchel, S. Johnson, and P. Clayton. 1995. Un- locking Clinical Data from Narrative Reports: A Study of Natural Language Processing. Annals of Internal Medicine, vol. 122 (9): 681–688. C. Lovis, P.-A. Michel, R. Baud, and J.-R. Scher- rer. 1995. Use of a conceptual semi-automatic ICD-9 encoding system in an hospital environ- ment. In Artificial Intelligence in Medicine, Proc. of AIME 95, pages 331–339. Springer-Verlag. M. Lyman, N. Sager, C. Friedman and E. Chi. 1985. Computer-structured Narrative in Ambu- latory Care: Its Use in Longitudinal Review of Clinical Data. In Proc. of SCAMC 85, pages 82– 86. M. Lyman, N. Sager, L. Tick, N.T. Nhàn, Y. Su, F. Borst, and J.-R. Scherrer. 1991. The applica- tion of natural-language processing to healthcare quality assessment. Medical Decision Making, (11 Suppl): S65–S68. A. McCray, J.-R. Scherrer, C. Safran, and C. Chute (eds.). 1995. Special Issue on Concepts, Knowledge, and Language in Healthcare Informa- tion Systems. Methods of Information in Medicine (34) 1/2. N.T. Nhàn, N. Sager, M. Lyman, L. Tick, F. Borst, and Y. Su. 1989. A medical language proces- sor for two indo-european languages. In Proc. of SCAMC 89, pages 554–558. N. Oliver. 1992. A sublanguage based medical lan- guage processing system for German. Ph.D. the- sis, Dept. of Computer Science. New York Univer- sity. N. Sager, C. Friedman and M. Lyman. 1981. Natu- grammar of English and its applications. Addison Wesley, Reading, Massachusetts. N. Sager, C. Friedman, and M. Lyman. 1987. Medi- cal Language Processing: Computer Management of Narrative Data. Addison Wesley, Reading, Massachusetts. N. Sager, M. Lyman, N. Nhàn, and L. Tick. 1995a. Medical language processing: Applications to pa- tient data representation and automatic encoding. Methods of Information in Medicine, (34):140– 146. N. Sager, N. Nhàn, M. Lyman, and L. Tick. 1996. Medical Language Processing with SGML display. In Proc. of the 1996 AMIA Annual Fall Sympo- sium, pages 547–551. J.R. Scherrer, R. Coté, and S. Mandil (eds.). 1989. Computerized Natural Medical Language Process- ing for Knowledge Representation. North Holland. P. Spyns and G. Adriaens. 1992. Applying and Improving the Restriction Grammar Approach for Dutch Patient Discharge Summaries. In Proc. of COLING 92, pages 1164–1168. P. Spyns. 1994. A robust category guesser for Dutch medical language. In Proc. of ANLP 94, pages 150– 155. ACL. P. Spyns. 1995. A contextual disambiguator for Dutch medical language. In Proc. of the BeNeLux Workshop on Logic Programming BENELOG 95, pages 20–24, Gent. P. Spyns. 1996a. Medical language processing and reusability of resources: a case study applied to Dutch. In Proc. of MIE 96, pages 1147–1152, Amsterdam. IOS Press. P. Spyns. 1996b. Natural language processing in medicine: An overview. Methods of Information in Medicine, (35):285–302. P. Spyns. 1996c. Natural Language Processing in the bio-medical area: Design and Implementation of an Analyser for Dutch. Ph.D. thesis, Dept. of Computer Science, K.U. Leuven. F. Wingert, D. Rothwell, and R. Coté. 1989. Au- tomated indexing into SNOMED and ICD. In (Scherrer et al., 1989), pages 1–5.38. P. Zweigenbaum, J. Bouaud, B. Bachimont, J. Charlet, B. Séroussi, J.F. Boisvieux. 1997. From Text to Knowledge: a Unifying Document- Oriented View of Analyzed Medical Language. In (Chute. 1997). pages 21–30. [ERROR: Failed to process this page - Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.]"
  },
  {
    "title": "Tagging Sentence Boundaries",
    "abstract": "In this paper we tackle sentence boundary disambiguation through a part-of-speech (POS) tagging framework. We describe necessary changes in text tokenization and the implementation of a POS tagger and provide results of an evaluation of this system on two corpora. We also describe an extension of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling. This made the resulting system robust to domain and topic shifts.",
    "content": "1 Introduction Sentence boundary disambiguation (SBD) is an im- portant aspect in developing virtually any practi- cal text processing application - syntactic parsing, Information Extraction, Machine Translation, Text Alignment, Document Summarization, etc. Seg- menting text into sentences in most cases is a sim- ple matter a period, an exclamation mark or a question mark usually signal a sentence boundary. However, there are cases when a period denotes a decimal point or is a part of an abbreviation and thus it does not signal a sentence break. Further- more, an abbreviation itself can be the last token in a sentence, in which case its period acts at the same time as part of this abbreviation and as the end-of-sentence indicator (fullstop). The first large class of sentence boundary disam- biguators uses manually built rules which are usually encoded in terms of regular expression grammars supplemented with lists of abbreviations, common words, proper names, etc. For instance, the Alem- bic workbench (Aberdeen et al., 1995) contains a sentence splitting module which employs over 100 regular-expression rules written in Flex. To put to- gether a few rules which do a job is fast and easy, but to develop a good rule-based system is quite a labour consuming enterprise. Another potential shortcom- ing is that such systems are usually closely tailored to a particular corpus and are not easily portable across domains. Automatically trainable software is generally soon as a way of producing systems quickly re-trainable for a new corpus, domain or even for another lan- guage. Thus, the second class of SBD systems em- ploys machine learning techniques such as decision tree classifiers (Riley, 1989), maximum entropy mod- eling (MAXTERMINATOR) (Reynar and Ratna- parkhi, 1997), neural networks (SATZ) (Palmer and Hearst, 1997), etc.. Machine learning systems treat the SBD task as a classification problem, using fea- tures such as word spelling, capitalization, suffix, word class, etc., found in the local context of poten- tial sentence breaking punctuation. There is, how- ever, one catch all machine learning approaches to the SBD task known to us require labeled examples for training. This implies an investment in the an- notation phase. corpus There are two corpora normally used for evalua- tion and development in a number of text process- ing tasks and in the SBD task in particular: the Brown Corpus and the Wall Street Journal (WSJ) corpus – both part of the Penn Treebank (Mar- cus, Marcinkiewicz, and Santorini, 1993). Words in both these corpora are annotated with part-of- speech (POS) information and the text is split into documents, paragraphs and sentences. This gives all necessary information for the development of an SBD system and its evaluation. State-of-the- art machine-learning and rule-based SBD systems achieve the error rate of about 0.8-1.5% measured on the Brown Corpus and the WSJ. The best per- formance on the WSJ was achieved by a combination of the SATZ system with the Alembic system - 0.5% error rate. The best performance on the Brown Cor- pus, 0.2% error rate, was reported by (Riley, 1989), who trained a decision tree classifier on a 25 million word corpus. 1.1 Word-based vs. Syntactic Methods The first source of ambiguity in end-of-sentence marking is introduced by abbreviations: if we know that the word which precedes a period is not an ab- breviation, then almost certainly this period denotes a sentence break. However, if this word is an ab- breviation, then it is not that easy to make a clear decision. The second major source of information for approaching the SBD task comes from the word which follows the period or other sentence splitting punctuation. In general, when the following word is punctuation, number or a lowercased word - the abbreviation is not sentence terminal. When the fol- lowing word is capitalized the situation is less clear. If this word is a capitalized common word - this sig- nals start of another sentence, but if this word is a proper name and the previous word is an abbrevia- tion, then the situation is truly ambiguous. Most of the existing SBD systems are word-based. They employ only lexical information (word capital- ization, spelling, suffix, etc.) to predict whether a capitalized word-token which follows a period is a proper name or is a common word. Usually this is implemented by applying the lexical lookup method where a word is assigned its category according to which word-list it belongs to. This, however, is clearly an oversimplification. For instance, the word \"Black\" is a frequent surname and at the same time it is a frequent common word, thus the lexical infor- mation is not very reliable in this case. But by em- ploying local context one can more robustly predict that in the context \"Black described..\" this word acts as a proper name and in the context \"Black umbrella..\" this word acts as a common word. It is almost impossible to robustly estimate con- texts larger than single focal word using word-based methods - even bigrams of words are too sparse. For instance, there are more than 50,000 distinct words in the Brown Corpus, thus there are 250,000 poten- tial word bigrams, but only a tiny fraction of them can be observed in the corpus. This is why words are often grouped into semantic classes. This, how- ever, requires large manual effort, is not scalable and still covers only a fraction of the lexica. Syntactic context is much easier to estimate because the num- ber of syntactic categories is much smaller than the number of distinct words. A standard way to identify syntactic categories for word-tokens is part-of-speech (POS) tagging. There syntactic categories are represented as POS tags e.g. NNS - plural noun, VBD - verb past form, JJR - com- parative adjective, etc. There exist several tag-sets which are currently in use - some of them reflect only the major syntactic information such as part- of-speech, number, tense, etc., whereas others reflect more refined information such as verb subcategoriza- tion, distinction between mass and plural nouns, etc. Depending on the level of detail one tag-set can incorporate a few dozen tags where another can in- corporate a few hundred, but still such tags will be considerably less sparse than individual words. For instance, there are only about 40 POS tags in the Penn Treebank tag-set, therefore there are only 240 potential POS bigrams. Of course, not every word combination and POS tag combination is possible. but these numbers give a rough estimation of the magnitude of required data for observing necessary contexts for words and POS tags. This is why the \"lexical lookup\" method is the major source of in- formation for word-based methods. The \"lexical lookup\" method for deciding whether a capitalized word in a position where capitalization is expected (e.g. after a fullstop) is a proper name or a common word gives about an 8% error rate on the Brown Corpus. We developed and trained a POS tagger which reduced this error more than by half - achieving just above a 3% error rate. On the WSJ corpus the POS tagging advantage was even greater: our tagger reduced the error rate from 15% of the lexical lookup approach to 5%. This suggests that the error rate of a sentence splitter can be reduced proportionally by using the POS tagging method- ology to predict whether a capitalized word after a period is a proper name or a common word. 1.2 The SATZ System (Palmer and Hearst, 1997) described an approach which recognized the potential of the local syntac- tic context for the SBD problem. Their system, SATZ, used POS information for words in the lo- cal context of potential sentence splitting punctu- ation. However, what is interesting is that they found difficulty in applying a standard POS tag- ging framework for determining POS information for the words: \"However, requiring a single part-of- speech assignment for each word introduces a pro- cessing circularity: because most part-of-speech tag- gers require predetermined sentence boundaries, the boundary disambiguation must be done before tag- ging. But if the disambiguations done before tag- ging, no part-of-speech assignments are available for the boundary determination system\". Instead, they applied a simplified method. The SATZ system mapped Penn Treebank POS tags into a set of 18 generic POS categories such as noun, ar- ticle, verb, proper noun, preposition, etc. Each word was replaced with a set of these generic categories that it can take on. Such sets of generic syntac- tic categories for three tokens before and three to- kens after the period constituted a context which was then fed into two kinds of classifiers (decision trees and neural networks) to make the predictions. This system demonstrated reasonable accuracy (1.0% error rate on the WSJ corpus) and also ex- hibited robustness and portability when applied to other domains and languages. However, the N- grams of syntactic category sets have two important disadvantages in comparison to the traditional POS tagging which is usually largely based (directly or indirectly) on the N-grams of POS tags. First, syn- tactic category sets are much sparser than syntactic categories (POS tags) and, thus, require more data for training. Second, in the N-grams-only method ...<W C='RB' A='N'>soon</W><W C='.'>.</W> <W A='Y' C='NNP'>Mr</W><W C='A'>.</W>... ...<W C='VBD'>said</W> <W C='NNP' A='Y'>Mr</W><W C='A'>.</W> <W C='NNP'>Brown</W>... ...<W C=','>, </W> <W C='NNP' A='Y'>Tex</W><W C='*'>.</W> <W C='DT'>The</W>... Figure 1: Example of tokenization and markup. Text is tokenized into tokens represented as XML elements with attributes: A='Y' - abbreviation, A='N' - not abbreviation, C - part-of-speech tag attribute, C='.' - fullstop, C='A' - part of abbreviation, C= '*' - a fullstop and part of abbreviation at the same time. no influence from the words outside the N-grams can be traced, thus, one has to adopt N-grams of suffi- cient length which in its turn leads either to sparse contexts or otherwise to sub-optimal discrimination. The SATZ system adopted N-grams of length six. In contrast to this, POS taggers can capture influ- ence of the words beyond an immediate N-gram and, thus, usually operate with N-grams of length two (bi- grams) or three (three-grams). Furthermore, in the POS tagging field there exist standard methods to cope with N-gram sparseness and unknown words. Also there have been developed methods for unsu- pervised training for some classes of POS taggers. 1.3 This Paper In this paper we report on the integration of the sentence boundary disambiguation functionality into the POS tagging framework. We show that sentence splitting can be handled during POS tagging and the above mentioned \"circularity\" can be tackled by us- ing a non-traditional tokenization and markup con- ventions for the periods. We also investigate reduc- ing the importance of pre-existing abbreviation lists and describe guessing strategies for unknown abbre- viations. 2 New Handling of Periods In the traditional Treebank schema, abbreviations are tokenized together with their trailing periods and, thus, stand-alone periods unambiguously sig- nal end-of-sentence. For handling the SBD task we suggest tokenizing periods separately from their ab- breviations and treating a period as an ambiguous token which can be marked as a fullstop ('.'), part- of-abbreviation ('A') or both ('*'). An example of such markup is displayed on Figure 1. Such markup allows us to treat the period similarly to all other words in the text: a word can potentially take on one of a several POS tags and the job of a tagger is to resolve this ambiguity. In our experiments we used the Brown Corpus and the Wall Street Journal corpus both taken from the Penn Treebank (Marcus, Marcinkiewicz, and San- torini, 1993). We converted both these corpora from the original format to our XML format (as displayed on Figure 1), split the final periods from the abbrevi- ations and assigned them with C='A' and C= '*' tags according to whether or not the abbreviation was the last token in a sentence. There were also quite a few infelicities in the original tokenization and tagging of the Brown Corpus which we corrected by hand. Using such markup it is straightforward to train a POS tagger which also disambiguates sentence boundaries. There is, however, one difference in the implementation of such tagger. Normally, a POS tagger operates on a text-span which forms a sen- tence and this requires performing the SBD before tagging. However, we see no good reason why such a text-span should necessarily be a sentence, because almost all the taggers do not attempt to parse a sen- tence and operate only in the local window of two to three tokens. The only reason why the taggers traditionally op- erate on the sentence level is because there exists a technical issue of handling long text spans. Sentence length of 30-40 tokens seems to be a reasonable limit and, thus, having sentences pre-chunked before tag- ging simplifies life. This issue, however, can be also addressed by breaking the text into short text-spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word-tokens can be terminated at an unambiguous word because this unambiguous word token will be the only history used in tagging of the next token. A trigram tagger operates within a window of three tokens, and thus a sequence of word-tokens can be terminated when two unambigu- ous words follow each other. 3 Tagging Experiment Using the modified treebank we trained a tri-gram POS tagger (Mikheev, 1997) based on a combination of Hidden Markov Models (HMM) and Maximum Entropy (ME) technologies. Words were clustered into ambiguity classes (Kupiec, 1992) according to sets of POS tags they can take on. This is a stan- dard technique that was also adopted by the SATZ system¹. The tagger predictions were based on the ambiguity class of the current word together with ¹The SATZ system operated with a reduced set of 18 generic categories instead of 40 POS tags of the Penn Tree- bank tag-set. Table 1: POS Tagging on sentence splitting punctuation and ambiguously capitalized words Tagger Feature Set Error on Sentence Punct. Error on Words in Mandatory Pos. Brown Corpus WSJ Corpus Brown Corpus WSJ Corpus Upper Bound 0.01 0.13 POS Tagger 0.25% 0.39% 3.15% 4.72% POS Tagger Enhanced 0.20% 0.31% 1.87% 3.22% POS Tagger/No abbr. list 0.98% 1.95% 3.19% 5.29% POS Tagger Enhanced/ No abbr. list 0.65% 1.39% 1.91% 3.28% the POS trigrams: hypothesized current POS tag and partially disambiguated POS tags of two previous word-tokens. We also collected a list of abbreviations as explained later in this paper and used the information about whether a word is an abbreviation, ordinary word or potential abbreviation (i.e. a word which could not be robustly classified in the first two categories). This tagger employed Maximum Entropy models for tag transition and emission estimates and Viterbi algorithm (Viterbi, 1967) for the optimal path search. Using the forward-backward algorithm (Baum, 1972) we trained our tagger in the unsupervised mode i.e. without using the annotation available in the Brown Corpus and the WSJ. For evaluation purposes we trained our tagger on the Brown Corpus and applied it to the WSJ corpus and vice versa. We preferred this method to ten-fold cross-validation because this allowed us to produce only two tagging models instead of twenty and also this allowed us to test the tagger in harsher conditions when it is applied to texts which are very distant from the ones it was trained on. In this research we concentrated on measuring the performance only on two categories of word-tokens: on periods and other sentence-ending punctuation and on word-tokens in mandatory positions. Mandatory positions are positions which might require a word to be capitalized e.g. after a period, quotes, brackets, in all-capitalized titles, etc. At the evaluation we considered proper nouns (NNP), plural proper nouns (NNPS) and proper adjectives² (JJP) to signal a proper name, all all other categories were considered to signal a common word or punctuation. We also did not consider as an error the mismatch between \".\" and \"*\" categories because both of them signal that a period denotes the end of sentence and the difference between them is only whether this period follows an abbreviation or a regular word. In all our experiments we treated embedded sentence boundaries in the same way as normal sentence boundaries. The embedded sentence boundary occurs when there is a sentence inside a sentence. This ² These are adjectives like \"American\" which are always written capitalized. We identified and marked them in the WSJ and Brown Corpus. can be a quoted direct speech sub-sentence inside a sentence, this can be a sub-sentence embedded in brackets, etc. We considered closing punctuation of such sentences equal to closing punctuation of ordinary sentences. There are two types of error the tagger can make when disambiguating sentence boundaries. The first one comes from errors made by the tagger in identifying proper names and abbreviations. The second one comes from the limitation of the POS tagging approach to the SBD task. This is when an abbreviation is followed by a proper name: POS information normally is not sufficient to disambiguate such cases and the tagger opted to resolve all such cases as \"not sentence boundary\". There are about 5-7% of such cases in the Brown Corpus and the WSJ and the majority of them, indeed, do not signal a sentence boundary. We can estimate the upper bound for our approach by pretending that the tagger was able to identify all abbreviations and proper names with perfect accuracy. We can simulate this by using the information available in the treebank. It turned out that the tagger marked all the cases when an abbreviation is followed by a proper name, punctuation, non-capitalized word or a number as \"not sentence boundary\". All other periods were marked as sentence-terminal. This produced 0.01% error rate on the Brown Corpus and 0.13% error rate on the WSJ as displayed in the first row of Table 1. In practice, however, we cannot expect the tagger to be 100% correct and the second row of Table 1 displays the actual results of applying our POS tagger to the Brown Corpus and the WSJ. General tagging performance on both our corpora was a bit better than a 4% error rate which is in line with the standard performance of POS taggers reported on these two corpora. On the capitalized words in mandatory positions the tagger achieved a 3.1-4.7% error rate which is an improvement over the lexical lookup approach by 2-3 times. On the sentence breaking punctuation the tagger performed extremely well - an error rate of 0.39% on the WSJ and 0.25% on the Brown Corpus. If we compare these results with the upper bound we see that the errors made by the tagger on the capitalized words and abbreviations instigated about a 0.25% error rate on the sentence boundaries. We also applied our tagger to single-case texts. We converted the WSJ and the Brown Corpus to upper-case only. In contrast to the mixed case texts where capitalization together with the syntactic in- formation provided very reliable evidence, syntactic information without capitalization is not sufficient to disambiguate sentence boundaries. For the ma- jority of POS tags there is no clear preference as to whether they are used as sentence starting or sen- tence internal. To minimize the error rate on single case texts, our tagger adopted a strategy to mark all periods which follow abbreviations as \"non-sentence boundaries\". This gave a 1.98% error rate on the WSJ and a 0.51% error rate on the Brown Corpus. These results are in line with the results reported for the SATZ system on single case texts. 4 Enhanced Feature Set (Mikheev, 1999) described a new approach to the disambiguation of capitalized words in mandatory positions. Unlike POS tagging, this approach does not use local syntactic context, but rather it applies the so-called document-centered approach. The essence of the document-centered approach is to scan the entire document for the contexts where the words in question are used unambiguously. Such contexts give the grounds for resolving ambiguous contexts. For instance, for the disambiguation of capital- ized words in mandatory positions the above rea- soning can be crudely summarized as follows: if we detect that a word has been used capitalized in an unambiguous context (not in a mandatory position), this increases the chances for this word to act as a proper name in mandatory positions in the same document. And, conversely, if a word is seen only lowercased, this increases the chances to downcase it in mandatory positions of the same document. By collecting sequences and unigrams of unambiguously capitalized and lowercased words in the document and imposing special ordering of their applications (Mikheev, 1999) reports that the document-centered approach achieved a 0.4-0.7% error rate with cover- age of about 90% on the disambiguation of capital- ized words in mandatory positions. We decided to combine this approach with our POS tagging system in the hope of achieving better accuracy on capitalized words after the periods and therefore improving the accuracy of sentence split- ting. Although the document-centered approach to capitalized words proved to be more accurate than POS tagging, the two approaches are complimentary to each other since they use different types of infor- mation. Thus, the hybrid system can bring at least two advantages. First, unassigned by the document- centered approach 10% of the ambiguously capital- ized words can be assigned using a standard POS tagging method based on the local syntactic con- text. Second, the local context can correct some of the errors made by the document-centered approach. To implement this hybrid approach we incorporated the assignments made by the document-centered ap- proach to the words in mandatory positions to our POS tagging model by simple linear interpolation. The third row of Table 1 displays the results of the application of the extended tagging model. We see an improvement on proper name recognition by about 1.5%: overall error rate of 1.87% on the Brown Corpus and overall error rate 3.22% on the WSJ. This in its turn allowed for better tagging of sen- tence boundaries: a 0.20% error rate on the Brown Corpus and a 0.31% error rate on the WSJ, which corresponds to about 20% cut in the error rate in comparision to the standard POS tagging. 5 Handling of Abbreviations Information about whether a word is an abbrevia- tion or not is absolutely crucial for sentence splitting. Unfortunately, abbreviations do not form a closed set, i.e., one cannot list all possible abbreviations. It gets even worse - abbreviations can coincide with ordinary words, i.e., \"in\" can denote an abbrevia- tion for \"inches\", \"no\" can denote an abbreviation for \"number\", \"bus\" can denote an abbreviation for \"business\", etc. Obviously, a practical sentence splitter which in our case is a POS tagger, requires a module that can guess unknown abbreviations. First, such a module can apply a well-known heuristic that single-word abbreviations are short and normally do not include vowels (Mr., Dr., kg.). Thus a word without vowels can be guessed to be an abbreviation unless it is writ- ten in all capital letters which can be an acronym (e.g. RPC). A span of single letters, separated by periods forms an abbreviation too (e.g. Y.M.C.A.). Other words shorter than four characters and un- known words shorter than five characters should be treated as potential abbreviations. Although these heuristics are accurate they manage to identify only about 60% of all abbreviations in the text which translates at 40% error rate as shown in the first row of Table 2. These surface-guessing heuristics can be supple- mented with the document-centered approach (DCA) to abbreviation guessing, which we call Positional Guessing Strategy (PGS). Although a short word which is followed by a period can potentially be an abbreviation, the same word when occurring in the same document in a different context can be unam- biguously classified as an ordinary word if it is used without a trailing period, or it can be unambigu- ously classified as an abbreviation if it is used with a Table 2: Error rate for different abbreviation identification methods Corpus WSJ Brown surface guess 39% 40% surface guess and DCA 8.4% 9.5% surface guess and DCA and abbr. list 0.8% 1.2% trailing period and is followed by a lowercased word or a comma. This allows us to assign such words accordingly even in ambiguous contexts of the same document, i.e., when they are followed by a period. For instance, the word \"Kong\" followed by a pe- riod and then by a capitalized word cannot be safely classified as a regular word (non-abbreviation) and therefore it is a potential abbreviation. But if in the same document we detect a context \"lived in Hong Kong in 1993\" this indicates that \"Kong\" is nor- mally written without a trailing period and hence is not an abbreviation. Having established that, we can apply this findings to the non-evident con- texts and classify \"Kong\" as a regular word (non- abbreviation) throughout the document. However, if we detect a context such as \"Kong., said\" this in- dicates that in this document \"Kong\" is normally written with a trailing period and hence is an ab- breviation. This gives us grounds to classify \"Kong\" as an abbreviation in all its occurrences within the same document. The positional guessing strategy relies on the assumption that there is a consistency of writing within the same document. Different authors can write \"Mr\" or \"Dr\" with or without trailing period but we assume that the same author (the author of a document) will write consistently. However, there can occur a situation when a potential abbre- viation is used as a regular word and as an abbre- viation within the same document. This is usually the case when an abbreviation coincides with a reg- ular word e.g. \"Sun.\" (meaning Sunday) and \"Sun\" (the name of a newspaper). To tackle this prob- lem, our strategy is to collect not only unigrams of potential abbreviations in unambiguous contexts as explained earlier but also their bigrams with the pre- ceding word. Now the positional guessing strategy can assign ambiguous instances on the basis of the bigrams it collected from the document. For instance, if in a document the system found a context \"vitamin C is\" it stores the bigram \"vitamin C\" and the unigram \"C\" with the information that it is a regular word. If in the same document the system also detects a context \"John C. later said\" it stores the bigram \"John C.\" and the unigram \"C\" with the information that it is an abbreviation. Here we have conflicting information for the word \"C\" it was detected as acting as a regular word and as an abbreviation within the same document so there is not enough information to resolve ambiguous cases purely using the unigram. However, some cases can be resolved on the basis of the bigrams e.g. the sys- tem will assign \"C\" as an abbreviation in an ambigu- ous context \"... John C. Research ...\" and it will assign \"C\" as a regular word (non-abbreviation) in an ambiguous context \"... vitamin C. Research ...\". When neither unigrams nor bigrams can help to resolve an ambiguous context for a potential abbre- viation, the system decides in favor of the more fre- quent category deduced from the current document for this potential abbreviation. Thus if the word \"In\" was detected as acting as a non-abbreviation (preposition) five times in the current document and two times as abbreviation (for the state Indiana), in a context where neither of the bigrams collected from the document can be applied, \"In\" is assigned as a regular word (non-abbreviation). The last re- sort strategy is to assign all non-resolved cases as non-abbreviations. Apart from the ability of finding abbreviations be- yond the scope of the surface guessing heuristics, the document-centered approach also allows for the clas- sification of some potential abbreviations as ordinary words, thus reducing the ambiguity for the sentence splitting module. The second row of Table 2 shows the results when we supplemented the surface guess- ing heuristics with the document-centered approach. This alone gave a huge improvement over the surface guessing heuristics. Using our abbreviation guessing module and an unlabeled corpus from New York Times 1996 of 300,000 words, we compiled a list of 270 abbrevia- tions which we then used in our tagging experiments together with the guessing module. In this list we included abbreviations which were identified by our guesser and which had a frequency of five or greater. When we combined the guessing module together with the induced abbreviation list and applied it to the Brown Corpus and the WSJ we measured about 1% error rate on the identification of abbreviation as can be seen in the third row of Table 2. We also tested our POS tagger and the extended tagging model in conjunction with the abbreviation guesser only, when the system was not equipped with the list of abbreviations. The error rate on capital- ized words went just a bit higher while the error rate on the sentence boundaries increased by two- three times but still stayed reasonable. In terms of absolute numbers, the tagger achieved a 0.98% error rate on the Brown Corpus and a 1.95% er- ror rate on the WSJ when disambiguating sentence boundaries. The extended system without the ab- breviation list was about 30% more accurate and achieved a 0.65% error rate on sentence splitting on the Brown Corpus and 1.39% on the WSJ corpus as shown in the last row of Table 1. The larger im- pact on the WSJ corpus can be explained by the fact that it has a higher proportion of abbreviations than the Brown Corpus. In the Brown Corpus, 8% of potential sentence boundaries come after abbre- viations. The WSJ is richer in abbreviations and 17% of potential sentence boundaries come after ab- breviations. Thus, unidentified abbreviations had a higher impact on the error rate in the WSJ. 6 Conclusion In this paper we presented an approach which treats the sentence boundary disambiguation problem as part of POS tagging. In its \"vanilla\" version the sys- tem performed above the results recently quoted in the literature for the SBD task. When we combined the \"vanilla\" model with the document-centered ap- proach to proper name handling we measured about a 20% further improvement in the performance on sentence splitting and about a 40% improvement on capitalized word assignment. POS tagging approach to sentence splitting pro- duces models which are highly portable across differ- ent corpora: POS categories are much more frequent than individual words and less affected by unseen words. This differentiates our approach from word- based sentence splitters. In contrast to (Palmer and Hearst, 1997), which also used POS categories as predictive features, we relied on a proper POS tag- ging technology, rather than a shortcut to POS tag estimation. This ensured higher accuracy of the POS tagging method which cut the error rate of the SATZ system by 69%. On the other hand because of its simplicity the SATZ approach is probably easier to implement and faster to train than a POS tagger. On single-case texts the syntactic approach did not show a considerable advantage to the word-based methods: all periods which followed abbreviations were assigned as \"sentence internal\" and the results achieved by our system on the single-case texts were in line with that of the other systems. The abbreviation guessing module which com- bines the surface guessing heuristics with the doc- ument centered approach makes our system very ro- bust to new domains. The system demonstrated strong performance even without being equipped with a list of known abbreviations which, to our knowledge, none of previously described SBD sys- tems could achieve. Another important advantage of our approach we see is that it requires potentially a smaller amount of training data and this training data does not need to be labeled in any way. In training a conventional sentence splitter one usually collects periods with the surrounding context and these samples have to be manually labeled. In our case a POS tagging model is trained on all available words, so syntactic dependencies between words which can appear in a local context of a period can be established from other parts of the text. Our system does not require annotated data for training and can be unsupervis- edly trained from raw texts of approximately 300,000 words or more. There are ways for further improvement of the performance of our system by combining it with a word-based system which encodes specific behavior for individual words. This is similar to how the SATZ system was combined with the Alembic sys- tem. This addresses the limitation of our syntactic approach in treating cases when an abbreviation is followed by a proper name always as \"non sentence boundary\". In fact we encoded one simple rule that an abbreviation which stands for an American state (e.g. Ala. or Kan.) always is sentence terminal if followed by a proper name. This reduced the error rate on the WSJ from 0.31% to 0.25%. Another av- enue for further development is to extend the system to other languages. References Aberdeen, J., J Burger, D. Day, L. Hirschman, P. Robinson, and M. Vilain. 1995. Mitre: De- scription of the alembic system used for muc-6. In The Proceedings of the Sixth Message Under- standing Conference (MUC-6), Columbia, Mary- land. Morgan Kaufmann. Baum, L.E. 1972. An inequality and associated maximization techique in statistical estimation for probabilistic functions of a Markov process. In- equalities 3 (1972) 1-8. Kupiec, Julian. 1992. Robust part-of-speech tagging using a hidden markov model. Computer Speech and Language. Marcus, Mitchell, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large anno- tated corpus of english: The penn treebank. Com- putational Linguistics, 19(2):313-329. Mikheev, A. 1999. A knowledge-free method for capitalized word disambiguation. In Proceedings of the 37th Conference of the Association for Computational Linguistics (ACL'99), pages 159- 168. University of Maryland. Mikheev, A., 1997. LT POS - the LTG part of speech tagger. Language Technology Group, University of Edinburgh. www.ltg.ed.ac.uk/software/pos. Palmer, D. D. and M. A. Hearst. 1997. Adaptive multilingual sentence boundary disambiguation. Computational Linguistics. Reynar, J. C. and A. Ratnaparkhi. 1997. A max- imum entropy approach to identifying sentence boundaries. In Proceedings of the Fifth ACL Con- ference on Applied Natural Language Processing (ANLP'97), Washington, D.C. Riley, M.D. 1989. Some applications of tree-based modeling to speech and language indexing. In Proceedings of the DARPA Speech and Natu- ral Language Workshop, pages 339-352. Morgan Kaufman. Viterbi, A.J. 1967. Error bounds for convolutional codes and an asymptomatically optimal decod- ing algorithm. IEEE Transactions on Information Theory."
  },
  {
    "title": "An Interactive Translation Support Facility for Non-Professional Users",
    "abstract": "We present an interactive translation method to support non-professional users to write an original document. The method, combining dictionary lookup function and user-guided stepwise interactive machine translation, allows the user to obtain clear result with an easy operation. We implemented the method as an English writing support facility that serves as a translation support front-end to an arbitrary application.",
    "content": "1 Introduction With the steady decrease of network communica- tion cost and equipment prices, world-wide com- puter networks and the number of its users are growing very rapidly. However, there is a large obstacle against global communication over net- works, namely the language barrier, especially for non English-speaking people. This is a major reason personal EJ (English to Japanese) machine transla- tion systems are gaining popularity in Japan. They help the user to quickly grasp the content of web pages, by providing rough translation. Since speed and lexical coverage are most important require- ments, conventional automatic machine translation systems developed so far are useful for this purpose. Contrary to the EJ direction, the major task in JE (Japanese to English) direction will be writing short original documents, such as e-mail. The most impor- tant requirement will be translation quality, because the reader is usually different from the MT user. To control quality, some kind of human interaction will be inevitable. However, interactive support for conventional MT systems doesn't seem suitable for these users, since they are primarily intended for professional translators. Their post-editing function often requires working in a special environment that requires special training. An interactive, easy-to- use translation support facility, targeted for non- professional translators, is desirable. We may expect that these users have basic knowl- edge and ability to read and understand English. This expectation is natural and realistic in a coun- try like Japan, where all high-school graduates are supposed to have completed six year course in En- glish. Their reading skill and grammar knowledge is usually enough to judge the quality of current MT systems, but they may need help from MT systems when browsing the Internet. For the JE direction, they will not be satisfied with the raw output of con- ventional MT systems, but it will be too laborious to write down English sentence from scratch. For these users, online dictionaries have been used be- cause of the reliability of the result. However, in spite of abundant information within the dictionary such as inflection, verbal case frame, idioms and so on, the only electronically available part is spelling of translation equivalents (through copy & paste). Other information is only presented to be read as in the case of a paper dictionary, with all further work left to the user. In this article, we present an interactive trans- lation method and its implementation, which has advantages of both a dictionary look-up tool and a machine translation system. The system has an in- teractive interface similar to Kana-Kanji conversion method, and initially serves as a dictionary look- up tool. After dictionary lookup, the user can in- voke syntactic transformation in terms of grammat- ical information in the dictionary. Syntactic trans- formation proceeds step by step in a bottom-up manner, combining smaller translation components into larger ones. This \"dictionary-based interactive translation\" approach allows the user to fully utilize syntactic information in the dictionary while main- taining clarity of the result. In the next section, we give a simple example of translation steps and provide a general idea of the method. In section 3, we describe the basic model and associated operations. Section 4 gives further explanation about disambiguation capability of the interactive operations. Section 5 discusses exten- sions to the basic model to treat linguistic phenom- ena such as idiomatic expressions. Section 6 de- scribes the current implementation as a front-end to an arbitrary application. In section 7, the method is compared with former approaches. The final section is the conclusion. 2 An Example In this section we show the basic steps of simple sentence translation, in order to give a general idea about how the method works. Consider a situation where the user is writing a message in English, using an editor of a mail pro- gram. Our system is running as a daemon. While the user is typing English characters, the system does nothing special and let them through to the editor window. The system wakes up when the user toggles on Japanese input. The moment the first Japanese character is typed in, the main translation window is opened, and all subsequent characters are typed in to that window instead of the editor win- dow. Suppose the input sentence is one shown in fig- ure 1 (a). As soon as (a) is entered, the dictionary look-up function is invoked automatically. A mor- phological analyzer recognizes word boundaries in the sentence, looks up corresponding word entries in the system dictionary, and shows the result in the main window (b).¹ Content words are replaced by a translation equivalent assumed most plausible, while functional words are left unchanged. This representation step, in which English words (content words) and Japanese words (functional words) are mixed, separates steps for word trans- lation and later syntactic transformation, making translation steps clearer. Since word order and func- tional words carrying grammatical functions are un- changed, the user can easily recognize the skeleton of the sentence, and clearly grasp the correspondence between the original word and its translation equiv- alent. This representation is not only to show the re- sults of dictionary look-up like (Canisius, 1977), but also carries all interactive operations of the method with it, and has a double role of showing information and being objects of interactive manipulation. Translation equivalent alternatives for the cursor position word (focus word) are displayed in an alter- natives window, appearing nearby that word. Fig- ure 2 is a snapshot of the alternatives window for ronbun (paper). The second line is highlighted to show that it is the current selection. The user can ¹Slanted words show romaji transcription of respec- tive Japanese words. They don't appear on a real window. (a) 私 は彼に論文を渡した watashi wa kare ni ronbun -o watashi -ta I TOP he DAT paper OBJ give PAST (b) Iはhe に paper をgiveた -wa -ni -0 (c) I gave him a paper -ta Figure 1: Translation of a simple sentence ronbun paper [noun] [typical word] thesis [noun] [for degree] essay [noun] [general] dissertation [noun] [for degree] Figure 2: Alternatives Window for ronbun change the selection simply by a cursor movement or mouse click on this window, then the corresponding translation equivalent on the main window changes synchronously. To see alternatives for another word, the user has only to move the cursor to that word on the main window. There is also a CD-ROM dictio- nary access function, making translation equivalent selection easier. In addition, the user can change an inflection in a similar manner on an inflection selec- tion window, opened by the user's request. If the user needs only the result of dictionary lookup, s/he can signal the end of translation at this point, just after choosing the translation equivalent. If a translation is necessary, the user needs to go one more step. At the same time as initial translation equivalent selection, the system predicts an appro- priate area for translation, as shown by an underline (b). Just as the translation equivalent selection can be freely changed, the area can be changed by drag- ging the left or right edge of the underline. After the user confirms selections of translation equiva- lents and translation area on (b), the user invokes translation. The system performs syntactic trans- formation using syntactic information in the dictio- nary such as the verbal case frame of the main verb in the area, shows the result on the main window, and replaces the original sentence with the result (c). When there is more than one possible transla- tion, the different possibilities are shown in an al- ternatives window similar to figure 2, allowing the user to change the choice. When the user triggers translation end, the result is sent to the original ed- itor window. The user can continue to work in the editor after turning off Japanese input. 3 Description of the Method Most important characteristics of this interactive translation method is that the Japanese input is con- verted to English in several steps allowing user inter- action at each step. In intermediate steps, a mixture of target language expression and source language expression are shown to give the current status of the interactive translation. Translation proceeds from a smaller unit as word to a larger unit as sentence, step by step in a bottom-up manner. The result can be checked and corrected at each step, making it easier to obtain a desired result. Interactive operations are similar to those of Kana-Kanji conversion, although they are further extended to be capable of control- ling syntactic transformations. We first describe the basic model that determines the scope and timing of interaction, then the set of interactive operations. 3.1 Basic Model The basic model of our method is the syntax- directed translation scheme with bottom-up at- tribute evaluation (see chapter 5 of (Aho et al., 1986)). In this scheme, an attribute of a syntax tree node is calculated from that of the children nodes by a semantic rule, associated with the syntax rule used to build the node from children. Attributes represent partial translation result for the structure below the node, and attribute calculation proceeds from the lexical nodes to the root node in a bottom- up manner. Interactivity is integrated with this model by al- lowing interactive operation when attribute is cal- culated at each node. Before each calculation, the system pauses to show an interpretation of the un- derlying structure, and allows the user to examine and change it if necessary. When interaction is fin- ished, the system chooses a next node and pauses there. This process repeats until the system reaches the root node. Any translation method can be used as long as it is compatible with this general scheme. Although basic model is as described, it is appar- ently too bothersome to give an operation at every node. In addition, some nodes only have a formal role in the grammar, and are not meaningful to the user. For this reason, the nodes at which the system automatically pauses for interaction are restricted to the node marked as a sentence, and the node that dominates a relative clause and its antecedent: in short, just restricted to contain one predicate. We remark that this restriction is effective only on de- fault decision of which node to pause at, and does not restrict operations by the user. The system looks for a minimal node marked as above, then pauses for user operation. At this time, attributes of the focus node and lower nodes are still undetermined except for lexical nodes. When the user triggers translation, undetermined attributes are calculated, then the re- sult replaces the tree under the focus node. That node serves as a kind of lexical node in subsequent translation. 3.2 Interactive Operations The basic interaction model of the method is that the system shows current interpretation in the form of translation equivalents and translation area, while the user responds to it by changing these initial se- lections. This set of operations is essentially the same as the Kana-Kanji conversion method, and its obvious advantage is that everybody who can use Kana-Kanji conversion is expected to be well accus- tomed to these operations. When the system pauses for interaction, it shows initial selection of translation equivalents and trans- lation area, as in figure 1 (b). Translation equivalent selection for all content words, and the designated region to be translated next, is shown in a compact manner, allowing the user to examine and change them before translation. This mixed representation level of the source and target language expression serves as a playground for all subsequent interac- tions. After confirming all selections, the user triggers translation. Then the original area is replaced with resulting English expression. If there are more than one possible translation, the system presents them in a similar window as alternatives window as in fig- ure 2, and the user is allowed to change the system's selection by the same interface as translation equiv- alent selection. Translation equivalent selection enables the user to directly manipulate target language expression. Selecting before translation is much easier than after translation, because the word order and understood syntactic structure is that of the user's native lan- guage. The meaning of translation area selection is also clear. The user should choose the area so that it contains necessary and sufficient words to be one meaningful expression. Technically, it is bracketing by the user. If the user changes the area, the system changes the analysis according to the new constraint. Further disambiguation capability of this operation will be discussed in section 4. Other possible interactive operations include edit- ing and undoing the translation. The user can freely modify the displayed characters at any time, and the system responds by invoking an appropriate proce- dure, such as morphological analysis. Also, the user can go back to any former steps by canceling former translations. All these operations are optional, except for trans- lation triggers to invoke next translation. The amount of interaction and timing of translation trig- ger is completely up to the user, and s/he can even proceed without any modification to the system's initial choice. Steps of interactive translation can be summarized as below. 3.3 Examples Next we turn to more complex examples, and show how more than one translation units are combined. 3.3.1 A Relative Clause Figure 3 shows translation steps for a sentence with a relative clause. This sentence has a depen- dency ambiguity, so we also show how to resolve it through interaction. The original sentence (a) contains a relative clause with verb kau (buy) with an antecedent hon (book). Since Japanese is head- final, the sentence-initial case element kare-ga (he- SUBJ) can be the subject of either kau (buy) or yomu (read), causing syntactic ambiguity. First, let's suppose kare-ga is assumed to be the subject of the relative clause by the system. Then the system pauses showing (b), as soon as (a) is in- put. In (b), the translation region is assumed to be \"he-ga buy-ta book\". After translation trigger, the system pauses showing (c). Please note that the underlined part in (b) is replaced by its equiva- lent English expression \"the book he bought\", and the whole sentence is underlined now. After another translation trigger, (d) is obtained, with missing sub- ject filled by some default word. Suppose after obtaining (d) the user noticed that this interpretation is not what s/he wants, and the case element kare-ga should be the subject of the verb of the matrix sentence. Then the user triggers undo of translation twice, returning to (b). Then s/he notice that \"he -ga buy -ta book\" is treated as one phrase, against his/her interpretation. Then s/he changes the underlined area to \"buy-ta book\", excluding \"he -ga\" from the region (e), because this is the \"correct meaningful phrase\" in the user's in- terpretation. After translation trigger, (f) follows. Note that the subject of the relative clause is supple- mented by a default element. Then (g), the desired result, follows. Generally, if two syntax tree nodes share a child leaf node, one is an ancestor of the other. This property guarantees that two overlapping transla- tion units can always be combined in our stepwise bottom-up translation method. 3.3.2 A Conjunction Figure 4 shows translation steps for two sentences joined by a subordinate conjunct node (because). (a) 彼が買った本を読んだ kare -ga kat -ta hon -0 yon -da he SUBJ buy PAST book OBJ read PAST he が buyたbookをread だ -da (b) (c) the book he bought をread だ -ga -ta -0 -0 -da (d) Someone read the book he bought (e) he が buyたbookをread だ -ga -ta -0 -da (f) he が the book someone bought をread だ -ga -0 (g) He read the book someone bought -da Figure 3: Relative Clause and Syntactic Ambiguity (a) 彼女が来たので私はうれしい kanojo -ga ki -ta -node watashi -wa ureshii she SUBJ come PAST because I TOP glad (b) she が comeたのでIはglad -ga -ta-node -wa (c) She came のでIはglad -node -wa (d) She came のでI am glad -node (e) I am glad because she came Figure 4: Treatment of Conjunction Component sentences are translated first (c, d), then they are combined to produce a complex sentence. Here \"because\" is assumed to be the first alternative as translation equivalent for node. 4 More on Interactive Operations The selection of an equivalent translation is more than simply choosing among synonyms, as shown in (Yamabana et al., 1995). First, part-of-speech of translation equivalent may be specified through this operation, since translation equivalents with differ- ent part-of-speech appear distinctly in the alterna- tives window. Second, the translation equivalent for functional words can be specified, which can affect the syntactic structure of the result. Although func- tional words remain unchanged in the intermediate representation, some words provide an alternatives window when the cursor is placed on them. Third, a whole unit with more than one word can be de- tected and selected in the same interface as transla- tion equivalent for a single word. An example for the first and second point is found in a translation equivalent set for an auxiliary rareru, which is known to be highly ambiguous. Even af- ter leaving aside less frequent \"spontaneity\" usage and indirect passivization, there are still at least three general interpretations: direct passivization, possibility, and honorific. Automatic disambigua- tion requires detailed semantic information, espe- cially when some case elements are missing or hid- den. rareru be -ed can possibly [passive] [honorific] [auxiliary] [capable] [adverb] [capable] [adjective] [capable] it is possible that [adjective] [capable] be able to Figure 5: Alternatives Window for rareru Figure 5 shows the content of the translation equivalent alternatives window for rareru. It ap- pears when the cursor is placed on that word. If \"be -ed\" is chosen, the auxiliary is interpreted as a pas- sive morpheme and treated as such in translation. If the second alternative is chosen, it is interpreted as honorific. In this case, as the translation equivalent is shown as a blank, no morpheme appears in the translation. By choosing the third alternative, it is translated to an auxiliary \"can\", showing capability. The fourth morpheme translates it to \"possibly\", an adverb. By choosing the fifth alternative, the user can specify the result to be a complement of an ad- jective \"possible\". A tree for the structure \"it is pos- sible that\", coded in the dictionary, is synthesized in the generation module. The third point will be discussed in section 5.2. 5 Extension from Basic Model As explained in section 3.1, the method basically as- sumes simple compositionality of translation. How- ever, this assumption apparently needs modification to be applied to broader phenomena. There are two major sources of violation. One is inherited attributes, corresponding to constraints posed by higher nodes to lower ones. Another is idiosyncratic violation of compositionality assumption, such as id- iomatic expressions. In this section we describe how the basic model is extended to treat phenomena that violates this assumption. 5.1 Constraints from Higher Nodes One obvious example of this type of violation is in- flection. It is not an intrinsic property of a word, but a constraint by dominating or governing element. For this reason, its calculation is delayed until the last phase of generation, when all information are gathered at the lexical node. In addition, inflec- tion are re-calculated in every translation, even if the translation of that word has been already fixed by a former translation. Another example is constraint posed by a verb subcategorization frame to subordinate elements. Although syntactic cases can be processed by inflec- tion mechanism, constraint of sentence styles, such (a) 私は彼が本を読むのを助ける watashi wa kare -ga hon -o yomu -no -o tasukeru I TOP he SUBJ book OBJ read COMP OBJ help (b) Iはhe が bookをreadのをhelp -wa -ga -wo -no-wo (c) Iは he reads a book をhelp -wa -0 (d) I help him to read a book Figure 6: Change of Generation Style as to-infinitive or gerund, can not be treated in a similar manner. Since the sentence is a default paus- ing node, subcategorized sentence usually is already fixed as a finite form before the constraint is ap- plied. To cope with this problem, we provide a bookkeeping mechanism that preserves all partial syntax trees generated during translation. When some grammatical constraint is newly introduced on an already translated expression, and if it requires structural deformation, the system looks for the reg- istered structure and generates it again so that it meets the new constraint. Figure 6 shows steps to obtain a sentence with an embedded clause \"I help him to read a book\". As soon as the original sentence (a) is entered, transla- tion equivalent selection and translation region se- lection is presented (b). The first region is the com- plement sentence \"he ga book wo read no\", where no is a complement marker. After translation, (c) is obtained. Then whole sentence is assumed to be the translation region, and (d) is obtained finally. Please note the change in the embedded sentence from a fi- nite form \"he read a book\" in (c) to an to-infinitive form \"him to read a book\" in (d), in accordance with the grammatical constraint posed by the verb \"help\". 5.2 Idiomatic Expression There are some sets of words that acquire special syntactic/semantic behavior when appearing simul- taneously. These idiomatic expressions are another major source that violates the compositionality as- sumption of the method. Hereafter, the word \"id- iomatic expression\" is used in a rather broad sense: if translation of a combination of words is not pre- dictable from their individual behavior, we call it an idiomatic expression. In one case, cooccurring words determines trans- lations of one another, even though their mean- ing can be understood compositionally. For exam- ple, renraku-wo toru (contact-OBJ take) should be translated to \"make a contact\", not \"take a con- tact\" nor \"get a contact\". In another case, the whole expression can turn into completely another mean- ing. For example, ashi-wo arau (foot-OBJ wash) can be interpreted as either \"wash (one's) foot\" or. \"wash one's hands\", the latter case losing the orig- inal meaning of respective words. Although these idiomatic expressions must be recognized and trans- lated as one thing, they cannot be registered as one word in the dictionary, since their elements can ap- pear in a distant position, or they can also have a purely compositional interpretation. To cope with this problem, we extended the trans- lation equivalent selection interface so that transla- tion equivalents can be specified as a set for these expressions. Translation equivalent for the compo- nent words of an idiomatic expression changes syn- chronously when one of them is altered. Also, we expanded the dictionary and morphological analyzer to allow such multi-word translation unit correspon- dence. We give an example with denwa-wo kakeru, an equivalent expression for \"make a phone call\". This is idiomatic because the correspondence between kakeru and \"make\" is peculiar to this interpreta- tion. When the expression denwa-wo kakeru is en- tered, the morphological analyzer recognizes it as an idiomatic expression and retrieves information from the idiom dictionary. Figure 7 is a snapshot of al- ternatives window for \"kakeru\", in the idiomatic in- terpretation. The second line is highlighted as the current selection. The leftmost word \"make\" shows that the current translation equivalent for \"kakeru\", and the third column shows the current translation equivalent for the whole expression is \"make a phone call\", an idiomatic interpretation. The alternatives window for \"denwa\" is shown in Figure 8. Here, the word \"phone call\" is highlighted corresponding to the interpretation as \"make a phone call\". When the user triggers translation, denwa becomes \"a phone call\", kakeru becomes \"make\", producing \"make a phone call\" in whole. If the user changes the selection to another alter- native, say \"telephone\" at the third line in the al- ternatives window kakeru, then the selection in the alternatives window denwa also changes to the third line synchronously. Translation of denwa as denwa shows this word will simply vanish after translation. Then the translation of whole expression becomes an one word verb phrase \"telephone\". At the first line of both alternatives window, the whole original Japanese expression is shown, with a slash at the boundaries of words, like denwa/wo/kakeru. This alternative allows the user to switch from idiomatic interpretation to non- idiomatic interpretation. If the user chooses this al- ternative, a new alternatives window containing lit- eral translation appears as in figure 9. At the same time the alternatives window for denwa changes and shows literal translations for denwa. The user can denwa/wo/kakeru make [verb] [make a phone call] telephone [verb] [telephone] call [verb] [call up] Figure 7: Alternatives for kakeru as an Idiom phone call [countable] [make a phone call] denwa [telephone] denwa [call up] Figure 8: Alternatives for denwa as an Idiom go back to the idiomatic interpretation by choosing the alternative denwa+wo+kakeru, at the last line of these alternatives windows. We remark that this mechanism provides a general means to treat translation unit with more than one component word. 6 Implementation The method is realized as an English writing support software on personal computers. The main function is divided into two modules, the interface module and the translation module. The interface module is in charge of user interaction, morphological analy- sis and predicting translation equivalent and region, as well as function as a front-end. The translation module performs translation of the specified region, obeying user specification passed by the interface module. The most important requirement for the translation module is robustness, in the sense that it doesn't drop a word even when specifications are contradictory. In that case, the system should serve as a simple online dictionary. A prominent feature is added in this implementa- tion: it works as a language conversion front-end to an arbitrary application. The system is placed be- tween the keyboard and an application in the data flow. It captures Japanese input before they are en- tered to an application, converts it into English, and then sends the result to the application (figure 10). kakeru hang [verb] put [verb] denwa+wo+kakeru Figure 9: Alternatives for kakeru in literal interpre- tation Any Applications (Mail, Word Processors, etc.) ↑ Interactive JE Conversion ↑ Any Kana-Kanji Conversion Program ↑ Keyboard Figure 10: Relation to Other Programs This function is realized using a standard hook and IME API of the operating system, Microsoft Win- dows 95. This feature allows this system used as an add-on function of any application, enabling the user to work in a familiar document writing environment. The system dictionary contains about 100,000 Japanese entries and 15,000 idiomatic expressions. Since there was no source available to build an id- iom dictionary of this size, they were collected man- ually from scratch following a method described in (Tamura et al., 1996). The essence of this method is limiting search space utilizing distinguished word classes characteristic to idiomatic expressions, re- vealed by an intensive analysis of these expressions. A CD-ROM online dictionary accessing function is also provided to help user's translation equivalent selection. This software is currently available either as a package software or a pre-installed software on per- sonal computers. 7 Discussion Interactive method in machine translation have been pursued by many people (Kay, 1973; Melby et al., 1980; Tomita, 1984; Huang, 1990; Somers et al., 1990a; Boitet et al., 1995). In these approaches, the system asks a set of questions to the user to resolve ambiguities not solvable by itself. Among problems of this approach are, as Melby pointed out, exces- sive interaction and necessity for special training for interactive operations. In our method, interactive operations are initi- ated and guided by the user and all interactive op- erations are optional, except for a small number of translation triggers needed for translating compo- nent sentences. The system provides its prediction as a default selection, and other possibilities as sec- ond or third choices, but the user is free to obey or ignore them. If the selection is wrong, the transla- tion result becomes wrong, which is a feedback to the user. Then the user can undo the translation, correct selections, and try again (for example, see figure 3). On the other hand, the user has only to repeat \"next\" instruction to obtain a result of au- tomatic translation quality. Frequency and content of interaction are determined by the user. In this manner, the user and the system are essentially co- operative, avoiding the problem of excessive ques- tioning by the system. The problem of difficulty in learning interactive operations is also avoided since our interactions are essentially those of simple Kana- Kanji conversion operations. We believe an average user can easily learn operations of our system. An interactive dependency parser reported in (Maruyama et al., 1990), is based on an interface like Kana-Kanji conversion, and shares character- istics described above². However, their method is limited to syntactic dependency disambiguation by explicitly specifying the words in the dependency re- lation, and it is difficult to expand the method to handle the types of ambiguity discussed in this pa- per. A user-driven approach to interactive translation, proposed by (Somers et al., 1990b), is based on cut and paste operations, where the content of copy buffer is translated when it is pasted. This method seems to leave too much burden to the user, since the user must explicitly specify which portions of the text should be translated, and in what order. Also it is not clear how to combine partial translations of two overlapping expressions, except for direct edit- ing. Our stepwise conversion scheme, in which conver- sion proceeds from smaller structures to larger ones, is a natural conclusion of our try-and-error-based conversion approach. As Melby says, a post-editor will only improve by a certain increment: if the re- sult is completely wrong, s/he will simply abandon the whole result. Since it is easier to obtain an ap- propriate result for a shorter and simpler structure, a result obtained by stepwise conversion tends to be of better quality than a result obtained by translat- ing the whole structure at one step. In other words, our system divides the translation step into smaller pieces, and allows post-editing at every step. As described before, target users of our method are those who have basic knowledge to read and un- derstand the target language. According to the tar- get language skill of the user, useful support func- tion will be different. For example, for a user who is competent in English, our system will be useful as an online dictionary. While writing in English, the user can look up the system dictionary only by entering a Japanese word. Then s/he can enjoy easy- to-use interactive operations for translation equiva- lent selection, inflection selection and CD-ROM dic- tionary access. When the user find an appropriate word, s/he only has to push the return key to enter the word into the original application. These users ²These characteristics are inherited essentially from a Kana-Kanji conversion interface. will also find it useful to obtain a translation equiv- alent expression for an idiomatic expression. These idiomatic expressions, either of source language or target language, are hard to translate since they do not allow literal translation and difficult to find in other dictionaries. By combining this idiom dictio- nary and translation function, the user can obtain a useful skeleton for target language expression. For many users, however, the translation function will be considered helpful to produce a result of the quality level that matches their English reading skill. Suit- able usage will be balanced between the user's skill and the capability of the system. The function as an add-on function to an arbitrary software will be an advantage equally for all users, enabling them to work in their familiar environment, compared to conventional machine-aided translation systems that force them to work in an independent unfamiliar environment. Finally we discuss some remaining problems and direction of further work. Translation quality needs continuous effort for im- provement, in both linguistic coverage and preci- sion. Precision of initial prediction of translation equivalent and translation area is crucial to the per- formance of the system, since they determine the quality of default translation. In our experience, the users are willing to use interactive operation to im- prove translation quality, but never to recover from incomprehensible output. We also have to mention some ambiguities difficult to resolve though basic operations of the method. An example is grammatical relation ambiguity be- tween a case element and a verb, when the case marker is hidden. Generally, the system treats these cases by producing all possibilities in the order of pri- ority and allowing the user to choose one. However, when such ambiguities are multiplied, the number of possibilities easily grows large, making selection difficult. One possible solution would be to pro- vide more disambiguation information, possibly a sequence of dialogues, to help the user to make de- cision. An important requirement here is that these dialogues must not force a response. The user should be able to ignore them unless they want to. Another further work is expanding the dictionary, especially idiomatic expressions. We are also plan- ning to add translation examples to the knowledge base, so that translation can be performed either using grammars or examples in the knowledge base. These examples are effective to guarantee correct- ness of the result, hence will be useful even for users not very familiar in the target language. In this di- rection, our system would be expanded as a kind of interactive example-based translation support sys- tem. 8 Conclusion We presented an interactive machine-aided trans- lation method to support writing in a foreign lan- guage, which is a combination of dictionary lookup and interactive machine translation. The transla- tion proceeds as a cooperative process between the system and the user, through interactive operations similar to Kana-Kanji conversion method. We im- plemented the method as a front-end language con- version software to an arbitrary application. References Aho, A. et al. 1986. Compilers. Principles, Techniques, and Tools. Addison-Wesley Publishing. Boitet, C. et al. 1995. Multilingual Dialogue-Based MT for Monolingual Authors: the LIDIA Project and a First Mockup. In Machine Translation, vol.9(2), pages 99-132. Canisius, P. 1977. Automatic Partial Translation in a Multilingual Information System. In Overcoming the Language Barrier (Third European Congress on Information Systems and Networks), vol.1, pages 259- 269. Huang, X. 1990. A Machine Translation System for the Target Language Inexpert. In Proceedings of COLING-90, pages 364-367. Kay, M. 1973. The MIND System. In Natural Language Processing, Algorithmic Press. Maruyama, H. et al. 1990. An Interactive Japanese Parser for Machine Translation. In Proceedings of COLING-90, pages 257-262. Melby, A. et al. 1980. ITS: Interactive Translation Sys- tem. In Proceedings of COLING-80, pages 424-429. Muraki, K. et al. 1994. TWP: How to assist English pro- duction on Japanese word processor. In Proceedings of COLING-94, pages 283-298. Somers, H. et al. 1990a. Machine Translation without a source text. In Proceedings of COLING-90, pages 271-276. Somers, H. et al. 1990b. A user-driven interaс- tive machine translation system. In Proceedings of SICONLP-90, pages 140-143. Tamura, S. et al. 1996. Collecting of Verbal Id- iomatic Expressions and Development of a Large Dic- tionary for Japanese-to-English Machine Translation. (In Japanese) In Proceedings of 2nd Annual Conven- tion of Association for NLP, pages 45-48. Tomita, M. 1984. Disambiguating grammatically ambiguous sentences by asking. In Proceedings of COLING-84, pages 476-480. Yamabana, K. et al. 1995. Interactive machine-aided translation reconsidered - Interactive disambiguation in TWP- In Proceedings of NLPRS-95, pages 368- 373"
  },
  {
    "title": "Minimizing Word Error Rate in Textual Summaries of Spoken Language",
    "abstract": "Automatic generation of text summaries for spoken language faces the problem of containing incorrect words and passages due to speech recognition errors. This paper describes comparative experiments where passages with higher speech recognizer confidence scores are favored in the ranking process. Results show that a relative word error rate reduction of over 10% can be achieved while at the same time the accuracy of the summary improves markedly.",
    "content": "1 Introduction The amount of audio data on-line has been grow- ing rapidly in recent years, and so methods for ef- ficiently indexing and retrieving non-textual infor- mation have become increasingly important (see, e.g., the TREC-7 branch for \"Spoken Document Re- trieval\" (Garofolo et al., 1999)). One way of compressing audio information is the automatic creation of textual summaries which can be skimmed much faster and stored much more effi- ciently than the audio itself. There has been plenty of research in the area of summarizing written lan- guage (see (Mani and Maybury, 1999) for a compre- hensive overview). So far, however, very little atten- tion has been given to the question how to create and evaluate a summary of spoken audio based on automatically generated transcripts from a speech recognizer. One fundamental problem with those summaries is that they contain incorrectly recog- nized words, i.e., the original text is to some extent \"distorted\". Several research groups have developed interac- tive \"browsing\" tools, where audio (and possibly video) can be accessed together with various types of textual information (transcripts, summaries) via a graphical user interface (Waibel et al., 1998; Valenza et al., 1999; Hirschberg et al., 1999). With these tools, the problem of misrecognitions is alleviated in the sense that the user can always easily listen to the audio recording corresponding to a passage in a textual summary. In some instances, however, this approach may not be feasible or too expensive to pursue, and a short, stand-alone textual repre- sentation of the spoken audio may be preferred or even required. This paper addresses in particular this latter case and (a) explores means of making textual summaries less distorted (i.e., reducing their word error rate (WER)), and (b) assesses how the accuracy of the summaries changes when methods for word error rate reduction are applied. Summary accuracy will be a function of how much relevant information is present in the summary. Our results from experiments on four television shows with multiple speakers show that it is possi- ble to reduce word error rate while at the same time also improving the accuracy of the summary. Fur- thermore, this paper presents a novel method for evaluation of textual summaries from spoken lan- guage data. The paper is organized as follows: In the next section, we review related work on spoken language summarization. In section 3 we describe our sum- marizer. Next, we present and discuss our proposal for an audio summarization evaluation metric (sec- tion 4). In section 5 we describe the corpus that we use for our experiments and how it was annotated. Sections 6 and 7 describe experiments on both hu- man and machine generated transcripts of the audio data. Finally, we discuss and summarize the results in sections 8 and 9. 2 Related work (Waibel et al., 1998) report results of their sum- marization system on automatically transcribed SWITCHBOARD data (Godfrey et al., 1992), the word error rate being about 30%. In a question-answer test with summaries of five dialogues, subjects could identify most of the key concepts using a summary size of only five turns. However, the results vary widely across five different dialogues tested in this experiment (between 20% and 90% accuracy). (Valenza et al., 1999) went one step further and report that they were able to reduce the word error rate in summaries (as opposed to full texts) by using speech recognizer confidence scores. They combined inverse frequency weights with confidence scores for each recognized word. Using summaries composed of one 30-gram per minute (approximately 15% length of the full text), the WER dropped from 25% for the full text to 10% for these summaries. They also conducted a qualitative study where human subjects were given summaries of n-grams of different length and also summaries with speaker utterances as min- imal units, either giving a high weight to the inverse frequency scores or to the confidence scores. The utterance summaries were considered best, followed closely by 30-gram summaries, both using high con- fidence score weights. This suggests that not only does the WER drop by extracting passages that are more likely to be correctly recognized but also do summaries seem to be \"better\" which are generated that way. While the results of (Valenza et al., 1999) are in- dicative for their approach, we want to investigate the benefits of using speech recognizer confidence scores in more detail and particularly find out about the trade-off between WER and summarization ac- curacy when we vary the influence of the confidence scores. To our knowledge, this paper addresses this trade-off for the first time in a clear, numerically de- scribable way. To be able to obtain numerical values for summary accuracy, we had our corpus annotated for relevance (section 5) and devised an evaluation scheme that allows the calculation of summary ac- curacy for both human and machine generated tran- scripts (section 4). 3 Summarization system Prior to summarizing, the input text is cleaned up for disfluencies, such as hesitations, filled pauses, and repetitions.¹ In the context of multi-topical recordings we use for our experiments, summaries are generated for each topical segment separately. The segment boundaries were determined to be at those places where the majority (at least half) of the human annotators agreed (see section 5). Intercoder agreement for topical boundaries is fairly good (and higher than the agreement on relevant words or pas- sages).2 To determine the content of the summaries, we use a “maximal marginal relevance\" (MMR) based summarizer with speaker turns as minimal units (cf. (Carbonell and Goldstein, 1998)). The MMR formula is given in equation 1. It gen- erates a list of turns ranked by their relevance and states that the next turn to be put in this ranked list will be taken from the turns which were not yet ranked (tnr) and has the following properties: it is (a) maximally similar to a \"query\" and (b) max- imally dissimilar to the turns which were already ¹More details about this component and other parts of the summarization system can be found in (Zechner and Waibel, 2000). ²For details see (Zechner, 2000). ranked (tr). As \"query\" we use a frequency vector for all content words within a topical segment. The λ-parameter (0.0 ≤ λ ≤ 1.0) is used to trade off the influence of (a) vs. (b). Both similarity metrics (sim₁, sim₂) are inner vector products of (stemmed) term frequencies (see equations 2 to 4); tf, is a vector of stem frequencies in a turn; f, are in-segment frequencies of a stem; fsmaz are maximal segment frequencies of any stem in the topical segment. sim₁ can be normalized or not. The formulae for tf, (equation 4) are inspired from Cornell's SMART system (Salton, 1971); we will call these parameters \"smax\", \"log\", and \"freq\", respectively. nextturn = arg max(λsim₁ (tnr,j, query) tnr,j -(1-λ) max sim₂ (tnr,j, tr,k)) (1) tr,k sim₁ = tf, tf tftfe or (2) sim₂ = tf,tf2 |tf||tf₂|| (3) tfi,s = 0.5 + 0.5 fi,s fmax or 1 + log fi,s (4) or fi,s Using the MMR algorithm, we obtain a list of ranked turns for each topical segment. We com- pute this both for human and machine generated transcripts of the audio files (\"reference text” vs. \"hypothesis text\").³ 4 Evaluation metrics The challenge of devising a meaningful evaluation metric for the task of audio summarization is that it has to be applicable to both the reference (hu- man transcript) and the hypothesis transcripts (au- tomatic speech recognizer (ASR) transcripts). We want to be able to assess the quality of the sum- mary with respect to the relevance markings of the human annotators (see section 5), as well as to re- late this \"summary accuracy\" to the word error rate present in the ASR transcripts. The approach we take is to align the words in the summary with the words in the reference transcript (wa). For ASR transcripts, word substitutions are aligned with their \"true original\" and word inser- tions are aligned with a NIL dummy. That way, ³The human reference is considered to be an \"optimal\" or \"ideal\" rendering of the words which were actually said in a conversation. Human transcription errors do occur, but are marginal and hence ignored in the context of this paper. [ERROR: Failed to process this page - Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.] TURN 1: rel: 0.5 0.5 0.5 0.5 REF: this is to illustrate the idea HYP: this is to ILLUMINATE *** idea err: C C C S D con: 1 1 1 0.9 0.75 0.75 *** *** AND I 0.8 0.8 TURN 2: rel: 0 1 1 1 1 1 REF: and here we have very relevant information HYP: and HE ** BEHAVES **** IRREVERENT FORMATION err: C S DS D S S con: 0.8 0.7 0.8 0.8 0.9 Figure 1: Simplified example of two turns (for score computation) BACK 19CENT BUCHANAN GRAY (i) sum -0.43 -0.51 -0.12 -0.03 (ii) average -0.53 -0.52 -0.43 -0.42 (iii) scores > 0.95 -0.55 -0.48 -0.35 -0.25 (iv) normalized (iii) -0.58 -0.48 -0.48 -0.44 (v) geometric mean -0.53 -0.53 -0.42 -0.38 Table 1: Pearson r correlation between WER and confidence scores racy when the word error rate is reduced, this is not necessarily the case. For example, it could turn out that while we can reduce WER by \"boosting\" passages with higher confidence scores, those pas- sages might have (much) fewer words marked rele- vant than those being present in the summary with- out boosting. This way, it would be conceivable to create low word error summaries that contain also very few relevant pieces of information. However, as we will see later, WER reduction goes hand in hand with an increase of summary accuracy. 5 Data characteristics and annotation Table 2 describes the main features of the corpus we used for our experiments: we selected four audio ex- cerpts from four television shows, together with hu- man generated textual transcripts. All these shows are conversations between multiple speakers. The audio was sampled at 16kHz and then also automat- ically transcribed using a gender independent, vo- cal tract length normalized, large vocabulary speech recognizer which was trained on about 80 hours of Broadcast News data (Yu et al., 1999). The average word error rates for our 4 recordings ranged from 25% to 50%. The reference transcripts of the four recordings were given to six human annotators who had to seg- ment them into topically coherent regions and to de- cide on the \"most relevant phrases\" to be included in a summary for each topical region. Those phrases usually do not coincide exactly with speaker turns and the annotators were encouraged to mark sec- tions of text freely such that they would form mean- ingful, concise, and informative phrases. Three an- notators could listen to the audio while annotat- ing the corpus, the other three only had the hu- man generated transcripts available. 2 of the 6 an- notators only finished the NewsHour data, so we have the opinion of 4 annotators for the recordings BUCHANAN and GRAY and of 6 annotators for BACK and 19CENT. 6 Experiments on human generated transcripts We created summaries of the reference transcripts using different parameters for the MMR computa- tion: For tf we used \"freq\", \"log\", and \"smax\"; fur- ther, we did or did not normalize these weights; fi- nally, we varied the MMR-A from 0.85 to 1.0. Sum- marization accuracy was determined at 5%, 10%, 15%, 20%, and 25% of the text length of each sum- marized topical segment and then averaged over all sample points in all segments. Since these were word-based lengths, words were added incrementally to the summary in the order of the turns ranked via MMR; turns were cut off when the length limit was reached. As explained in the example in section 4, the accuracy score is defined as the fraction of the sum of all individual word relevance scores (as de- --- BACK 19CENT BUCHANAN GRAY TV show NewsHour NewsHour Crossfire Crossfire number of speakers 5 2 4 5 speaker turns 24 27 69 70 words in transcript 1216 1281 3252 2205 length in minutes 8.6 8.6 17.3 11.9 topical segments 4 4 4 3 word error rate (in %) 25.6 32.6 32.5 49.8 Table 2: Characteristics of the corpus BACK 19CENT BUCHANAN GRAY average 0.533 0.596 0.513 0.443 0.522 Table 3: Reference summarization accuracy of MMR summaries termined by human annotators) over the maximum possible score given the current number of words in the summary. Table 3 shows the summary accuracy results for the best parameter setting (tf=log, no normaliza- tion). 7 Experiments on automatically generated transcripts Using the same summarizer as before, we now cre- ated summaries from ASR transcripts. Addition- ally to the summary accuracy, we evaluate now also the WER for each evaluation point. Again, we ran a series of experiments for different parameters of the MMR formula (tf=log, smax, freq; with/without normalization). As before, we achieved the best re- sults for non normalized scores and tf=log. We var- ied a from 0.0 to 10.0 to see how much of an effect we would get from the \"boosting\" of turns with many high confidence scores (see equations 5 and 6). The EXP formula yielded better results than MULT (Table 4), the optimum for EXP was reached for a = 3.0 with a WER of 26.6%, an absolute improve- ment of over 8% over the average of WER=35.1% for the complete ASR transcripts (non-summarized). The summarization accuracy peaks at 0.47, a 9% absolute improvement over the a = 0.0-baseline and only about 5% absolute lower than for reference sum- maries (Table 4 and Figure 2). When we compare the baseline of a = 0.0 (i.e., no \"boosting\" of high confidence turns) to the best re- sult (a = 3.0), we see that the WER drops markedly by about 12% relative from 30.1 to 26.6%. At the same time, the summarization accuracy increases by about 18% relative form 0.401 to 0.472. $^6$If we use non-normalized scores, the value of the MMR-λ does not have any measurable effect; we assigned it to be 0.95 for all subsequent experiments. Summary accuracy vs. Word error rate 0.4 EXP formula word error rate 0.35 alpha=0 0.3 alpha=7 alpha=3 0.25 0.2 0.35 0.4 0.45 0.5 0.55 summary accuracy Figure 2: Summary accuracy vs. word error rates with EXP boosting (0 ≤ α ≤ 7) Results for the MULT formula confirm this trend, but it is considerably weaker: approximately 6% WER reduction and 14% accuracy improvement for α = 10.0 over the α = 0.0 baseline. An appendix (section 11) provides an example of actual summaries generated by our system for the first topical segment of the BACK conversation. It illustrates how WER reduction and summary ac- curacy improvement can be achieved by using our confidence boosting method. 8 Discussion The most significant result of our experiments is, in our opinion, the fact that the trade-off between word and summary accuracy indeed leads to an op- timal parameter setting for the creation of textual summaries for spoken language (Figure 2). Using a formula which emphasizes turns containing many high confidence scores leads to an average WER re- duction of over 10% and to an average improvement in summary accuracy of over 15%, compared to the baseline of a standard MMR-based summary. Comparing our results to those reported in (Valenza et al., 1999), we find that their relative --- BACK 19CENT BUCHANAN GRAY average α = 0.0 EXP (a = 3.0) acc WER 0.411 26.2 0.648 18.8 acc WER acc WER 0.501 26.7 0.412 30.6 0.501 26.7 0.444 26.9 MULT (a = 10.0) 0.575 21.5 0.501 26.7 acc WER acc WER 0.280 36.9 0.401 30.1 0.296 34.0 0.472 26.6 0.429 29.6 0.317 35.7 0.456 28.3 Table 4: Effect of a on summary accuracy vs. WER (in %) transcripts with EXP and MULT boosting methods BACK 19CENT avgth -0.79 -0.11 BUCHANAN GRAY -0.43 -0.03 Table 5: Correlation between WER and confidence scores on a turn basis WER reduction for summaries over full texts was considerably larger than ours (60% vs. 24%). We conjecture that reasons for this may be due to the different nature and quality of the confidence scores, and (not unrelated), to the different absolute WER of the two corpora (25% vs. 35%): in transcripts with higher WER, the confidence scores are usually less reliable (cf. Table 1). Looking at the four audio recordings individually, we see that the improvements vary strongly across different recordings. We conjecture that one reason for this fact may be due to the high variation in the correlation between WER and confidence scores on a turn basis (Table 5). This would explain why, e.g., BACK's improvements are much stronger than those of the BUCHANAN recording or why there are no improvements for the 19CENT recording. How- ever, GRAY does improve despite its very low abso- lute correlation. 9 Summary In this paper, we presented experiments on sum- maries of both human and machine generated tran- scripts from four recordings of spoken language. We explored the trade-off of word accuracy vs. summary accuracy (relevance) using speech recognizer confi- dence scores to rank passages with lower word error rate higher in the summarization process. Results comparing our approach to a simple MMR ranking show that while the WER can be reduced by over 10%, summarization accuracy improves by over 15% as measured against transcripts with relevance annotations. 10 Acknowledgements We thank the six human annotators for their tedious work of annotating the corpus with topical segment boundaries and relevance information. We also want to thank Alon Lavie and the three anonymous re- viewers for useful feedback and comments on earlier drafts of this paper. This work was funded in part by ATR Inter- preting Telecommunications Research Laboratories of Japan, and the US Department of Defense. References Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceed- ings of the 21st ACM-SIGIR International Con- ference on Research and Development in Informa- tion Retrieval, Melbourne, Australia. John S. Garofolo, Ellen M. Voorhees, Cedric G. P. Auzanne, and Vincent M. Stanford. 1999. Spoken document retrieval: 1998 evaluation and investi- gation of new metrics. In Proceedings of the ESCA workshop: Accessing information in spoken audio, pages 1-7. Cambridge, UK, April. J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992. SWITCHBOARD: telephone speech corpus for research and development. In Proceedings of the ICASSP-92, volume 1, pages 517-520. Julia Hirschberg, Steve Whittaker, Don Hindle, Fer- nando Pereira, and Amit Singhal. 1999. Finding information in audio: A new paradigm for audio browsing/retrieval. In Proceedings of the ESCA workshop: Accessing information in spoken audio, pages 117-122. Cambridge, UK, April. Inderjeet Mani and Mark T. Maybury, editors. 1999. Advances in automatic text summarization. MIT Press, Cambridge, MA. Gerard Salton, editor. 1971. The SMART Retrieval System Experiments in Automatic Text Pro- cessing. Prentice Hall, Englewood Cliffs, New Jer- sey. Robin Valenza, Tony Robinson, Marianne Hickey, and Roger Tucker. 1999. Summarisation of spo- ken audio through information extraction. In Pro- ceedings of the ESCA workshop: Accessing in- formation in spoken audio, pages 111-116. Cam- bridge, UK, April. Alex Waibel, Michael Bett, and Michael Finke. 1998. Meeting browser: Tracking and summa- rizing meetings. In Proceedings of the DARPA Broadcast News Workshop. Hua Yu, Michael Finke, and Alex Waibel. 1999. Progress in automatic meeting transcription. In Proceedings of EUROSPEECH-99, Budapest, Hungary, September. α relative sa WER in % turns in summary 0.0 0.428 3.0 0.885 29.2 11.8 2, 1[beginning] 1, 5[beginning] Table 6: Relative summary accuracy, WER, and se- lected turns by the summarizer for (a) no boosting and (b) EXP boosting. turn avg. relevance score higher WER scores, case (b) (a = 3.0) successfully ranks turn 1 first due to its higher confidence scores and hence both summary accuracy and WER scores improve. 1 0.663 2 0.369 3 0.149 4 0.212 5 0.274 WER in % avgth 9.5 0.84 27.5 0.40 26.9 0.39 11.1 0.08 27.7 0.17 Table 7: Average relevance scores, WER, and confi- dence values for the five turns of BACK's first topical segment. Klaus Zechner and Alex Waibel. 2000. Dia- summ: Flexible summarization of spontaneous dialogues in unrestricted domains. Available from http://www.cs.cmu.edu/~zechner/publications.html. Klaus Zechner. 2000. A word-based annota- tion and evaluation scheme for summariza- tion of spontaneous speech. Available from http://www.cs.cmu.edu/~zechner/publications.html. 11 Appendix: Example summaries This appendix provides summaries for the first topi- cal segment of the BACK conversation. The contents of this conversation revolves around former Illinois congressman Dan Rostenkowski who had been re- leased from prison and was ready to re-enter public life. Figure 3 shows the human transcript of this seg- ment which is about two minutes long and con- sists of 5 speaker turns. Figure 4 contrasts the machine generated summaries for this segment (a) without confidence boosting (a = 0.0) and (b) using the optimal confidence boosting (a = 3.0, method EXP). Insertions and substitutions are capitalized and marked with I- or S- prefixes. Table 6 compares the relative summary accuracies (sa) and word error rates (WER in %) for these two summaries (aver- age over the 5 sample points from 5% to 25% sum- mary length). Additionally, the turns that show up in the summaries are listed in their ranking order. Table 7 provides the average relevance scores, word error rates, and confidence scores (\"avgth\") for each turn of this topical segment. We observe that the most relevant turn is turn 1 which has, incidentally, also the lowest WER. Whereas in case (a) (a = 0.0), turn 2 is ranked first and therefore dominates the lower relevance and 1 elizabeth: it has been eight months since dan rostenkowski walked out of a wisconsin federal prison six months since he left a halfway house in chicago the former chairman of the house ways and means 2 committee is ready to step back into the public eye elizabeth: the reception was warm the banquet hall packed with the city's movers and shakers the thirty five dollars a plate invitation referred to rostenkowski as mr. chairman rostenkovski made no reference to his conviction for misusing federal funds only a brief reference to his fifteen months of prison time 3 dan: i graduated from oxford and i really had a rhodes scholarship the past three years have been a constantly challenging time for me change never comes easily and given the circumstances of my situation that was particularly true for me at times things have been downright bleak and i wouldn't want to wish my experience on my worst enemy but there were some silver linings i've had an opportunity to read and reflect in a way that wasn't possible when i was in constant moment in these remarks today i'd like to share some of my conclusions 4 elizabeth: the conclusions did not dwell on the demise of dan rostenkowski's career but 5 dan: the demise of party politics those who say that the president's political power has been weakened by scandal have truly short memories the sad fact is that president clinton has never had a democratic base in congress a group of people whom one could support the white house on any given issue are not there Figure 3: Human transcript of first topical segment (BACK) 1 elizabeth: has been eight months since dan rostenkowski walked out of wisconsin federal prison I-MAYBE 2 elizabeth: was S-ALARMED the banquet hall packed with the city's S-COMMUTERS S-IN S-CHAMBERS S-WHICH thirty five S-DOLLAR a plate S-IMITATION referred to rostenkowski as S-MR. chairman I-MIS S-USING federal rostenkowski made no reference to his conviction for I-LET I-ME I-ASK funds only a brief reference to S-IS fifteen months of prison time 1 elizabeth: has been eight months since dan rostenkowski walked out of wisconsin federal prison I-MAYBE six months since he left S-THE halfway house in chicago the former chairman of the house ways and means committee ready to step back into the public eye 5 dan: S-ALSO say that the president's political power has been weakened by scandal S-RIGHT S-ESPECIALLY short S-MEMORY S-THAT S-DISSATISFACTION that president clinton has never Figure 4: Machine generated summaries for (a) a = 0.0 and (b) a = 3.0 (25% of text length)"
  },
  {
    "title": "INTEGRATING TOP-DOWN AND BOTTOM-UP STRATEGIES IN A TEXT PROCESSING SYSTEM",
    "abstract": "The SCISOR system is a computer program designed to scan naturally occurring texts in constrained domains, extract information, and answer questions about that information. The system currently reads newspaper stories in the domain of corporate mergers and acquisitions. The language analysis strategy used by SCISOR combines full syntactic (bottom-up) parsing and conceptual expectation-driven (top-down) parsing. Four knowledge sources, including syntactic and semantic information and domain knowledge, interact in a flexible manner. This integration produces a more robust semantic analyzer designed to deal gracefully with gaps in lexical and syntactic knowledge, transports easily to new domains, and facilitates the extraction of information from texts.",
    "content": "INTRODUCTION The System for Conceptual Information Summarization, Organization and Retrieval (SCISOR) is an implemented system designed to extract information from naturally oc- curring texts in constrained domains. The derived infor- mation is stored in a conceptual knowledge base and re- trieved using a natural language analyzer and generator. Conceptual information extracted from texts has a number of advantages over other information-retrieval techniques [Rau, 1987a], in addition to allowing for the automatic generation of databases from texts. The integration of top-down, expectation driven pro- cessing, and bottom-up, language-driven parsing is impor- tant for text understanding. Bottom-up strategies identify surface linguistic relations in the input and produce con- ceptual structures from these relations. With the input \"ACE made ACME an offer\", a good \"bottom-up\" linguistic analyzer can identify the subject, verb, direct and indirect objects. It also can determine that ACME was the recipi- ent of an offer, rather than being made into an offer, as in \"ACE made ACME a subsidiary\". Top-down methods use extensive knowledge of the context of the input, practical constraints, and conceptual expectations based on previous events to fit new informa- tion into an existing framework. A good \"top-down\" an- alyzer might determine from \"ACE made ACME an offer\" that ACME is the target of a takover (which is not obvious from the language, since the offer could be for something that ACME owns), and relate the offer to other events (pre- vious rumors or competing offers). Bottom-up methods tend to produce more accurate parses and semantic interpretations, account for subtleties in linguistic expression, and detect inconsistencies and lexi- cal gaps. Top-down methods are more tolerant of unknown words or grammatical lapses, but are also more apt to de- rive erroneous interpretations, fail to detect inconsisten- cies between what is said and how it is interpreted, and often cannot produce any results when the text presents unusual or unexpected information. Integration of these two approaches can improve the depth and accuracy of the understanding process. SCISOR is unique in its integration of the bottom-up processing performed by its analyzer, TRUMP (Trans- portable Understanding Mechanism Package) [Jacobs, 1986], with other sources of information in the form of conceptual expectations. In this paper, four information sources are described that are used by SCISOR to produce meaning represen- tations from texts. The actual processing sequence and timing of the application of these sources are illustrated. THE SCISOR SYSTEM The SCISOR system is currently being tested with news- paper stories about corporate takeovers. The domain provides interesting subject matter as well as some rich language. The gradual development of the stories over time motivates a natural language approach, while the re- stricted nature of the material allows us to encode concep- tual expectations necessary for top-down processing. The following is an example of the operation of SCISOR on a simple news story: W ACQUISITION UPS BID FOR WARNACO Warnaco received another merger offer, valued at $36 a share, or $360 million. The buyout offer for the apparel maker was made by the W Acquisition Corporation of Delaware. User: Who took over Warnaco? System: W Acquisition offered $36 per share for Warnaco. User: What happened to Warnaco last Tuesday? System: Warnaco rose 2 1/2 as a result of rumors. 129 The system has been demonstrated with a small set of input texts, and is being expanded to handle large numbers of newswire stories using a large domain knowledge base and substantial vocabulary. SOURCES OF INFORMATION Text processing in SCISOR is accomplished through the integration of sources of knowledge and types of processing. The four sources of knowledge that SCISOR uses to extract meaning from text are as follows: A. Role-filler Expectations: Constraints on what can fill a conceptual role are the primary source of infor- mation used in top-down processing. B. Event Expectations: Expectations about events that may occur in the future are cre- ated from previous stories, and used to predict values in the expected events if they occur. C. Linguistic: Grammatical, lexical and phrasal knowl- edge is used whenever it is available and reliable. Sub- language (domain-specific) linguistic information may also be used if available. D. World Knowledge Expectations: World knowledge expectations can disambiguate multiple in-. terpretations through domain-specific heuristics. SCISOR can operate with any combination of these infor- mation sources. When one or more sources are lacking, the information extracted from the texts may be more superfi- cial, or less reliable. The flexibility in depth of processing provided by these multiple information sources is an inter- esting feature in its own right, in addition to forming the foundations for a system to \"skim\" efficiently when a new text contains material already processed. As an example of each source of information, consider the following segment from the text printed previously: Warnaco received another merger offer, valued at $36 a share, or $360 million. Role-filler expectations allow SCISOR to make reli- able interpretations of the dollar figures in spite of incom- plete lexical knowledge of the syntactic roles they play in the sentence. This is accomplished because prices of stock are constrained to be \"small\" numbers, whereas fillers of takeover-bid value roles are constrained to be \"large\" quan- tities. Event expectations lead to the deeper interpretation that this offer is an increase over a previous offer because one expects some kind of rebuttal to an offer to occur in the future. An increased offer is one such rebuttal. World knowledge might allow the system to predict whether the offer was an increase or a competing offer, depending on what other information was available. A unique feature of SCISOR is that partial linguistic knowledge contributes to all of these interpretations, and to the understanding of \"received\" in this context. This is noteworthy because general knowledge about \"receive\" in this case interacts with domain knowledge in understand- ing the role of Warnaco in the offer. A robust parser and semantic interpreter could obtain these features from the texts without the use of expecta- tions. This would make top-down processing unnecessary. Robust in-depth analysis of texts, however, is beyond the near-term capabilities of natural language technology; thus SCISOR is designed with the understanding that there will always be gaps in the system's knowledge base that must be dealt with gracefully. Now the four sources of information used to extract information are described in more detail, followed by a discussion of how they interact in the processing of two sample texts. A. Role-filler Expectations The simplest kind of expectation-driven information that can be used is termed \"role-filler\" expectations. These ex- pectations take the form of constraints on the filler of a conceptual role. This is the primary source of processing power in expectation-driven systems such as FRUMP [De- Jong, 1979]. The following list illustrates some examples of constraints on certain fillers of roles used in the corporate takeover domain. ROLE FILLER-CONSTRAINT EXAMPLE target company-agent ACE suitor company-agent ACME price-per-share small number $45 total value large number $45 million This information is encoded declaratively in the knowledge base of the system. During the processing of a text, roles may be filled with more than one hypothesis; however, as soon as a filler for a role is certain, the process of elimi- nation is used to aid in the resolution of other bindings. Thus, if SCISOR determines that ACE is a takeover target, it will assume by default that ACME is the suitor if the two companies appear in the same story and there is no additional information to aid in the disambiguation. B. Event Expectations Expectations that certain events will occur in the future are a second source of information available to aid in the in- terpretation of new events. These expectations arise from the events in previous stories. For example, when the sys- tem reads that rumors have been swirling around ACE as a takeover target, an event expectation is set up that antici- pates an offer for ACE in some future story. When an offer has been made, expectations are set up that some kind of rebuttal will take place. This rebuttal may be a rejection or an acceptance of the offer. The acceptance of the offer option carries with it the event expectation that the total value of the takeover will be the amount of the offer. Event expectations are implemented as domain- dependent, declarative properties of the events in the do- 130 main. They are derived from the script-like [Schank and Abelson, 1977] representations of typical event sequences. C. Linguistic Analysis The most important source of information used in text processing is a full bottom-up parser. TRUMP is a flexible language analyzer consisting of a syntactic processor and semantic interpreter [Jacobs, 1986, Jacobs, 1987a]. The system is designed to fill conceptual roles using linguistic, conceptual, and metaphorical relationships distributed in a knowledge hierarchy. Within SCISOR, TRUMP identifies linguistic rela- tionships in the input, using lexical and syntactic knowl- edge. Knowledge structures produced by TRUMP are passed through an interface to the top-down processing components. Pieces of knowledge structures may then be tied together with the expectation-driven processing com- ponents. In the case of a complete parse of an input sentence, the knowledge structures produced by TRUMP contain most of the structure of the final interpretation, although expectations often further refine the analysis. In the case of partial parses, more of the structure is determined by role- filler expectations. The following are two simple examples of this division of labor: Input: W Acquisition offered $36 a share for Warnaco. Partial parser output: (Basic-sentence (NP (Name-NP (Name (C-Name W_Acquisition)))) (VP (Adjunct-VP (VP (Transitive-VP (Verb-part (Basic-verb-part (V offered))) (NP (Postmodified-NP (NP ($-NP $36)) (MOD (Ratio-modifier (R-word a) (N share))))))) (PP (Basic-PP (P for) (NP (Name-NP (G-Name Warnaco)))))))) TRUMP interpretation: (offer (offerer W-Acq-Co) (offeree Warnaco) (offer (dollars (quantity 36) (denominator share)))) Final interpretation: (corp-takeover-offer (suitor W-Acq-Co) (target Warnaco) (dps (quantity 36))) Input: Warnaco received another merger offer, valued at $36 a share. Partial parser output: (Subj-verb-relation (Subj (NP (Name-NP (Name (C-Name W-Acquisition)))) (Verb (V received))) (N offer) (NP (Postmodified-NP (NP ($-NP $36)) (MOD (Ratio-modifier (R-word (det a)) (Noun-part (N share)))))) TRUMP interpretation: (offer) (transfer-event (recipient Warnaco)) (dollars (quantity 36) (denominator share)) Final interpretation: (corp-takeover-offer (target Warnaco) (dps (quantity 36))) In the first example above, TRUMP succeeds in pro- ducing a complete syntactic parse, along with the corre- sponding semantic interpretation. The domain knowledge helps only to specify the verb sense of \"offer\". In the sec- ond example, however, more of the work is done by the domain-dependent expectations. In this case, the unknown words prevent TRUMP from completing the parse, so the output from the parser is a set of linguistic relations. These relations allow the semantic interpreter to produce some superficial conceptual structures, but the final conceptual roles are filled using domain knowledge. The distinction between the general offer and the more specific corp-takeover-offer is essential for under- standing texts of this type. In general, an offer may be made for something to someone, but it is only in the cor- porate takeover domain that the target of the takeover (the for role) is by default the same as the recipient of the of- fer (the to role). Since TRUMP is a domain-independent analyzer, it cannot itself fill such roles appropriately. The knowledge sources at work in SCISOR and the timing of the information exchange in the system are de- scribed in the next section. D. World Knowledge Expectations If all the above sources of information are still insufficient to determine or satisfactorily disambiguate potential re- lationships between items in the text, so called \"world knowledge\" can be called into play. This world knowledge takes the form of domain-dependent generalizations, im- plemented as declarative relationships between concepts. For example, in the corporate takeover domain, a piece of world knowledge that can aid in the determination of what company is taking over what company is the following: 131 If it is ambiguous whether: Company A is taking over Company B or Company B is taking over Company A Choose the larger company to be the suitor and the smaller company to be the target This example uses the knowledge that it is almost always the case that the suitor is larger than the target company. The utilization of this generalization (that typically larger companies take over smaller companies) requires knowl- edge of the company sizes, assumed to be present in the knowledge base of the system. Another example is: If it is ambiguous whether: value A is a previous offer or present stock price and value B is a new offer or vice versa, Choose the larger offer for the new offer or present stock price, and the smaller offer for the previous offer In this case, a company rarely would decrease their of- fer unless something unexpected happened to the target company before the takeover was completed. Similarly, an offer is almost always for more than the current value of the stock on the stock market. These heuristics incorporate expectations that arise from potentially complex explanations. For example, the reason why a new offer is higher than an old offer rests on a complex understanding of the plan of the suitor to reach their goal of taking over the target company. The world knowledge presented here represents a compilation of this complex reasoning into simple heuristics for text understanding, albeit ad hoc. Although this type of information is shown in a rule- like form, it is implemented with special relationship links that contain information as to how to compute the truth value of the relationship. When this type of knowledge is needed to disambiguate an input, the system checks if any objects have these \"world knowledge constraints\". If so, they are activated and applied to the situation under consideration. The intuition underlying the inclusion of heuristics of this sort is that there is a great deal of \"common sense\" in- formation that can increase an understanding mechanism's ability to extract meaning. This type of information is a last resort for determining conceptual relations when other more principled sources of information are exhausted. KNOWLEDGE INTEGRATION Each of the four sources of information described above is utilized at different points in the processing of the input text, and with different degrees of confidence. The follow- ing algorithm describes a particular instantiation of this order for a hypothetical event sequence involving rumors about ACE, followed by an offer by ACME for ACE. In general, event expectations are set up as soon as an event that has an expectation property is detected. That is, as soon as the system sees a rumor, it sets up an ex- pectation that there will be an offer for the company the rumor was about sometime in the future. When that event-expectation is confirmed, those ex- pectations are realized and the information expected is added to the meaning extracted from the text being pro- cessed. Note that these realized expectations may later be retracted given additional information. Role-filler expec- tations then create multiple hypotheses about which items may fill what roles. These are narrowed down by any con- straints already present by event expectations. Linguistic analysis, when it provides a complete final meaning representation for a portion of the text containing features of interest, always supercedes a conflicting role- filler expectation. For example, if a role-filler expectation hypothesized that ACE was the target in a takeover, and the parser determined that ACME was the object of the takeover, ACME alone would be included as the target. World knowledge expectations are invoked only in the case of conflicting or ambiguous interpretations. For ex- ample, if after all the processing is finished and the system does not know whether ACE is taking over ACME or vice versa, the expectation that the larger company is typically the suitor is invoked and used in the final disambiguation. Below are the sample input texts, followed by the se- quence of steps that are taken by the program. ACE, an apparel maker planning a leveraged buyout, rose $2 1/2 to $35 3/8, as a rumor spread that another buyer might appear. The company said there were no corporate develop- ments to account for the rise, and the rumor could not be confirmed. ACE received another merger offer, valued at $36 a share, or $360 million. The buyout offer for the apparel maker was made by the ACME Corporation of Delaware. ACE closed yesterday at $35 3/8. 1. System reads first story and extracts information that there are rumors about ACE and that the stock price is currently $35 3/8, using role-filler expectations. 2. An event expectation is set up that there will be an offer-event, with ACE as the target of the takeover offer. 3. System begins reading story involving a takeover offer and ACE. 4. Target slot of offer is filled with ACE from the event expectation. 5. An event expectation is set up that there will be a rebuttal to the offer sometime in the future. 6. System encounters ACME which it knows to be a com- pany. Suitor slot of offer is thus filled with ACME via a role-filler expectation. 132 7. $36 a share is parsed with the phrasal lexicon. 8. $36 a share is added as a candidate for either the stock's current price on the stock market or the amount of the ACME offer, due to role-filler expec- tations. 9. $360 million is parsed with the phrasal lexicon. 10. $360 million is added as candidate for the total value of the offer due to a role-filler expectation that expects total values to be large numbers. 11. Syntactic and semantic analysis determine that the offerer is ACME, and the target is ACE. This reinforces the interpretations previously hypothesized. 12. Syntactic and semantic analysis determine the loca- tion of the ACME Corporation to be Delaware. 13. $35 3/8 is encountered, which is taken to be a price- per-share amount, due to a role-filler expectation that expects prices per share to be small numbers. 14. $35 3/8 a share is added as a candidate for either the stock's current price on the stock market or the amount of the ACME offer. 15. $35 3/8 is taken to be the stock's current price and $36 is taken to be the amount of the ACME offer, due to the world knowledge expectation that expects the offer to exceed the current trading price. The contribution of the various sources of knowledges varies with the amount of knowledge they can be brought to bear on the language being analyzed. That is, given more syntactic and semantic knowledge, TRUMP could have done more work in the analyses of these stories. Given more detailed conceptual expectations, the bottom- up mechanism also could have extracted more meaning. Together, the two mechanisms should combine to produce a deeper and more complete meaning representation than either one could alone. IMPLEMENTATION SCISOR consists of a variety of programs and tools, op- erating in conjunction with a declarative knowledge base of domain-independent linguistic, grammatical and world knowledge and domain-dependent lexicon and domain knowledge. A brief overview of the system may be found in [Rau, 1987c], and a more complete description in [Rau, 1987b]. The natural language input is processed with the TRUMP parser and semantic interpreter [Jacobs, 1986]. Linguistic knowledge is represented using the Ace linguis- tic knowledge representation framework [Jacobs and Rau, 1985]. Answers to user's questions and event expectations are retrieved using the retrieval mechanism described in [Rau, 1987b]. Responses to the user will be generated with the KING [Jacobs, 1987b] natural language gener- ator when that component is integrated with SCISOR; currently output is \"canned\". The events in SCISOR are represented using the KODIAK knowledge representation language [Wilensky, 1986], augmented with some scriptal knowledge of typical events in the domain. SYSTEM STATUS All the components of SCISOR described here have been implemented, although not all have been connected to- gether. The system can, as of this writing, process a num- ber of stories in the domain. The processing entails the combined expectation-driven and language driven capabil- ities described here. For questions that the system can understand, SCISOR retrieves conceptual answers to in- put questions. These answers are currently output using pseudo-natural language, but we are in the process of in- tegrating the KING generator. SCISOR is currently being connected to an automatic source of on-line information (a newswire) for extensive testing and experimentation. The goal of this effort is to prove the utility of the system for processing large bodies of text in a limited domain. Although there will undoubtedly be many lessons in extending SCISOR to handle thousands of texts, SCISOR's first few stories have already demonstrated some of the advantages of the approach described here: 1. Much of the knowledge used in analyzing these stories is domain-independent. 2. Where top-down strategies fail, SCISOR can still ex- tract some information from the texts and use this information in answering questions. 3. Unknown words (lexical gaps) and grammatical lapses are tolerated. These three characteristics simply cannot be achieved with- out combining top-down and bottom-up strategies. The major barrier to the practical success of text pro- cessing systems like SCISOR is the vast amount of knowl- edge required to perform accurate analysis of any body of text. This bottleneck has been partially overcome by the graceful integration of processing strategies in the sys- tem; the program currently operates using only hundreds of known words. However, SCISOR is designed to benefit ultimately from an extended vocabulary (i. e. thousands of word roots) and increased domain knowledge. The vo- cabulary and knowledge base of the system are constantly being extended using a combination of manual and auto- mated techniques. EXTENSIBILITY AND PORTABILITY Our research has combined some of the advantages of top-down language processing methods (tolerance of un- known inputs, understanding in context) with the assets of bottom-up strategies (broader linguistic capabilities, par- tial results in the absence of expectations). The system described here competently answers questions about con- strained texts, uses the same language analyzer for text processing and question answering, and has been applied to other domains as well as the corporate takeover sto ries. SCISOR is thus a state-of-the-art system, but like 133 other text processing systems the main chore that remains is to allow for the practical extraction of information from thousands of real texts. The following are the main issues involved in making such a system a reality and how we address them: Lexicon design: The size of the text-processing lexicon is important, but sheer vocabulary is not of much help. What is needed is a lexicon that accounts both for the basic meanings of common words and the specialized use of terms in a given context. We use a hierarchical phrasal lexicon [Besemer and Jacobs, 1987, Dyer and Zernik, 1986] to allow domain-specific vocabulary to take advantage of existing linguistic knowledge and ul- timately to facilitate automatic language acquisition. Grammar: A disadvantage of many approaches to text processing is that it is counterintuitive to assume that most language processing is domain-specific. While specialized knowledge is essential, a portable gram- mar, like a core lexicon, is indispensable. Language is too complex to be reduced to a few domain-specific heuristics. Because specialized constructs may inherit from general grammatical rules, TRUMP allows spe- cialized sublanguage grammar to interact with \"core\" grammar. It is still a challenge, however, to deal gracefully with constructs in a sublanguage that would ordinarily be extragrammatical. Conceptual Knowledge: The KODIAK knowledge rep- resentation, used for conceptual knowledge in SCISOR, allows for multiple inheritance as well as structured relationships among conceptual roles. This representation is useful for the retrieval of conceptual information in the system. A broader base of \"com- mon sense\" knowledge in KODIAK will be used to increase the robustness of SCISOR. Our strategy has been to attack the robustness prob- lem by starting with the underlying knowledge represen- tation issues. There will be no way to avoid the work involved in scaling up a system, but with this strategy we hope that much of this work will be useful for text process- ing in general, as well as for analysis within a specialized domain. FUTURE DIRECTIONS In the immediate future, we hope to connect SCISOR to a continuous source of on-line information to begin collect- ing large amounts of conceptually analyzed material, and extensively testing the system. We also plan to dramatically increase the size of the lexicon through the addition of an on-line source of dic- tionary and thesaurus information. The system grammar also will increase in coverage over time, as we extend and improve the capabilities of the bottom-up TRUMP parser. Another interesting extension is the full implementa- tion of a parser skimming mode. This mode of operation, triggered when the system recognizes input events that are identical to events it has already read about, will cause the parser to perform very superficial processing of the text. This superficial or skimming processing will continue until the parser reaches a point in the text where the story is no longer reporting on events the system has already read about. RELATED RESEARCH The bulk of the research on natural language text pro- cessing adheres to one of the two approaches integrated in SCISOR. The practical issue for text processing sys- tems is that it is still far from feasible to design a program that processes extended, unconstrained text. Within the \"bottom-up\" framework, one of the most successful strate- gies, in light of this issue, is to define a specialized domain \"sublanguage\" [Kittredge, 1982] that allows robust pro- cessing so long as the texts use prescribed vocabulary and linguistic structure. The \"top-down\" approach similarly relies heavily on the constraints of the textual domain, but in this approach the understanding process is bound by constraints on the knowledge to be derived rather than restrictions on the linguistic structures. The bottom-up, or language-driven strategy, has the advantage of covering a broad class of linguistic phe- nomena and processing even the more intricate details of a text. Many systems [Grishman and Kittredge, 1986] have depended on this strategy for processing messages in constrained domains. Other language-driven programs [Hobbs, 1986] do not explicitly define a sublanguage but rely on a robust syntax and semantics to understand the constrained texts. These systems build upon existing grammars, which may make the semantic interpretation of the texts difficult. The top-down, or expectation-driven, approach, of- fers the benefit of being able to \"skim\" texts for particu- lar pieces of information, passing gracefully over unknown words or constructs and ignoring some of the complexities of the language. A typical, although early, effort at skim- ming news stories was implemented in FRUMP [DeJong, 1979], which accurately extracted certain conceptual in- formation from texts in preselected topic areas. FRUMP proved that the expectation-driven strategy was useful for scanning texts in constrained domains. This strategy in- cludes the banking telex readers TESS [Young and Hayes, 1985] and ATRANS [Lytinen and Gershman, 1986]. These programs all can be easily \"fooled\" by unusual texts, and can obtain only the expected information. The difficulty of building a flexible understanding sys- tem inhibits the integration of the two strategies, although some of those mentioned above have research efforts di- rected at integration. Dyer's BORIS system [Dyer, 1983], a program designed for in-depth analysis of narratives rather than expository text scanning, integrates multiple knowl- edge sources and, like SCISOR, does some dynamic com- bination of top-down and bottom-up strategies. The lin- guistic knowledge used by BORIS is quite different from that of TRUMP, however. It lacks explicit syntactic struc- 134 tures and thus, like the sublanguage approach, relies more heavily on domain-specific linguistic knowledge. Lytinen's MOPTRANS [Lytinen, 1986] integrates syntax and seman- tics in understanding, but the syntactic coverage of the system is in no way comparable to the bottom-up pro- grams. SCISOR is, to our knowledge, the first text pro- cessing system to integrate full language-driven processing with conceptual expectations. CONCLUSION The analysis of extended texts presents an extremely dif- ficult problem for artificial intelligence systems. Bottom- up processing, or linguistic analysis, is necessary to avoid missing information that may be explicitly, although sub- tly, conveyed by the text. Top-down, or expectation-driven processing, is essential for the understanding of language in context. Most text analysis systems have relied too heavily on one strategy. SCISOR represents a unique integration of knowledge sources to achieve robust and reliable extraction of in- formation from naturally occurring texts in constrained domains. Its ability to use lexical and syntactic knowl- edge when available separates it from purely expectation- driven semantic analyzers. At the same time, its lack of reliance on any single source of information and multiple \"fall-back\" heuristics give the system the ability to focus attention and processing on those items of particular in- terest to be extracted. REFERENCES [Besemer and Jacobs, 1987] David Besemer and Paul S. Jacobs. FLUSH: a flexible lexicon design. In Proceed- ings of the 25th Meeting of the Association for Com- putational Linguistics, Palo Alto, California, 1987. [DeJong, 1979] Gerald DeJong. Skimming Stories in Real Time: An Experiment in Integrated Understanding. Research Report 158, Department of Computer Sci- ence, Yale University, 1979. [Dyer, 1983] Michael G. Dyer. In-Depth Understanding. MIT Press, Cambridge, MA, 1983. [Dyer and Zernik, 1986] Michael G. Dyer and Uri Zernik. Encoding and acquiring meanings for figurative phrases. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, New York, 1986. [Grishman and Kittredge, 1986] Raplh Grishman and Richard Kittredge, editors. Analyzing Language in Restricted Domains: Sublanguage Description and Processing. Lawrence Erlbaum, Hillsdale, NJ, 1986. [Hobbs, 1986] Jerry R. Hobbs. Site report: overview of the TACITUS project. Computational Linguistics, 12(3):220-222, 1986. [Jacobs, 1986] Paul S. Jacobs. Language analysis in not- so-limited domains. In Proceedings of the Fall Joint Computer Conference, Dallas, Texas, 1986. [Jacobs, 1987a] Paul S. Jacobs. A knowledge framework for natural language analysis. In Proceedings of the Tenth International Joint Conference on Artificial In- telligence, Milan, Italy, 1987. [Jacobs, 1987b] Paul S. Jacobs. Knowledge-intensive nat- ural language generation. Artificial Intelligence, 33(3):325-378, November 1987. [Jacobs and Rau, 1985] Paul S. Jacobs and Lisa F. Rau. Ace: associating language with meaning. In Tim O'Shea, editor, Advances in Artificial Intelligence, pages 295-304, North Holland, Amsterdam, 1985. [Kittredge, 1982] Richard Kittredge. Variation and homo- geneity of sublanguages. In Richard Kittredge and John Lehrberger, editors, Sublanguages: Studies of Language in Restricted Domains, pages 107-137, Wal- ter DeGruyter, New York, 1982. [Lytinen, 1986] Steven Lytinen. Dynamically combining syntax and semantics in natural language processing. In Proceedings of the Fifth National Conference on Artificial Intelligence, Philadelphia, 1986. [Lytinen and Gershman, 1986] Steven Lytinen and Ana- tole Gershman. ATRANS: automatic processing of money transfer messages. In Proceedings of the Fifth National Conference on Artificial Intelligence, Philadelphia, 1986. [Rau, 1987a] Lisa F. Rau. Information retrieval in never- ending stories. In Proceedings of the Sixth National Conference on Artificial Intelligence, pages 317-321, Los Altos, CA, Morgan Kaufmann Inc., Seattle, Washington, July 1987. [Rau, 1987b] Lisa F. Rau. Knowledge organization and access in a conceptual information system. Infor- mation Processing and Management, Special Issue on Artificial Intelligence for Information Retrieval, 23(4):269-283, 1987. [Rau, 1987c] Lisa F. Rau. SCISOR: a system for effec- tive information retrieval from text. In Poster Session Proceedings of the Third IEEE Conference on Artifi- cial Intelligence Applications, Orlando, FL, 1987. [Schank and Abelson, 1977] Roger C. Schank and Robert P. Abelson. Scripts, Plans, Goals, and Un- derstanding. Lawrence Erlbaum, Halsted, NJ, 1977. [Wilensky, 1986] R. Wilensky. Knowledge representation - a critique and a proposal. In Janet Kolodner and Chris Riesbeck, editors, Experience, Memory, and Reasoning, pages 15-28, Lawrence Erlbaum Asso- ciates, Hillsdale, New Jersey, 1986. [Young and Hayes, 1985] S. Young and P. Hayes. Auto- matic classification and summarization of banking telexes. In The Second Conference on Artificial In- telligence Applications, pages 402-208, IEEE Press, 1985."
  },
  {
    "title": "Nymble: a High-Performance Learning Name-finder",
    "abstract": "This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model. We present our justification for the problem and our approach, a detailed discussion of the model itself and finally the successful results of this new approach.",
    "content": "1. Introduction In the past decade, the speech recognition commu- nity has had huge successes in applying hidden Markov models, or HMM's to their problems. More recently, the natural language processing community has effectively employed these models for part-of- speech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993). We would now propose that HMM's have success- fully been applied to the problem of name-finding. We have built a named-entity (NE) recognition system using a slightly-modified version of an HMM; we call our system \"Nymble\". To our knowledge, Nymble out-performs the best published results of any other learning name-finder. Further- more, it performs at or above the 90% accuracy level, often considered \"near-human performance\". The system arose from the NE task as specified in the last Message Understanding Conference (MUC), where organization names, person names, location names, times, dates, percentages and money amounts were to be delimited in text using SGML-markup. We will describe the various models employed, the methods for training these models and the method for \"decoding\" on test data (the term \"decoding\" borrowed from the speech recognition community, since one goal of traversing an HMM is to recover the hidden state sequence). To date, we have successfully trained and used the model on both English and Spanish, the latter for MET, the multi-lingual entity task. 2. Background 2.1 Name-finding as an Information- theoretic Problem The basic premise of the approach is to consider the raw text encountered when decoding as though it had passed through a noisy channel, where it had been originally marked with named entities.<sup>1</sup> The job of the generative model is to model the original process which generated the name-class-annotated words, before they went through the noisy channel. More formally, we must find the most likely sequence of name-classes (NC) given a sequence of words (W): Pr(NC | W) (2.1) In order to treat this as a generative model (where it generates the original, name-class-annotated words), we use Bayes' Rule: Pr(NC | W) = Pr(W, NC) Pr(W) (2.2) and since the a priori probability of the word sequence—the denominator—is constant for any given sentence, we can maxi-mize Equation 2.2 by maximizing the numerator alone. <sup>1</sup> See (Cover and Thomas, 1991), ch. 2, for an excellent overview of the principles of information theory. 2.2 Previous Approaches to Name- finding Previous approaches have typically used manually constructed finite state patterns (Weischedel, 1995, Appelt et al., 1995). For every new language and every new class of new information to spot, one has to write a new set of rules to cover the new language and to cover the new class of information. A finite-state pattern rule attempts to match against a sequence of tokens (words), in much the same way as a general regular expression matcher. In addition to these finite- state pattern approaches, a variant of Brill rules has been applied to the problem, as outlined in (Aberdeen et al., 1995). 2.3 Interest in Problem and Potential Applications The atomic elements of information extraction— indeed, of language as a whole—could be considered the who, where, when and how much in a sentence. A name-finder performs what is known as surface- or lightweight-parsing, delimiting sequences of tokens that answer these important questions. It can be used as the first step in a chain of processors: a next level of processing could relate two or more named entities, or perhaps even give semantics to that relationship using a verb. In this way, further processing could discover the \"what\" and \"how\" of a sentence or body of text. Furthermore, name-finding can be useful in its own right: an Internet query system might use name- finding to construct more appropriately-formed queries: \"When was Bill Gates born?\" could yield the query \"Bill Gates\"+born. Also, name-finding can be directly employed for link analysis and other information retrieval problems. 3. Model We will present the model twice, first in a conceptual and informal overview, then in a more- detailed, formal description of it as a type of HMM. The model bears resemblance to Scott Miller's novel work in the Air Traffic Information System (ATIS) task, as documented in (Miller et al., 1994). 3.1 Conceptual Model Figure 3.1 is a pictorial overview of our model. Informally, we have an ergodic HMM with only eight internal states (the name classes, including the NOT-A-NAME class), with two special states, the START- and END-OF-SENTENCE states. Within each of the name-class states, we use a statistical bigram language model, with the usual one-word-per-state emission. This means that the number of states in each of the name-class states is equal to the vocabulary size, |V|. The generation of words and name-classes proceeds in three steps: 1. Select a name-class NC, conditioning on the previous name-class and the previous word. 2. Generate the first word inside that name-class, conditioning on the current and previous name- classes. 3. Generate all subsequent words inside the current name-class, where each subsequent word is conditioned on its immediate predecessor. These three steps are repeated until the entire observed word sequence is generated. Using the Viterbi algorithm, we efficiently search the entire space of all possible name-class assignments, maximizing the numerator of Equation 2.2, Pr(W, NC). Informally, the construction of the model in this manner indicates that we view each type of \"name\" to be its own language, with separate bigram probabilities for generating its words. While the number of word-states within each name-class is equal 2 to |V|, this \"interior\" bigram language model is ergodic, i.e., there is a probability associated with every one of the |V|² transitions. As a parameter- ized, trained model, if such a transition were never observed, the model \"backs off\" to a less-powerful model, as described below, in §3.3.3 on p. 4. 3.2 Words and Word-Features Throughout most of the model, we consider words to be ordered pairs (or two-element vectors), composed of word and word-feature, denoted (w, f). The word feature is a simple, deterministic computa- tion performed on each word as it is added to or Word Feature twoDigitNum fourDigitNum containsDigitAndAlpha feature computation is an extremely small part of the implementation, at roughly ten lines of code. Also, most of the word features are used to distinguish types of numbers, which are language-independent.2 The rationale for having such features is clear: in Roman languages, capitalization gives good evidence of names.3 3.3 Formal Model This section describes the model formally, discussing the transition probabilities to the word- states, which \"generate\" the words of each name-class. 3.3.1 Top Level Model Example Text 90 1990 A8956-67 As with most trained, probabilistic models, we containsDigitAndDash 09-96 containsDigitAndSlash 11/9/89 containsDigitAndComma 23,000.00 containsDigitAndPeriod 1.00 otherNum 456789 allCaps BBN capPeriod Μ. firstWord first word of sentence initCap Sally lowerCase can other , Intuition Two-digit year Four digit year Product code Date Date Monetary amount Monetary amount, percentage Other number Organization Person name initial No useful capitalization information Capitalized word Uncapitalized word Punctuation marks, all other words Table 3.1 Word features, examples and intuition behind them looked up in the vocabulary. It produces one of the fourteen values in Table 3.1. These values are computed in the order listed, so that in the case of non-disjoint feature-classes, such as containsDigitAndAlpha and containsDigitAndDash, the former will take precedence. The first eight features arise from the need to distinguish and annotate monetary amounts, percentages, times and dates. The rest of the features distinguish types of capitalization and all other words (such as punctuation marks, which are separate tokens). In particular, the firstWord feature arises from the fact that if a word is capitalized and is the first word of the sentence, we have no good information as to why it is capitalized (but note that allcaps and capPeriod are computed before firstWord, and therefore take precedence). The word feature is the one part of this model which is language-dependent. Fortunately, the word have a most accurate, most powerful model, which will \"back off\" to a less-powerful model when there is insufficient training, and ultimately back-off to unigram probabilities. In order to generate the first word, we must make a transition from one name-class to another, as well as calculate the likelihood of that word. Our intuition was that a word preceding the start of a name-class (such as \"Mr.\", \"President\" or other titles preceding the PERSON name-class) and the word following a name-class would be strong indicators of the subsequent and preceding name-classes, respectively. 2 Non-english languages tend to use the comma and period in the reverse way in which English does, i.e., the comma is a decimal point and the period separates groups of three digits in large numbers. However, the re-ordering of the precedence of the two relevant word-features had little effect when decoding Spanish, so they were left as is. 3 Although Spanish has many lower-case words in organization names. See §4.1 on p, 6 for more details. Accordingly, the probabilitiy for generating the first word of a name-class is factored into two parts: Pr(NC | NC-1, -1). Pr((W, f) first | NC, NC-1). (3.1) The top level model for generating all but the first word in a name-class is Pr((w, f) | (w, f), NC). (3.2) There is also a magical \"+end+\" word, so that the probability may be computed for any current word to be the final word of its name-class, i.e., Pr(+end+,other) | (w, f) final, NC). (3.3) As one might imagine, it would be useless to have the first factor in Equation 3.1 be conditioned off of the +end+ word, so the probability is conditioned on the previous real word of the previous name-class, i.e., we compute Pr(NC | NC-1, W-1) w w_₁ = +end + if -1 NC_₁ = START - OF - SENTENCE w_₁ = last observed word otherwise (3.4) Note that the above probability is not conditioned on the word-feature of W_1, the intuition of which is that in the cases where the previous word would help the model predict the next name-class, the word feature-capitalization in particular is not impor- tant: \"Mr.\" is a good indicator of the next word beginning the PERSON name-class, regardless of capitalization, especially since it is almost never seen as \"mr.\". 3.3.2 Calculation of Probabilities The calculation of the above probabilities is straightforward, using events/sample-size: Pr(NC | NC, W₁) = C(NC,NC-1,W-1) C(NC-1,W_1) Pr((w, f) first | NC, NC-1) = = c(NC, NC_₁) (3.5) c((w,f) surat, NC, NC-1) (3.6) Pr((w,f) | (w,f), NC) = c({w.f), (w.f), NC) (3.7) c((w,f), NC) -1' where c() represents the number of times the events occurred in the training data (the count). 3.3.3 Back-off Models and Smoothing Ideally, we would have sufficient training (or at least one observation of!) every event whose conditional probability we wish to calculate. Also, ideally, we would have sufficient samples of that upon which each conditional probability is conditioned, e.g., for Pr(NC | NC_1, w_₁), we would like to have seen sufficient numbers of NC-1, W-1 Unfortunately, there is rarely enough training data to compute accurate probabilities when \"decoding\" on new data. 3.3.3.1 Unknown Words The vocabulary of the system is built as it trains. Necessarily, then, the system knows about all words for which it stores bigram counts in order to compute the probabilities in Equations 3.1 3.3. The question arises how the system should deal with unknown words, since there are three ways in which they can appear in a bigram: as the current word, as the previous word or as both. A good answer is to train a separate, unknown word-model off of held-out data, to gather statistics of unknown words occurring in the midst of known words. Typically, one holds out 10-20% of one's training for smoothing or unknown word-training. In order to overcome the limitations of a small amount of training data-particularly in Spanish-we hold out 50% of our data to train the unknown word- model (the vocabulary is built up on the first 50%), save these counts in training data file, then hold out the other 50% and concatentate these bigram counts with the first unknown word-training file. This way, we can gather likelihoods of an unknown word appearing in the bigram using all available training data. This approach is perfectly valid, as we are trying to estimate that which we have not legitimately seen in training. When decoding, if either word of the bigram is unknown, the model used to estimate the probabilities of Equations 3.1-3 is the unknown word model, otherwise it is the model from the normal training. The unknown word-model can be viewed as a first level of back-off, therefore, since it is used as a backup model when an unknown word is encountered, and is necessarily not as accurate as the bigram model formed from the actual training. 3.3.3.2 Further Back-off Models and Smoothing Whether a bigram contains an unknown word or not, it is possible that either model may not have seen this bigram, in which case the model backs off to a less-powerful, less-descriptive model. Table 3.2 shows a graphic illustration of the back-off scheme: Name-class Bigrams Pr(NC | NC_₁, W₁) : Pr(NC | NC_₁) Pr(NC) : : 1 number of name - classes First-word Bigrams Pr((w, f)<sub>first</sub> | NC, NC<sub>-1</sub>) : Pr((w,f) | (+begin+, other), NC) : Pr((w,f)| NC) : Pr(w| NC). Pr(f | NC) Non-first-word Bigrams Pr((w, f) | (w, f)<sub>-1</sub>, NC) : Pr((w, f) | NC) : Pr(w | NC). Pr(f | NC) : 1 1 |V| number of word features : 1 1 |V| number of word features Table 3.2 Back-off strategy The weight for each back-off model is computed on- the-fly, using the following formula: If computing Pr(XIY), assign weight of λ to the direct computation (using one of the formulae of §3.3.2) and a weight of (1 – λ) to the back-off model, where λ = 1 - (1 - <sup>old c(Y)</sup>⁄<sub>c(Y)</sub>) <sup>1</sup>⁄<sub>(1 + <sup>unique outcomes of Y</sup>⁄<sub>c(Y)</sub>)</sub> , (3.8) where \"old c(Y)\" is the sample size of the model from which we are backing off. This is a rather simple method of smoothing, which tends to work well when there are only three or four levels of back-off.<sup>4</sup> This method also overcomes the problem when a back-off model has roughly the same amount of training as the current model, via the first factor of Equation 3.8, which essentially ignores the back-off model and puts all the weight on the primary model, in such an equi-trained situation. As an example-disregarding the first factor-if we saw the bigram \"come hither\" once in training and we saw \"come here\" three times, and nowhere else did we see the word \"come\" in the NOT-A-NAME class, when computing Pr(\"hither\" | \"come\", NOT-A-NAME), we would back off to the unigram probability Pr(\"hither\" | NOT-A-NAME) with a weight of <sup>1</sup>⁄<sub>3</sub>. since the number of unique outcomes for the word-state for \"come\" would be two, and the total number of times \"come\" had been the preceding word in a bigram would be four (a <sup>4</sup>Any more levels of back-off might require a more sophisticated smoothing technique, such as deleted interpolation. No matter what smoothing technique is used, one must remember that smoothing is the art of estimating the probability of that which is unknown (i.e., not seen in training). 1/(1+<sup>2</sup>⁄<sub>4</sub>) = <sup>2</sup>⁄<sub>3</sub> weight for the bigram probability, a 1 - <sup>2</sup>⁄<sub>3</sub> = <sup>1</sup>⁄<sub>3</sub> weight for the back-off model). 3.4 Comparison with a traditional HMM Unlike a traditional HMM, the probability of generating a particular word is 1 for each word-state inside each of the name-class states. An alternative- and more traditional-model would have a small number of states within each name-class, each having, perhaps, some semantic significance, e.g., three states in the PERSON name-class, representing a first, middle and last name, where each of these three states would have some probability associated with emitting any word from the vocabulary. We chose to use a bigram language model because, while less semantically appealing, such n-gram language models work remarkably well in practice. Also, as a first research attempt, an n-gram model captures the most general significance of the words in each name-class, without presupposing any specifics of the structure of names, à la the PERSON name-class example, above. More important, either approach is mathematically valid, as long as all transitions out of a given state sum to one. 3.5 Decoding All of this modeling would be for naught were it not for the existence of an efficient algorithm for finding the optimal state sequence, thereby \"decoding\" the original sequence of name-classes. The number of possible state sequences for N states in an ergodic model for a sentence of m words is N<sup>m</sup>, but, using dynamic programming and an appropriate merging of multiple theories when they converge on a particular state—the Viterbi decoding algorithm—a sentence can be \"decoded\" in time linear to the number of tokens in the sentence, O(m) (Viterbi, 1967). Since we are interested in recovering the name-class state sequence, we pursue eight theories at every given step of the algorithm. 4. Implementation and Results 4.1 Development History Initially, the word-feature was not in the model; instead the system relied on a third-level back-off part- of-speech tag, which in turn was computed by our stochastic part-of-speech tagger. The tags were taken at face value: there were not k-best tags; the system treated the part-of-speech tagger as a \"black box\". Although the part-of-speech tagger used capitalization to help it determine proper-noun tags, this feature was only implicit in the model, and then only after two levels of back-off! Also, the capitalization of a word was submerged in the muddiness of part-of-speech tags, which can \"smear\" the capitalization probability mass over several tags. Because it seemed that capitalization would be a good name-predicting feature, and that it should appear earlier in the model, we eliminated the reliance on part-of-speech altogether, and opted for the more direct, word-feature model described above, in §3. Originally, we had a very small number of features, indicating whether the word was a number, the first word of a sentence, all uppercase, inital-capitalized or lower-case. We then expanded the feature set to its current state in order to capture more subtleties related mostly to numbers; due to increased performance (although not entirely dramatic) on every test, we kept the enlarged feature set. Contrary to our expectations (which were based on our experience with English), Spanish contained many examples of lower-case words in organization and location names. For example, departamento (\"Department\") could often start an organization name, and adjectival place-names, such as coreana (\"Korean\") could appear in locations and by convention are not capitalized. 4.2 Current Implementation The entire system is implemented in C++, atop a \"home-brewed\", general-purpose class library, providing a rapid code-compile-train-test cycle. In fact, many NLP systems suffer from a lack of software and computer-science engineering effort: run- time efficiency is key to performing numerous experiments, which, in turn, is key to improving performance. A system may have excellent performance on a given task, but if it takes long to compile and/or run on test data, the rate of improvement of that system will be miniscule compared to that which can run very efficiently. On a Sparc20 or SGI Indy with an appropritae amount of RAM, Nymble can compile in 10 minutes, train in 5 minutes and run at 6MB/hr. There were days in which we had as much as a 15% reduction in error rate, to borrow the performance measure used by the speech community, where error rate = 100% − F- measure. (See §4.3 for the definition of F-measure.) 4.3 Results of evaluation In this section we report the results of evaluating the final version of the learning software. We report the results for English and for Spanish and then the results of a set of experiments to determine the impact of the training set size on the algorithm's performance in both English and Spanish. For each language, we have a held-out development test set and a held-out, blind test set. We only report results on the blind test set for each respective language. 4.3.1 F-measure The scoring program measures both precision and recall, terms borrowed from the information-retrieval community, where P = number of correct responses and number responses R = number of correct responses number correct in key (4.1) Put informally, recall measures the number of \"hits\" vs. the number of possible correct answers as specified in the key file, whereas precision measures how many answers were correct ones compared to the number of answers delivered. These two measures of performance combine to form one measure of performance, the F-measure, which is computed by the weighted harmonic mean of precision and recall: F = (β² +1)RP (B²R) + P (4.2) where β represents the relative weight of recall to precision (and typically has the value 1). To our knowledge, our learned name-finding system has achieved a higher F-measure than any other learned system when compared to state-of-the-art manual (rule-based) systems on similar data. 4.3.2 English and Spanish Results Our test set of English data for reporting results is that of the MUC-6 test set, a collection of 30 WSJ documents (we used a different test set during development). Our Spanish test set is that used for MET, comprised of articles from the news agency AFP. Table 4.1 illustrates Nymble's performance as compared to the best reported scores for each category. Best Reported 100 Case Language Score Nymble 90 Mixed English 96 93 80 Upper English 89 91 70 Mixed Spanish 93 90 60 50 Table 4.1 F-measure Scores 40 Data 30 20 10 0 4.3.3 The Amount of Training Required With any learning technique one of the important questions is how much training data is required to get acceptable performance. More generally how does performance vary as the training set size is increased or decreased? We ran a sequence of experiments in English and in Spanish to try to answer this question for the final model that was implemented. For English, there were 450,000 words of training data. By that we mean that the text of the document itself (including headlines but not including SGML tags) was 450,000 words long. Given this maximum size of training available to us, we successfully divided the training material in half until we were using only one eighth of the original training set size or a training set of 50,000 words for the smallest experiment. To give a sense of the size of 450,000 words, that is roughly half the length of one edition of the Wall Street Journal. The results are shown in a histogram in Figure 4.1 below. The positive outcome of the experiment is that half as much training data would have given almost equivalent performance. Had we used only one quarter of the data or approximately 100,000 words, performance would have degraded slightly, only about 1-2 percent. Reducing the training set size to 50,000 words would have had a more significant decrease in the performance of the system; however, the performance is still impressive even with such a small training set. Figure 4.1: 12.5% Training 25% Training 50% Training 100% Training Impact of Various Training Set Sizes on Performance in English. The learning algorithm performs remarkable well, nearly comparable to handcrafted systems with as little as 100,000 words of training data. On the other hand, the result also shows that merely annotating more data will not yield dramatic improvement in the performance. With increased training data it would be possible to use even more detailed models that require more data and could achieve significantly improved overall system performance with those more detailed models. For Spanish we had only 223,000 words of training data. We also measured the performance of the system with half the training data or slightly more than 100,000 words of text. Figure 4.2 shows the results. There is almost no change in performance by using as little as 100,000 words of training data. Therefore the results in both languages were comparable. As little as 100,000 words of training data produces performance nearly comparable to handcrafted systems. 100 90 80 70 60 50 40 30 20 10 0 50% Training 100% Training Figure 4.2: Impact of Training Set Size on Performance in Spanish 5. Further Work While our initial results have been quite favorable, there is still much that can be done potentially to improve performance and completely close the gap between learned and rule-based name-finding systems. We would like to incorporate the following into the current model: • lists of organizations, person names and locations • an aliasing algorithm, which dynamically updates the model (where e.g. IBM is an alias of International Business Machines) • longer-distance information, to find names not captured by our bigram model 6. Conclusions We have shown that using a fairly simple probabilistic model, finding names and other numerical entities as specified by the MUC tasks can be performed with \"near-human performance\", often likened to an F of 90 or above. We have also shown that such a system can be trained efficiently and that, given appropriately and consistently marked answer keys, it can be trained on languages foreign to the trainer of the system; for example, we do not speak Spanish, but trained Nymble on answer keys marked by native speakers. None of the formalisms or techniques presented in this paper is new; rather, the approach to this task—the model itself—is wherein lies the novelty. Given the incredibly difficult nature of many NLP tasks, this example of a learned, stochastic approach to name-finding lends credence to the argument that the NLP community ought to push these approaches, to find the limit of phenomena that may be captured by probabilistic, finite-state methods. 7. References Aberdeen, J., Burger, J., Day, D., Hirschman, L., Robinson, P. and Vilain, M. (1995) In Proceedings of the Sixth Message Understanding Conference (MUC-6)Morgan Kaufmann Publishers, Inc., Columbia, Maryland, pp. 141-155. Appelt, D. E., Jerry R. Hobbs, Bear, J., Israel, D., Kameyama, M., Kehler, A., Martin, D., Myers, K. and Tyson, M. (1995) In Proceedings of the Sixth Message Understanding Conference (MUC-6)Morgan Kaufmann Publishers, Inc., Columbia, Maryland, pp. 237-248. Church, K. (1988) In Second Conference on Applied Natural Language Processing, Austin, Texas. Cover, T. and Thomas, J. A. (1991) Elements of Information Theory, John Wiley & Sons, Inc., New York. Miller, S., Bobrow, R., Schwartz, R. and Ingria, R. (1994) In Human Language Technology Workshop, Morgan Kaufmann Publishers, Plainsboro, New Jersey, pp. 278-282. Viterbi, A. J. (1967) IEEE Transactions on Information Theory, IT-13(2), 260-269. Weischedel, R. (1995) In Proceedings of the Sixth Message Understanding Conference (MUC- 6)Morgan Kaufmann Publishers, Inc., Columbia, Maryland, pp. 55-69. Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L. and Palmucci, J. (1993) Computational Linguistics, 19(2), 359-382. 8. Acknowledgements The work reported here was supported in part by the Defense Advanced Research Projects Agency; a technical agent for part of the work was Fort Huachucha under contract number DABT63-94-C- 0062. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government. We would also like to give special acknowledge- ment to Stuart Shieber, McKay Professor of Computer Science at Harvard University, who endorsed and helped foster the completion of this, the first phase of Nymble's development."
  },
  {
    "title": "Morphological Tagging: Data vs. Dictionaries",
    "abstract": "Part of Speech tagging for English seems to have reached the human levels of error, but full morphological tagging for inflectionally rich languages, such as Romanian, Czech, or Hungarian, is still an open problem, and the results are far from being satisfactory. This paper presents results obtained by using a universalized exponential feature-based model for five such languages. It focuses on the data sparseness issue, which is especially severe for such languages (the more so that there are no extensive annotated data for those languages). In conclusion, we argue strongly that the use of an independent morphological dictionary is the preferred choice to more annotated data under such circumstances.",
    "content": "1 Full Morphological Tagging English Part of Speech (POS) tagging has been widely described in the recent past, starting with the (Church, 1988) paper, followed by numerous others using various methods: neural networks (Ju- lian Benello and Anderson, 1989), HMM tagging (Merialdo, 1992), decision trees (Schmid, 1994), transformation-based error-driven learning (Brill, 1995), and maximum entropy (Ratnaparkhi, 1996), to select just a few. However different the methods were, English dominated in these tests. Unfortunately, English is a morphologically \"im- poverished\" language: there are no complicated agreement relations, word order variation is mini- mal, and the morphological categories are either ex- tremely simple (-s for plural of nouns, for example), or (almost) nonexistent (cases expressed by inflec- tion, for example) - with not too many exceptions and irregularities. Therefore the number of tags se- lected for an English tagset is not that large (40-75 in the typical case). Also, the average ambiguity is low (2.32 tags per token on the manually tagged * The work described herein has been started and largely done within author's home institution, the Institute of For- mal and Applied Linguistics, Charles University, Prague, CZ, within the project VS96151 of the Ministry of Education of the Czech Republic and partially also under the grant 405/96/K214 of the Grant Agency of the Czech Republic. Wall Street Journal part in the Penn Treebank, for example). Highly inflective and agglutinative languages are different. Obviously we can limit the number of tags to the major part-of-speech classes, plus some (like the Xerox Language Tools (Chanod, 1997) for such languages do), and in fact achieve similar perfor- mance, but that limits the usefulness of the results thus obtained for further analysis. These languages, obviously, do not use the rich inflection just for the amusement (or embarrassment) of their speakers (or NLP researchers): the inflectional categories carry important information which ought to be known at a later time (e.g., during parsing). Thus one wants not only to tell apart verbs from nouns, but also nominative from genitive, masculine animate from inanimate, singular from plural - all of them being often ambiguous one way or the other. The average tagset, as found even in a moderate corpus, contains between 500 and 1,000 distinct tags whereas the size of the set of possible and plausible tags can reach 3,000 to 5,000. Obviously, any of the statistical methods used for English (even if fully supervised) clash with (or, fall through) the data sparseness problem (see below Table 1 for details). There have been attempts to solve this problem for some of the highly inflectional European lan- guages ((Daelemans et al., 1996), (Erjavec et al., 1999), (Tufis, 1999), and also our own in (Hajič and Hladká, 1997), (Hajič and Hladká, 1998), see also below), but so far no method nor a tagger has been evaluated against a larger number of those lan- guages in a similar setting, to allow for a side-by- side comparison of the difficulty (or ease) of full morphological tagging of those languages. Thanks to the Multext-East project (Véronis, 1996a), there are now five annotated corpora available (which are manually fully morphologically tagged) to perform such experiments. 2 The Languages Used and The Training Data We use the Multext-East-annotated version of the Orwell's 1984 novel in Czech, Estonian, Hungarian, Romanian and Slovene¹. The annotation uses a sin- gle SGML-based formal scheme, and even common guidelines for tagset design and annotation, nev- ertheless the tagsets differ substantially since the languages differ as well: Romanian is a French-like romance language, Hungarian is agglutinative, and the other languages are more or less inflectional- type languages². The annotated data contains about 100k tokens (including punctuation) for each lan- guage; out of those, the first 20k tokens has been used for testing, the rest for training. We have also extended the tag identifiers by appending a string of hyphens ('-') to suit the exponential tagger which expects the tags to be of equal length; the mapping was 1:1 for all tags in all languages, since the \"long\" tags are in fact the Multext-East standard. From the tagging point of view, the language char- acteristics displayed in Table 1 are the most rele- vant³. 3 The Methodology The main tagger used for the comparison experiment is the probabilistic exponential-model-based, error- driven learner we described in detail in (Hajič and Hladká, 1998). Modifications had to be made, how- ever, to make it more universal across languages. 3.1 Structure of the Model The model described in (Hajič and Hladká, 1998) is a general exponential (specifically, a log-linear) model (such as the one used for Maximum Entropy- based models): PAC(yx) = exp(∑i=1 λifi(y, x)) Z(x) (1) where fi(y,x) is a binary-valued feature of the event value being predicted and its context, λi is a weight of the feature fi, and Z(x) is the natural normalization factor. This model is then essentially reduced to Naive Bayes by the approximation of the There are more languages involved in the Multext-East project, but only these five languages have been really care- fully tagged; English is unfortunately tagged using Eric Brill's tagger trained in unsupervised mode, leaving multiple output at almost every ambiguous token, and Bulgarian is totally unusable since it has been tagged automatically with only a baseline tagger. The English results reported below thus come from the Penn Treebank data, from which we have used roughly 100,000 words to match the training data sizes for the remaining languages. For Czech, Hungarian, and Slovene we use later versions of the annotated data (than those found on the Multext-East CD) which we obtained directly from the authors of the annotations after the Multext-CD had been published, since the new data contain rather substantial im- provements over the originally published data. ²For detailed account of the lexical characteristics of these languages, see (Véronis, 1996b). ³We have included English here for comparison purposes, since these characteristics are independent of the annotatio λi parameters, which is done because there are mil- lions of possible features in the pool and thus the full entropy maximization is prohibitively expensive, if we want to select a small number of features instead of keeping them all. The tags are predicted separately for each mor- phological category (such as POS, NUMBER, CASE, DEGREE OF COMPARISON, etc.). The model makes an extensive use of so-called \"ambigu- ity classes\" (ACs). An ambiguity class is a set of values (such as genitive and accusative) of a single category (such as CASE) which arises for some word forms as a result of morphological analysis. For un- ambiguous word forms (unambiguous from the point of view of a certain category), the ambiguity class set contains only a single value; for ambiguous forms, there are 2 or more values in the AC. For example, let's suppose we use part-of-speech (POS), number and tense as morphological categories for English; then the word form \"borrowed\" is 2-way ambiguous in POS ({V, J} for verb and adjective, respectively), unambiguous in number (linguistic arguments apart, number is typically regarded \"not applicable\" to ad- jectives as well as to almost all forms of verbs in English), and 3-way ambiguous in tense ({P,N,-} for past tense, past participle, and \"not applicable\" in the adjective form). The predictions of the models are always condi- tioned on the ambiguity class of the category (POS, NUMBER, ...) in question. In other words, there is a separate model for each category and an ambigu- ity class from that category. Naturally, there is no model for unambiguous ACs classes. However, even though the ambiguity classes bring very valuable in- formation about the word form being tagged and a reliable information about the context (since they are fixed during tagging), using ACs causes also an unwelcome effect of partitioning the already scarce data and also effectively ignores statistics of the un- ambiguous cases. The context of features uses the neighboring words (original word forms) and ambiguity classes on sub- tags, where their relative position in text might be either fixed (0, -1, +1) or \"variable\", using a value of the POS subtag as the \"stop here\" criterion, up to 4 text positions (words) apart. 3.2 General Subtag Features The original model uses the ambiguity classes not only for conditioning on context in features, but also for the individual models based on category and an AC. More general features have been introduced, which do not depend on the ambiguity class of the subtag being predicted any more. This allows to learn also from unambiguous tokens. However, the training time is increased dramatically by doing so since all cuonte ue to be take Table 1: Training data in numbers Language Training Size Tagset Size Ambiguous Tokens English⁴ 99903 139 38.65% Czech 87071 970 45.97% Estonian 81383 476 40.24% Hungarian 102992 401 21.58% Romanian 104583 486 40.00% Slovene 94457 1033 38.01% into consideration, as opposed to the case of training the small AC-based model, when only those training events which contain the particular AC are used. 3.3 Variable Distance Condition The \"stop\" criterion for finding the appropriate relative ative position was originally based on hard coded choices suitable for the Czech language only, and of course it depended on the tagset as well. This depen- dency has been removed by selecting the appropriate conditions automatically when building the pool of possible features at the initialization phase⁵ (using the relative frequency of the POS ambiguity classes, and a threshold to cut off less frequent categories to limit the size of the feature pool). 3.4 Weight Variation Even though the full computation of the appropriate feature weight is still prohibitive (the more so when the general features are added), the learner is now allowed to vary the weights (in several discrete steps) during feature selection, as a (somewhat crude) at- tempt to depart from the Naive Bayes simplification to the approximation of the \"correct\" Maximum En- tropy estimation. 3.5 Handling Unknown Words In order to compare the effects of (not) using an in- dependent dictionary, we have added an unknown word handling module to the code⁶. It extracts the prefix and suffix frequency information (and the combination thereof) from the training data. Then, for each of the combinations, it selects the most fre- quent set of tags seen in the training data and stores it for later use. When tagging, the data is first piped through a \"guesser\" which assigns to the unknown words such a set of possible tags which is stored with the longest matching prefix/suffix combination. ⁵Also, the use of variable-distance context may be switched off entirely. ⁶Originally, the code relied exclusively on the use of such an independent dictionary. Since the coverage of the Czech dictionary we have used is extensive, we have been simply ignoring the unknown word problem altogether in the past. 4 The Results 4.1 Reporting Error Rate: Words vs. Tokens Since \"best-only\" tagging has been carried out, the error rate (i.e, 100 − accuracy in %) measure has been used throughout as the only evaluation crite- rion. However, since some results reported previ- ously were apparently obtained using only the \"real\" words as the total for accuracy evaluation, whereas in other experiments every token counts (including punctuation⁷, for example)⁸, we have computed both and report them separately⁸. 4.2 Availability of Dictionary Information We use two methods to obtain the set of possible tags for any given word form (i.e., to analyze it mor- phologically). Both methods include handling un- known words. First, we use only information which may be obtained automatically from the manually annotated corpus (we call this method automatic). This is the way the Maximum Entropy tagger (Rat- naparkhi, 1996) runs if one uses the binary version from the website (see the comparison in Section 5). However, it is not unreasonable to assume that a larger independent dictionary exists which can help to obtain a list of possible tags for each word form in test data. This is what we have at our disposal for the languages in question, since the development of such a dictionary was part of the Multext-East project. We can thus assume a dictionary info is available for unknown words in the test data, i.e., even though there is no statistics available for them (since they did not appear in the training data), all possible tags for (almost)⁹ every test token are avail- able. This method is referred to as independent in the following text. We have also used a third method of obtain- ing a dictionary information (called mixed), namely, by using only the words from the training data, ⁷And sometimes a separate token for sentence boundary ⁸Table 1 has been computed using all tokens. In fact, the languages differ significantly in the proportion of punctuation: from about 18% (English) to 30% (Estonian). ⁹Depending on the quality of the independent dictionary. Of course, the tagsets must match, which could be a problem per se. Here it is simple, since the dictionaries have been developed using the same tagsets as the tagged data. but complementing the information about them ob- tained from the training data by including all other possible tags for such words. Therefore the net result is that during testing, we have only training words at our disposal, but with a complete dictionary in- formation (as if coming from a full morphological dictionary)<sup>10</sup>. The results on the full training data set are sum- marized in Table 2. The baseline error rate is computed as follows. First of all, we use the independent dictionary for obtaining the possible tags for each word. Then we extract only the lexical information from the current position<sup>11</sup> and counts used for smoothing (which is based on the ambiguity classes only and it does not use lexical information). The system is then trained normally, which means it uses the lexical information only if the AC-based smoothing<sup>12</sup> alone does not work. This baseline method is thus very close to the usual baseline method of using simple conditional distribution of tags given words. The message of Table 2 seems to be obvious; but before we jump to conclusions, let's present another set of experiments. In view of the recent interest in dealing with \"small languages\", and with regard to the questions of cost-effectiveness of using \"human\" resources (i.e. annotation vs. rule-writing vs. tools development etc.), we have also performed experiments with re- duced training data size (but with an enriched fea- ture pool – by lowering thresholds, adding more of the \"general features\" as described above, etc. – as allowed by reasonable time/space constraints).<sup>13</sup> These results are summarized in Table 3 (using only the dictionary derived from the training data), Table 4 (using words from training data with mor- phological information complemented from a dictio- nary) and Table 5 (using the \"independent\" dictio- nary). In all cases, we again count only true words (no punctuation). Accordingly, the major POS er- ror rate is reported, too (12 POS tags to be dis- tinguished only: Noun, Verb, Adjective, ...; see Ta- bles 6, 7, and 8). <sup>10</sup>This arrangement removes the \"closed vocabulary\" phe- nomenon from the test data, since for the Multext-East data, we did not have a truly independent vocabulary available. <sup>11</sup>Words from the training data which are not singletons (freq > 1) are used. Surprisingly enough, it would not hurt to use them too. We believe it is due to the smoothing method used. Even though this is valid only for the baseline experi- ment, we have observed in general that this form of exponen- tial model (with error-driven training, that is) is remarkably resistant to overtraining. <sup>12</sup>Using ACs linearly interpolated with global unigram sub- tag distribution and finally the uniform distribution. <sup>13</sup>By reasonable we mean less than a day of CPU for train- ing. Table 9: Exponential w/feature selection vs. Max- imum Entropy tagger (Words-only Error Rate, no dictionary) Language Tagger Exp. MaxEnt English 9.18% 6.38% Czech 18.83% 17.77% Estonian 13.95% 14.92% Hungarian 8.16% 8.55% Romanian 7.76% 7.66% Slovene 16.26% 17.44% 4.3 Tagger Comparison The work (Erjavec et al., 1999) consistently com- pares several taggers (HMM, Brill's Transformation- based Tagger, Ratnaparkhi's Maximum Entropy tagger, and the Daelemans et al.'s Memory-based Tagger) on Slovene. We have chosen the Maximum Entropy tagger (Ratnaparkhi, 1996) for a compari- son with our universal tagger, since it achieved (by a small margin) the best overall result on Slovene as reported there (86.360% on all tokens) of tag- gers available to us (MBT, the best overall, was not freely available to us at the time of writing). We have trained and tested the Maximum Entropy Tag- ger on exactly the same data, using the off-the-shelf (java binary only) version. The results are compared in Table 9. Since we want to show how a tagger accuracy is influenced by the amount of training data available, we have run a series of experiments comparing the results of the exponential tagger to the maximum entropy tagger when there is only a limited amount of data available. The results are summarized in Table 10. Since the public version of the MaxEnt tagger cannot be modified to take advantage of nei- ther the mixed nor the independent dictionary, we have compared it only to the automatic dictionary version of the exponential tagger. To save space, the results are tabulated only for the training data sizes of 2000, 5000 and 20000 words. Again, only the \"true\" word error rate is reported. As the tables show, for the languages we tested, the exponential, feature-based tagger we adapted from (Hajič and Hladká, 1998) achieves similar re- sults as the Maximum Entropy tagger<sup>14 15</sup>. (using exactly the same (full) training data; the \"score\" is 3:3, with the MaxEnt tagger being substantially better on English; probably the development lan- <sup>14</sup>Otherwise the acknowledged leader in English tagging <sup>15</sup>The only substantial difference we noticed was in tagging speed. The runtime speed of the MaxEnt tagger is lower, only about 10 words per second vs. almost 500 words per second; it should be noted however that we are comparing MaxEnt's java bytecode and C. Table 2: Results (Error rate, ER) on full training data, only true words counted (no punctuation) Dictionary: Automatic Mixed Independent Language Baseline Full Baseline Full Baseline Full English 11.42% 9.18% 11.40% 7.91% 7.07% 3.58% Czech 23.02% 18.83% 22.61% 14.78% 19.40% 9.59% Estonian 16.12% 13.95% 16.19% 12.98% 9.94% 5.34% Hungarian 8.35% 8.16% 8.31% 8.00% 3.55% 2.58% Romanian 10.87% 7.76% 10.81% 7.34% 7.49% 3.35% Slovene 20.53% 16.26% 20.01% 13.29% 17.29% 9.00% Table 3: Error rate on reduced training data, dictionary: automatic Language Training data size 1000 2000 5000 10000 20000 Full English 36.20% 29.36% 23.47% 18.27% 14.46% 9.18% Czech 48.22% 42.95% 36.54% 30.97% 27.08% 18.83% Estonian 48.14% 42.10% 32.44% 26.81% 21.51% 13.95% Hungarian 39.68% 32.21% 23.94% 18.04% 13.92% 8.16% Romanian 40.61% 35.02% 25.06% 19.26% 15.16% 7.76% Slovene 45.84% 39.58% 33.12% 28.60% 24.50% 16.26% Table 4: Error rate on reduced training data, dictionary: mixed Language Training data size 1000 2000 5000 10000 20000 Full English 36.15% 29.58% 22.93% 17.70% 14.00% 7.91% Czech 48.97% 41.93% 34.37% 28.10% 23.31% 14.78% Estonian 48.24% 42.79% 32.98% 26.60% 21.02% 12.98% Hungarian 39.87% 32.71% 23.63% 17.98% 13.82% 8.00% Romanian 42.85% 35.70% 25.46% 19.23% 14.81% 7.34% Slovene 46.74% 39.88% 32.00% 26.20% 21.73% 13.29% Table 5: Error rate on reduced training data, dictionary: \"independent\" Language Training data size 1000 2000 5000 10000 20000 Full English 10.29% 7.64% 5.53% 4.54% 3.83% 3.58% Czech 22.51% 18.07% 17.33% 15.10% 12.62% 9.59% Estonian 13.11% 11.95% 10.70% 9.29% 8.10% 5.34% Hungarian 6.84% 5.35% 4.29% 4.07% 3.48% 2.58% Romanian 13.11% 9.47% 7.81% 6.18% 5.07% 3.35% Slovene 24.63% 19.17% 16.17% 14.12% 12.62% 9.00% guage bias shows here16). However, when the train- ing data size goes down, the advantage of predicting the single morphological categories separately favors the exponential tagger (with the notable and sub- stantial exception of English). The less data, the larger the difference (Tab 10). 16On the other hand, the Exponential tagger has been de- veloped on Czech originally and it lost on this language. It should be noted that the original version of the exponen- tial tagger did contain more Czech-specific features, and thus might in fact do better. The resulting accuracy (of both taggers) is still unsatisfactory not only from the point of view of results obtained on English, but also from the prac- tical point of view: approx. 85% accuracy (Czech, Slovene) typically means that about five out of six 10-word sentences contain at least one error in it. That is bad news e.g. for parsing projects involving tagging as a preliminary step. Table 6: POS Error rate on reduced training data, dictionary: automatic Training data size Language 1000 2000 5000 10000 20000 Full English 26.77% 20.82% 16.11% 11.86% 9.48% 5.64% Czech 24.32% 20.20% 13.46% 9.70% 7.22% 3.72% Estonian 35.81% 30.52% 23.02% 18.26% 14.31% 8.46% Hungarian 30.54% 24.99% 18.09% 13.15% 10.29% 5.81% Romanian 31.33% 27.59% 19.24% 14.51% 11.25% 5.21% Slovene 27.16% 23.15% 17.01% 12.89% 9.74% 5.61% Table 7: POS Error rate on reduced training data, dictionary: mixed Language Training data size 1000 2000 5000 10000 20000 Full English 26.69% 21.09% 15.82% 11.53% 9.08% 4.94% Czech 24.32% 20.61% 13.47% 10.19% 7.37% 3.76% Estonian 36.48% 31.76% 23.55% 18.21% 14.32% 8.20% Hungarian 30.28% 25.25% 17.59% 12.89% 10.15% 5.64% Romanian 33.56% 28.34% 20.03% 14.52% 11.03% 5.04% Slovene 27.58% 23.30% 16.85% 12.59% 9.88% 5.12% Table 8: POS Error rate on reduced training data, dictionary: \"independent\" Language Training data size 1000 2000 5000 10000 20000 Full English 6.42% 5.36% 3.63% 3.02% 2.53% 2.43% Czech 3.21% 2.85% 2.17% 2.01% 1.65% 1.12% Estonian 6.71% 6.32% 5.27% 4.31% 3.77% 2.36% Hungarian 5.35% 4.42% 3.39% 3.18% 2.75% 2.04% Romanian 9.51% 6.54% 5.36% 4.00% 3.18% 1.89% Slovene 6.10% 5.19% 4.04% 3.59% 3.25% 2.08% 5 Conclusions 5.1 The Differences Among Languages The following discussion abstracts from the tagset design, relying on the fact that the Multext-East project has been driven by common tagset guidelines to an unprecedented extent, given the very different languages involved. At the same time, we acknowl- edge that even so, their design for the individual languages might have influenced the results. Also, the quality of the annotation is an important factor; we believe though that the later data we obtained for the experiments described here are within the range of usual human error and do not suffer from negligence<sup>17</sup>. First of all, it is clear that these languages differ substantially just by looking at the simple training <sup>17</sup>Specifically, we are sure that the post-release Czech, Slovene and Hungarian data we are using are without anno- tation defects beyond the usual occasional annotation error, as they have been double checked, and we also believe that the other two languages are reasonably clean. Bulgarian, al- though present on the CD, is unfortunately unusable since it has not been manually annotated; for English, see above. data statistics, where the number of unique tags seen in a relatively small collection of about 100k tokens is high - from 401 (Hungarian) to 1033 (Slovene); com- pare that to English with only 139 tags. However, it is interesting to see that the average per-token ambi- guity is much more narrowly distributed, and in fact English ranks 3rd (after Hungarian and Slovene), Czech being the last with almost every other token ambiguous on average. This ambiguity does not cor- respond with the results obtained: Slovene, being the second least ambiguous, is the second most dif- ficult to tag. Only Czech behaves consistently by tailing the pack in both cases. 5.2 Comparison to Previous Results Any comparison is necessarily difficult due to differ- ent evaluation methodologies, even within the \"best- only\", accuracy-based reporting. Nevertheless, we will try. For Romanian, Tufis in his recent work (Tufis, 1999) reports 98.5% accuracy (i.e. 1.5% error rate) on Romanian, using the classifier combination ap- proach advocated by e.g. (Brill and Wu, 1998). His Table 10: Error rate comparison on reduced training data, automatic dictionary Training data size 2000 5000 20000 Language ME Exp ME Exp ME Exp English 26.03% 29.36% 17.70% 23.47% 9.61% 14.46% Czech 50.77% 42.95% 41.95% 36.54% 28.16% 27.08% Estonian 51.08% 42.10% 40.09% 32.44% 25.50% 21.51% Hungarian 41.12% 32.21% 30.68% 23.94% 17.27% 13.92% Romanian 42.88% 35.02% 30.07% 25.06% 16.67% 15.16% Slovene 49.46% 39.58% 39.34% 33.12% 27.77% 24.50% results are well above the 3.29% error rate achieved here (with even a larger tagset of 1391 vs. 486 here), but the paper does not say how this number has been computed (training data size, the all-token/words- only question) thus making any conclusions difficult to make. He also argues that his method is language independent but no results are mentioned for other languages. For Czech, previous work achieved similar results (6.20% on newspaper text using the all-tokens-based error rate computation, on 160,000 training tokens; vs. 7.04% here on approx. half that amount of train- ing data; same handling of unknown words). This is in line with the expectations, since the same method- ology (tagging as well as evaluation) has been used, except the features used in that work were specifi- cally tuned to Czech. The most detailed account of Slovene (Erjavec et al., 1999) reports various results, which might not be directly comparable because it is unclear whether they use the all-tokens-based or words-only compu- tation of the error rate. They report 6.421% error rate on the full tagset on known words, and 13.583% on all words (tokens?) including unknown words (the exponential tagger we used achieved 13.82% on all tokens, 16.26% on words only). They use almost the same data (Orwell's 1984, but leaving out the Appendices)<sup>18</sup>. They also report that the original Czech-specific exponential tagger used as a basis for the work reported here achieved 7.28% error rate on Slovene on full tags on the same data, which means that by the changes to the exponential tagger aimed at its language independence we introduced in Sec- tion 3, we have not achieved any improvement (on Slovene) of the exp. tagger (the error rate stayed at 7.26% – using all-tokens-based evaluation numbers, dictionary available; but the data was not exactly the same, presumably). 5.3 Dictionary vs. Training Data This is, according to our opinion, the most interest- ing result of the experiments described so far. As <sup>18</sup>Their tag count is lower (1021) than here (1033), but that's not really relevant. They do not report the average ambiguity or a similar measure. already Table 2 clearly suggests, even the baseline tagging results obtained with the help of an indepen- dent dictionary are comparable (if not better) than the fully-trained tagger on 100k words, but without the dictionary information. The situation is even clearer when comparing the POS-only results: here the \"independent\" dictionary results are better by far, with almost no training data needed. Looking at the characteristics of the languages, it is apparent that the inflections cause the problem: the coverage of a previously unseen text is inferior to the usual coverage of English or another analytical language. Therefore, unless we can come up with a really clever way of learning rules for dealing with previously unseen words, it is clearly strongly prefer- able to work on a morphological dictionary<sup>19</sup>, rather than to try to annotate more data. 6 Future Work We would like to compare more taggers using still other methodologies, especially the MBT tagger, which achieved the best results on Slovene but which was not available to us at the time of writing this paper. Obviously, we would also like to use the clas- sifier combination method on them, to confirm the really surprisingly good results on Romanian and test it on the other languages as well. We would also like to enrich the best taggers avail- able today (such as the Maximum Entropy tagger) by using the dictionary information available and compare the results with the exponential feature- based tagger we have been using in the experiments here. For Czech and Slovene, the results are still far be- low what one would like to see (in absolute terms). It seems that the key lies in the initial feature set defi- nition - including statistical tagset clustering, which might potentially lead to more reliable estimates of certain parameters while using still the same size of training data. <sup>19</sup>Not necessarily manually - apparently, even a partially supervised method would be of tremendous help. 7 Acknowledgements The author wishes to thank many Multext-East participants for their efforts to improve the original data, especially to Niki Petkevič, Tomaz Erjavec, Heiki-Jaan Kaalep and Gábor Prószéky, and for providing the final versions of the annotated data for the experiments. Any errors and mistakes are solely to be blamed on the author, not the annotators, of course. References Eric Brill and Jun Wu. 1998. Classifier combination for improved lexical disambiguation. In Proceedings of ACL/COLING'98, pages 191-195, Montreal, Canada. ACL/ICCL. Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21:543-565. Jean-Pierre Chanod. 1997. Current developments for Central & Eastern European languages. In Proceedings of EU Project meeting TELRI I, Romania. Kenneth W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143, Austin, Texas. ACL. Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. 1996. MBT: A memory-based part of speech tagger generator. In Proceedings of WVLC 4, pages 14-27. ACL. Tomaz Erjavec, Saso Dzeroski, and Jakub Zavrel. 1999. Morphosyntactic Tagging of Slovene: Evaluating PoS Taggers and Tagsets. Technical Report IJS-DP 8018, Dept. for Intelligent Systems, Jozef Stefan Institute, Ljubljana, Slovenia, April 2nd. Jan Hajič and Barbora Hladká. 1997. Tagging of inflective languages: a comparison. In Proceedings of ANLP'97, pages 136-143, Washington, DC. ACL. Jan Hajič and Barbora Hladká. 1998. Tagging inflective languages: Prediction of morphological categories for a rich, structured tagset. In Proceedings of ACL/COLING'98, pages 483-490, Montreal, Canada. ACL/ICCL. Andrew W. Mackie Julian Benello and James A. Anderson. 1989. Syntactic category disambiguation with neural networks. Computer Speech and Language, 3:203-217. Bernard Merialdo. 1992. Tagging text with a probabilistic model. Computational Linguistics, 20(2):155-171. Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of EMNLP 1, pages 133-142. ACL. Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, pages 44-49, Manchester, England. Dan Tufis. 1999. Tiered tagging and combined language models classifiers. In Proceedings of Text, Speech and Dialogue '99, Mariánské Lázně, Czech Republic, Sept. 15-18. Jean Véronis. 1996a. Multext-East (Copernicus 106). http://www.lpl.univ- aix.fr/projects/multext-east. Jean Véronis. 1996b. Multext-East language- specific resources (Copernicus 106). http://www.lpl.univ-aix.fr/projects/multext- east/MTE2.html."
  },
  {
    "title": "Do the Benefits of Joint Models for Relation Extraction Extend to Document-level Tasks?",
    "abstract": "Two distinct approaches have been proposed for relational triple extraction - pipeline and joint. Joint models, which capture interactions across triples, are the more recent development, and have been shown to outperform pipeline models for sentence-level extraction tasks. Document-level extraction is a more challenging setting where interactions across triples can be long-range, and individual triples can also span across sentences. Joint models have not been applied for document-level tasks so far. In this paper, we benchmark state-of-the-art pipeline and joint extraction models on sentence-level as well as document-level datasets. Our experiments show that while joint models outperform pipeline models significantly for sentence-level extraction, their performance drops sharply below that of pipeline models for the document-level dataset.",
    "content": "1 Introduction Relation extraction is a crucial NLP task for con- structing and enriching knowledge bases. Tradi- tional pipeline approaches (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2014, 2015; Nayak and Ng, 2019; Jat et al., 2017) first identify entities followed by relation identification one en- tity pair at a time. In contrast, more recent joint ap- proaches (Zeng et al., 2018; Takanobu et al., 2019; Nayak and Ng, 2020; Wei et al., 2020; Wang et al., 2020b; Zhong and Chen, 2021; Zheng et al., 2021; Li et al., 2021; Wei et al., 2020; Yan et al., 2021; Shang et al., 2022) not only identify entities and re- lations for the same triple together but also extract all relational triples together. Thus, these recent approaches are better suited for capturing complex interactions. Joint models for relation extraction outper- form traditional pipeline models for sentence-level datasets such as NYT (Riedel et al., 2010). A more natural and complex setting for relation extraction is at the document-level. In the document-level task, relational triples may also span across sen- tences. Further, there may be long range interac- tions between different triples across sentences. As a result, the search space for joint models blows up with document size. So far, research for document- level datasets such as DocRED (Yao et al., 2019) has used pipeline approaches and avoided the joint approach. In this paper, we investigate if the benefits of the joint approach extrapolate from sentence- level to document-level tasks. We benchmark 5 SOTA joint models and 3 SOTA pipeline models on sentence-level (NYT) and document-level (Do- CRED) datasets. We observe that the benefits of the SOTA joint models do not extend to document- level tasks. While performance of both classes of models drop sharply, joint models fall signifi- cantly below that of pipeline models. We perform extensive analysis to identify the short-comings of the two classes of models highlighting areas of improvement. 2 Relation Extraction Approaches Pipeline RE approaches solve the RE task in two sequential steps. In Step 1, they use an NER model to identify the entities and entity mentions in the input text. In Step 2, they take the predicted enti- ties and entity mentions as input, and predict all possible relations from a pre-defined relation set between pairs of entities. We use PL-Marker (Ye et al., 2022) as the NER module and KD-DocRE (Tan et al., 2022), SSAN (Xu et al., 2021a), and experiment with SAIS (Xiao et al., 2022) as rela- tion classification models for our experiments, and train these for specific datasets. Following standard practice, we use gold standard entity mentions for training and validation of the relation classification models, while for inference of the test instances, we naturally use the predicted entity mentions as in- put. Note that in both steps, these models perform independent classification for each entity mention Original Kiato (, Sidirodromikos Stathmos Kiatou ) is a railway station in Kiato in the northern Peloponnese, Greece The station is located a kilometre west of the town, near the Greek National Road 8A (Patras - Corinth highway). It opened on 9 July 2007 as the western terminus of the line from Athens Airport. Initially the station served as an exchange point for passengers to Patras on the old metre gauge SPAP line to Patras, but all traffic was suspended indefinitely in December 2010 for cost reasons. The nearby old Kiato station was also closed. Passengers for Patras must now change to bus services at Kiato. The station is served by one train per hour to Piraeus . Processed Kiato (, Kiato) is a railway station in Kiato in the northern Peloponnese, Greece. The station is located a kilometre west of the town, near the Greek National Road 8A (Peloponnese - Corinth highway). It opened on 9 July 2007 as the western terminus of the line from Athens Airport. Initially the station served as an exchange point for passengers to Peloponnese on the old metre gauge SPAP line to Peloponnese, but all traffic was suspended indefinitely in December 2010 for cost reasons. The nearby old Kiato station was also closed. Passengers for Peloponnese must now change to bus services at Kiato. The station is served by one train per hour to Peloponnese. Table 1: A sample document from DocRED and its processed version. The different mentions of the entity 'Kiato' and 'Peloponnese' are marked with blue and red respectively. In the processed text, different mentions are normalized with the first mention of these two entities which are 'Kiato' and 'Peloponnese'. Dataset # Relations Train Validation Test #Context # Triples #Context #Triples #Context #Triples NYT 24 56,196 94,222 5,000 8,489 5,000 8,616 DocRED 96 2,572 20,233 284 2,187 924 7,337 Table 2: Statistics of the NYT24 and DocRED datasets. Context refers to a sentence or document. and relation. Joint RE approaches identify the entities and relations in a relational triple in an end-to-end fash- ion. Further, they consider the entire input text (sentence or document) and output a set of rela- tional triples together, thus, capturing complex in- teractions across triples in theory. The flip side, naturally, is that they need to explore a signif- icantly larger space of candidates, which grows combinatorially with the length of the input text. We use 5 SOTA models for experiments. PtrNet (Nayak and Ng, 2020) and REBEL (Huguet Cabot and Navigli, 2021) use the Seq2Seq approach. Ptr- Net generates the index position of entities in text whereas REBEL generates the tokens for the triples. OneRel (Shang et al., 2022) uses a table-based tag- ging approach. The tagging approaches of BiRTE (Ren et al., 2022) and GRTE (Ren et al., 2021) have a separate entity extraction process in their end-to-end modeling. We train these models in an end-to-end fashion as described in the respective papers. 3 Extraction Settings and Datasets The original and simpler setting for relation ex- traction is sentence-level. This setting consists of individual sentences containing one or more rela- tions as context. NYT (Riedel et al., 2010) is a large-scale and popular benchmark for sentence- level RE, and we use this dataset as it is for our sentence-level experiments. This setting, however, is restrictive since a large fraction of relations in natural text spans across multiple sentences. This is captured in the document-level relation extraction setting. Here, relational triples may be intra-sentence or inter- sentence, meaning that the head and tail entities of the relational triple can span across multiple sentences and require reasoning across multiple sentences to identify them. The task is to predict all these relations given an entire document as con- text. DocRED (Yao et al., 2019) is a benchmark document-level dataset that we use for our experi- ments. Contexts in DocRED (avg. number of tokens around 197) are much longer than in NYT (avg. number of tokens around 37). However, training data size is much larger for NYT. Since the rela- tion labels of the DocRED test set are not released, we use the original validation set as test set and split the training data for training and validation. DocRED has mostly been used for pipeline mod- els. We needed additional processing to make it tractable for joint models. We remove the doc- uments with overlapping entity mentions in the training and validation set. We get 2,856 docu- ments from the training set and 924 documents from the validation set. Then, we replace all entity mentions with the first occurring entity mention for each entity so that co-reference resolution is not required. We include an example of document processing for DocRED in Table 1. The details of the dataset splits of the NYT and DocRED for our experiments are included in Table 21. Evaluation Metric: We use 'strict' criteria for evaluation. We consider an extracted relational triple as correct only if two entities and relation exactly match with a ground truth triple. We report triple level precision, recall and F1 scores for the models. Parameter Settings: We use BERTBASE (cased) (Devlin et al., 2019) for document encod- ing for all the models except REBEL for which we have used BARTBASE (Lewis et al., 2019). We used NVIDIA Tesla V100 32GB GPU to train the models. We used other hyper-parameters as per provided in their respective papers. 4 Results and Discussion Through our experiment, we try to find out the answers to following research questions (RQ). RQ1: How do the two classes of models perform at sentence and documents scales? End-to-end performance of the joint models and the pipeline models on the NYT and DocRED datasets is shown in Table 3. On the sentence- level NYT dataset, both joint models and pipeline models achieve close to 0.90 F1 score. But, on Do- CRED, we see a huge drop in the F1 score for both categories. The pipeline models score below 0.60 whereas among the joint models GRTE and BiRTE perform around 0.45, the others drop to 0.20 or below. One reason for the drop for DocRED is the smaller training data size. But, it does not explain the gap of 10% F1 score between the pipeline and joint models when they performed almost at par for NYT. This suggests that joint models struggle with longer context and cross-sentence relations of documents. We investigate this in more detail next. RQ2: Are some joint models better than others at document scale? Out of the 5 joint models, we see significantly higher drop in F1 score for OneRel and PtrNet than REBEL, GRTE, and BiRTE models. Given a document with L tokens and K predefined rela- tions, OneRel maintains a three-dimensional matrix MLXKXL and assigns tags for all possible triples. Our processed DocRED dataset is available at https://github.com/pratiksaini4/nyt-docred-joint-pipeline- comparison 45 When context length grows as in documents, M has many more negative tags and very few positive tags, which seems to affect OneRel performance. BIRTE and GRTE, on the other hand, extract the en- tities first separately and then classify the relations. While this is done in an end-to-end fashion, it is still similar to pipeline approach. This may be the reason for the smaller performance drop compared to pipelines models on DocRED. PtrNet and REBEL are Seq2Seq models which use a decoder to extract the triples, so they possi- bly need more training data to learn from longer document contexts. Additionally, PtrNet extracts index positions for the entities. Since an entity may appear more than once in a document, we mark the first occurring index of the entity-mention to train this model. This very likely contributes to its poorer performance. On the other hand, REBEL outputs entities as text, and does not have this train- ing issue. RQ3: How different are performances for intra vs inter-sentence extraction? The fundamental difference between NYT and DocRED is that NYT contains only intra-sentence triples, whereas DocRED contains both intra and inter-sentence (cross-sentence) triples. In Table 5, we first show intra vs inter sentence relational triple distribution for the gold and model predictions on the DocRED dataset. Pipeline models have nearly the same distribution for gold and prediction. But, joint models are skewed towards intra-sentence re- lations. This suggests that joint models are very good at extracting intra-sentence triples but they struggle with inter-sentence triples. This is why joint models perform very well on the NYT dataset and fail to do so on DocRED. In Table 4, we have reported the intra-sentence vs inter-sentence relations performance of the top- performing models on the DocRED test dataset. We see that all the models perform way better at intra-sentence extraction as compared to inter- sentence extraction, it demonstrates that inter- sentence extraction is significantly harder. We also see that pipeline models achieve around 10% higher F1 scores than the joint models for both intra and inter categories on DocRED. This shows that even for the familiar intra-sentence setting joint models face more difficulties compared to pipeline models when encountered with longer context and smaller training volume. Lastly, we investigate the impact of the distance --- Joint Models GRTE PtrNet Pipeline Models SSAN SAIS NYT24 DocRED Model P R F1 P R F1 OneRel 0.926 0.918 0.922 0.513 0.130 0.208 BIRTE 0.914 0.920 0.917 0.522 0.402 0.454 0.929 0.924 0.926 0.586 0.373 0.456 0.898 0.894 0.896 0.222 0.145 0.175 Rebel 0.881 0.885 0.883 0.466 0.356 0.404 KD-DocRE 0.895 0.910 0.902 0.620 0.556 0.586 0.781 0.798 0.789 0.576 0.529 0.552 0.864 0.879 0.872 0.640 0.545 0.589 Table 3: Performance of the SOTA models on end-to-end relation extraction on NYT24 and DocRED datasets. Joint Models Pipeline Models Model Intra Inter P R F1 P R F1 0.600 0.425 0.497 0.420 0.366 0.391 0.677 0.407 0.508 0.460 0.320 0.378 BIRTE GRTE KD-DocRE 0.666 0.601 0.631 0.545 0.485 0.513 SAIS 0.697 0.594 0.641 0.548 0.467 0.504 Table 4: Performance of top performing SOTA models on Intra vs Inter relational triples on DocRED dataset. Predict Model Intra % Inter % Gold 61 39 BIRTE 56 44 Joint Models GRTE 58 42 KD-DOCRE 62 38 Pipeline Models SAIS 62 38 Table 5: Intra vs Inter relational triple distribution of SOTA models predictions. We include top two models from the joint and pipeline class for this analysis. Pipeline Models Joint Models # Hops KD-DocRE SAIS BIRTE GRTE 1 0.50 0.45 0.38 0.33 2 0.44 0.44 0.34 0.31 3 0.48 0.49 0.36 0.31 4 0.48 0.49 0.37 0.31 5 0.51 0.54 0.39 0.32 6 0.42 0.43 0.29 0.32 Table 6: Recall score of Pipeline and Joint SOTA mod- els on inter-sentence relations with respect to distance between subject and object entities. between subject and object mentions in the context on the performance of inter-sentence relations. In Table 6, we record recall of SOTA models on inter- sentence relations for different subject-object hop distances. Hop distance k refers to the minimum sentence-level distance between the subject and object entity of a triple within the document being k. Again, we see that pipeline models outperform joint models by ~ 12% for all hop distances, and 46 not just for longer ones. RQ4: How is performance affected by training data size? Next, we analyze how training volume affects performance for the two model classes for the sim- pler intra-sentence extraction task. Note that NYT contains such relations exclusively. Since DocRED has both categories, we prepare DocRED-Intra in- cluding only intra-sentence triples and the corre- sponding sentences. The size of these datasets are significantly different. DocRED-Intra has only ~ 6.5K training instances compared to 94K for NYT. We train all the models with these intra- sentence triples and record their performance for DocRED-Intra in Table 7. Corresponding NYT performance is in Table 3. We observe a big gap of ~ 42 -50% for joint models and ~ 33% for pipeline models in the performance between NYT and DocRED-Intra. This is due to the smaller train- ing volume associated with a larger number of re- lations in DocRED. The notable disparity between pipeline and joint models in the case of DocRED- Intra demonstrates that joint models are not as effec- tive at generalization compared to pipeline models, particularly when working with limited training data volumes and longer contexts. RQ5: How different are entity extraction per- formances at sentence and document scales? Finally, we aim to analyze if the huge gap in the performance of pipeline and joint models on Model P R F1 Joint Models BIRTE 0.527 0.462 0.492 GRTE 0.544 0.346 0.423 Pipeline Models KD-DocRE 0.524 0.619 0.567 SAIS 0.485 0.610 0.540 Table 7: Performance of top performing SOTA models on DocRED-Intra dataset. DocRED is affected by their performance on NER subtask of relation extraction. In Table 8, we in- clude the performance of these models on the en- tity extraction task. Pipeline models have a sepa- rate NER model. The performance of this model - PL-Marker - on NER task is similar to that of the BIRTE and GRTE models for NYT dataset. But, for DocRED, BIRTE and GRTE perform much worse than PL-Marker, the drop in F1 score being around 25%. This, in turn, hurts their performance on the relational triple extraction. Though train- ing data volume for DocRED is smaller, note that the PL-Marker model is trained on the DocRED dataset itself as are the BiRTE/GRTE models. This shows that, aside from overall extraction perfor- mance, joint models struggle with the NER subtask as well when training data is limited. This suggests that a separate NER model may be more useful in such settings. Dataset Model P R F1 NYT24 PL-Marker 0.948 0.955 0.952 BIRTE 0.955 0.954 0.954 GRTE 0.958 0.956 0.957 DocRED PL-Marker 0.942 0.934 0.938 BIRTE 0.73 0.647 0.686 GRTE 0.757 0.576 0.654 Table 8: Performance of PL-Marker, BiRTE and GRTE on the NER task for NYT24 and DocRED dataset. 5 Conclusion While joint models for relational triple extraction have been shown to outperform pipeline models for sentence-level extraction settings, in this paper we have demonstrated, with extensive experimentation, that these benefits do not extend to the more real- istic and natural document-level extraction setting, which entails longer contexts and cross-sentence re- lations. Experimenting with 5 SOTA joint models and 3 SOTA pipeline models, we have shown that while performance of both classes of models drops significantly in the more complex document setting, joint models suffer significantly more with longer context, inter-sentence relations and limited train- ing data for the overall task as well as for subtasks such as NER. This aids in establishing a research agenda for joint models to extend the promised benefits of joint entity identification, relation classi- fication, and joint extraction of all triples from the context. This pertains to the more challenging yet natural and crucial setting for relation extraction. 6 Limitations The major limitation of this work is that we could only analyze 3 pipeline models and 5 joint models. Recently, many models have been proposed for this task both in pipeline and joint class. Out of these, we chose different kinds of SOTA models to cover the different design choices made by these mod- els. We chose PtrNet (Nayak and Ng, 2020) and REBEL (Huguet Cabot and Navigli, 2021) as they used Seq2Seq model for this task. OneRel (Shang et al., 2022) used table-filling method whereas BiRTE (Ren et al., 2022) and GRTE (Ren et al., 2021) used sequentially extracting entities and re- lations in their end-to-end model. References Pere-Lluís Huguet Cabot and Roberto Navigli. 2021. Rebel: Relation extraction by end-to-end language generation. In EMNLP. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In NAACL. Zhijiang Guo, Yan Zhang, and Wei Lu. 2019. Atten- tion guided graph convolutional networks for relation extraction. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 241-251, Florence, Italy. Association for Com- putational Linguistics. Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge- based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. Kevin Huang, Peng Qi, Guangtao Wang, Tengyu Ma, and Jing Huang. 2021. Entity and evidence guided document-level relation extraction. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 307-315, Online. As- sociation for Computational Linguistics. Pere-Lluís Huguet Cabot and Roberto Navigli. 2021. REBEL: Relation extraction by end-to-end language 47 generation. In Findings of the Association for Com- putational Linguistics: EMNLP 2021, pages 2370- 2381, Punta Cana, Dominican Republic. Association for Computational Linguistics. Sharmistha Jat, Siddhesh Khandelwal, and Partha Taluk- dar. 2017. Improving distantly supervised relation ex- traction using word and entity based attention. In Pro- ceedings of the 6th Workshop on Automated Knowl- edge Base Construction. Robin Jia, Cliff Wong, and Hoifung Poon. 2019. Document-level n-ary relation extraction with multi- scale representation learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers), pages 3693–3704, Minneapolis, Min- nesota. Association for Computational Linguistics. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdel rahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for nat- ural language generation, translation, and compre- hension. In Annual Meeting of the Association for Computational Linguistics. Xianming Li, Xiaotian Luo, Cheng Jie Dong, Daichuan Yang, Beidi Luan, and Zhen He. 2021. TDEER: An efficient translating decoding schema for joint extraction of entities and relations. In EMNLP. Makoto Miwa and Mohit Bansal. 2016. End-to-end re- lation extraction using LSTMs on sequences and tree structures. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics. Guoshun Nan, Zhijiang Guo, Ivan Sekulic, and Wei Lu. 2020. Reasoning with latent structure refinement for document-level relation extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1546–1557, Online. Association for Computational Linguistics. Tapas Nayak and Hwee Tou Ng. 2019. Effective at- tention modeling for neural relation extraction. In Proceedings of the Conference on Computational Natural Language Learning. Tapas Nayak and Hwee Tou Ng. 2020. Effective mod- eling of encoder-decoder architecture for joint entity and relation extraction. In Proceedings of The Thirty- Fourth AAAI Conference on Artificial Intelligence. Xutan Peng, Chenghua Lin, and Mark Stevenson. 2021. Cross-lingual word embedding refinement by l₁ norm optimisation. In Proceedings of the 2021 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, pages 2690–2701, Online. As- sociation for Computational Linguistics. Chris Quirk and Hoifung Poon. 2017. Distant super- vision for relation extraction beyond the sentence boundary. In Proceedings of the 15th Conference of the European Chapter of the Association for Compu- tational Linguistics: Volume 1, Long Papers, pages 1171-1182, Valencia, Spain. Association for Compu- tational Linguistics. Feiliang Ren, Longhui Zhang, Shujuan Yin, Xiaofeng Zhao, Shilei Liu, Bochao Li, and Yaduo Liu. 2021. A novel global feature-oriented relational triple ex- traction model based on table filling. In EMNLP. Feiliang Ren, Longhui Zhang, Xiaofeng Zhao, Shujuan Yin, Shilei Liu, and Bochao Li. 2022. A simple but effective bidirectional framework for relational triple extraction. Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining. Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions with- out labeled text. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases. Y. Shang, Heyan Huang, and Xian-Ling Mao. 2022. OneRel: Joint entity and relation extraction with one module in one step. In AAAI. Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. N-ary relation extraction using graph- state LSTM. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, pages 2226-2235, Brussels, Belgium. Associa- tion for Computational Linguistics. Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, Xian- grong Zeng, and Shengping Liu. 2020. Joint entity and relation extraction with set prediction networks. CoRR, abs/2011.01675. Ryuichi Takanobu, Tianyang Zhang, Jiexi Liu, and Min- lie Huang. 2019. A hierarchical framework for rela- tion extraction with reinforcement learning. In Pro- ceedings of The Thirty-Third AAAI Conference on Artificial Intelligence. Qingyu Tan, Ruidan He, Lidong Bing, and Hwee Tou Ng. 2022. Document-level relation extraction with adaptive focal loss and knowledge distillation. In Findings of the Association for Computational Lin- guistics: ACL 2022, pages 1672–1681, Dublin, Ire- land. Association for Computational Linguistics. Hengzhu Tang, Yanan Cao, Zhenyu Zhang, Jiangxia Cao, Fang Fang, Shi Wang, and Pengfei Yin. 2020. HIN: hierarchical inference network for document- level relation extraction. CoRR, abs/2003.12754. Difeng Wang, Wei Hu, Ermei Cao, and Weijian Sun. 2020a. Global-to-local neural networks for document-level relation extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3711– 3721, Online. Association for Computational Lin- guistics. Hong Wang, Christfried Focke, Rob Sylvester, Nilesh Mishra, and William Yang Wang. 2019. Fine- tune bert for docred with two-step process. CoRR, abs/1909.11898. Yijun Wang, Changzhi Sun, Yuanbin Wu, Hao Zhou, Lei Li, and Junchi Yan. 2021. UniRE: A unified label space for entity relation extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 220-231, Online. Association for Computational Linguistics. Yucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu, and Limin Sun. 2020b. TPLinker: Single-stage joint extraction of entities and relations through token pair linking. In COL- ING. Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, and Yi Chang. 2020. A novel cascade binary tagging framework for relational triple extraction. In ACL. Yuxin Xiao, Zecheng Zhang, Yuning Mao, Carl Yang, and Jiawei Han. 2022. SAIS: Supervising and aug- menting intermediate steps for document-level re- lation extraction. In Proceedings of the 2022 Con- ference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies, pages 2395-2409, Seattle, United States. Association for Computational Lin- guistics. Yiqing Xie, Jiaming Shen, Sha Li, Yuning Mao, and Jiawei Han. 2022. Eider: Empowering document- level relation extraction with efficient evidence ex- traction and inference-stage fusion. In Findings of the Association for Computational Linguistics: ACL 2022, pages 257–268, Dublin, Ireland. Association for Computational Linguistics. Benfeng Xu, Quan Wang, Yajuan Lyu, Yong Zhu, and Zhendong Mao. 2021a. Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction. Proceedings of the AAAI Conference on Artificial Intelligence. Wang Xu, Kehai Chen, and Tiejun Zhao. 2021b. Dis- criminative reasoning for document-level relation ex- traction. In Findings of the Association for Com- putational Linguistics: ACL-IJCNLP 2021, pages 1653-1663, Online. Association for Computational Linguistics. Wang Xu, Kehai Chen, and Tiejun Zhao. 2021c. Document-level relation extraction with reconstruc- tion. Proceedings of the AAAI Conference on Artifi- cial Intelligence, 35(16):14167-14175. Zhiheng Yan, Chong Zhang, Jinlan Fu, Qi Zhang, and Zhongyu Wei. 2021. A partition filter network for joint entity and relation extraction. In EMNLP. Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. 2019. DocRED: A large-scale document-level relation extraction dataset. In Pro- ceedings of ACL 2019. Deming Ye, Yankai Lin, Peng Li, and Maosong Sun. 2022. Packed levitated marker for entity and relation extraction. In Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4904–4917, Dublin, Ireland. Association for Computational Linguistics. Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceed- ings of the 2015 Conference on Empirical Methods in Natural Language Processing. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via con- volutional deep neural network. In Proceedings of the 25th International Conference on Computational Linguistics. Shuang Zeng, Yuting Wu, and Baobao Chang. 2021. SIRE: Separate intra- and inter-sentential reasoning for document-level relation extraction. In Findings of the Association for Computational Linguistics: ACL- IJCNLP 2021, pages 524-534, Online. Association for Computational Linguistics. Shuang Zeng, Runxin Xu, Baobao Chang, and Lei Li. 2020. Double graph based reasoning for document- level relation extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1630-1640, On- line. Association for Computational Linguistics. Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu, and Jun Zhao. 2018. Extracting relational facts by an end-to-end neural model with copy mechanism. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Meishan Zhang, Yue Zhang, and Guohong Fu. 2017. End-to-end neural relation extraction with global op- timization. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730-1740, Copenhagen, Denmark. Associa- tion for Computational Linguistics. Heng Zheng, Rui Wen, Xi Chen, Yifan Yang, Yunyan Zhang, Ziheng Zhang, Ningyu Zhang, Bin Qin, Ming Xu, and Yefeng Zheng. 2021. PRGC: Potential rela- tion and global correspondence based joint relational triple extraction. In ACL. Zexuan Zhong and Danqi Chen. 2021. A frustratingly easy approach for entity and relation extraction. In North American Chapter of the Association for Com- putational Linguistics. Wenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing Huang. 2021. Document-level relation extraction with adaptive thresholding and localized context pool- ing. Proceedings of the AAAI Conference on Artifi- cial Intelligence, 35(16):14612-14620. A Appendix A.1 Details of Joint Models A.1.1 PtrNet (Nayak and Ng, 2020) PtrNet utilizes a seq2seq approach along with the pointer network-based decoding for jointly extract- ing entities and relations. Each triple contains the start and end index of the subject and object entities, along with the relation class label. Their decoder has two-pointer networks to identify the start and end index of the two entities and a classifier to iden- tify the relation between the two entities. Decoder extracts a relational triple at each time steps and continue the process till there is no triple to extract. To ensure parity with other SOTA models, their BILSTM encoder is replaced with BERT encoder. A.1.2 REBEL (Huguet Cabot and Navigli, 2021) REBEL utilizes an auto-regressive seq2seq model that streamlines the process of relation extraction by presenting triples as a sequence of text and uses special separator tokens, as markers, to achieve the linearization.. WordDecoder model of (Nayak and Ng, 2020) uses a similar approach using LSTMs whereas REBEL is a BART-based Seq2Seq model that utilizes the advantages of transformer model and pre-training. REBEL uses a more compact representation for the relational triples over Word- Decoder model. A.1.3 GRTE (Ren et al., 2021) GRTE utilizes individual tables for each relation. The cell entries of a table denote the presence or absence of relation between the associated token pairs. It uses enhanced table-filling methods by introducing two kinds of global features. The first global feature is for the association of entity pairs and the second is for relations. Firstly, a table feature is generated for each relation, which is then consolidated with the features of all relations. This integration produces two global features related to the subject and object, respectively. These two global features are refined multiple times. Finally, the filled tables are utilized to extract all relevant triples. 50 A.1.4 OneRel (Shang et al., 2022) OneRel frames the joint entity and relation ex- traction task as a fine-grained triple classification problem. It uses a three-dimensional matrix with relation-specific horns tagging strategy. The rows in this matrix refer to the head entity tokens and the columns in this matrix refer to the tail entity tokens from the original text. The scoring-based classi- fier checks the accuracy of the decoded relational triples. It discards the triples with low confidence. A.1.5 BiRTE (Ren et al., 2022) In this paper, a bidirectional tagging approach with multiple stages is utilized. BiRTE first discovers the subject entities and then identifies the object entities based on the subject entities. It then does this in the reverse direction, first discovering the object entities and identifying subject entities for the object entities. The final stage involves the relation classification of subject-object pairs. They perform these tasks jointly in a single model. A.2 Details of Pipeline Models A.2.1 SSAN (Xu et al., 2021a) This paper frames the structure of entities as de- fined by the specific dependencies between the en- tity mention pairs in a document. The proposed approach, SSAN, integrates these structural depen- dencies with the self-attention mechanism at the encoding stage. To achieve this, two transforma- tion modules are included in each self-attention building block, which generates attentive biases to regulate the attention flow adaptively. This ap- proach achieved SOTA performance on the Do- CRED dataset. A.2.2 KD-DocRE (Tan et al., 2022) This paper suggests a semi-supervised framework for extracting document-level relations. To achieve this, they exploit the inter-dependency among the relational triples through the implementation of an axial attention module. This approach leads to en- hanced performance when dealing with two-hop relations. In addition to this, an adaptive focal loss is proposed as a means of resolving the is- sue of imbalanced label distribution for long-tail classes. Finally, to account for the difference be- tween human-annotated data and distantly super- vised data, knowledge distillation is utilized. Their experiments on the DocRED show the effectiveness of the approach. A.2.3 SAIS (Xiao et al., 2022) The objective of this paper is to train the model to identify relevant contexts and entity types by using the Supervising and Augmenting Intermedi- ate Steps (SAIS) approach for relation extraction. The SAIS framework proposed in this paper results in the extraction of relations that are of superior quality, owing to its more efficient supervision. By utilizing evidence-based data augmentation and en- semble inference, SAIS also improves the accuracy of the supporting evidence retrieval process while minimizing the computational cost. A.3 Details of NER Model A.3.1 PL-Marker (Ye et al., 2022) In their approach for span representation, PL- Marker strategically uses levitated markers to con- sider the interrelation between pairs of spans. They propose a packing strategy that factors in neigh- bouring spans to improve the modeling of entity boundary information. Additionally, they utilize a subject-oriented packing approach, which groups each subject with its objects to effectively model the interrelations between the same subject span pairs. A.4 Related Work Sentence-level Relation Extraction: Early ap- proaches for relation extraction use two steps pipeline approach. The first step, Named Entity Recognition (NER), extracts entities from the text. The second step, Relation Classification (RC), iden- tifies pairwise relations between the extracted enti- ties (Zeng et al., 2014, 2015; Jat et al., 2017; Nayak and Ng, 2019). Pipeline methods fail to capture the implicit correlation between the two sub-tasks. They suffer from error propagation between the two stages. They cannot model the interaction among the relational triples. To mitigate the drawbacks of pipeline ap- proaches, recent works have focused on Joint en- tities and relation extraction. Joint approaches re- ferred to as End-to-End Relation Extraction (RE) accomplish both tasks jointly. Training simulta- neously on both NER and RC tasks allows for capturing more complex interactions among the multiple relational triples present in the context. Miwa and Bansal (2016) proposed a model that trained the NER and RC module in a single model. Nayak and Ng (2020); Cabot and Navigli (2021) propose seq2seq models for extracting the triples in a sequence. Sui et al. (2020) casts the joint extraction task as a set prediction problem rather than a sequence extraction problem. Zhang et al. (2017); Wang et al. (2020b, 2021); Shang et al. (2022) formulate the NER and RC tasks as table filling problem where each cell of the table repre- sents the interaction between two tokens. Ren et al. (2022); Zheng et al. (2021); Li et al. (2021); Yan et al. (2021); Wei et al. (2020) have separate NER and RC modules in the same model trained in an end-to-end fashion. Document-level Relation Extraction: Re- cently, there has been a shift of interest towards document-level RE (Yao et al., 2019). Document- level relation extraction (DocRE) is known to be a more complex and realistic task compared to the sentence-level counterpart. DocRE typically in- volves large volumes of data, which can be compu- tationally expensive to handle using joint models. Recent work in DocRE has avoided using joint models for this task as joint models are not scal- able for long documents. In DocRE, there can be multiple mentions of an entity with different sur- face forms across the document and the evidence of the relations can spread across multiple sen- tences. In document-level RE, mostly pipeline ap- proaches are proposed, joint extraction approaches are not explored for this task. Earlier works (Peng et al., 2021; Quirk and Poon, 2017; Song et al., 2018; Jia et al., 2019) used dependency graph be- tween the two entities to find the relations. Recent works (Guo et al., 2019; Nan et al., 2020; Wang et al., 2020a; Zeng et al., 2020, 2021; Xu et al., 2021c,b) proposed graph-based approaches that use advanced neural techniques to do multi-hop reason- ing. More recent Transformer-based approaches (Wang et al., 2019; Tang et al., 2020; Huang et al., 2021; Xu et al., 2021a; Zhou et al., 2021; Xie et al., 2022) use pre-trained language models to encode long-range contextual dependencies in the docu- ments. Huang et al. (2021); Xie et al. (2022); Xiao et al. (2022); Tan et al. (2022) use neural classifier to identify the evidences for relations along with re- lation classification for performance improvement. 51"
  },
  {
    "title": "Contextual Spelling Correction Using Latent Semantic Analysis",
    "abstract": "Contextual spelling errors are defined as the use of an incorrect, though valid, word in a particular sentence or context. Traditional spelling checkers flag misspelled words, but they do not typically attempt to identify words that are used incorrectly in a sentence. We explore the use of Latent Semantic Analysis for correcting these incorrectly used words and the results are compared to earlier work based on a Bayesian classifier.",
    "content": "1 Introduction Spelling checkers are now available for all major word processing systems. However, these spelling checkers only catch errors that result in misspelled words. If an error results in a different, but incor- rect word, it will go undetected. For example, quite may easily be mistyped as quiet. Another type of er- ror occurs when a writer simply doesn't know which word of a set of homophones¹ (or near homophones) is the proper one for a particular context. For ex- ample, the usage of affect and effect is commonly confused. Though the cause is different for the two types of errors, we can treat them similarly by examining the contexts in which they appear. Consequently, no effort is made to distinguish between the two er- ror types and both are called contextual spelling er- rors. Kukich (1992a; 1992b) reports that 40% to 45% of observed spelling errors are contextual er- rors. Sets of words which are frequently misused or mistyped for one another are identified as confusion sets. Thus, from our earlier examples, {quiet, quite} and {affect, effect} are two separate confusion sets. In this paper, we introduce Latent Semantic Anal- ysis (LSA) as a method for correcting contextual spelling errors for a given collection of confusion sets. ¹ Homophones are words that sound the same, but are spelled differently. LSA was originally developed as a model for infor- mation retrieval (Dumais et al., 1988; Deerwester et al., 1990), but it has proven useful in other tasks too. Some examples include an expert Expert lo- cator (Streeter and Lochbaum, 1988) and a confer- ence proceedings indexer (Foltz, 1995) which per- forms better than a simple keyword-based index. Recently, LSA has been proposed as a theory of se- mantic learning (Landauer and Dumais, (In press)). Our motivation in using LSA was to test its effec- tiveness at predicting words based on a given sen- tence and to compare it to a Bayesian classifier. LSA makes predictions by building a high-dimensional, \"semantic\" space which is used to compare the sim- ilarity of the words from a confusion set to a given context. The experimental results from LSA predic- tion are then compared to both a baseline predic- tor and a hybrid predictor based on trigrams and a Bayesian classifier. 2 Related Work Latent Semantic Analysis has been applied to the problem of spelling correction previously (Kukich, 1992b). However, this work focused on detect- ing misspelled words, not contextual spelling errors. The approach taken used letter n-grams to build the semantic space. In this work, we use the words di- rectly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of prob- lems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of prob- lems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992). Recently, Golding and Schabes (1996) de- scribed a system, Tribayes, that combines a trigram model of the words' parts of speech with a Bayesian classifier. The trigram component of the system is used to make decisions for those confusion sets that documents D' S 0 terms X = T 0 0 txd txr rxr rxd Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D'. contain words with different parts of speech. The Bayesian component is used to predict the correct word from among same part-of-speech words. Golding and Schabes selected 18 confusion sets from a list of commonly confused words plus a few that represent typographical errors. They trained their system using a random 80% of the Brown cor- pus (Kučera and Francis, 1967). The remaining 20% of the corpus was used to test how well the system performed. We have chosen to use the same 18 con- fusion sets and the Brown corpus in order to compare LSA to Tribayes. 3 Latent Semantic Analysis Latent Semantic Analysis (LSA) was developed at Bellcore for use in information retrieval tasks (for which it is also known as LSI) (Dumais et al., 1988; Deerwester et al., 1990). The premise of the LSA model is that an author begins with some idea or information to be communicated. The selection of particular lexical items in a collection of texts is simply evidence for the underlying ideas or informa- tion being presented. The goal of LSA, then, is to take the \"evidence\" (i.e., words) presented and un- cover the underlying semantics of the text passage. Because many words are polysemous (have multi- ple meanings) and synonymous (have meanings in common with other words), the evidence available in the text tends to be somewhat \"noisy.\" LSA at- tempts to eliminate the noise from the data by first representing the texts in a high-dimensional space and then reducing the dimensionality of the space to only the most important dimensions. This pro- cess is described in more detail in Dumais (1988) or Deerwester (1990), but a brief description is pro- vided here. A collection of texts is represented in matrix for- mat. The rows of the matrix correspond to terms and the columns represent documents. The indi- vidual cell values are based on some function of the term's frequency in the corresponding document and its frequency in the whole collection. The func- tion for selecting cell values will be discussed in sec- tion 4.2. A singular value decomposition (SVD) is performed on this matrix. SVD factors the origi- nal matrix into the product of three matrices. We'll identify these matrices as T, S, and D' (see Figure 1). The T matrix is a representation of the original term vectors as vectors of derived orthogonal factor val- ues. D' is a similar representation for the original document vectors. S is a diagonal matrix² of rank r. It is also called the singular value matrix. The sin- gular values are sorted in decreasing order along the diagonal. They represent a scaling factor for each dimension in the Tand D'matrices. Multiplying T, S, and D'together perfectly repro- duces the original representation of the text collec- tion. Recall, however, that the original representa- tion is expected to be noisy. What we really want is an approximation of the original space that elim- inates the majority of the noise and captures the most important ideas or semantics of the texts. An approximation of the original matrix is created by eliminating some number of the least important singular values in S. They correspond to the least important (and hopefully, most noisy) dimensions in the space. This step leaves a new matrix (So) of rank k.³ A similar reduction is made in T and D by retaining the first k columns of T and the first k rows of D'as depicted in Figure 2. The product of the resulting To, So, and D', matrices is a least squares best fit reconstruction of the original matrix (Eckart and Young, 1939). The reconstructed ma- trix defines a space that represents or predicts the frequency with which each term in the space would appear in a given document or text segment given an infinite sample of semantically similar texts (Lan- 2A diagonal matrix is a square matrix that contains non-zero values only along the diagonal running from the upper left to the lower right. The number of factors k to be retained is generally selected empirically. documents terms X = T<sub>0</sub> ⋅ S<sub>0</sub> D'<sub>0</sub> t x d t x k k x k k x d Figure 2: Results of reducing the T, S and D' matrices produced by SVD to rank k. Recombining the reduced matrices gives X, a least squares best fit reconstruction of the original matrix. dauer and Dumais, (In press)). New text passages can be projected into the space by computing a weighted average of the term vectors which correspond to the words in the new text. In the contextual spelling correction task, we can generate a vector representation for each text passage in which a confusion word appears. The similarity between this text passage vector and the confusion word vectors can be used to predict the most likely word given the context or text in which it will appear. 4 Experimental Method 4.1 Data Separate corpora for training and testing LSA's ability to correct contextual word usage errors were created from the Brown corpus (Kučera and Francis, 1967). The Brown corpus was parsed into individual sentences which are randomly assigned to either a training corpus or a test corpus. Roughly 80% of the original corpus was assigned as the training corpus and the other 20% was reserved as the test corpus. For each confusion set, only those sentences in the training corpus which contained words in the confusion set were extracted for construction of an LSA space. Similarly, the sentences used to test the LSA space's predictions were those extracted from the test corpus which contained words from the confusion set being examined. The details of the space construction and testing method are described below. 4.2 Training Training the system consists of processing the training sentences and constructing an LSA space from them. LSA requires the corpus to be segmented into documents. For a given confusion set, an LSA space is constructed by treating each training sentence as a document. In other words, each training sentence is used as a column in the LSA matrix. Before being processed by LSA, each sentence undergoes the following transformations: context reduction, stemming, bigram creation, and term weighting. Context reduction is a step in which the sentence is reduced in size to the confusion word plus the seven words on either side of the word or up to the sentence boundary. The average sentence length in the corpus is 28 words, so this step has the effect of reducing the size of the data to approximately half the original. Intuitively, the reduction ought to improve performance by disallowing the distantly located words in long sentences to have any influence on the prediction of the confusion word because they usually have little or nothing to do with the selection of the proper word. In practice, however, the reduction we use had little effect on the predictions obtained from the LSA space. We ran some experiments in which we built LSA spaces using the whole sentence as well as other context window sizes. Smaller context sizes didn't seem to contain enough information to produce good predictions. Larger context sizes (up to the size of the entire sentence) produced results which were not significantly different from the results reported here. However, using a smaller context size reduces the total number of unique terms by an average of 13%. Correspondingly, using fewer terms in the initial matrix reduces the average running time and storage space requirements by 17% and 10% respectively. Stemming is the process of reducing each word to its morphological root. The goal is to treat the different morphological variants of a word as the same entity. For example, the words smile, smiled, smiles, smiling, and smilingly (all from the corpus) are reduced to the root smile and treated equally. We tried different stemming algorithms and all improved the predictive performance of LSA. The results presented in this paper are based on Porter's (Porter, 1980) algorithm. Bigram creation is performed for the words that were not removed in the context reduction step. Bigrams are formed between all adjacent pairs of words. The bigrams are treated as additional terms during the LSA space construction process. In other words, the bigrams fill their own row in the LSA ma- trix. Term weighting is an effort to increase the weight or importance of certain terms in the high dimensional space. A local and global weighting is given to each term in each sentence. The local weight is a combination of the raw count of the par- ticular term in the sentence and the term's prox- imity to the confusion word. Terms located nearer to the confusion word are given additional weight in a linearly decreasing manner. The local weight of each term is then flattened by taking its log2. The global weight given to each term is an attempt to measure its predictive power in the corpus as a whole. We found that entropy (see also (Lochbaum and Streeter, 1989)) performed best as a global mea- sure. Furthermore, terms which did not appear in more than one sentence in the training corpus were removed. While LSA can be used to quickly obtain satisfac- tory results, some tuning of the parameters involved can improve its performance. For example, we chose (somewhat arbitrarily) to retain 100 factors for each LSA space. We wanted to fix this variable for all confusion sets and this number gives a good average performance. However, tuning the number of factors to select the \"best\" number for each space shows an average of 2% improvement over all the results and up to 8% for some confusion sets. 4.3 Testing Once the LSA space for a confusion set has been cre- ated, it can be used to predict the word (from the confusion set) most likely to appear in a given sen- tence. We tested the predictive accuracy of the LSA space in the following manner. A sentence from the test corpus is selected and the location of the confu- sion word in the sentence is treated as an unknown word which must be predicted. One at a time, the words from the confusion set are inserted into the sentence at the location of the word to be predicted and the same transformations that the training sen- tences undergo are applied to the test sentence. The inserted confusion word is then removed from the sentence (but not the bigrams of which it is a part) because its presence biases the comparison which oc- curs later. A vector in LSA space is constructed from the resulting terms. The word predicted most likely to appear in a sen- tence is determined by comparing the similarity of each test sentence vector to each confusion word vec- tor from the LSA space. Vector similarity is evalu- ated by computing the cosine between two vectors. The pair of sentence and confusion word vectors with the largest cosine is identified and the corresponding confusion word is chosen as the most likely word for the test sentence. The predicted word is compared to the correct word and a tally of correct predictions is kept. 5 Results The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996). Seven of the 18 confusion sets contain words that are all the same part of speech and the remaining 11 con- tain words with different parts of speech. Golding and Schabes (1996) have already shown that using a trigram model to predict words from a confusion set based on the expected part of speech is very effec- tive. Consequently, we will focus most of our atten- tion on the seven confusion sets containing words of the same part of speech. These seven sets are listed first in all of our tables and figures. We also show the results for the remaining 11 confusion sets for comparison purposes, but as expected, these aren't as good. We, therefore, consider our system com- plementary to one (such as Tribayes) that predicts based on part of speech when possible. 5.1 Baseline Prediction System We describe our results in terms of a baseline predic- tion system that ignores the context contained in the test sentence and always predicts the confusion word that occurred most frequently in the training corpus. Table 1 shows the performance of this baseline pre- dictor. The left half of the table lists the various con- fusion sets. The next two columns show the training and testing corpus sentence counts for each confu- sion set. Because the sentences in the Brown corpus are not tagged with a markup language, we identi- fied individual sentences automatically based on a small set of heuristics. Consequently, our sentence counts for the various confusion sets differ slightly from the counts reported in (Golding and Schabes, 1996). The right half of Table 1 shows the most frequent word in the training corpus from each confusion set. Following the most frequent word is the baseline performance data. Baseline performance is the per- centage of correct predictions made by choosing the given (most frequent) word. The percentage of cor- rect predictions also represents the frequency of sen- tences in the test corpus that contain the given word. The final column lists the training corpus frequency of the given word. The difference between the base- line performance column and the training corpus frequency column gives some indication about how evenly distributed the words are between the two corpora. For example, there are 158 training sentences for the confusion set {principal, principle) and 34 test sentences. Since the word principle is listed in the right half of the table, it must have appeared more frequently in the training set. From the final column, Confusion Set Train Test Most Freq. Base (Train Freq.) principal principle 158 34 principle 41.2 (57.6) raise rise 117 36 rise 72.2 (65.0) affect effect 193 53 effect 88.7 (85.0) peace piece 257 62 peace 58.1 (59.5) country county 389 91 country 59.3 (71.0) amount number 480 122 number 75.4 (73.8) among between 853 203 between 62.1 (66.7) accept except 189 62 except 67.7 (73.5) begin being 623 161 being 88.8 (89.4) lead led 197 63 led 50.8 (52.3) passed past 353 81 past 64.2 (63.2) quiet quite 280 76 quite 88.2 (76.1) weather whether 267 67 whether 73.1 (79.0) cite sight site 128 32 sight 62.5 (54.7) it's its 1577 391 its 84.7 (84.9) than then 2497 578 than 58.8 (55.3) you're your 734 220 your 86.8 (84.5) their there they're 4176 978 there 53.4 (53.1) Table 1: Baseline performance for 18 confusion sets. The table is divided into confusion sets containing words of the same part of speech and those which have different parts of speech. we can see that it occurred in almost 58% of the training sentences. However, it occurs in only 41% of the test sentences and thus the baseline predictor scores only 41% for this confusion set. 5.2 Latent Semantic Analysis Table 2 shows the performance of LSA on the con- textual spelling correction task. The table provides the baseline performance information for compari- son to LSA. In all but the case of {amount, number}, LSA improves upon the baseline performance. The improvement provided by LSA averaged over all con- fusion sets is about 14% and for the sets with the same part of speech, the average improvement is 16%. Table 2 also gives the results obtained by Tribayes as reported in (Golding and Schabes, 1996). The baseline performance given in connection with Trib- ayes corresponds to the partitioning of the Brown corpus used to test Tribayes. It should be noted that we did not implement Tribayes nor did we use the same partitioning of the Brown corpus as Tribayes. Thus, the comparison between LSA and Tribayes is an indirect one. The differences in the baseline predictor for each system are a result of different partitions of the Brown corpus. Both systems randomly split the data such that roughly 80% is allocated to the train- ing corpus and the remaining 20% is reserved for the test corpus. Due to the random nature of this process, however, the corpora must differ between the two systems. The baseline predictor presented in this paper and in (Golding and Schabes, 1996) are based on the same method so the correspond- ing columns in Table 2 can be compared to get an idea of the distribution of sentences that contain the most frequent word for each confusion set. Examination of Table 2 reveals that it is difficult to make a direct comparison between the results of LSA and Tribayes due to the differences in the par- titioning of the Brown corpus. Each system should perform well on the most frequent confusion word in the training data. Thus, the distribution of the most frequent word between the the training and the test corpus will affect the performance of the system. Because the baseline score captures infor- mation about the percentage of the test corpus that should be easily predicted (i.e., the portion that con- tains the most frequent word), we propose a com- parison of the results by examination of the respec- tive systems' improvement over the baseline score reported for each. The results of this comparison are charted in Figure 3. The horizontal axis in the figure represents the baseline predictor performance for each system (even though it varies between the two systems). The vertical bar thus represents the performance above (or below) the baseline predictor for each system on each confusion set. LSA performs slightly better, on average, than Tribayes for those confusion sets which contain words of the same part of speech. Tribayes clearly out-performs LSA for those words of a different part of speech. Thus, LSA is doing better than the Bayesian component of Tribayes, but it doesn't in- clude part of speech information and is therefore not capable of performing as well as the part of speech trigram component of Tribayes. Consequently, we believe that LSA is a competitive alternative to --- Confusion Set Baseline LSA Baseline Tribayes principal principle 41.2 91.2 58.8 88.2 raise rise 72.2 80.6 64.1 76.9 affect effect 88.7 94.3 91.8 95.9 peace piece 58.1 83.9 44.0 90.0 country county 59.3 81.3 91.9 85.5 amount number 75.4 56.6 71.5 82.9 among between 62.1 80.8 71.5 75.3 accept except 67.7 82.3 70.0 82.0 begin being 88.8 93.2 93.2 97.3 lead led 50.8 73.0 46.9 83.7 passed past 64.2 80.3 68.9 95.9 quiet quite 88.2 90.8 83.3 95.5 weather whether 73.1 85.1 86.9 93.4 cite sight site 62.5 78.1 64.7 70.6 it's its 84.7 92.8 91.3 98.1 than then 58.8 90.5 63.4 94.9 you're your 86.8 91.4 89.3 98.9 their there they're 53.4 73.9 56.8 97.6 Table 2: LSA performance for 18 confusion sets. The results of Tribayes (Golding and Schabes, 1996) are also given. Tribayes and LSA performance compared to baseline predictor 50 40 30 20 10 0 -10 -20 Raw improvement over baseline principle rise effect peace country number between except being lead past quite whether sight its then your there Tribayes LSA Figure 3: Comparison of Tribayes vs. LSA performance above the baseline metric. --- a Bayesian classifier for making predictions among words of the same part of speech. 5.3 Performance Tuning The results that have been presented here are based on uniform treatment for each confusion set. That is, the initial data processing steps and LSA space con- struction parameters have all been the same. How- ever, the model does not require equivalent treat- ment of all confusion sets. In theory, we should be able to increase the performance for each confusion set by tuning the various parameters for each confu- sion set. In order to explore this idea further, we selected the confusion set {amount, number} as a testbed for performance tuning to a particular confusion set. As previously mentioned, we can tune the number of factors to a particular confusion set. In the case of this confusion set, using 120 factors increases the performance by 6%. However, tuning this param- eter alone still leaves the performance short of the baseline predictor. A quick examination of the context in which both words appear reveals that a significant percentage (82%) of all training instances contain either the bi- gram of the confusion word preceded by the, fol- lowed by of, or in some cases, both. For exam- ple, there are many instances of the collocation the+number+of in the training data. However, there are only one third as many training instances for amount (the less frequent word) as there are for number. This situation leads LSA to believe that the bigrams the amount and amount+of have more dis- crimination power than the corresponding bigrams which contain number. As a result, LSA gives them a higher weight and LSA almost always predicts amount when the confusion word in the test sen- tence appears in this context. This local context is a poor predictor of the confusion word and its pres- ence tends to dominate the decision made by LSA. By eliminating the words the and of from the train- ing and testing process, we permit the remaining context to be used for prediction. The elimination of the poor local context combined with the larger number of factors increases the performance of LSA to 13% above the baseline predictor (compared to 11% for Tribayes). This is a net increase in perfor- mance of 32%! 6 Conclusion We've shown that LSA can be used to attack the problem of identifying contextual misuses of words, particularly when those words are the same part of speech. It has proven to be an effective alternative to Bayesian classifiers. Confusions sets whose words are different parts of speech are more effectively han- dled using a method which incorporates the word's part of speech as a feature. We are exploring tech- niques for introducing part of speech information into the LSA space so that the system can make better predictions for those sets on which it doesn't yet measure up to Tribayes. We've also shown that for the cost of experimentation with different param- eter combinations, LSA's performance can be tuned for individual confusion sets. While the results of this experiment look very nice, they still don't tell us anything about how useful the technique is when applied to unedited text. The testing procedure assumes that a confusion word must be predicted as if the author of the text hadn't supplied a word or that writers misuse the confusion words nearly 50% of the time. For example, consider the case of the confusion set {principal, principle}. The LSA prediction accuracy for this set is 91%. However, it might be the case that, in practice, peo- ple tend to use the correct word 95% of the time. LSA has thus introduced a 4% error into the writing process. Our continuing work is to explore the error rate that occurs in unedited text as a means of as- sessing the \"true\" performance of contextual spelling correction systems. 7 Acknowledgments The first author is supported under DARPA con- tract SOL BAA95-10. We gratefully acknowledge the comments and suggestions of Thomas Landauer and the anonymous reviewers. References Scott Deerwester, Susan T. Dumais, George W. Fur- nas, Thomas K. Landauer, and Richard A. Harsh- man. 1990. Indexing by Latent Semantic Analy- sis. Journal of the American Society for Informa- tion Science, 41(6):391-407, September. Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Scott Deerwester, and Richard Harsh- man. 1988. Using Latent Semantic Analysis to improve access to textual information. In Human Factors in Computing Systems, CHI'88 Confer- ence Proceedings (Washington, D.C.), pages 281- 285, New York, May. ACM. Carl Eckart and Gale Young. 1939. A principle axis transformation for non-hermitian matrices. American Mathematical Society Bulletin, 45:118- 121. Peter W. Foltz. 1995. Improving human- proceedings interaction: Indexing the CHI index. In Human Factors in Computing Systems: CHI'95 Conference Companion, pages 101-102. Associa- tions for Computing Machinery (ACM), May. William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26(5-6):415-439, Dec. Andrew R. Golding. 1995. A Bayesian hybrid method for context-sensitive spelling correction. In Proceedings of the Third Workshop on Very Large Corpora, Cambridge, MA. Andrew R. Golding and Yves Schabes. 1996. Com- bining trigram-based and feature-based methods for context-sensitive spelling correction. In Pro- ceedings of the 34th Annual Meeting of the Associ- ation for Computational Linguistics, Santa Clara, CA, June. Association for Computational Linguis- tics. Karen Kukich. 1992a. Spelling correction for the telecommunications network for the deaf. Com- munications of the ACM, 35(5):80-90, May. Karen Kukich. 1992b. Techniques for automatically correcting words in text. ACM Computing Sur- veys, 24(4):377-439, Dec. Henry Kučera and W. Nelson Francis. 1967. Com- putational Analysis of Present-Day American En- glish. Brown University Press, Providence, RI. Thomas K. Landauer and Susan T. Dumais. (In press). A solution to Plato's problem: The La- tent Semantic Analysis theory of acquisition, in- duction, and representation of knowledge. Psy- chological Review. Karen E. Lochbaum and Lynn A. Streeter. 1989. Comparing and combining the effectiveness of La- tent Semantic Indexing and the ordinary vector space model for information retrieval. Informa- tion Processing & Management, 25(6):665-676. Susan W. McRoy. 1992. Using multiple knowledge sources for word sense disambiguation. Computa- tional Linguistics, 18(1):1-30, March. M. F. Porter. 1980. An algorithm for suffix strip- ping. Program, 14(3):130-137, July. Lynn A. Streeter and Karen E. Lochbaum. 1988. An expert/expert-locating system based on au- tomatic representation of semantic structure. In Proceedings of the Fourth Conference on Artifi- cial Intelligence Applications, pages 345-350, San Diego, CA, March. IEEE. David Yarowsky. 1994. Decision lists for lexical am- biguity resolution: Application to accent restora- tion in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Com- putational Linguistics, pages 88-95, Las Cruces, NM, June. Association for Computational Lin- guistics."
  },
  {
    "title": "Cut and Paste Based Text Summarization",
    "abstract": "We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts. The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resulting phrases together as coherent sentences. Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer.",
    "content": "1 Introduction There is a big gap between the summaries produced by current automatic summarizers and the abstracts written by human professionals. Certainly one fac- tor contributing to this gap is that automatic sys- tems can not always correctly identify the important topics of an article. Another factor, however, which has received little attention, is that automatic sum- marizers have poor text generation techniques. Most automatic summarizers rely on extracting key sen- tences or paragraphs from an article to produce a summary. Since the extracted sentences are discon- nected in the original article, when they are strung together, the resulting summary can be inconcise, incoherent, and sometimes even misleading. We present a cut and paste based text sum- marization technique, aimed at reducing the gap between automatically generated summaries and human-written abstracts. Rather than focusing on how to identify key sentences, as do other re- searchers, we study how to generate the text of a summary once key sentences have been extracted. The main idea of cut and paste summarization is to reuse the text in an article to generate the summary. However, instead of simply extracting sentences as current summarizers do, the cut and paste system will \"smooth\" the extracted sentences by editing them. Such edits mainly involve cutting phrases and pasting them together in novel ways. The key features of this work are: (1) The identification of cutting and past- ing operations. We identified six operations that can be used alone or together to transform extracted sentences into sentences in human-written abstracts. The operations were identified based on manual and automatic comparison of human-written abstracts and the original articles. Examples include sentence reduction, sentence combination, syntactic transfor- mation, and lexical paraphrasing. (2) Development of an automatic system to perform cut and paste operations. Two opera- tions - sentence reduction and sentence combination - are most effective in transforming extracted sen- tences into summary sentences that are as concise and coherent as in human-written abstracts. We implemented a sentence reduction module that re- moves extraneous phrases from extracted sentences, and a sentence combination module that merges the extracted sentences or the reduced forms resulting from sentence reduction. Our sentence reduction model determines what to cut based on multiple sources of information, including syntactic knowl- edge, context, and statistics learned from corpus analysis. It improves the conciseness of extracted sentences, making them concise and on target. Our sentence combination module implements combina- tion rules that were identified by observing examples written by human professionals. It improves the co- herence of extracted sentences. (3) Decomposing human-written summary sentences. The cut and paste technique we propose here is a new computational model which we based on analysis of human-written abstracts. To do this analysis, we developed an automatic system that can match a phrase in a human-written abstract to the corresponding phrase in the article, identifying its most likely location. This decomposition program allows us to analyze the construction of sentences in a human-written abstract. Its results have been used to train and test the sentence reduction and sentence combination module. In Section 2, we discuss the cut and paste tech- nique in general, from both a professional and com- putational perspective. We also describe the six cut and paste operations. In Section 3, we describe the system architecture. The major components of the system, including sentence reduction, sentence com- bination, decomposition, and sentence selection, are described in Section 4. The evaluation results are shown in Section 5. Related work is discussed in Section 6. Finally, we conclude and discuss future work. 2 Cut and paste in summarization 2.1 Related work in professional summarizing Professionals take two opposite positions on whether a summary should be produced by cutting and past- ing the original text. One school of scholars is opposed; \"(use) your own words... Do not keep too close to the words before you\", states an early book on abstracting for American high school stu- dents (Thurber, 1924). Another study, however, shows that professional abstractors actually rely on cutting and pasting to produce summaries: \"Their professional role tells abstractors to avoid inventing anything. They follow the author as closely as pos- sible and reintegrate the most important points of a document in a shorter text\" (Endres-Niggemeyer et al., 1998). Some studies are somewhere in be- tween: \"summary language may or may not follow that of author's\" (Fidel, 1986). Other guidelines or books on abstracting (ANSI, 1997; Cremmins, 1982) do not discuss the issue. Our cut and paste based summarization is a com- putational model; we make no claim that humans use the same cut and paste operations. 2.2 Cut and paste operations We manually analyzed 30 articles and their corre- sponding human-written summaries; the articles and their summaries come from different domains (15 general news reports, 5 from the medical domain, 10 from the legal domain) and the summaries were written by professionals from different organizations. We found that reusing article text for summarization is almost universal in the corpus we studied. We de- fined six operations that can be used alone, sequen- tially, or simultaneously to transform selected sen- tences from an article into the corresponding sum- mary sentences in its human-written abstract: (1) sentence reduction Remove extraneous phrases from a selected sen- tence, as in the following example 1: 1 All the examples in this section were produced by human professionals Document sentence: When it arrives some- time next year in new TV sets, the V-chip will give parents a new and potentially revolution- ary device to block out programs they don't want their children to see. Summary sentence: The V-chip will give par- ents a device to block out programs they don't want their children to see. The deleted material can be at any granularity: a word, a phrase, or a clause. Multiple components can be removed. (2) sentence combination Merge material from several sentences. It can be used together with sentence reduction, as illustrated in the following example, which also uses paraphras- ing: Text Sentence 1: But it also raises serious questions about the privacy of such highly personal information wafting about the digital world. Text Sentence 2: The issue thus fits squarely into the broader debate about privacy and se- curity on the internet, whether it involves pro- tecting credit card number or keeping children from offensive information. Summary sentence: But it also raises the is- sue of privacy of such personal information and this issue hits the head on the nail in the broader debate about privacy and security on the internet. (3) syntactic transformation In both sentence reduction and combination, syn- tactic transformations may be involved. For exam- ple, the position of the subject in a sentence may be moved from the end to the front. (4) lexical paraphrasing Replace phrases with their paraphrases. For in- stance, the summaries substituted point out with note, and fits squarely into with a more picturesque description hits the head on the nail in the previous examples. (5) generalization or specification Replace phrases or clauses with more general or specific descriptions. Examples of generalization and specification include: Generalization: \"a proposed new law that would require Web publishers to obtain parental consent before collecting personal in- formation from children\" → \"legislation to protect children's privacy on-line\" Specification: \"the White House's top drug official\" → \"Gen. Barry R. McCaffrey, the White House's top drug official\" Input article Sentence extraction extracted key sentences Parser Cut and paste based generation Corpus of human-written abstracts Co-reference Sentence reduction WordNet, decomposition Sentence combination Combined lexicon (6) reordering Output summary Figure 1: System architecture Change the order of extracted sentences. For in- stance, place an ending sentence in an article at the beginning of an abstract. In human-written abstracts, there are, of course, sentences that are not based on cut and paste, but completely written from scratch. We used our de- composition program to automatically analyze 300 human-written abstracts, and found that 19% of sen- tences in the abstracts were written from scratch. There are also other cut and paste operations not listed here due to their infrequent occurrence. 3 System architecture The architecture of our cut and paste based text summarization system is shown in Figure 1. Input to the system is a single document from any domain. In the first stage, extraction, key sentences in the ar- ticle are identified, as in most current summarizers. In the second stage, cut and paste based generation, a sentence reduction module and a sentence combina- tion module implement the operations we observed in human-written abstracts. The cut and paste based component receives as input not only the extracted key sentences, but also the original article. This component can be ported to other single-document summarizers to serve as the generation component, since most current sum- marizers extract key sentences - exactly what the extraction module in our system does. Other resources and tools in the summarization system include a corpus of articles and their human- written abstracts, the automatic decomposition pro- gram, a syntactic parser, a co-reference resolution system, the WordNet lexical database, and a large- scale lexicon we combined from multiple resources. The components in dotted lines are existing tools or resources; all the others were developed by ourselves. 4 Major components The main focus of our work is on decomposition of summaries, sentence reduction, and sentence com- bination. We also describe the sentence extraction module, although it is not the main focus of our work. 4.1 Decomposition of human-written summary sentences The decomposition program, see (Jing and McKe- own, 1999) for details, is used to analyze the con- struction of sentences in human-written abstracts. The results from decomposition are used to build the training and testing corpora for sentence reduc- tion and sentence combination. The decomposition program answers three ques- tions about a sentence in a human-written abstract: (1) Is the sentence constructed by cutting and past- ing phrases from the input article? (2) If so, what phrases in the sentence come from the original arti- cle? (3) Where in the article do these phrases come from? We used a Hidden Markov Model (Baum, 1972) solution to the decomposition problem. We first mathematically formulated the problem, reducing it to a problem of finding, for each word in a summary Summary sentence: (FO:S1 arthur b sackler vice president for law and public policy of time warner inc) (F1:S-1 and) (F2:S0 a member of the direct marketing association told) (F3:S2 the com- munications subcommittee of the senate commerce committee) (F4:S-1 that legislation) (F5:S1to protect) (F6:54 children's) (F7:S4 privacy) (F8:S4 online) (F9:50 could destroy the spontaneous nature that makes the internet unique) Source document sentences: Sentence 0: a proposed new law that would require web publishers to obtain parental consent before collecting personal information from children (F9 could destroy the spontaneous nature that makes the internet unique) (F2 a member of the direct marketing association told) a senate panel thursday Sentence 1: (F0 arthur b sackler vice president for law and public policy of time warner inc) said the association supported efforts (F5 to protect) children online but he urged lawmakers to find some middle ground that also allows for interactivity on the internet Sentence 2: for example a child's e-mail address is necessary in order to respond to inquiries such as updates on mark mcguire's and sammy sosa's home run figures this year or updates of an online magazine sackler said in testimony to (F3 the communications subcommittee of the senate commerce committee) Sentence 4: the subcommittee is considering the (F6 children's) (F8 online) (F7 privacy ) protection act which was drafted on the recommendation of the federal trade commission Figure 2: Sample output of the decomposition program sentence, a document position that it most likely comes from. The position of a word in a document is uniquely identified by the position of the sentence where the word appears, and the position of the word within the sentence. Based on the observation of cut and paste practice by humans, we produced a set of general heuristic rules. Sample heuristic rules in- clude: two adjacent words in a summary sentence are most likely to come from two adjacent words in the original document; adjacent words in a summary sentence are not very likely to come from sentences that are far apart in the original document. We use these heuristic rules to create a Hidden Markov Model. The Viterbi algorithm (Viterbi, 1967) is used to efficiently find the most likely document position for each word in the summary sentence. Figure 2 shows sample output of the program. For the given summary sentence, the program cor- rectly identified that the sentence was combined from four sentences in the input article. It also di- vided the summary sentence into phrases and pin- pointed the exact document origin of each phrase. A phrase in the summary sentence is annotated as (FNUM:SNUM actual-text), where FNUM is the se- quential number of the phrase and SNUM is the number of the document sentence where the phrase comes from. SNUM = -1 means that the compo- nent does not come from the original document. The phrases in the document sentences are annotated as (FNUM actual-text). 4.2 Sentence reduction The task of the sentence reduction module, de- scribed in detail in (Jing, 2000), is to remove extra- neous phrases from extracted sentences. The goal of reduction is to \"reduce without major loss\"; that is, we want to remove as many extraneous phrases as possible from an extracted sentence so that it can be concise, but without detracting from the main idea that the sentence conveys. Ideally, we want to re- move a phrase from an extracted sentence only if it is irrelavant to the main topic. Our reduction module makes decisions based on multiple sources of knowledge: (1) Grammar checking. In this step, we mark which components of a sentence or a phrase are obligatory to keep it grammatically correct. To do this, we traverse the sentence parse tree, produced by the English Slot Grammar (ESG) parser devel- oped at IBM (McCord, 1990), in top-down order and mark for each node in the parse tree, which of its children are obligatory. The main source of knowledge the system relies on in this step is a large-scale, reusable lexicon we combined from mul- tiple resources (Jing and McKeown, 1998). The lexi- con contains subcategorizations for over 5,000 verbs. This information is used to mark the obligatory ar- guments of verb phrases. (2) Context information. We use an extracted sentence's local context in the article to decide which components in the sentence are likely to be most relevant to the main topic. We link the words in the extracted sentence with words in its local context, if they are repetitions, morphologically related, or linked with each other in WordNet through certain type of lexical relation, such as synonymy, antonymy, or meronymy. Each word in the extracted sentence gets an importance score, based on the number of links it has with other words and the types of links. Each phrase in the sentence is then assigned a score Example 1: Original sentence : When it arrives sometime next year in new TV sets, the V-chip will give parents a new and potentially revolutionary device to block out programs they don't want their children to see. Reduction program: The V-chip will give parents a new and potentially revolutionary device to block out programs they don't want their children to see. Professionals : The V-chip will give parents a device to block out programs they don't want their children to see. Example 2: Original sentence : Som and Hoffman's creation would allow broadcasters to insert multiple ratings into a show, enabling the V-chip to filter out racy or violent material but leave unexceptional portions of a show alone. Reduction Program: Som and Hoffman's creation would allow broadcasters to insert multiple rat- ings into a show. Professionals : Som and Hoffman's creation would allow broadcasters to insert multiple rat- ings into a show. Figure 3: Sample output of the sentence reduction program by adding up the scores of its children nodes in the parse tree. This score indicates how important the phrase is to the main topic in discussion. (3) Corpus evidence. The program uses a cor- pus of input articles and their corresponding reduced forms in human-written abstracts to learn which components of a sentence or a phrase can be re- moved and how likely they are to be removed by professionals. This corpus was created using the de- composition program. We compute three types of probabilities from this corpus: the probability that a phrase is removed; the probability that a phrase is reduced (i.e., the phrase is not removed as a whole, but some components in the phrase are removed); and the probability that a phrase is unchanged at all (i.e., neither removed nor reduced). These cor- pus probabilities help us capture human practice. (4) Final decision. The final reduction decision is based on the results from all the earlier steps. A phrase is removed only if it is not grammatically obligatory, not the focus of the local context (indi- cated by a low context importance score), and has a reasonable probability of being removed by humans. The phrases we remove from an extracted sentence include clauses, prepositional phrases, gerunds, and to-infinitives. The result of sentence reduction is a shortened version of an extracted sentence 2. This shortened text can be used directly as a summary, or it can be fed to the sentence combination module to be merged with other sentences. Figure 3 shows two examples produced by the re- duction program. The corresponding sentences in human-written abstracts are also provided for com- parison. 2 It is actually also possible that the reduction program decides no phrase in a sentence should be removed, thus the result of reduction is the same as the input. 4.3 Sentence combination To build the combination module, we first manu- ally analyzed a corpus of combination examples pro- duced by human professionals, automatically cre- ated by the decomposition program, and identified a list of combination operations. Table 1 shows the combination operations. To implement a combination operation, we need to do two things: decide when to use which com- bination operation, and implement the combining actions. To decide when to use which operation, we analyzed examples by humans and manually wrote a set of rules. Two simple rules are shown in Fig- ure 4. Sample outputs using these two simple rules are shown in Figure 5. We are currently exploring using machine learning techniques to learn the com- bination rules from our corpus. The implementation of the combining actions in- volves joining two parse trees, substituting a subtree with another, or adding additional nodes. We im- plemented these actions using a formalism based on Tree Adjoining Grammar (Joshi, 1987). 4.4 Extraction Module The extraction module is the front end of the sum- marization system and its role is to extract key sen- tences. Our method is primarily based on lexical re- lations. First, we link words in a sentence with other words in the article through repetitions, morpholog- ical relations, or one of the lexical relations encoded in WordNet, similar to step 2 in sentence reduction. An importance score is computed for each word in a sentence based on the number of lexical links it has with other words, the type of links, and the direc- tions of the links. After assigning a score to each word in a sentence, we then compute a score for a sentence by adding up the scores for each word. This score is then normal- Categories Add descriptions or names for people or organizations Aggregations Substitute incoherent phrases Substitute phrases with more general or specific information Mixed operations Combination Operations add description (see Figure 5) add name extract common subjects or objects (see Figure 5) change one sentence to a clause add connectives (e.g., and or while) add punctuations (e.g., \";\") substitute dangling anaphora substitute dangling noun phrases substitute adverbs (e.g., here) remove connectives substitute with more general information substitute with more specific information combination of any of above operations (see Figure 2) Table 1: Combination operations Rule 1: IF: ((a person or an organization is mentioned the first time) and (the full name or the full descrip- tion of the person or the organization exists somewhere in the original article but is missing in the summary)) THEN: replace the phrase with the full name plus the full description Rule 2: IF: ((two sentences are close to each other in the original article) and (their subjects refer to the same entity) and (at least one of the sentences is the reduced form resulting from sentence reduc- tion)) THEN: merge the two sentences by removing the subject in the second sentence, and then com- bining it with the first sentence using connective \"and\". Figure 4: Sample sentence combination rules ized over the number of words a sentence contains. The sentences with high scores are considered im- portant. The extraction system selects sentences based on the importance computed as above, as well as other indicators, including sentence positions, cue phrases, and tf*idf scores. 5 Evaluation Our evaluation includes separate evaluations of each module and the final evaluations of the overall sys- tem. We evaluated the decomposition program by two experiments, described in (Jing and McKeown, 1999). In the first experiment, we selected 50 human-written abstracts, consisting of 305 sentences in total. A human subject then read the decomposi- tion results of these sentences to judge whether they are correct. 93.8% of the sentences were correctly decomposed. In the second experiment, we tested the system in a summary alignment task. We ran the decomposition program to identify the source document sentences that were used to construct the sentences in human-written abstracts. Human sub- jects were also asked to select the document sen- tences that are semantic-equivalent to the sentences in the abstracts. We compared the set of sentences identified by the program with the set of sentences selected by the majority of human subjects, which is used as the gold standard in the computation of pre- cision and recall. The program achieved an average 81.5% precision, 78.5% recall, and 79.1% f-measure for 10 documents. The average performance of 14 human judges is 88.8% precision, 84.4% recall, and 85.7% f-measure. Recently, we have also tested the system on legal documents (the headnotes used by Westlaw company), and the program works well on those documents too. The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts. 400 sentences were used to compute corpus probabili- ties and 100 sentences were used for testing. The results show that 81.3% of the reduction decisions made by the system agreed with those of humans. The humans reduced the length of the 500 sentences by 44.2% on average, and the system reduced the length of the 100 test sentences by 32.7%. The evaluation of sentence combination module is not as straightforward as that of decomposition or reduction since combination happens later in the pipeline and it depends on the output from prior Example 1: add descriptions or names for people or organization Original document sentences: \"We're trying to prove that there are big benefits to the patients by involving them more deeply in their treatment\", said Paul Clayton, Chairman of the Department dealing with comput- erized medical information at Columbia. \"The economic payoff from breaking into health care records is a lot less than for banks\", said Clayton at Columbia. Combined sentence: \"The economic payoff from breaking into health care records is a lot less than for banks\", said Paul Clayton, Chairman of the Department dealing with computerized medical information at Columbia. Professional: (the same) Example 2: extract common subjects Original document sentences: The new measure is an echo of the original bad idea, blurred just enough to cloud prospects both for enforcement and for court review. Unlike the 1996 act, this one applies only to commercial Web sites thus sidestepping 1996 objections to the burden such regulations would pose for museums, libraries and freewheeling conversation deemed \"indecent\" by somebody somewhere. The new version also replaces the vague \"indecency\" standard, to which the court objected, with the better-defined one of material ruled \"harmful to minors.\" Combined sentences: The new measure is an echo of the original bad idea. The new version applies only to commercial web sites and replaces the vague \"indecency\" standard with the better-defined one of material ruled \"harmful to minors.\" Professional: While the new law replaces the \"indecency\" standard with \"harmful to minors\" and now only applies to commercial Web sites, the \"new measure is an echo of the original bad idea.\" Figure 5: Sample output of the sentence combination program modules. To evaluate just the combination compo- nent, we assume that the system makes the same reduction decision as humans and the co-reference system has a perfect performance. This involves manual tagging of some examples to prepare for the evaluation; this preparation is in progress. The eval- uation of sentence combination will focus on the ac- cessment of combination rules. The overall system evaluation includes both in- trinsic and extrinsic evaluation. In the intrinsic eval- uation, we asked human subjects to compare the quality of extraction-based summaries and their re- vised versions produced by our sentence reduction and combination modules. We selected 20 docu- ments; three different automatic summarizers were used to generate a summary for each document, pro- ducing 60 summaries in total. These summaries are all extraction-based. We then ran our sentence reduction and sentence combination system to re- vise the summaries, producing a revised version for each summary. We presented human subjects with the full documents, the extraction-based summaries, and their revised versions, and asked them to com- pare the extraction-based summaries and their re- vised versions. The human subjects were asked to score the conciseness of the summaries (extraction- based or revised) based on a scale from 0 to 10 the higher the score, the more concise a summary is. They were also asked to score the coherence of the summaries based on a scale from 0 to 10. On aver- age, the extraction-based summaries have a score of 4.2 for conciseness, while the revised summaries have a score of 7.9 (an improvement of 88%). The average improvement for the three systems are 78%, 105%, and 88% respectively. The revised summaries are on average 41% shorter than the original extraction- based summaries. For summary coherence, the aver- age score for the extraction-based summaries is 3.9, while the average score for the revised summaries is 6.1 (an improvement of 56%). The average improve- ment for the three systems are 69%, 57%, and 53% respectively. We are preparing a task-based evaluation, in which we will use the data from the Summariza- tion Evaluation Conference (Mani et al., 1998) and compare how our revised summaries can influence humans' performance in tasks like text categoriza- tion and ad-hoc retrieval. 6 Related work (Mani et al., 1999) addressed the problem of revising summaries to improve their quality. They suggested three types of operations: elimination, aggregation, and smoothing. The goal of the elimination opera- tion is similar to that of the sentence reduction op- eration in our system. The difference is that while elimination always removes parentheticals, sentence- initial PPs and certain adverbial phrases for every extracted sentence, our sentence reduction module aims to make reduction decisions according to each case and removes a sentence component only if it considers it appropriate to do so. The goal of the aggregation operation and the smoothing operation is similar to that of the sentence combination op- eration in our system. However, the combination operations and combination rules that we derived from corpus analysis are significantly different from those used in the above system, which mostly came from operations in traditional natural language gen- eration. 7 Conclusions and future work This paper presents a novel architecture for text summarization using cut and paste techniques ob- served in human-written abstracts. In order to auto- matically analyze a large quantity of human-written abstracts, we developed a decomposition program. The automatic decomposition allows us to build large corpora for studying sentence reduction and sentence combination, which are two effective op- erations in cut and paste. We developed a sentence reduction module that makes reduction decisions us- ing multiple sources of knowledge. We also investi- gated possible sentence combination operations and implemented the combination module. A sentence extraction module was developed and used as the front end of the summarization system. We are preparing the task-based evaluation of the overall system. We also plan to evaluate the porta- bility of the system by testing it on another corpus. We will also extend the system to query-based sum- marization and investigate whether the system can be modified for multiple document summarization. Acknowledgment We thank IBM for licensing us the ESG parser and the MITRE corporation for licensing us the co- reference resolution system. This material is based upon work supported by the National Science Foun- dation under Grant No. IRI 96-19124 and IRI 96-18797. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. References ANSI. 1997. Guidelines for abstracts. Technical Re- port Z39.14-1997, NISO Press, Bethesda, Mary- land. L. Baum. 1972. An inequality and associated max- imization technique in statistical estimation of probabilistic functions of a markov process. In- equalities, (3):1-8. Edward T. Cremmins. 1982. The Art of Abstracting. ISI Press, Philadelphia. Brigitte Endres-Niggemeyer, Kai Haseloh, Jens Müller, Simone Peist, Irene Santini de Sigel, Alexander Sigel, Elisabeth Wansorra, Jan Wheeler, and Brünja Wollny. 1998. Summarizing Information. Springer, Berlin. Raya Fidel. 1986. Writing abstracts for free-text searching. Journal of Documentation, 42(1):11- 21, March. Hongyan Jing and Kathleen R. McKeown. 1998. Combining multiple, large-scale resources in a reusable lexicon for natural language generation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, volume 1, pages 607-613, Université de Montréal, Quebec, Canada, August. Hongyan Jing and Kathleen R. McKeown. 1999. The decomposition of human-written summary sentences. In Proceedings of the 22nd In- ternational ACM SIGIR Conference on Re- search and Development in Information Re- trieval(SIGIR'99), pages 129-136, University of Berkeley, CA, August. Hongyan Jing. 2000. Sentence reduction for au- tomatic text summarization. In Proceedings of ANLP 2000. Aravind.K. Joshi. 1987. Introduction to tree- adjoining grammars. In A. Manaster-Ramis, ed- itor, Mathematics of Language. John Benjamins, Amsterdam. Inderjeet Mani, David House, Gary Klein, Lynette Hirschman, Leo Obrst, Therese Firmin, Michael Chrzanowski, and Beth Sundheim. 1998. The TIPSTER SUMMAC text summarization eval- uation final report. Technical Report MTR- 98W0000138, The MITRE Corporation. Inderjeet Mani, Barbara Gates, and Eric Bloedorn. 1999. Improving summaries by revising them. In Proceedings of the 37th Annual Meeting of the As- sociation for Computational Linguistics(ACL'99), pages 558-565, University of Maryland, Mary- land, June. Michael McCord, 1990. English Slot Grammar. IBM. Samuel Thurber, editor. 1924. Précis Writing for American Schools. The Atlantic Monthly Press, INC., Boston. A.J. Viterbi. 1967. Error bounds for convolution codes and an asymptotically optimal decoding al- gorithm. IEEE Transactions on Information The- ory, 13:260-269."
  },
  {
    "title": "An Annotation Scheme for Free Word Order Languages",
    "abstract": "We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages. Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme. The resulting scheme reflects a stratificational notion of language, and makes only minimal assumptions about the interrelation of the particular representational strata.",
    "content": "1 Introduction The work reported in this paper aims at providing syntactically annotated corpora ('treebanks') for stochastic grammar induction. In particular, we focus on several methodological issues concerning the annotation of non-configurational languages. In section 2, we examine the appropriateness of existing annotation schemes. On the basis of these considerations, we formulate several additional requirements. A formalism complying with these requirements is described in section 3. Section 4 deals with the treatment of selected phenomena. For a description of the annotation tool see section 5. 2 Motivation 2.1 Linguistically Interpreted Corpora Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods. Real- world texts annotated with different strata of linguistic information can be used for grammar induction. The data-drivenness of this approach presents a clear advantage over the traditional, idealised notion of competence grammar. 2.2 Existing Treebank Formats Corpora annotated with syntactic structures are commonly referred to as treebanks. Existing tree- bank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained. Theory-independence: Annotations should not be influenced by theory-specific considerations. Nevertheless, different theory-specific representations shall be recoverable from the annotation, cf. (Marcus et al., 1994). Multi-stratal representation: Clear separation of different description levels is desirable. Data-drivenness: The scheme must provide representational means for all phenomena occurring in texts. Disambiguation is based on human processing skills (cf. (Marcus et al., 1994), (Sampson, 1995), (Black et al., 1996)). The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies. The underlying argument structure is not represented directly, but can be recovered from the tree and trace-filler annotations. Syntactic category is encoded in node labels. Grammatical functions constitute a complex label system (cf. (Bies et al., 1995), (Sampson, 1995)). Part-of-Speech is annotated at word level. Thus the context-free constituent backbone plays a pivotal role in the annotation scheme. Due to the substantial differences between existing models of constituent structure, the question arises of how the theory independence requirement can be satisfied. At this point the importance of the underlying argument structure is emphasised (cf. (Lehmann et al., 1996), (Marcus et al., 1994), (Sampson, 1995)). 2.3 Language-Specific Features Treebanks of the format described in the above section have been designed for English. Therefore, the solutions they offer are not always optimal for other language types. As for free word order languages, the following features may cause problems: • local and non-local dependencies form a con- tinuum rather than clear-cut classes of pheno- mena; • there exists a rich inventory of discontinuous constituency types (topicalisation, scrambling, clause union, pied piping, extraposition, split NPs and PPs); • word order variation is sensitive to many fac- tors, e.g. category, syntactic function, focus; • the grammaticality of different word permuta- tions does not fit the traditional binary 'right- wrong' pattern; it rather forms a gradual tran- sition between the two poles. In light of these facts, serious difficulties can be ex- pected arising from the structural component of the existing formalisms. Due to the frequency of discon- tinuous constituents in non-configurational langua- ges, the filler-trace mechanism would be used very often, yielding syntactic trees fairly different from the underlying predicate-argument structures. Consider the German sentence (1) daran wird ihn Anna erkennen, daß er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: S Finally, the structural handling of free word or- der means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious. Since most methods of handling discontinuous constituents make the for- malism more powerful, the efficiency of processing deteriorates, too. An alternative solution is to make argument struc- ture the main structural component of the forma- lism. This assumption underlies a growing num- ber of recent syntactic theories which give up the context-free constituent backbone, cf. (McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995). These approaches provide an ade- quate explanation for several issues problematic for phrase-structure grammars (clause union, extrapo- sition, diverse second-position phenomena). 2.4 Annotating Argument Structure Argument structure can be represented in terms of unordered trees (with crossing branches). In order to reduce their ambiguity potential, rather simple, 'flat' trees should be employed, while more information can be expressed by a rich system of function labels. Furthermore, the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions, e.g. every syn- tactic structure has a unique head. Thus, notions such as head should be distinguished at the level of syntactic functions rather than structures. This re- quirement speaks against the traditional sort of de- pendency trees, in which heads are represented as non-terminal nodes, cf. (Hudson, 1984). A tree meeting these requirements is given below: VP S#1 S VP AdvP#3 Adv V NP#2 NP daran en wird ihn Anna #2 #3 erkennen, dass er weint The fairly short sentence contains three non-local dependencies, marked by co-references between tra- ces and the corresponding nodes. This hybrid repre- sentation makes the structure less transparent, and therefore more difficult to annotate. Apart from this rather technical problem, two fur- ther arguments speak against phrase structure as the structural pivot of the annotation scheme: • Phrase structure models stipulated for non- configurational languages differ strongly from each other, presenting a challenge to the inten- ded theory-independence of the scheme. • Constituent structure serves as an explanatory device for word order variation, which is difficult to reconcile with the descriptivity requirement. AdvP Adv V NP NP V CPL NP V daran wird ihn Anna erkennen, dass er weint Such a word order independent representation has the advantage of all structural information being en- coded in a single data structure. A uniform repre- sentation of local and non-local dependencies makes the structure more transparent¹. 3 The Annotation Scheme 3.1 Architecture We distinguish the following levels of representation: 1A context-free constituent backbone can still be re- covered from the surface string and argument structure by reattaching 'extracted' structures to a higher node. Argument structure, represented in terms of un- ordered trees. Grammatical functions, encoded in edge labels, e.g. SB (subject), MO (modifier), HD (head). Syntactic categories, expressed by category la- bels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals. 3.2 Argument Structure A structure for (2) is shown in fig. 2. (2) schade, daß kein Arzt anwesend ist, der pity that no doctor present is who sich auskennt is competent 'Pity that no competent doctor is here' Note that the root node does not have a head de- scendant (HD) as the sentence is a predicative con- struction consisting of a subject (SB) and a predi- cate (PD) without a copula. The subject is itself a sentence in which the copula (ist) does occur and is assigned the tag HD2. The tree resembles traditional constituent struc- tures. The difference is its word order independence: structural units (\"phrases\") need not be contiguous substrings. For instance, the extraposed relative clause (RC) is still treated as part of the subject NP. As the annotation scheme does not distinguish dif- ferent bar levels or any similar intermediate catego- ries, only a small set of node labels is needed (cur- rently 16 tags, S, NP, AP..). 3.3 Grammatical Functions Due to the rudimentary character of the argument structure representations, a great deal of information has to be expressed by grammatical functions. Their further classification must reflect different kinds of linguistic information: morphology (e.g., case, in- flection), category, dependency type (complementa- tion vs. modification), thematic role, etc.3 However, there is a trade-off between the granu- larity of information encoded in the labels and the speed and accuracy of annotation. In order to avoid inconsistencies, the corpus is annotated in two sta- ges: basic annotation and refinement. While in the first phase each annotator has to annotate structures as well as categories and functions, the refinement can be done separately for each representation level. During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the follo- wing areas of information: 2CP stands for complementizer, OA for accusative object and RC for relative clause. NK denotes a 'kernel NP' component (v. section 4.1). 3For an extensive use of grammatical functions cf. (Karlsson et al., 1995), (Voutilainen, 1994). Dependency type: complements are further clas- sified according to features such as category and case: clausal complements (OC), accusa- tive objects (OA), datives (DA), etc. Modifiers are assigned the label MO (further classification with respect to thematic roles is planned). Se- parate labels are defined for dependencies that do not fit the complement/modifier dichotomy, e.g., pre- (GL) and postnominal genitives (GR). Headedness versus non-headedness: Headed and non-headed structures are distin- guished by the presence or absence of a branch labeled HD. Morphological information: Another set of la- bels represents morphological information. PM stands for morphological particle, a label for German infinitival zu and superlative am. Se- parable verb prefixes are labeled SVP. During the second annotation stage, the annota- tion is enriched with information about thematic ro- les, quantifier scope and anaphoric reference. As al- ready mentioned, this is done separately for each of the three information areas. 3.4 Structure Sharing A phrase or a lexical item can perform multiple func- tions in a sentence. Consider equi verbs where the subject of the infinitival VP is not realised syntac- tically, but co-referent with the subject or object of the matrix equi verb: (3) er bat mich zu kommen he asked me to come (mich is the understood subject of kommen). In such cases, an additional edge is drawn from the embed- ded VP node to the controller, thus changing the syntactic tree into a graph. We call such additional edges secondary links and represent them as dotted lines, see fig. 4, showing the structure of (3). 4 Treatment of Selected Phenomena As theory-independence is one of our objectives, the annotation scheme incorporates a number of widely accepted linguistic analyses, especially in the area of verbal, adverbial and adjectival syntax. However, some other standard analyses turn out to be proble- matic, mainly due to the partial, idealised character of competence grammars, which often marginalise or ignore such important phenomena as 'deficient' (e.g. headless) constructions, appositions, temporal expressions, etc. In the following paragraphs, we give annotations for a number of such phenomena. 4.1 Noun Phrases Most linguistic theories treat NPs as structures hea- ded by a unique lexical item (noun). However, this idealised model needs several additional assumpti- ons in order to account for such important pheno- mena as complex nominal NP components (cf. (4)) or nominalised adjectives (cf. (5)). (4) my uncle Peter Smith (5) der sehr Glückliche the very happy 'the very happy one' In (4), different theories make different headedness predictions. In (5), either a lexical nominalisation rule for the adjective Glückliche is stipulated, or the existence of an empty nominal head. Moreover, the so-called DP analysis views the article der as the head of the phrase. Further differences concern the attachment of the degree modifier sehr. Because of the intended theory-independence of the scheme, we annotate only the common mini- mum. We distinguish an NP kernel consisting of determiners, adjective phrases and nouns. All com- ponents of this kernel are assigned the label NK and treated as sibling nodes. The difference between the particular NK's lies in the positional and part-of-speech information, which is also sufficient to recover theory-specific structures from our 'underspecified' representations. For in- stance, the first determiner among the NK's can be treated as the specifier of the phrase. The head of the phrase can be determined in a similar way ac- cording to theory-specific assumptions. In addition, a number of clear-cut NP components can be defined outside that juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RCC), clausal and sentential complements (OC). They are all treated as siblings of NK's re- gardless of their position (in situ or extraposed). 4.2 Attachment Ambiguities Adjunct attachment often gives rise to structural ambiguities or structural uncertainty. However, full or partial disambiguation takes place in context, and the annotators do not consider unrealistic readings. In addition, we have adopted a simple convention for those cases in which context information is insuf- ficient for total disambiguation: the highest possible attachment site is chosen. A similar convention has been adopted for con- structions in which scope ambiguities have syntac- tic effects but a one-to-one correspondence between scope and attachment does not seem reasonable, cf. focus particles such as only or also. If the scope of such a word does not directly correspond to a tree node, the word is attached to the lowest node domi- nating all subconstituents appearing in its scope. 4.3 Coordination A problem for the rudimentary argument structure representations is the use of incomplete structures in natural language, i.e. phenomena such as coor- dination and ellipsis. Since a precise structural de- scription of non-constituent coordination would re- quire a rich inventory of incomplete phrase types, we have agreed on a sort of underspecified representa- tions: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links. Fig. 3 shows the re- presentation of the sentence: (6) sie wurde von preußischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preußischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorpo- rated into Prussia in 1887' The category of the coordination is labeled CVP here, where C stands for coordination, and VP for the actual category. This extra marking makes it easy to distinguish between 'normal' and coordina- ted categories. Multiple coordination as well as enumerations are annotated in the same way. An explicit coordinating conjunction need not be present. Structure-sharing is expressed using secondary links. 5 The Annotation Tool 5.1 Requirements The development of linguistically interpreted cor- pora presents a laborious and time-consuming task. In order to make the annotation process more effi- cient, extra effort has been put into the development of an annotation tool. The tool supports immediate graphical feedback and automatic error checking. Since our scheme per- mits crossing edges, visualisation as bracketing and indentation would be insufficient. Instead, the com- plete structure should be represented. The tool should also permit a convenient hand- ling of node and edge labels. In particular, variable tagsets and label collections should be allowed. 5.2 Implementation As the need for certain functionalities becomes ob- vious with growing annotation experience, we have decided to implement the tool in two stages. In the first phase, the main functionality for building and displaying unordered trees is supplied. In the se- cond phase, secondary links and additional structu- ral functions are supported. The implementation of the first phase as described in the following para- graphs is completed. As keyboard input is more efficient than mouse input (cf. (Lehmann et al., 1996)) most effort has been put in developing an efficient keyboard inter- face. Menus are supported as a useful way of getting help on commands and labels. In addition to pure annotation, we can attach comments to structures. Figure 1 shows a screen dump of the tool. The largest part of the window contains the graphical re- presentation of the structure being amotated. The following commands are available: • group words and/or phrases to a new phrase; • ungroup a phrase; • change the name of a phrase or an edge; • re-attach a node; • generate the postscript output of a sentence. The three tagsets used by the annotation tool (for words, phrases, and edges) are variable and are stored together with the corpus. This allows easy modification if needed. The tool checks the appro- priateness of the input. For the implementation, we used Tcl/Tk Version 4.1. The corpus is stored in a SQL database. 5.3 Automation The degree of automation increases with the amount of data available. Sentences annotated in previous steps are used as training material for further pro- cessing. We distinguish five degrees of automation: 0) Completely manual annotation. 1) The user determines phrase boundaries and syntactic categories (S, NP, etc.). The program automatically assigns grammatical function la- bels. The annotator can alter the assigned tags. 2) The user only determines the components of a new phrase, the program determines its syntac- tic category and the grammatical functions of its elements. Again, the annotator has the op- tion of altering the assigned tags. 3) Additionally, the program performs simple bracketing, i.e., finds 'kernel' phrases. 4) The tagger suggests partial or complete parses. So far, about 1100 sentences of our corpus have been annotated. This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above). 5.4 Assigning Grammatical Function Labels Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf. e.g. (Cutting et al., 1992) and (Feldweg, 1995)). For a phrase Q with children of type T.,.,.Tk and grammatical functions G.,..., Gk, we use the lexical probabilities PQ(GTi) and the contextual (trigram) probabilities PQ(Ti|Ti-1, Ti-2) The lexical and contextual probabilities are deter- mined separately for each type of phrase. During annotation, the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure, i.e., we calculate k argmax I PQ(Ti|Ti-1, Ti-2)- PQ(GTi). G i=1 To keep the human annotator from missing errors made by the tagger, we additionally calculate the strongest competitor for each label G₁. If its pro- bability is close to the winner (closeness is defined by a threshold on the quotient), the assignment is regarded as unreliable, and the annotator is asked to confirm the assignment. For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%). The procedure was repeated 10 times with different partitionings. The tagger rates 90% of all assignments as reliable and carries them out fully automatically. Accuracy for these cases is 97%. Most errors are due to wrong identification of the subject and different kinds of objects in sentences and VPs. Accuracy of the unre- liable 10% of assignments is 75%, i.e., the annotator has to alter the choice in 1 of 4 cases when asked for confirmation. Overall accuracy of the tagger is 95%. Owing to the partial automation, the average an- notation efficiency improves by 25% (from around 4 minutes to 3 minutes per sentence). 6 Conclusion As the annotation scheme described in this paper fo- cusses on annotating argument structure rather than constituent trees, it differs from existing treebanks in several aspects. These differences can be illustrated by a comparison with the Penn Treebank annotation scheme. The following features of our formalism are then of particular importance: • simpler (i.e. 'flat') representation structures • complete absence of empty categories • no special mechanisms for handling disconti- nuous constituency The current tagset comprises only 16 node labels and 34 function tags, yet a finely grained classifica- tion will take place in the near future. We have argued that the selected approach is bet- ter suited for producing high quality interpreted cor- pora in languages exhibiting free constituent order. In general, the resulting interpreted data also are closer to semantic annotation and more neutral with respect to particular syntactic theories. As modern linguistics is also becoming more aware of the importance of larger sets of naturally occur- General: Corpus: RefCorpus Testkopie Editor: Thorsten Parser Ok Reload Exit Sentence: No.: 4 / 1269 Comment: Origin: refcorp.tt Last edited: Thorsten, 07/02/97, 17:39:29 Move: Prev Next Go to: -10 +10 -100 +100 Filter Mask... Matches: 0 Switching to sentence no. 4... Done. Figure 1: Screen dump of the annotation tool ring data, interpreted corpora are a valuable re- source for theoretical and descriptive linguistic re- search. In addition the approach provides empiri- cal material for psycholinguistic investigation, since preferences for the choice of certain syntactic con- structions, linearizations, and attachments that have been observed in online experiments of language pro- duction and comprehension can now be put in rela- tion with the frequency of these alternatives in larger amounts of texts. Syntactically annotated corpora of German have been missing until now. In the second phase of the project Verbmobil a treebank for 30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created. We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation. Since the combinatorics of syntactic constructions creates a demand for very large corpora, efficiency of annotation is an important criterion for the success of the developed methodology and tools. Our anno- tation tool supplies efficient manipulation and im- mediate visualization of argument structures. Par- tial automation included in the current version si- gnificantly reduces the manual effort. Its extension is subject to further investigations. 7 Acknowledgements This work is part of the DFG Sonderforschungs- bereich 378 Resource-Adaptive Cognitive Processes, Project C3 Concurrent Grammar Processing. We wish to thank Tania Avgustinova, Berthold Crysmann, Lars Konieczny, Stephan Oepen, Karel Oliva, Christian Weiß and two anonymous reviewers for their helpful comments on the content of this paper. We also wish to thank Robert MacIntyre and Ann Taylor for valuable discussions on the Penn Treebank annotation. Special thanks go to Oliver Plaehn, who implemented the annotation tool, and to our fearless annotators Roland Hendriks, Kerstin Klöckner, Thomas Schulz, and Bernd-Paul Simon. References Ann Bies et al. 1995. Bracketing Guidelines for Treebank II Style Penn Treebank Project. Techni- cal report, University of Pennsylvania. Ezra Black et al. 1996. Beyond Skeleton Par- sing: Producing a Comprehensive Large-Scale General-English Treebank With Full Grammati- cal Analysis. In The 16th International Confe- rence on Computational Linguistics, pages 107 — 113, Copenhagen, Denmark. Doug Cutting, Julian Kupiec, Jan Pedersen, and Pe- nelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the 3rd Conference on Applied Natural Language Processing (ACL), pa- ges 133-140. David Dowty. 1989. Towards a minimalist theory of syntactic structure. In Tilburg Conference on Discontinuous Constituency. Helmut Feldweg. 1995. Implementation and evalua- tion of a German HMM for POS disambiguation. In Proceedings of EACL-SIGDAT-95 Workshop, Dublin, Ireland. Richard Hudson. 1984. Word Grammar. Basil Blackwell Ltd. Fred Karlsson, Atro Voutilainen, Juha Heikkila, and Arto Anttila. 1995. Constraint Grammar. A Language-Independent System for Parsing Unre- stricted Text. Mouton de Gruyter, Berlin, New York. Kathol, Andreas and Carl Pollard. 1995. Extrapo- sition via Complex Domain Formation. In Pro- ceedings of the 33rd Annual Meeting of the ACL, pages 174-180, Cambridge, MA. Association for Computational Linguistics. Sabine Lehmann et al. 1996. TSNLP — Test Sui- tes for Natural Language Processing. In The 16th International Conference on Computational Lin- guistics, pages 711 — 717, Copenhagen, Denmark. Mitchell Marcus et al. 1994. The Penn Treebank: Annotating Predicate Argument Structure. In Proceedings of the Human Language Technology Workshop, San Francisco. Morgan Kaufmann. James McCawley. 1987. Some additional evidence for discontinuity. In Huck and Ojeda (eds.), Dis- continuous Constituency: Syntax and Semantics, pp 185-200. New York, Academic Press. Mike Reape. 1993. A Formal Theory of Word Or- der: A Case Study in West Germanic. Ph.D. the- sis, University of Edinburgh. Geoffrey Sampson. 1995. English for the Compu- ter. The SUSANNE Corpus and Analytic Scheme. Clarendon Press, Oxford. Atro Voutilainen. 1994. Designing a Parsing Gram- mar. University of Helsinki, Dept. of General Lin- guistics. Publications No. 22. PD Schade ADJD SB HD CP S SB SB PD HD NP NK NK RC S SB OA HD da\"s kein Arzt $. KOUS PIAT NN anwesend ADJD ist VAFIN der sich auskennt $. PRELS PAF VVFIN $. Figure 2: Headed and non-headed structures, extraposition OC CVP VP SBP HD MO PP AC NK NK VP OD NF NK NK NK HD Sie wurde PPER VAFIN APPR von preu'sische Truppen besetzt und 1887 dem preu\"sischen Staatsverband angegliedert ADJA NN VVPP KON CARD ART ADJA NN WPP $. Figure 3: Coordination S SB HD DA OC VP HD SB VZ PM HD Er PPER bat mich zu kommen VVFIN PPER PTKZU VVINF $. Figure 4: Equi construction"
  },
  {
    "title": "A Novel Use of Statistical Parsing to Extract Information from Text",
    "abstract": "Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",
    "content": "1 Introduction Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard. Yet, relatively few have embedded one of these algorithms in a task. Chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition. In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction. The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998. Several technical challenges confronted us and were solved: • How could the limited semantic interpretation required in information extraction be integrated into the statistical learning algorithm? We were able to integrate both syntactic and semantic information into the parsing process, thus avoiding potential errors of syntax first followed by semantics. • Would TREEBANKing of the variety of news sources in MUC-7 be required? Or could the University of Pennsylvania's TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire, which includes dozens of newspapers? Manually creating source- specific training data for syntax was not required. Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation. • Would semantic annotation require computational linguists? We were able to specify relatively simple guidelines that students with no training in computational linguistics could annotate. 2 Information Extraction Tasks We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998). The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts). For each organization in an article, one must identify all of its names as used in the article, its type (corporation, government, or other), and any significant description of it. For each person, one must find all of the person's names within the document, his/her type (civilian or military), and any significant descriptions (e.g., titles). For each location, one must also give its type (city, province, county, body of water, etc.). For the following example, the template element in. Figure 1 was to be generated: \"...according to the report by Edwin Dorn, under secretary of defense for personnel and readiness. ...Dorn's conclusion that Washington...\" <ENTITY-9601020516-13> := ENT_NAME: \"Edwin Dorn\" \"Dorn\" ENT_TYPE: PERSON ENT_DESCRIPTOR: \"under secretary of defense for personnel and readiness\" ENT_CATEGORY: PER_CIV Figure 1: An example of the information to be extracted for TE. The Template Relations (TR) task involves identifying instances of three relations in the text: • the products made by each company • the employees of each organization, • the (headquarters) location of each organization. TR builds on TE in that TR reports binary relations between elements of TE. For the following example, the template relation in Figure 2 was to be generated: \"Donald M. Goldstein, a historian at the University of Pittsburgh who helped write...\" <EMPLOYEE_OF-9601020516-5> := PERSON: <ENTITY-9601020516-18> ORGANIZATION: <ENTITY- 9601020516-9> <ENTITY-9601020516-9> := ENT_NAME: \"University of Pittsburgh\" ENT_TYPE: ORGANIZATION ENT_CATEGORY: ORG_CO <ENTITY-9601020516-18> := ENT_NAME: \"Donald M. Goldstein\" ENT_TYPE: PERSON ENT_DESCRIPTOR: \"a historian at the University of Pittsburgh\" Figure 2: An example of information to be extracted for TR 3 Integrated Sentential Processing Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones. Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: 1. part-of-speech tagging 2. name finding 3. syntactic analysis, often limited to noun and verb group chunking 4. semantic interpretation, usually based on pattern matching Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility. However, pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline. For example, an error made during part-of-speech-tagging may cause a future error in syntactic analysis, which may in turn cause a semantic interpretation failure. There is no opportunity for a later stage, such as parsing, to influence or correct an earlier stage such as part-of-speech tagging. An integrated model can limit the propagation of errors by making all decisions jointly. For this reason, we focused on designing an integrated model in which tagging, name- finding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other. A second consideration influenced our decision toward an integrated model. We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al. 1997). Finally, our newly constructed parser, like that of (Collins 1997), was based on a generative statistical model. Thus, each component of what would be the first three stages of our pipeline was based on the same general class of statistical model. Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model. If the single generalized model could then be extended to semantic analysis, all necessary sentence level processing would be contained in that model. Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties - especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs - would also benefit semantic analysis. 4 Representing Syntax and Semantics Jointly Our integrated model represents syntax and semantics jointly using augmented parse trees. In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations. An example of an augmented parse tree is shown in Figure 3. The five key facts in this example are: • \"Nance\" is the name of a person. • \"A paid consultant to ABC News\" describes a person. • \"ABC News\" is the name of an organization. • The person described as \"a paid consultant to ABC News\" is employed by ABC News. • The person named \"Nance\" and the person described as \"a paid consultant to ABC News\" are the same person. Here, each \"reportable\" name or description is identified by a \"-r\" suffix attached to its semantic label. For example, \"per-r\" identifies \"Nance\" as a named person, and \"per-desc-r\" identifies \"a paid consultant to ABC News\" as a person description. Other labels indicate relations among entities. For example, the co- reference relation between \"Nance\" and \"a paid consultant to ABC News\" is indicated by \"per-desc-of.\" In this case, because the argument does not connect directly to the relation, the intervening nodes are labeled with semantics \"-ptr\" to indicate the connection. Further details are discussed in the section Tree Augmentation. 5 Creating the Training Data To train our integrated model, we required a large corpus of augmented parse trees. Since it was known that the MUC-7 evaluation data would be drawn from a variety of newswire sources, and that the articles would focus on rocket launches, it was important that our training corpus be drawn from similar sources and that it cover similar events. Thus, we did not consider simply adding semantic labels to the existing Penn TREEBANK, which is drawn from a single source - the Wall Street Journal - and is impoverished in articles about rocket launches. Instead, we applied an information retrieval system to select a large number of articles from the desired sources, yielding a corpus rich in the desired types of events. The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus. Initially, we tried to annotate the training corpus by hand marking, for each sentence, the entire augmented tree. It soon became painfully obvious that this task could not be performed in the available time. Our annotation staff found syntactic analysis particularly complex and slow going. By necessity, we adopted the strategy of hand marking only the semantics. Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed. To produce a corpus of augmented parse trees, we used the following multi-step training procedure which exploited the Penn TREEBANK S per/np per-desc-of/sbar-Ink per-desc-ptr/sbar per-desc-ptr/vp vp per-desc-r/np emp-of/pp-Ink org-ptr/pp per-r/np whnp advp per-desc/np org-r/np per/nnp wp vbz rb det vbn per-desc/nn to org'/nnp org/nnp vbd Nance , who is also a paid consultant to ABC News said , Figure 3: An example of an augmented parse tree. 1. The model (see Section 7) was first trained on purely syntactic parse trees from the TREEBANK, producing a model capable of broad-coverage syntactic parsing. 2. Next, for each sentence in the semantically annotated corpus: a. The model was applied to parse the sentence, constrained to produce only parses that were consistent with the semantic annotation. A parse was considered consistent if no syntactic constituents crossed an annotated entity or description boundary. b. The resulting parse tree was then augmented to reflect semantic structure in addition to syntactic structure. coreference employee relation person-descriptor person organization Nance , who is also a paid consultant to ABC News , Figure 4: An example of semantic annotation. said Applying this procedure yielded a new version of the semantically annotated corpus, now annotated with complete augmented trees like that in Figure 3. 6 Tree Augmentation In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machine- generated syntactic parse trees. For each sentence, combining these two sources involved five steps. These steps are given below: Tree Augmentation Algorithm 1. Nodes are inserted into the parse tree to distinguish names and descriptors that are not bracketed in the parse. For example, the parser produces a single noun phrase with no internal structure for \"Lt. Cmdr. David Edwin Lewis\". Additional nodes must be inserted to distinguish the description, \"Lt. Cmdr.,\" and the name, \"David Edwin Lewis.\" 2. Semantic labels are attached to all nodes that correspond to names or descriptors. These labels reflect the entity type, such as person, organization, or location, as well as whether the node is a proper name or a descriptor. 3. For relations between entities, where one entity is not a syntactic modifier of the other, the lowermost parse node that spans both entities is identified. A semantic tag is then added to that node denoting the relationship. For example, in the sentence \"Mary Fackler Schiavo is the inspector general of the U.S. Department of Transportation,\" a co-reference semantic label is added to the S node spanning the name, \"Mary Fackler Schiavo,\" and the descriptor, \"the inspector general of the U.S. Department of Transportation.\" 4. Nodes are inserted into the parse tree to distinguish the arguments to each relation. In cases where there is a relation between two entities, and one of the entities is a syntactic modifier of the other, the inserted node serves to indicate the relation as well as the argument. For example, in the phrase \"Lt. Cmdr. David Edwin Lewis,\" a node is inserted to indicate that \"Lt. Cmdr.\" is a descriptor for \"David Edwin Lewis.\" 5. Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes. These labels serve to form a continuous chain between the relation and its argument. 7 Model Structure In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997). The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process. For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward. Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created. Word features are introduced primarily to help with unknown words, as in (Weischedel et al. 1993). We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3. At each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection dependent on particular features of previously generated elements. We pick up the derivation just after the topmost S and its head word, said, have been produced. The next steps are to generate in order: 1. A head constituent for the S, in this case a VP. 2. Pre-modifier constituents for the S. In this case, there is only one: a PER/NP. 3. A head part-of-speech tag for the PER/NP, in this case PER/NNP. 4. A head word for the PER/NP, in this case nance. 5. Word features for the head word of the PER/NP, in this case capitalized. 6. A head constituent for the PER/NP, in this case a PER-R/NP. 7. Pre-modifier constituents for the PER/NP. In this case, there are none. 8. Post-modifier constituents for the PER/NP. First a comma, then an SBAR structure, and then a second comma are each generated in turn. This generation process is continued until the entire tree has been produced. We now briefly summarize the probability structure of the model. The categories for head constituents, $c_h$, are predicted based solely on the category of the parent node, $c_p$: $P(c_h | c_p)$, e.g. $P(vp|s)$ Modifier constituent categories, $c_m$, are predicted based on their parent node, $c_p$, the head constituent of their parent node, $c_{hp}$, the previously generated modifier, $c_{m-1}$, and the head word of their parent, $w_p$. Separate probabilities are maintained for left (pre) and right (post) modifiers: $P_L(c_m | c_p, c_{hp}, c_{m-1}, w_p)$, e.g. $P_L(per | np | s, vp, null, said)$ $P_R(c_m | c_p, c_{hp}, c_{m-1}, w_p)$, e.g. $P_R(null | s, vp, null, said)$ Part-of-speech tags, $t_m$, for modifiers are predicted based on the modifier, $c_m$, the part- of-speech tag of the head word, $t_h$, and the head word itself, $w_h$: $P(t_m | c_m, t_h, w_h)$, e.g. $P(per | nnp | per / np, vbd, said)$ Head words, $w_m$, for modifiers are predicted based on the modifier, $c_m$, the part-of-speech tag of the modifier word, $t_m$, the part-of- speech tag of the head word, $t_h$, and the head word itself, $w_h$: $P(w_m | c_m, t_m, t_h, w_h)$, e.g. $P(nance | per / np, per / nnp, vbd, said)$ Finally, word features, $f_m$, for modifiers are predicted based on the modifier, $c_m$, the part- of-speech tag of the modifier word, $t_m$, the part-of-speech tag of the head word, $t_h$, the head word itself, $w_h$, and whether or not the modifier head word, $w_m$, is known or unknown. $P(f_m | c_m, t_m, t_h, w_h, known(w_m))$, e.g. $P(cap | per / np, per / nnp, vbd, said, true)$ The probability of a complete tree is the product of the probabilities of generating each element in the tree. If we generalize the tree components (constituent labels, words, tags, etc.) and treat them all as simply elements, $e$, and treat all the conditioning factors as the history, $h$, we can write: $P(tree) = \\prod_{e \\in tree} P(e|h)$ 8 Training the Model Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus. However, because these estimates are too sparse to be relied upon, we use interpolated estimates consisting of mixtures of successively lower- order estimates (as in Placeway et al. 1993). For modifier constituents, the mixture components are: $P'(c_m | c_p, c_{hp}, c_{m-1}, w_p) = $ $\\lambda_1 P(c_m | c_p, c_{hp}, c_{m-1}, w_p)$ $+$ $\\lambda_2 P(c_m | c_p, c_{hp}, c_{m-1})$ For part-of-speech tags, the mixture components are: $P'(t_m | c_m, t_h, w_h) = \\lambda_1 P(t_m | c_m, w_h)$ $+$ $\\lambda_2 P(t_m | c_m, t_h)$ $+$ $\\lambda_3 P(t_m | c_m)$ For head words, the mixture components are: $P'(w_m | c_m, t_m, t_h, w_h) = \\lambda_1 P(w_m | c_m, t_m, w_h)$ $+$ $\\lambda_2 P(w_m | c_m, t_m, t_h)$ $+$ $\\lambda_3 P(w_m | c_m, t_m)$ $+$ $\\lambda_4 P(w_m | t_m)$ Finally, for word features, the mixture components are: P'(fm|Cmstmoth, w, known(wm)) = λ₁ P(fm|Cm,t,wh, known(wm)) +λ₂ P(fm|Cm,t,mth, known(wm)) +λ₃ P(fm|Cm,t, known(wm)) +λ₄ P(fm|m, known(wm)) 9 Searching the Model Given a sentence to be analyzed, the search program must find the most likely semantic and syntactic interpretation. More precisely, it must find the most likely augmented parse tree. Although mathematically the model predicts tree elements in a top-down fashion, we search the space bottom-up using a chart- based search. The search is kept tractable through a combination of CKY-style dynamic programming and pruning of low probability elements. 9.1 Dynamic Programming Whenever two or more constituents are equivalent relative to all possible later parsing decisions, we apply dynamic programming, keeping only the most likely constituent in the chart. Two constituents are considered equivalent if: 1. They have identical category labels. 2. Their head constituents have identical labels. 3. They have the same head word. 4. Their leftmost modifiers have identical labels. 5. Their rightmost modifiers have identical labels. 9.2 Pruning Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a threshold of the highest scoring constituent are maintained; all others are pruned. For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman, 1997). We can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node. Thus, the scores used in pruning can be considered as the product of: 1. The probability of generating a constituent of the specified category, starting at the topmost node. 2. The probability of generating the structure beneath that constituent, having already generated a constituent of that category. Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence. The semantics - that is, the entities and relations - can then be directly extracted from these sentential trees. 10 Experimental Results Our system for MUC-7 consisted of the sentential model described in this paper, coupled with a simple probability model for cross-sentence merging. The evaluation results are summarized in Table 1. In both Template Entity (TE) and Template Relation (TR), our system finished in second place among all entrants. Nearly all of the work was done by the sentential model; disabling the cross-sentence model entirely reduced our overall F-Score by only 2 points. Task Recall Precision F-Score Entities (TE) 83% 84% 83.49% Relations (TR) 64% 81% 71.23% Table 1: MUC-7 scores. Task Part-of-Speech Tagging Parsing (sentences < 40 words) Name Finding Score 95.99 (% correct) 85.06 (F-Score) 92.28 (F-Score) Table 2: Component task performance. While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding. We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97), and evaluated name finding accuracy on the MUC- 7 named entity test. The results are summarized in Table 2. While performance did not quite match the best previously reported results for any of these three tasks, we were pleased to observe that the scores were at or near state-of-the-art levels for all cases. 11 Conclusions We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFG- HR) can be used effectively for information extraction. A single model proved capable of performing all necessary sentential processing, both syntactic and semantic. We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required. The semantic training corpus was produced by students according to a simple set of guidelines. This simple semantic annotation was the only source of task knowledge used to configure the model. Acknowledgements The work reported here was supported in part by the Defense Advanced Research Projects Agency. Technical agents for part of this work were Fort Huachuca and AFRL under contract numbers DABT63-94-C-0062, F30602-97-C-0096, and 4132-BBN-001. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government. We thank Michael Collins of the University of Pennsylvania for his valuable suggestions. References Bikel, Dan; S. Miller; R. Schwartz; and R. Weischedel. (1997) \"NYMBLE: A High- Performance Learning Name-finder.\" In Proceedings of the Fifth Conference on Applied Natural Language Processing, Association for Computational Linguistics, pp. 194-201. Collins, Michael. (1996) \"A New Statistical Parser Based on Bigram Lexical Dependencies.\" In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pp. 184-191. Collins, Michael. (1997) \"Three Generative, Lexicalised Models for Statistical Parsing.\" In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pp. 16-23. Marcus, M.; B. Santorini; and M. Marcinkiewicz. (1993) \"Building a Large Annotated Corpus of English: the Penn Treebank.\" Computational Linguistics, 19(2):313-330. Goodman, Joshua. (1997) \"Global Thresholding and Multiple-Pass Parsing.\" In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, pp. 11-25. Placeway, P., R. Schwartz, et al. (1993). \"The Estimation of Powerful Language Models from Small and Large Corpora.\" IEEE ICASSP Weischedel, Ralph; Marie Meteer; Richard Schwartz; Lance Ramshaw; and Jeff Palmucci. (1993) \"Coping with Ambiguity and Unknown Words through Probabilistic Models.\" Computational Linguistics, 19(2):359-382."
  },
  {
    "title": "Learning to Predict Problematic Situations in a Spoken Dialogue System: Experiments with How May I Help You?",
    "abstract": "Current spoken dialogue systems are deficient in their strategies for preventing, identifying and repairing problems that arise in the conversation. This paper reports results on learning to automatically identify and predict problematic human-computer dialogues in a corpus of 4774 dialogues collected with the How May I Help You spoken dialogue system. Our expectation is that the ability to predict problematic dialogues will allow the system's dialogue manager to modify its behavior to repair problems, and even perhaps, to prevent them. We train a problematic dialogue classifier using automatically obtainable features that can identify problematic dialogues significantly better (23%) than the baseline. A classifier trained with only automatic features from the first exchange in the dialogue can predict problematic dialogues 7% more accurately than the baseline, and one trained with automatic features from the first two exchanges can perform 14% better than the baseline.",
    "content": "1 Introduction Spoken dialogue systems promise efficient and nat- ural access to a large variety of information sources and services from any phone. Systems that sup- port short utterances to select a particular function (through a statement such as \"Say credit card, col- lect or person-to-person\") are saving companies mil- lions of dollars. Research prototypes exist for appli- cations such as personal email and calendars, travel and restaurant information, and personal banking (Baggia et al., 1998; Walker et al., 1998; Seneff et al., 1995; Sanderman et al., 1998; Chu-Carroll and Carpenter, 1999) inter alia. Yet there are still many research challenges: current systems are limited in the interaction they support and brittle in many re- spects. We show how spoken dialogue systems can learn to support more natural interaction on the ba- sis of their previous experience. One way that current spoken dialogue systems are quite limited is in their strategies for detecting and repairing problems that arise in conversation. If a problem can be detected, the system can either transfer the call to a human operator or modify its dialogue strategy in an attempt to repair the prob- lem. We can train systems to improve their ability to detect problems by exploiting dialogues collected in interactions with human users. The initial segments of these dialogues can be used to predict that a prob- lem is likely to occur. We expect that the ability to predict that a dialogue is likely to be problematic will allow the system's dialogue manager to apply more sophisticated strategies to repairing problems, and even perhaps, to prevent them. This paper reports experiments on predicting problems in spoken dialogue interaction by train- ing a problematic dialogue predictor on a corpus of 4774 dialogues collected in an experimental trial of AT&T's How May I Help You (HMIHY) spoken dia- logue system (Gorin et al., 1997; Riccardi and Gorin, to appear; E. Ammicht and Alonso, 1999). In this trial, the HMIHY system was installed at an AT&T customer care center. HMIHY answered calls from live customer traffic and successfully automated a large number of customer requests. An example of a dialogue that HMIHY completed successfully is shown in Figure 1.1 S1: AT&T How may I help you? U1: I need to [ uh) put a call on my calling card please S2: May I have your card number, please? U2: 76543210987654 S3: What number would you like to call? U3:8147776666 (misrecognized) S4: May I have that number again? U4:8147776666 S5: Thank you. Figure 1: Sample TASKSUCCESS Dialogue We shall refer to the dialogues with a TASKSUC- CESS outcome, in which HMIHY successfully auto- mates the customer's call, as the TASKSUCCESS dia- logues. In addition to the TASKSUCCESS dialogues, there are several other call outcomes, to be described in detail below, which we consider problematic. The phone numbers, card numbers, and pin numbers in the sample dialogues are artificial. This paper reports results from a set of experi- ments that test whether we can learn to automat- ically predict that a dialogue will be problematic on the basis of information the system has: (1) early on in the dialogue; and (2) in real time. We train an automatic classifer for predicting problem- atic dialogues from features that can be automat- ically extracted from the HMIHY corpus. The re- sults show that we can learn to predict problematic dialogues using fully automatic features with an ac- curacy ranging from 72% to 87%, depending on how much of the dialogue the system has seen so far. Section 2 describes HMIHY and the dialogue corpus which the experiments are based on. Section 3 de- tails the encoding of the dialogues and the methods used for utilizing the machine learning program RIP- PER to train an automatic problematic dialogue pre- dictor (Cohen, 1996). Section 4 presents the results and section 5 summarizes the paper and describes future work. 2 Experimental System and Data HMIHY is a spoken dialogue system based on the no- tion of call routing (Gorin et al., 1997; Chu-Carroll and Carpenter, 1999). In the HMIHY call rout- ing system, services that the user can access are classified into 14 categories, plus a category called other for calls that cannot be automated and must be transferred to a human operator (Gorin et al., 1997).2 Each category describes a different task, such as person-to-person dialing, or receiving credit for a misdialed number. The system determines which task the caller is requesting on the basis of its understanding of the caller's response to the open-ended system greeting AT&T, How May I Help You?. Once the task has been determined, the infor- mation needed for completing the caller's request is obtained using dialogue submodules that are specific for each task (Abella and Gorin, 1999). In addition to the TASKSUCCESS dialogues in which HMIHY successfully automates the cus- tomer's call, illustrated in Figure 1, and the calls that are transferred to a human operator, there are three other possible outcomes for a call, all of which are problematic. The first category, which we call HANGUP, results from a customer's decision to hang up on the system. A sample HANGUP dialogue is in Figure 2. A caller may hang up because s/he is frustrated with the system; our goal is to learn from the corpus which system behaviors led to the caller's frustration. The second problematic category (WIZARD), re- sults from a human customer care agent's decision to take over the call from the system. Because HMIHY is experimental, each call was monitored by a human 2 One potential use of our results is to provide better rules for making the transfer decision. S1: AT&T How may I help you? U1: I need credit please. S2: Okay. What is the reason for the credit? U2: Miss, uh, Miss, different area code than I needed. S3: Sorry. I need to know whether you need credit for a wrong number, bad connection or a call that was cut off. U3: It is a wrong number. (impatient humming) S4: What was the number that you dialed? USER HANGS UP Figure 2: Sample HANGUP Dialogue agent serving as a wizard who could override the system. There were a number of agents who partici- pated as wizards during the trial of HMIHY and each wizard was simply told to take over the call if s/he perceived problems with the system's performance. The wizard's decision was logged by the experimen- tal setup, resulting in labelling the call as one that the wizard took over. A dialogue where the wizard decided that the dialogue was problematic and took over the call is shown in Figure 3. S1: AT&T How may I help you? Ul: (silence) S2: Sorry. Please briefly tell me how I may help you? U2: I'm trying to call 18 hundred call ATT. S3: Do you want to place a call? U3: Through my calling card. S4: May I have your card number, please? U4: It's 8 7 6543210 and then my pin number is 8 765. (misrecognized) S5: Sorry. Please enter or say your card number again. WIZARD STEPS IN Figure 3: Sample WIZARD Dialogue The third problematic category, the TASKFAILURE dialogues, are cases where the system completed the call, but carried out a task that was not the task that the customer was actually requesting. An ex- ample TASKFAILURE dialogue is given in Figure 4: HMIHY interpreted utterance U2 as a request to make a third-party call e.g. to bill it to my home phone. HMIHY then asked the caller for the infor- mation it needed to carry out this task, the caller complied, and the system completed the call. The corpus of 4774 dialogues used in our exper- iments was collected in several experimental trials of HMIHY on live customer traffic (Riccardi and Gorin, to appear; E. Ammicht and Alonso, 1999), and is referred to as HM2 in (Riccardi and Gorin, to appear)). During the trial, all of the system behav- iors were automatically recorded in a log file, and later the dialogues were transcribed by humans and labelled with a semantic category representing the 3 Of course we can only infer what might have motivated the wizard to take over the call, but we assume that the wizard had good reason for doing so. S1: AT&T How may I help you? Ul: (silence) S2: Sorry. Please briefly tell me how I may help you? U2: I wanta find out how much my bill's worth. S3: What phone number would you like the call billed to? U3: Um the access number. S4: Sorry. Could I have that phone number again? U4: 18 hundred 2255288. S5: What number would you like to call? U5: 800 225 5288 (touchtone input) S6: Thank you. Figure 4: Sample TASKFAILURE Dialogue task that the caller was asking HMIHY to perform, on a per utterance basis. The logfiles also included labels indicating whether the wizard had taken over the call or the user had hung up. 3 Training an Automatic Problematic Dialogue Predictor Our experiments apply the machine learning pro- gram RIPPER (Cohen, 1996) to automatically induce a \"problematic dialogue\" classification model. RIP- PER takes as input the names of a set of classes to be learned, the names and ranges of values of a fixed set of features, and training data specifying the class and feature values for each example in a training set. Its output is a classification model for predicting the class of future examples. In RIPPER, the classifica- tion model is learned using greedy search guided by an information gain metric, and is expressed as an ordered set of if-then rules. To apply RIPPER, the dialogues in the corpus must be encoded in terms of a set of classes (the output classification) and a set of input features that are used as predictors for the classes. We start with the dialogue categories described above, but since our goal is to develop algorithms that predict/identify problematic dialogues, we treat HANGUP, WIZARD and TASKFAILURE as equivalently problematic. Thus we train the classifier to distinguish between two classes: TASKSUCCESS and PROBLEMATIC. Note that our categorization is inherently noisy because we do not know the real reasons why a caller hangs up or a wizard takes over the call. The caller may hang up because she is frustrated with the system, or she may simply dislike automation, or her child may have started crying. Similarly, one wizard may have low confidence in the system's ability to recover from er- rors and use a conservative approach that results in taking over many calls, while another wizard may be more willing to let the system try to recover. Nev- ertheless we take these human actions as a human labelling of these calls as problematic. Given this classification, approximately 36% of the calls in the corpus of 4774 dialogues are PROBLEMATIC and 64% are TASKSUCCESS. Next, we encoded each dialogue in terms of a set of 196 features that were either automatically logged by one of the system modules, hand-labelled by hu- mans, or derived from raw features. We use the hand-labelled features to produce a TOPLINE, an es- timation of how well a classifier could do that had access to perfect information. The entire feature set is summarized in Figure 5. • Acoustic/ASR Features — recog, recog-numwords, ASR-duration, dtmf- flag, rg-modality, rg-grammar • NLU Features — a confidence measure for all of the possible tasks that the user could be trying to do — salience-coverage, inconsistency, context-shift, top-task, nexttop-task, top-confidence, diff- confidence • Dialogue Manager Features — sys-label, utt-id, prompt, reprompt, confirma- tion, subdial — running tallies: num-reprompts, num- confirms, num-subdials, reprompt%, confir- mation%, subdialogue% • Hand-Labelled Features — tscript, human-label, age, gender, user- modality, clean-tscript, cltscript-numwords, rsuccess • Whole-Dialogue Features — num-utts, num-reprompts, percent-reprompts, num-confirms, percent-confirms, num- subdials, percent-subdials, dial-duration. Figure 5: Features for spoken dialogues. There are 8 features that describe the whole dia- logue, and 47 features for each of the first four ex- changes. We encode features for the first four ex- changes because we want to predict failures before they happen. Since 97% of the dialogues in our cor- pus are five exchanges or less, in most cases, any potential problematic outcome will have occurred by the time the system has participated in five ex- changes. Because the system needs to be able to predict whether the dialogue will be problematic us- ing information it has available in the initial part of the dialogue, we train classifiers that only have ac- cess to input features from exchange 1, or only the features from exchange 1 and exchange 2. To see whether our results generalize, we also experiment with a subset of features that are task-independent. We compare results for predicting problematic dia- logues, with results for identifying problematic di- alogues, when the classifier has access to features representing the whole dialogue. We utilized features logged by the system because they are produced automatically, and thus could be used during runtime to alter the course of the dia- logue. The system modules that we collected infor- mation from were the acoustic processer/automatic speech recognizer (ASR) (Riccardi and Gorin, to ap- pear), the natural language understanding (NLU) module (Gorin et al., 1997), and the dialogue man- ager (DM) (Abella and Gorin, 1999). Below we de- scribe each module and the features obtained from it. ASR takes as input the acoustic signal and outputs a potentially errorful transcription of what it believes the caller said. The ASR features for each of the first four exchanges were the output of the speech recognizer (recog), the number of words in the recognizer output (recog-numwords), the duration in seconds of the input to the recognizer (asr-duration), a flag for touchtone input (dtmf-flag), the input modality expected by the recognizer (rg-modality) (one of: none, speech, touchtone, speech+touchtone, touchtone- card, speech+touchtone-card, touchtone-date, speech+touchtone-date, or none-final-prompt), and the grammar used by the recognizer (rg-grammar). The motivation for the ASR features is that any one of them may have impacted performance. For example, it is well known that longer utterances are less likely to be recognized correctly, thus asr- duration could be a clue to incorrect recognition re- sults. In addition, the larger the grammar is, the more likely an ASR error is, so the name of the grammar rg-grammar could be a predictor of incor- rect recognition. The natural language understanding (NLU) mod- ule takes as input a transcription of the user's utter- ance from ASR and produces 15 confidence scores representing the likelihood that the caller's task is one of the 15 task types. It also extracts other relevant information, such as phone or credit card numbers. Thus 15 of the NLU features for each ex- change represent the 15 confidence scores. There are also features that the NLU module calculates based on processing the utterance. These include an intra-utterance measure of the inconsistency be- tween tasks that the user appears to be requesting (inconsistency), a measure of the coverage of the utterance by salient grammar fragments (salience- coverage), a measure of the shift in context between utterances (context-shift), the task with the highest confidence score (top-task), the task with the second highest confidence score (nexttop-task), the value of the highest confidence score (top-confidence), and the difference in values between the top and next- to-top confidence scores (diff-confidence). The motivation for these NLU features is to make use of information that the NLU module has based on processing the output of ASR and the current dis- course context. For example, for utterances that fol- low the first utterance, the NLU module knows what task it believes the caller is trying to complete. If it appears that the caller has changed her mind, then the NLU module may have misunderstood a previ- ous utterance. The context-shift feature indicates the NLU module's belief that it may have made an error (or be making one now). The dialogue manager (DM) takes the output of NLU and the dialogue history and decides what it should say to the caller next. It decides whether it believes there is a single unambiguous task that the user is trying to accomplish, and how to resolve any ambiguity. The DM features for each of the first four exchanges are the task-type label which includes a label that indicates task ambiguity (sys-label), utter- ance id within the dialogue (implicit in the encod- ing), the name of the prompt played before the user utterance (prompt), and whether that prompt was a reprompt (reprompt), a confirmation (confirm), or a subdialogue prompt (subdial), a superset of the re- prompts and confirmation prompts. The DM features are primarily motivated by pre- vious work. The task-type label feature is to cap- ture the fact that some tasks may be harder than others. The utterance id feature is motivated by the idea that the length of the dialogue may be impor- tant, possibly in combination with other features like task-type. The different prompt features for initial prompts, reprompts, confirmation prompts and sub- dialogue prompts are motivated by results indicating that reprompts and confirmation prompts are frus- trating for callers and that callers are likely to hy- perarticulate when they have to repeat themselves, which results in ASR errors (Shriberg et al., 1992; Levow, 1998). The DM features also include running tallies for the number of reprompts (num-reprompts), number of confirmation prompts (num-confirms), and num- ber of subdialogue prompts (num-subdials), that had been played up to each point in the dialogue, as well as running percentages (percent-reprompts, percent- confirms, percent-subdials). The use of running tal- lies and percentages is based on the assumption that these features are likely to produce generalized pre- dictors (Litman et al., 1999). The features obtained via hand-labelling were hu- man transcripts of each user utterance (tscript), a set of semantic labels that are closely related to the system task-type labels (human-label), age (age) and gender (gender) of the user, the actual modality of the user utterance (user-modality) (one of: nothing, speech, touchtone, speech+touchtone, non-speech), and a cleaned transcript with non-word noise infor- mation removed (clean-tscript). From these features we calculated two derived features. The first was the number of words in the cleaned transcript (cltscript numwords), again on the assumption that utterance length is strongly correlated with ASR and NLU er- rors. The second derived feature was based on cal- culating whether the human-label matches the sys- label from the dialogue manager (rsuccess). There were four values for rsuccess: rcorrect, rmismatch, rpartial-match and rvacuous-match, indicating re- spectively correct understanding, incorrect under- standing, partial understanding, and the fact that there had been no input for ASR and NLU to oper- ate on, either because the user didn't say anything or because she used touch-tone. The whole-dialogue features derived from the per- utterance features were: num-utts, num-reprompts, percent-reprompts, num-confirms, percent-confirms, num-subdials, and per-cent-subdials for the whole di- alogue, and the duration of the entire dialogue in seconds (dial-duration). In the experiments, the features in Figure 5 except the Hand-Labelled features are referred to as the AU- TOMATIC feature set. We examine how well we can identify or predict problematic dialogues using these features, compared to the full feature set including the Hand-Labelled features. As mentioned earlier, we wish to generalize our problematic dialogue pre- dictor to other systems. Thus we also discuss how well we can predict problematic dialogues using only features that are both automatically acquirable dur- ing runtime and independent of the HMIHY task. The subset of features from Figure 5 that fit this qualification are in Figure 6. We refer to them as the AUTO, TASK-INDEP feature set. The output of each RIPPER experiment is a clas- sification model learned from the training data. To evaluate these results, the error rates of the learned classification models are estimated using the resam- pling method of cross-validation. In 5-fold cross- validation, the total set of examples is randomly di- vided into 5 disjoint test sets, and 5 runs of the learn- ing program are performed. Thus, each run uses the examples not in the test set for training and the re- maining examples for testing. An estimated error rate is obtained by averaging the error rate on the testing portion of the data from each of the 5 runs. Since we intend to integrate the rules learned by RIPPER into the HMIHY system, we examine the precision and recall performance of specific hy- potheses. Because hypotheses from different cross- validation experiments cannot readily be combined together, we apply the hypothesis learned on one randomly selected training set (80% of the data) to that set's respective test data. Thus the precision and recall results reported below are somewhat less • Acoustic/ASR Features recog, recog-numwords, ASR-duration, dtmf- flag, rg-modality • NLU Features salience-coverage, inconsistency, context-shift, top-confidence, diff-confidence • Dialogue Manager Features num- utt-id, reprompt, confirmation, subdial running tallies: num-reprompts, confirms, num-subdials, reprompt%, confir- mation%, subdialogue% Figure 6: Automatic task independent (AUTO, TASK-INDEP) features available at runtime. reliable than the error rates from cross-validation. 4 Results We present results for both predicting and identi- fying problematic dialogues. Because we are inter- ested in predicting that a dialogue will be problem- atic at a point in the dialogue where the system can do something about it, we compare prediction ac- curacy after having only seen the first exchange of the dialogue with prediction accuracy after having seen the first two exchanges, with identification ac- curacy after having seen the whole dialogue. For each of these situations we also compare results for the AUTOMATIC and AUTO, TASK-INDEP feature sets (as described earlier), with results for the whole fea- ture set including hand-labelled features. Table 1 summarizes the results. The baseline on the first line of Table 1 repre- sents the prediction accuracy from always guess- ing the majority class. Since 64% of the dialogues are TASKSUCCESS dialogues, we can achieve 64% ac- curacy from simply guessing TASKSUCCESS without having seen any of the dialogue yet. The first EXCHANGE 1 row shows the results of using the AUTOMATIC features from only the first exchange to predict whether the dialogue outcome will be TASKSUCCESS or PROBLEMATIC. The results show that the machine-learned classifier can predicts problematic dialogues 8% better than the baseline after having seen only the first user utterance. Using only task-independent automatic features (Figure 6) the EXCHANGE 1 classifier can still do nearly as well. The ALL row for EXCHANGE 1 indicates that even if we had access to human perceptual ability (the hand-labelled features) we would still only be able to distinguish between TASKSUCCESS and PROBLEM- ATIC dialogues with 77% accuracy after having seen the first exchange. Features Used Accuracy (SE) BASELINE (majority class) 64.0% 72.3% 1.04% EXCHANGE 1 AUTOMATIC AUTO, TASK-INDEP 71.6% 1.05% ALL 77.0% 0.56% EXCHANGES 1&2 AUTOMATIC 79.9% 0.58% AUTO, TASK-INDEP 78.6% 0.37% ALL 86.7% 0.33% FULL DIALOGUE AUTOMATIC 87.0% 0.72% AUTO, TASK-INDEP 86.7% 0.82% TOPLINE ALL 92.3% 0.72% Table 1: Results for predicting and identifying problematic dialogues (SE = Standard Error) The EXCHANGE 1&2 rows of Table 1 show the re- sults using features from the first two exchanges in the dialogue to predict the outcome of the dialogue.4 The additional exchange gives roughly an additional 7% boost in predictive accuracy using either of the AUTOMATIC feature sets. This is only 8% less than the accuracy we can achieve using these features af- ter having seen the whole dialogue (see below). The ALL row for EXCHANGE 1&2 shows that we could achieve over 86% accuracy if we had the ability to utilize the hand-labelled features. The FULL DIALOGUE row in Table 1 for AUTO- MATIC and AUTO, TASK-INDEP features shows the ability of the classifier to identify problematic dia- logues, rather than predict them, using features for the whole dialogue. The ALL row for the FULL DI- ALOGUE shows that we could correctly identify over 92% of the outcomes accurately if we had the ability to utilize the hand-labelled features. Note that the task-independent automatic fea- tures always perform within 2% error of the auto- matic features, and the hand-labelled features con- sistently perform with accuracies ranging from 6-8% greater. The rules that RIPPER learned on the basis of the Exchange 1 automatic features are below. Exchange 1, Automatic Features: if (el-top-confidence ≤ .924) ∧ (el-dtmf-flag = '1') then problematic, if (el-diff-confidence ≤ .916) ^ (el-asr-duration ≥ 6.92) then problematic, default is tasksuccess. According to these rules, a dialogue will be prob- lematic if the confidence score for the top-ranked Since 23% of the dialogues consisted of only two ex- changes, we exclude the second exchange features for those dialogues where the second exchange consists only of the sys- tem playing a closing prompt. We also excluded any features that indicated to the classifier that the second exchange was the last exchange in the dialogue. task (given by the NLU module) is moderate or low and there was touchtone input in the user utterance. The second rule says that if the difference between the top confidence score and the second-ranked con- fidence score is moderate or low, and the duration of the user utterance is more than 7 seconds, predict PROBLEMATIC. The performance of these rules is summarized in Table 2. These results show that given the first ex- change, this ruleset predicts that 22% of the dia- logues will be problematic, while 36% of them ac- tually will be. Of the dialogues that actually will be problematic, it can predict 41% of them. Once it predicts that a dialogue will be problematic, it is correct 69% of the time. As mentioned earlier, this reflects an overall improvement in accuracy of 8% over the baseline. The rules learned by training on the automatic task-independent features for exchanges 1 and 2 are given below. As in the first rule set, the features that the classifier appears to be exploiting are primarily those from the ASR and NLU modules. Exchanges 1&2, Automatic Independent Features: Task- if (e2-recog-numwords ≤ 0) ∧ (el-diff-confidence ≤ .95) then problematic. if (el-salience-coverage < .889) A (e2-recog contains \"I\") A (e2-asr-duration ≥ 7.48) then problematic. if (el-top-confidence ≤ .924) ∧ (e2-asr-duration ≥ 5.36) A (el-asr-duration ≥ 8.6) then problematic. if (e2-recog is blank) ^ (e2-asr-duration ≥ 2.8) then problematic. if (el-salience-coverage <.737) A (el-recog contains \"help\") A (el-asr-duration ≤ 7.04) then problematic. if (el-diff-confidence ≤ .924) ∧ (el-dtmf-flag = '1') A (el-asr-duration ≤ 6.68) then problematic. default is tasksuccess. The performance of this ruleset is summarized in Table 3. These results show that, given the first two exchanges, this ruleset predicts that 26% of the Class Occurred Predicted Recall Precision Success 64.1% 78.3% 89.44% 73.14% Problematic 35.9% 21.7% 41.47% 68.78% Table 2: Precision and Recall with Exchange 1 Automatic Features Class Occurred Predicted Recall Precision Success 64.1% 75.3% 91.42% 77.81% Problematic 35.9% 24.7% 53.53% 77.78% Table 3: Precision and Recall with Exchange 1&2 Automatic, Task-Independent Features dialogues will be problematic, while 36% of them actually will be. Of the problematic dialogues, it can predict 57% of them. Once it predicts that a dialogue will be problematic, it is correct 79% of the time. Compared with the classifier for the first utterance alone, this classifier has an improvement of 16% in recall and 10% in precision, for an overall improvement in accuracy of 7% over using the first exchange alone. One observation from these hypotheses is the clas- sifier's preference for the asr-duration feature over the feature for the number of words recognized (recog-numwords). One would expect longer utter- ances to be more difficult, but the learned rulesets indicate that duration is a better measure of utter- ance length than the number of words. Another ob- servation is the usefulness of the NLU confidence scores and the NLU salience-coverage in predicting problematic dialogues. These features seem to pro- vide good general indicators of the system's success in recognition and understanding. The fact that the main focus of the rules is detecting ASR and NLU errors and that none of the DM behaviors are used as predictors also indicates that, in all likelihood, the DM is performing as well as it can, given the noisy input that it is getting from ASR and NLU. To identify potential improvements in the prob- lematic dialogue predictor, we analyzed which hand- labelled features made large performance improve- ments, under the assumption that future work can focus on developing automatic features that ap- proximate the information provided by these hand- labelled features. The analysis indicated that the rsuccess feature alone improves the performance of the TOPLINE from 88.5%, as reported in (Langkilde et al., 1999), to 92.3%. Using rsuccess as the only feature results in 73.75% accuracy for exchange 1, 81.9% accuracy for exchanges 1&2 and 85.3% accu- racy for the full dialogue. In addition, for Exchanges 1&2, the accuracy of the AUTOMATIC, TASK-INDEP feature set plus the rsuccess feature is 86.5%, which is only 0.2% less than the accuracy of ALL the fea- tures for Exchanges 1&2 as shown in Table 1. The rules that RIPPER learns for Exchanges 1&2 when the AUTOMATIC, TASK-INDEP feature set is aug- mented with the single hand-labelled rsuccess fea- ture is shown below. Exchanges 1&2, Rsuccess + Automatic Task-Independent Features: if e2-salience-coverage ≤ 0.651 A e2-asr-duration ≥ 0.04 A e2-rsuccess=Rvacuous-match then problematic, if e2-rsuccess=Rmismatch then problematic, if e2-rsuccess=Rmismatch e2-salience-coverage ≥ 0.2 then problematic, el-top-confidence ≤ 0.909 e2-context-shift < 0.014 A e2-recog-numwords ≤ 12 ( if e2-rsuccess=Rmismatch A el-rsuccess=Rmismatch then problematic, if e2-rsuccess=Rmismatch e2-top-confidence < 0.803 A e2-asr-duration ≥ 2.68 e2-asr-duration ≤ 6.32 then problematic, if el-rsuccess=Rmismatch el-diff-confidence ≥ 0.83 then problematic, if e2-rsuccess=Rmismatch A e2-context-shift ≥ 0.54 then problematic, if e2-asr-duration ≥ 5.24 e2-salience-coverage < 0.833 A e2-top-confidence ≤ 0.801 A e2-recog-numwords ≤ 7 A e2-asr-duration ≤ 16.08 then problematic, if el-diff-confidence ≤ 0.794 A el-asr-duration ≥ 7.2 A el-inconsistency ≥ 0.024 A el-inconsistency ≥ 0.755 then problematic, default is tasksuccess Note that the rsuccess feature is frequently used in the rules and that RIPPER learns rules that combine the rsuccess feature with other features, such as the confidence, asr-duration, and salience-coverage fea- tures. 5 Discussion and Future Work In summary, our results show that: (1) All feature sets significantly improve over the baseline; (2) Us- ing automatic features from the whole dialogue, we can identify problematic dialogues 23% better than the baseline; (3) Just the first exchange provides sig- nificantly better prediction (8%) than the baseline; (4) The second exchange provides an additional sig- nificant (7%) improvement, (5) A classifier based on task-independent automatic features performs with less than 1% degradation in error rate relative to the automatic features. Even with current accuracy rates, the improved ability to predict problematic dialogues means that it may be possible to field the system without human agent oversight, and we ex- pect to be able to improve these results. The research reported here is the first that we know of to automatically analyze a corpus of logs from a spoken dialogue system for the purpose of learning to predict problematic situations. Our work builds on earlier research on learning to identify di- alogues in which the user experienced poor speech recognizer performance (Litman et al., 1999). How- ever, that work was based on a much smaller set of experimental dialogues where the notion of a good or bad dialogue was automatically approximated rather than being labelled by humans. In addition, because that work was based on features synthesized over the entire dialogues, the hypotheses that were learned could not be used for prediction during runtime. We are exploring several ways to improve the per- formance of and test the problematic dialogue pre- dictor. First, we noted above the extent to which the hand-labelled feature rsuccess improves classifier performance. In other work we report results from training an rsuccess classifier on a per-utterance level (Walker et al., 2000), where we show that we can achieve 85% accuracy using only fully automatic fea- tures. In future work we intend to use the (noisy) output from this classifier as input to our problem- atic dialogue classifier with the hope of improving the performance of the fully automatic feature sets. In addition, since it is more important to minimize errors in predicting PROBLEMATIC dialogues than er- rors in predicting TASKSUCCESS dialogues, we intend to experiment with RIPPER's loss ratio parameter, which instructs RIPPER to achieve high accuracy for the PROBLEMATIC class, while potentially reducing overall accuracy. Finally, we plan to integrate the learned rulesets into the HMIHY dialogue system to improve the system's overall performance. References A. Abella and A.L. Gorin. 1999. Construct algebra: An analytical method for dialog management. In Proc. of the Association for Computational Lin- guistics. P. Baggia, G. Castagneri, and M. Danieli. 1998. Field trials of the Italian Arise Train Timetable System. In Interactive Voice Technology for Telecommunications Applications, IVTTA, pages 97-102. J. Chu-Carroll and R. Carpenter. 1999. Vector- based natural language call routing. Computa- tional Linguistics, 25-3:361-387. W. Cohen. 1996. Learning trees and rules with set- valued features. In 14th Conference of the Amer- ican Association of Artificial Intelligence, AAAI. A.L. Gorin E. Ammicht and T. Alonso. 1999. Knowledge collection for natural language spoken dialog systems. In Proc. of EUROSPEECH 99. A.L. Gorin, G. Riccardi, and J.H. Wright. 1997. How may I Help You? Speech Communication, 23:113-127. I. Langkilde, M. Walker, J. Wright, A. Gorin, and D. Litman. 1999. Automatic prediction of prob- lematic human-computer dialogues in How May I Help You? In Proc. IEEE Workshop on Auto- matic Speech Recognition and Understanding. G. A. Levow. 1998. Characterizing and recogniz- ing spoken corrections in human-computer dia- logue. In Proc. of the 36th Annual Meeting of the Association of Computational Linguistics, COL- ING/ACL 98, pages 736-742. D. J. Litman, M. A. Walker, and M. J. Kearns. 1999. Automatic detection of poor speech recognition at the dialogue level. In Proc. of the 37th Annual Meeting of the Association of Computational Lin- guistics, ACL99, pages 309-316. G. Riccardi and A.L. Gorin. to appear. Spoken lan- guage adaptation over time and state in a natu- ral spoken dialog system. IEEE Transactions on Speech and Audio. A. Sanderman, J. Sturm, E. den Os, L. Boves, and A. Cremers. 1998. Evaluation of the Dutch Train Timetable Information System developed in the ARISE project. In Interactive Voice Technology for Telecommunications Applications, pages 91- 96. S. Seneff, V. Zue, J. Polifroni, C. Pao, L. Hethering- ton, D. Goddeau, and J. Glass. 1995. The pre- liminary development of a displayless PEGASUS system. In ARPA SLT Workshop. E. Shriberg, E. Wade, and P. Price. 1992. Human- machine problem solving using spoken language systems (SLS): Factors affecting performance and user satisfaction. In Proc. of the DARPA Speech and NL Workshop, pages 49-54. M. A. Walker, J. C. Fromer, and S. Narayanan. 1998. Learning optimal dialogue strategies: A case study of a spoken dialogue agent for email. In Proc. of the 36th Annual Meeting of the Association of Computational Linguistics, COL- ING/ACL 98, pages 1345-1352. M. Walker, I. Langkilde, and J. Wright. 2000. Us- ing NLP and Discourse features to identify under- standing errors in the How May I Help You spoken dialogue system. In Submission."
  },
  {
    "title": "An Empirical Assessment of Semantic Interpretation",
    "abstract": "We introduce a framework for semantic interpreta- tion in which dependency structures are mapped to conceptual representations based on a parsimonious set of interpretation schemata. Our focus is on the empirical evaluation of this approach to semantic in- terpretation, i.e., its quality in terms of recall and precision. Measurements are taken with respect to two real-world domains, viz. information technology test reports and medical finding reports.",
    "content": "1 Introduction Semantic interpretation has been an actively investi- gated issue on the research agenda of the logic-based paradigm of NLP in the late eighties (e.g., Charniak and Goldman (1988), Moore (1989), Pereira and Pollack (1991)). With the emergence of empirical methodologies in the early nineties, attention has al- most completely shifted away from this topic. Since then, semantic issues have mainly been dealt with under a lexical perspective, viz. in terms of the res- olution of lexico-semantic ambiguities (e.g., Schütze (1998), Pedersen and Bruce (1998)) and the gener- ation of lexical hierarchies from large text corpora (e.g., Li and Abe (1996), Hirakawa et al. (1996)) massively using statistical techniques. The research on semantic interpretation that was conducted in the pre-empiricist age of NLP was mainly driven by an interest in logical formalisms as carriers for appropriate semantic representations of NL utterances. With this representational bias, computational matters — how can semantic repre- sentation structures be properly derived from parse trees for a large variety of linguistic phenomena? — became a secondary issue. In particular, this re- search lacked entirely quantitative data reflecting the accuracy of the proposed semantic interpreta- tion mechanisms on real-world language data. One might be tempted to argue that recent eval- uation efforts within the field of information extrac- tion (IE) systems (Chinchor et al., 1993) are going to remedy this shortcoming. Given, however, the fixed number of knowledge templates and the restricted types of entities, locations, and events they encode as target information to be extracted, one readily re- alizes that such an evaluation framework provides, at best, a considerably biased, overly selective test environment for judging the understanding potential of text analysis systems which are not tuned for this special application. On the other hand, the IE experiments clearly in- dicate the need for a quantitative assessment of the interpretative performance of natural language un- derstanding systems. We will focus on this challenge and propose such a general evaluation framework. We first outline the model of semantic interpretation underlying our approach and then focus on its em- pirical assessment for two basic syntactic structures of the German language, viz. genitives and auxiliary constructions, in two domains. 2 The Basic Model for Semantic Interpretation The problem of semantic interpretation can be de- scribed as the mapping from syntactic to semantic (or conceptual) representation structures. In our ap- proach, the syntactic representation structures are given as dependency graphs (Hahn et al., 1994). Un- like constituency-based syntactic descriptions, de- pendency graphs consist of lexical nodes only, and these nodes are connected by vertices, each one of which is labeled by a particular dependency relation (cf. Figure 1). For the purpose of semantic interpretation, de- pendency graphs can be decomposed into semanti- cally interpretable subgraphs.¹ Basically, two types of semantically interpretable subgraphs can be dis- tinguished. The first one consists of lexical nodes which are labeled by content words only (lexical in- stances of verbs, nouns, adjectives or adverbs) and which are directly linked by a single dependency re- lation of any type whatsoever. Such a subgraph is illustrated in Figure 1 by Speicher - genatt - Com- puters. The second type of subgraph is also delim- ited by labels of content words but, in addition, a series of n = 1...4 intermediary lexical nodes may ¹ This notion and all subsequent criteria for interpretation are formally described in Romacker et al. (1999). propo: kann subject: verbpart: Speicher werden specifier: Der specifier: des genatt: Computers 1 verbpart erweitert mit ppadjunct: pobject: SDRAM-Modulen [The memory of the computer -- can -- with SDRAM-modules -- extended -- be] The memory of the computer can be extended with SDRAM-modules Figure 1: Dependency Graph for a Sample Sentence appear between these content words, all of which are labeled by non-content words (such as auxiliary or modal verbs, prepositions). Hence, in contrast to direct linkage we speak here of indirect linkage be- tween content words. Such a subgraph, with two intervening non-content words the modal \"kann\" and the auxiliary \"werden\" -, is given in Figure 1 by Speicher -- subject -- kann -- verbpart -- werden -- verbpart -- erweitert. Another subgraph with just one intervening non-content word -- the preposition \"mit\" -- is illustrated by erweitert - ppadjunct - mit -- pobject -- SDRAM-Modulen. From these consid- erations follows that, e.g., the subgraph spanned by Speicher and SDRAM-Modulen does not form a semantically interpretable subgraph, since the con- tent word erweitert intervenes on the linking path. Our approach to semantic interpretation sub- scribes to the principles of locality and composition- ality. It operates on discrete and well-defined units (subgraphs) of the parse tree, and the results of se- mantic interpretation are incrementally combined by fusing semantically interpretable subgraphs. As semantic target language we have chosen the framework of KL-ONE-type description logics (DL) (Woods and Schmolze, 1992). Since these logics are characterized by a settheoretical semantics we stay on solid formal ground. Furthermore, we take ad- vantage of the powerful inference engine of DL sys- tems, the description classifier, which turns out to be essential for embedded reasoning during the seman- tic interpretation process. By equating the semantic representation language with the conceptual one, we follow arguments discussed by Allen (1993). The basic idea for semantic interpretation is as follows: Each lexical surface form of a content word is associated with a set of concept identifiers repre- senting its (different) lexical meanings. This way, lexical ambiguity is accounted for. These concep- tual correlates are internal to the domain knowledge base, where they are described by a list of attributes or conceptual roles, and corresponding restrictions on permitted attribute values or role fillers are asso- ciated with them. Context: IT-DOMAIN X COMPUTER-SYSTEM. 02 MEMORY. 01 1 HAS-WORKING-MEMORY EXTENSION-PATIENT EXTENSION-CO-PATIENT POSSIBLE EXTENSION. 04 MODALITY PRESENT TENSE SDRAM-MODULE. 03 Figure 2: Concept Graph for a Sample Sentence As an example, consider the description for the concept COMPUTER-SYSTEM. It may be character- ized by a set of roles, such as HAS-HARD-DISK or HAS- WORKING-MEMORY, with corresponding restrictions on the concept types of potential role fillers. HAS- WORKING-MEMORY, e.g., sanctions only fillers of the concept type MEMORY. These conceptual con- straints are used for semantic filtering, i.e., for the elimination of syntactically admissible dependency graphs which, nevertheless, do not have a valid se- mantic interpretation. Semantic interpretation, in effect, boils down to finding appropriate conceptual relations in the do- main knowledge that link the conceptual correlates of the two content words spanning the semanti- cally interpretable subgraph, irrespective of whether a direct or an indirect linkage holds at the syn- tactic level. Accordingly, Figure 2 depicts the se- mantic/conceptual interpretation of the dependency structure given in Figure 1. Instances represent- ing the concrete discourse entities and events in the sample sentence are visualized as solid rectan- gles containing a unique identifier (e.g., COMPUTER- SYSTEM.02). Labeled and directed edges indicate instance roles. Dashed rectangles characterize sym- bols used as makers for tense and modality.2 Note that in Figure 2 each tuple of content words which configures a minimal subgraph in Figure 1 has already received an interpretation in terms of a relation linking the conceptual correlates. For exam- ple, Speicher - genatt - Computers (cf. Figure 1, box 1) is mapped to COMPUTER-SYSTEM.02 HAS- WORKING-MEMORY MEMORY.01 (cf. Figure 2, box 1). However, the search for a valid conceptual rela- tion is not only limited to a simple one-link slot-filler structure. We rather may determine conceptual re- lation paths between conceptual correlates of lexical items, the length of which may be greater than 1. 2 We currently do not further interpret the information con- tained in tense or modality markers. Verbal Lexeme Nominal Preposition VerbTrans Auxiliary Noun Pronoun: <subject: (agent patient}> <dirobject: (patient co-patient}> <genitive attribute:Ø> erweitern (extend) werden_passive Speicher (memory) <{patient co-patient}> mit (with) <{has-part instrument ...}> Figure 3: Fragment of the Lexeme Class Hierarchy (Thus, the need for role composition in the DL lan- guage becomes evident.) The directed search in the concept graph of the domain knowledge requires so- phisticated structural and topological constraints to be manageable at all. These constraints are encap- sulated in a special path finding and path evaluation algorithm specified in Markert and Hahn (1997). Besides these conceptual constraints holding in the domain knowledge, we further attempt to reduce the search space for finding relation paths by two kinds of syntactic criteria. First, the search may be constrained by the type of dependency relation hold- ing between the content words of the currently con- sidered semantically interpretable subgraph (direct linkage), or it may be constrained by the intervening lexical material, viz. the non-content words (indirect linkage). Each of these syntactic constraints has an immediate mapping to conceptual ones. For some dependency configurations, however, no syntactic constraints may apply. Such a case of un- constrained semantic interpretation (e.g., for geni- tive attributes directly linked by the genatt relation) leads to an exhaustive directed search in the knowl- edge base in order to find all conceptually compati- ble role fillings among the two concepts involved. Syntactic restrictions on semantic interpretation either come from lexeme classes or concrete lexemes. They are organized in terms of the lexeme class hi- erarchy superimposed on the fully lexicalized depen- dency grammar we use (Hahn et al., 1994). In the fragment depicted in Figure 3, the lexeme class of transitive verbs, VERBTRANS, requires that when- ever a subject dependency relation is encountered, semantic interpretation is constrained to the con- ceptual roles AGENT or PATIENT and all their sub- relations (such as EXTENSION-PATIENT). All other conceptual roles are excluded from the subsequent semantic interpretation. Exploiting the property in- heritance mechanisms provided by the hierarchic or- ganization of the lexicalized dependency grammar, all concrete lexemes subsumed by the lexeme class VERBTRANS, like \"erweitern\" (extend), inherit the corresponding constraint. However, there are lexeme classes such as NOUN which do not render any con- straints for dependency relations such as evidenced by gen[itive] att[ribute] (cf. Fig. 3). It may even happen that such restrictions can only be attached to concrete lexemes in order to avoid overgeneralization. Fortunately, we observed that this only happened to be the case for closed-class, i.e., non-content words. Accordingly, in Figure 3 the preposition \"with\" is characterized by the con- straint that only the conceptual roles HAS-PART, IN- STRUMENT, etc. must be taken into consideration for semantic interpretation. Since the constraints at the lexeme class or the lex- eme level are hard-wired in the class hierarchy, we refer to the mapping of dependency relations (or id- iosyncratic lexemes) to a set of conceptual relations (expanded to their transitive closure) as static inter- pretation. In contradistinction, the computation of relation paths for tuples of concepts during the sen- tence analysis process is called dynamic interpreta- tion, since the latter process incorporates additional conceptual constraints on the fly. The above-mentioned conventions allow the specification of high-level semantic interpretation schemata covering a large variety of different syntac- tic constructions by a single schema. For instance, each syntactic construction for which no conceptual constraints apply (e.g., the interpretation of geni- tives, most adjectives, etc.) receives its semantic interpretation by instantiating the same interpreta- tion schema (Romacker et al., 1999). The power of this approach comes from the fact that these high- level schemata are instantiated in the course of the parsing process by exploiting the dense specifications of the inheritance hierarchies both at the grammar level (the lexeme class hierarchy), as well as the con- ceptual level (the concept and role hierarchies). We currently supply up to ten semantic interpre- tation schemata for declaratives, relatives, and pas- sives at the clause level, complement subcategoriza- tion via PPs, auxiliaries, all tenses at the VP level, pre- and and postnominal modifiers at the NP level, and anaphoric expressions. We currently do not ac- count for control verbs (work in progress), coordina- tion and quantification. 3 The Evaluation of Semantic Interpretation In this section, we want to discuss, for two particular types of German language phenomena, the adequacy of our approach in the light of concrete language data taken from the two corpora we work with. This part of the enterprise, the empirical assessment of se- mantic interpretation, is almost entirely neglected in the literature (for two notable exceptions, cf. Bon- nema et al. (1997) and Bean et al. (1998)). Though similarities exist (viz. dealing with the performance of NLP systems in terms of their abil- ity to generate semantic/conceptual structures), the semantic interpretation (SI) task has to be clearly distinguished from the information extraction (IE) task and its standard evaluation settings (Chinchor et al., 1993). In the IE task, a small subset of the templates from the entire domain is selected into which information from the texts are mapped. Also, the design of these templates focus on particularly interesting facets (roles, in our terminology), so that an IE system does not have to deal with the full range of qualifications that might occur even re- lating to relevant, selected concepts. Note that in any case, a priori relevance decisions limit the range of a posteriori fact retrieval. The SI task, however, is far less restricted. We here evaluate the adequacy of the conceptual rep- resentation structures relating, in principle (only re- stricted, of course, by the limits of the knowledge ac- quisition devices), to the entire domain of discourse, with all qualifications mentioned in a text. Whether these are relevant or not for a particular application has to be determined by subsequent data/knowledge cleansing. In this sense, semantic interpretation might deliver the raw data for transformation into appropriate IE target structures. Only because of feasibility reasons, the designers of IE systems equate IE with SI. The cross-linking of IE and SI tasks, however, bears the risk of having to determine, in advance, what will be relevant or not for later re- trieval processes, assumptions which are likely to be flawed by the dynamics of domains and the unpre- dictability of the full range of interests of prospective users. 3.1 Methodological Issues Our methodology to deal with the evaluation of se- mantic interpretation is based on a triple division of test conditions. The first category relates to checks whether so-called static constraints, effected by the mapping from a single dependency relation to one or more conceptual relations, are valid (cf. Figure 3 for restrictions of this type). Second, one may in- vestigate the appropriateness of the results from the search of the domain knowledge base, i.e., whether a relation between two concepts can be determined at all, and, if so, whether that relation (or role chain) is adequate. The conceptual constraints which come into play at this stage of processing are here referred to as dynamic constraint propagation, since they are to be computed on the fly, while judging the valid- ity of the role chain in question. Third, interactions between the above-mentioned static constraints and dynamic constraint propagation may occur. This is the case for the interpretation of auxiliaries or prepositions, where intervening lexical material and associated constraints have to be accounted for si- multaneously. In our evaluation study, we investigated the effects of category II and category III phenomena by consid- ering genitives and modal as well as auxiliary verbs, respectively. The knowledge background is consti- tuted by a domain ontology that is divided into an upper generic part (containing about 1,500 concepts and relations) and domain-specific extensions. We here report on the two specialized domains we deal with a hardware-biased information technology (IT) domain model and an ontology covering parts of anatomical medicine (MED). Each of these two domain models adds roughly about 1,400 concepts and relations to the upper model. Corresponding lexeme entries in the lexicon provide linkages to the entire ontology. In order to avoid error chaining, we always assume a correct parse to be delivered for the semantic interpretation process. We took a random selection of 54 texts (compris- ing 18,500 words) from the two text corpora, viz. IT test reports and MEDical finding reports. For evaluation purposes (cf. Table 1), we concentrated on the interpretation of genitives (as an instance of direct linkage; GEN) and on the interpretation of periphrastic verbal complexes, i.e., passive, tempo- ral and modal constructions (as instances of indirect linkage; MODAUX). The choice of these two grammatical patterns al- lows us to ignore the problems caused by syntac- tic ambiguity, since in our data no structural am- 3 Note that computations at the domain knowledge level which go beyond mere type checking are usually located out- side the scope the semantic considerations. This is due to the fact that encyclopedic knowledge and its repercussions on the understanding process are typically not considered part of the semantic interpretation task proper. While this may be true from a strict linguistic point of view, from the com- putational perspective of NLP this position cannot seriously be maintained. Even more so, when semantic and conceptual representations are collapsed. biguities occurred. If one were to investigate the combined effects of syntactic ambiguity and seman- tic interpretation the evaluation scenario had to be changed. Methodologically, the first step were to ex- plore the precision of a semantic interpretation task without structural ambiguities (as we do) and then, in the next step, incorporate the treatment of syn- tactic ambiguities (e.g., by semantic filtering devices, cf. Bonnema et al. (1997)). Several guidelines were defined for the evaluation procedure. A major issue dealt with the correctness of a semantic interpretation. In cases with interpre- tation, we considered a semantic interpretation to be a correct one, if the conceptual relation between the two concepts involved was considered adequate by introspection (otherwise, incorrect). This qualifi- cation is not as subjective as it may sound, since we applied really strict conditions adjusted to the fine- grained domain knowledge.<sup>4</sup> Interpretations were considered to be correct in those cases which con- tained exactly one relation, as well as cases of se- mantical/conceptual ambiguities (up to three read- ings, the most), presumed the relation set contained the correct one.<sup>5</sup> A special case of incorrectness, called <i>nil</i>, occurred when no relation path could be determined though the two concepts under scrutiny were contained in the domain knowledge base and an interpretation should have been computed. We further categorized the cases where the sys- tem failed to produce an interpretation due to at least one concept specification missing (with respect to the two linked content words in a semantically interpretable subgraph). In all those cases with- out interpretation, insufficient coverage of the upper model was contrasted with that of the two domain models in focus, MED and IT, and with cases in which concepts referred to other domains, e.g., fash- ion or food. Ontological subareas that could nei- ther be assigned to the upper model nor to partic- ular domains were denoted by phrases referring to time (e.g., \"the beginning of the year\"), space (e.g., <sup>4</sup>The majority of cases were easy to judge. For instance, \"the infiltration of the stroma\" resulted in a correct reading - STROMA being the PATIENT of the INFILTRATION event -, as well as in an incorrect one - being the AGENT of the IN- FILTRATION. Among the incorrect semantic interpretations we also categorized, e.g., the interpretation of the expression \"the prices of the manufacturers\" as a conceptual linkage from PRICE via PRICE-OF to PRODUCT via HAS-MANUFACTURER to MANUFACTURER (this type of role chaining can be considered an intriguing example of the embedded reasoning performed by the description logic inference engine), since it did not ac- count for the interpretation that MANUFACTURERS fix PRICES as part of their marketing strategies. After all, correct inter- pretations always boiled down to entirely evident cases, e.g., HARD-DISK PART-OF COMPUTER. <sup>5</sup> At the level of semantic interpretation, the notion of se- mantic ambiguity relates to the fact that the search algorithm for valid conceptual relation paths retrieves more than a single relation (chain). \"the surface of the storage medium\"), and abstract notions (e.g., \"the acceptance of IT technology\"). Finally, we further distinguished evaluative expres- sions (e.g., \"the advantages of plasma display\") from figurative language, including idiomatic expressions (e.g., \"the heart of the notebook\"). At first glance, the choice of genitives may appear somewhat trivial. From a syntactic point of view, genitives are directly linked and, indeed, constitute an easy case to deal with at the dependency level. From a conceptual perspective, however, they pro- vide a real challenge. Since no static constraints are involved in the interpretation of genitives (cf. Figure 3, lexeme class NOUN) and, hence, no prescriptions of (dis)allowed conceptual relations are made, an un- constrained search (apart from connectivity condi- tions imposed on the emerging role chains) of the domain knowledge base is started. Hence, the main burden rests on the dynamic constraint processing part of semantic interpretation, i.e., the path find- ing procedure muddling through the complete do- main knowledge base in order to select the adequate conceptual reading(s). Therefore, genitives make a strong case for test category II mentioned above. Dependency graphs involving modal verbs or aux- iliaries are certainly more complex at the syntac- tic level, since the corresponding semantically in- terpretable subgraphs may be composed of up to six lexical nodes. However, all intervening non- content-word nodes accumulate constraints for the search of a valid relation for semantic interpretations and, hence, allows us to test category III phenom- ena. The search space is usually pruned, since only those relations that are sanctioned by the interven- ing nodes have to be taken into consideration. 3.2 Evaluation Data We considered a total of almost 250 genitives in all these texts, from which about 59%/33% (MED/IT) received an interpretation.<sup>6</sup> Out of the total loss due to incomplete conceptual coverage, 56%/58% (23 of 41 genitives/57 of 98 genitives) can be attributed to insufficient coverage of the domain models. Only the remaining 44%/42% are due to the residual factors listed in Table 1. In our sample, the number of syntactic construc- tions containing modal verbs or auxiliaries amout to 292 examples. Compared to genitives, we obtained a more favorable recall for both domains: 66% for MED and 40% for IT. As for genitives, lacking in- terpretations, in the majority of cases, can be at- tributed to insufficient conceptual coverage. For the IT domain, however, a dramatic increase in the num- ber of missing concepts is due to gaps in the upper model (78 or 63%) indicating that a large number of <sup>6</sup>Confidence intervals at a 95% reliability level are given in brackets in Table 1. --- MED-GEN IT-GEN MED-MODAUX IT-MODAUX # texts 29 25 29 25 # words 4,300 14,200 4,300 14,200 recall 57% 31% 66% 40% precision 97% 94% 95% 85% # occurrences 100 147 58 234 ... with interpretation 59 (59%) 49 (33%) 40 (69%) 111 (47%) [confidence intervals] [48%-67%] [24%-41%] [56%-81%] [40%-53%] ... correct (single reading) 53 (53%) 28 (19%) 38 (66%) 88 (38%) ... correct (multiple readings) 4 (4%) 18 (12%) 0 (0%) 6 (3%) ... incorrect 0 3 0 14 ... nil 2 0 2 3 ... without interpretation 41 (41%) 98 (67%) 18 (31%) 123 (53%) ... domain model (MED/IT) 23 (23%) 57 (39%) 11 (19%) 42 (34%) ... upper model 3 23 5 78 ... other domains 0 4 0 0 ... time 0 15 0 1 ... space 7 8 0 5 ... abstracta, generics 11 12 0 16 ... evaluative expressions 0 8 0 3 ... figurative language 1 17 2 24 ... miscellaneous 0 1 0 3 Table 1: Empirical Results for the Semantic Interpretation of Genitives (GEN) and Modal Verbs and Aux- iliaries (MODAUX) in the IT and MED domains essential concepts for verbs were not modeled. Also, figurative speech plays a more important role in IT with 24 occurrences. Both observations mirror the fact that IT reports are linguistically far less con- strained and are rhetorically more advanced than their MED counterparts. Another interesting observation which is not made explicit in Table 1 concerns the distribution of modal verbs and auxiliaries. In MED, we encountered 57 passives and just one modal verb and no temporal auxiliaries, i.e., our data are in line with prevailing findings about the basic patterns of medical sublan- guage (Dunham, 1986). For the IT domain, cor- responding occurrences were far less biased, viz. 80 passives, 131 modal verbs, and 23 temporal auxil- iaries. Finally, for the two domains 25 samples con- tained both modal verbs and auxiliaries, thus form- ing semantically interpretable subgraphs with four word nodes. One might be tempted to formulate a null hy- pothesis concerning the detrimental impact of the length of semantically interpretable subgraphs (i.e., the number of intervening lexical nodes carrying non-content words) on the quality of semantic inter- pretation. In order to assess the role of the length of the path in a dependency graph, we separately investigated the results for these subclasses of com- bined verbal complexes. From the entire four-node set (cf. Table 2) with 25 occurrences (3 for MED and 22 for IT), 16 received an interpretation (3 for MED, 13 for IT). While we neglect the MED data due to the small absolute numbers, the IT domain revealed recall precision # occurrences MED IT 4-nodes 4-nodes --- 59% 85% 3 22 ... with interpretation 3 13 ......correct 3 11 Table 2: Interpretation Results for Semantically In- terpretable Graphs Consisting of Four Nodes 59% recall and 85% precision. If we compare this to the overall figures for recall (40%) and precision (85%), the data might indicate a gain in recall for longer subgraphs, while precision keeps stable. The results we have worked out are just a first step into a larger series of broader and deeper evaluation efforts. The concrete values we present, sobering as they may be for recall (57%/31% for genitives and 66%/40% for modal verbs and auxiliaries), encour- aging, however, for precision (97%/94% for genitives and 95%/85% for modal verbs and auxiliaries), can only be interpreted relative to other data still lacking on a broader scale. As with any such evaluation, idiosyncrasies of the coverage of the knowledge bases are inevitably tied with the results and, thus, put limits on too far- reaching generalizations. However, our data reflect the intention to submit a knowledge-intensive text understander to a realistic, i.e., conceptually un- constrained and therefore \"unfriendly\" test environ- ment. Judged from the figures of our recall data, there is no doubt, whatsoever, that conceptual coverage of the domain constitutes the bottleneck for any knowledge-based approach to NLP.7 Sublanguage differences are also mirrored systematically in these data, since medical texts adhere more closely to well- established concept taxonomies and writing stan- dards than magazine articles in the IT domain. 4 Related Work After a period of active research within the logic- based paradigm (e.g., Charniak and Goldman (1988), Moore (1989), Pereira and Pollack (1991)), work on semantic interpretation has almost ceased with the emergence of the empiricist movement in NLP (cf. Bos et al. (1996) for one of the more recent studies dealing with logic-based semantic interpreta- tion in the framework of the VERBMOBIL project). Only few methodological proposals for semantic computations were made since then (e.g., higher- order colored unification as a mechanism to avoid over-generation inherent to unconstrained higher- order unification (Gardent and Kohlhase, 1996)). An issue which has lately received more focused at- tention are ways to cope with the tremendous com- plexity of semantic interpretations in the light of an exploding number of (scope) ambiguities. Within the underspecification framework of semantic repre- sentations, e.g., Dörre (1997) proposes a polynomial algorithm which constructs packed semantic repre- sentations directly from parse forests. All the previously mentioned studies (with the ex- ception of the experimental setup in Dörre (1997)), however, lack an empirical foundation of their var- ious claims. Though the MUC evaluation rounds (Chinchor et al., 1993) yield the flavor of an empiri- cal assessment of semantic structures, their scope is far too limited to count as an adequate evaluation platform for semantic interpretation. Nirenburg et al. (1996) already criticize the 'black-box' architec- ture underlying MUC-style evaluations, which pre- cludes to draw serious conclusions from the short- comings of MUC-style systems as far as single lin- guistic modules are concerned. More generally, in this paper the rationale underlying size (of the lex- icons, knowledge or rule bases) as the major assess- ment category is questioned. Rather dimensions re- lating to the depth and breadth of the knowledge sources involved in complex system behavior should be taken more seriously into consideration. This is exactly what we intended to provide in this paper. As far as evaluation studies are concerned dealing with the assessment of semantic interpretations, few 7 At least for the medical domain, we are currently actively pursuing research on the semiautomatic creation of large-scale ontologies from weak knowledge sources (medical terminolo- gies); cf. Schulz and Hahn (2000). have been carried out, some of which under severe restrictions. For instance, Bean et al. (1998) nar- row semantic interpretation down to a very limited range of spatial relations in anatomy, while Gomez et al. (1997) bias the result by preselecting only those phrases that were already covered by their domain models, thus optimizing for precision while shunting aside recall considerations. A recent study by Bonnema et al. (1997) comes closest to a serious confrontation with a wide range of real-world data (Dutch dialogues on a train travel domain). This study proceeds from a corpus of annotated parse trees to which are assigned type- logical formulae which express the corresponding se- mantic interpretation. The goal of this work is to compute the most probable semantic interpretation for a given parse tree. Accuracy (i.e., precision) is rather high and ranges between 89,2%-92,3% de- pending on the training size and depth of the parse tree. Our accuracy criterion is weaker (the intended meaning must be included in the set of all read- ings), which might explain the slightly higher rates we achieve for precision. However, this study does not distinguish between different syntactic construc- tions that undergo semantic interpretation, nor does it consider the level of conceptual interpretation (we focus on) as distinguished from the level of semantic interpretation to which Bonnema et al. refer. 5 Conclusions The evaluation of the quality and adequacy of se- mantic interpretation data is still in its infancy. Our approach which confronts semantic interpretation devices with a random sample of textual real-world data, without intentionally constraining the selec- tion of these language data, is a real challenge for the proposed methodology and it is unique in its experimental rigor. However, our work is just a step in the right di- rection rather than giving a complete picture or al- lowing final conclusions. Two reasons may be given for the lack of such experiments. First, interest in the deeper conceptual aspects of text interpretation has ceased in the past years, with almost all efforts devoted to robust and shallow syntactic processing of large data sets. This also results in a lack of so- phisticated semantic and conceptual specifications, in particular, for larger text analysis systems. Sec- ond, providing a gold standard for semantic inter- pretation is, in itself, an incredibly underconstrained and time-consuming process for which almost no re- sources have been allocated in the NLP community up to now. Acknowledgements. We want to thank the mem- bers of the group for close cooperation. Martin Ro- macker is supported by a grant from DFG (Ha 2097/5-1). References James F. Allen. 1993. Natural language, knowledge representation, and logical form. In M. Bates and R. M. Weischedel, editors, Challenges in Natural Language Processing, pages 146-175. Cambridge: Cambridge University Press. Carol A. Bean, Thomas C. Rindflesch, and Charles A. Sneiderman. 1998. Automatic seman- tic interpretation of anatomic spatial relationships in clinical text. In Proceedings of the 1998 AMIA Annual Fall Symposium., pages 897-901. Orlando, Florida, November 7-11, 1998. Remko Bonnema, Rens Bod, and Remko Scha. 1997. A DOP model for semantic interpretation. In Pro- ceedings of the 35th Annual Meeting of the Asso- ciation for Computational Linguistics & 8th Con- ference of the European Chapter of the ACL, pages 159-167. Madrid, Spain, July 7-12, 1997. Johan Bos, Björn Gambäck, Christian Lieske, Yoshiki Mori, Manfred Pinkal, and Karsten Worm. 1996. Compositional semantics in VERB- MOBIL. In COLING'96 - Proceedings of the 16th International Conference on Computational Lin- guistics, pages 131-136. Copenhagen, Denmark, August 5-9, 1996. Eugene Charniak and Robert Goldman. 1988. A logic for semantic interpretation. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pages 87-94. Buffalo, New York, U.S.A., 7-10 June 1988. Nancy Chinchor, Lynette Hirschman, and David D. Lewis. 1993. Evaluating message understanding systems: an analysis of the third Message Un- derstanding Conference (MUC-3). Computational Linguistics, 19(3):409-447. Jochen Dörre. 1997. Efficient construction of un- derspecified semantics under massive ambiguity. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics & 8th Conference of the European Chapter of the ACL, pages 386-393. Madrid, Spain, July 7-12, 1997. George Dunham. 1986. The role of syntax in the sublanguage of medical diagnostic statements. In R. Grishman and R. Kittredge, editors, Analyz- ing Language in Restricted Domains: Sublanguage Description and Processing, pages 175-194. Hills- dale, NJ & London: Lawrence Erlbaum. Claire Gardent and Michael Kohlhase. 1996. Higher-order coloured unification and natural lan- guage semantics. In ACL'96 - Proceedings of the 34th Annual Meeting of the Association for Com- putational Linguistics, pages 1-9. Santa Cruz, California, U.S.A., 24-27 June 1996. Fernando Gomez, Carlos Segami, and Richard Hull. 1997. Determining prepositional attach- ment, prepositional meaning, verb meaning and thematic roles. Computational Intell., 13(1):1-31. Udo Hahn, Susanne Schacht, and Norbert Bröker. 1994. Concurrent, object-oriented natural lan- guage parsing: the PARSETALK model. Inter- national Journal of Human-Computer Studies, 41(1/2):179-222. Hideki Hirakawa, Zhonghui Xu, and Kenneth Haase. 1996. Inherited feature-based similarity measure based on large semantic hierarchy and large text corpus. In COLING'96 - Proceedings of the 16th International Conference on Computational Lin- guistics, pages 508-513. Copenhagen, Denmark, August 5-9, 1996. Hang Li and Naoki Abe. 1996. Clustering words with the MDL principle. In COLING'96 - Pro- ceedings of the 16th International Conference on Computational Linguistics, pages 4-9. Copen- hagen, Denmark, August 5-9, 1996. Katja Markert and Udo Hahn. 1997. On the in- teraction of metonymies and anaphora. In IJ- CAI'97 - Proceedings of the 15th International Joint Conference on Artificial Intelligence, pages 1010-1015. Nagoya, Japan, August 23-29, 1997. Robert C. Moore. 1989. Unification-based seman- tic interpretation. In Proceedings of the 27th An- nual Meeting of the Association for Computa- tional Linguistics, pages 33-41. Vancouver, B.C., Canada, 26-29 June 1989. Sergei Nirenburg, Kavi Mahesh, and Stephen Beale. 1996. Measuring semantic coverage. In COL- ING'96 - Proceedings of the 16th International Conference on Computational Linguistics, pages 83-88. Copenhagen, Denmark, August 5-9, 1996. Ted Pedersen and Rebecca Bruce. 1998. Knowledge lean word-sense disambiguation. In AAAI'98 Proceedings of the 15th National Conference on Artificial Intelligence, pages 800-805. Madison, Wisconsin, July 26-30, 1998. Fernando C.N. Pereira and Martha E. Pollack. 1991. Incremental interpretation. Artificial Intelligence, 50(1):37-82. Martin Romacker, Katja Markert, and Udo Hahn. 1999. Lean semantic interpretation. In IJCAI'99 Proceedings of the 16th International Joint Con- ference on Artificial Intelligence, pages 868-875. Stockholm, Sweden, July 31 - August 6, 1999. Stefan Schulz and Udo Hahn. 2000. Knowledge en- gineering by large-scale knowledge reuse: experi- ence from the medical domain. In Proceedings of the 7th International Conference on Principles of Knowledge Representation and Reasoning. Breck- enridge, CO, USA, April 12-15, 2000. Hinrich Schütze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97-124. William A. Woods and James G. Schmolze. 1992. The KL-ONE family. Computers & Mathematics with Applications, 23(2/5):133-177."
  },
  {
    "title": "TRIPHONE ANALYSIS: A COMBINED METHOD FOR THE CORRECTION OF ORTHOGRAPHICAL AND TYPOGRAPHICAL ERRORS",
    "abstract": "Most existing systems for the correction of word level errors are oriented toward either typographical or orthographical errors. Triphone analysis is a new correction strategy which combines phonemic transcription with trigram analysis. It corrects both kinds of errors (also in combination) and is superior for orthographical errors.",
    "content": "1. INTRODUCTION 1.1 Error types Any method for the correction of word level errors in written texts must be carefully tuned. On the one hand, the number of probable corrections should be maximized; on the other hand, the number of unlikely corrections should be minimized. In order to achieve these goals, the characteristics of specific error types must be exploited as much as possible. In this article we distinguish two major types of word level errors: orthographical errors and typographical errors. They have some clearly different characteristics. Orthographical errors are cognitive errors consisting of the substitution of a deviant spelling for a correct one when the author either simply doesn't know the correct spelling for a correct spelling, forgot it, or misconceived it. An important characteristic of orthographical errors is that they generally result in a string which is phonologically identical or very similar to the correct string (e.g. indicies instead of indices¹). As a consequence, orthographical errors are dependent on the correspondence between spelling and pronunciation in a particular language. Another characteristic is that proper names, infrequent words and foreign words are particularly prone to ortho- graphical errors. ¹All examples of errors given in this article were actually found by the authors in texts written by native speakers of the language in question. Typographical errors are motoric errors caused by hitting the wrong sequence of keys. Hence their char- acteristics depend on the use of a particular keyboard rather than on a particular language. Roughly eighty percent of these errors can be described as single dele- tions (e.g. continous) insertions (e.g. explaination), substitutions (e.g. anyboby) or transpositions (e.g. autoamtically) while the remaining twenty percent are complex errors (Peterson, 1980). Some statistical facts about typographical errors are that word-initial errors are rare, and doubling and undoubling (e.g. succeeed, discusion) are common. In general, typographical errors do not lead to a string which is homophonous with the correct string. Most of the correction methods currently in use in spelling checkers are biased toward the correction of typographical errors. We argue that this is not the right thing to do. Even if orthographical errors are not as frequent as typographical errors, they are not to be neglected for a number of good reasons. First, orthographical errors are cognitive errors, so they are more persistent than typographical errors: proof- reading by the author himself will often fail to lead to correction. Second, orthographical errors leave a worse impression on the reader than typographical errors. Third, the use of orthographical correction for standardization purposes (e.g. consistent use of either British or American spelling) is an important application appreciated by editors. In this context, our research pays special attention to Dutch, which has a preferred standard spelling but allows alternatives for a great many foreign words, e.g. architect (preferred) vs. architekt (allowed and commonly used in Dutch). Editors of books generally prefer a consistent use of the standard spelling. Finally, we would like to point out that methods for orthographical error correction can not only be applied in text processing, but also in database retrieval. In fact, our research was prompted partly by a project proposal for a user interface to an electronic encyclopedia. One or our experiments involving a lists of some five thousand worldwide geographical names (mainly in Dutch spelling, e.g. Noordkorea, Nieuwzeeland) has yielded very positive results. In this context, the correction of orthographical errors is obviously more important than the correction of typographical errors. 1.2 Correction strategies Daelemans, Bakker & Schotel (1984) distinguish between two basic kinds of strategies: statistical and linguistic strategies. Statistical strategies are based on string comparison techniques, often augmented by specific biases using statistical characteristics of some error types, such as the fact that typographical errors do not frequently occur in the beginning of a word. Since these strategies do not exploit any specific linguistic knowledge, they will generally work better for typographical errors than for orthographical errors. Linguistic strategies exploit the fact that orthog- raphical errors often result in homophonous strings (sound-alikes. e.g. consistancy and consistency). They normally involve some kind of phonemic tran- scription. Typographical errors which do not severely affect the pronunciation, such as doubling and undoubling, may be covered as well, but in general, linguistic strategies will do a poor job on all other typographical errors. Because each type of strategy is oriented toward one class of errors only, what is needed in our opinion is a combined method for orthographical and typo- graphical errors. Our research has explored one approach to this problem, namely, the combination of a linguistic strategy with a statistical one. The remainder of this document is structured as follows. First we will discuss and criticize some existing statistical and linguistic correction methods. Then we will introduce triphone analysis. Finally we will report some results of an experiment with this method. 2. SOME EXISTING CORRECTION METHODS 2.1 Spell In Peterson's SPELL (Peterson, 1980), all probable corrections are directly generated from an incorrect string by considering the four major single error types. The program first makes a list of all strings from which the incorrect string can be derived by a single deletion, insertion, substitution or trans- position. This list is then matched against the dictionary: all strings occuring in both the list and the dictionary are considered probable corrections. Although the number of derivations is relatively small for short strings, they often lead to several probable corrections because many of them will actually occur in the dictionary. For longer strings, many possible derivations are considered but most of those will be non-existent words. An advantage of SPELL with respect to all other methods is that short words can be corrected equally well as long ones. A disadvantage is that all complex errors and many orthographical errors fall outside the scope of SPELL. 2.2 Speedcop SPEEDCOP (Pollock & Zamora, 1984) uses a special technique for searching and comparing strings. In order to allow a certain measure of similarity, strings are converted into similarity keys which inten- tionally blur the characteristics of the original strings. The key of the misspelling is looked up in a list of keys for all dictionary entries. The keys found in the list within a certain distance of the target key are considered probable corrections. The blurring of the similarity keys must be carefully finetuned. On the one hand, if too much information is lost, too many words collate to the same key. If, on the other hand, too much infor- mation is retained, the key will be too sensitive to alterations by misspellings. Two similarity keys are used in SPEEDCOP: a skeleton key and an omission key. These keys are carefully designed in order to partially preserve the characters in a string and their interrelationships. The information contained in the key is ordered according to some characteristics of typographical errors, e.g. the fact that word-initial errors are infrequent and that the sequence of consonants is often undisturbed. The skeleton key contains the first letter of a string, then the remaining consonants and finally the remaining vowels (in order, without duplicates). E.g. the skeleton key of information would be infrmtoa. The advantage of using this key is that some frequent error types such as doubling and undoubling of characters as well as transpositions involving one consonant and one vowel (except for an initial vowel) results in keys which are identical to the keys of the original strings. The most vulnerable aspect of the skeleton key is its dependence on the first few consonants. This turned out to be a problem, especially for omissions. Therefore, a second key, the omission key, was developed. According to Pollock & Zamora (1984), consonants are omitted in the following declining or- der of frequency: RSTNLCHDPGMFBYWVZXQKJ. The omission key is construed by first putting the consonants in increasing order of omission frequency and adding the vowels in order of occurrence. E.g. the omission key for information is fmntrioa. SPEEDCOP exploits the statistical properties of typographical errors well, so it deals better with frequent kinds of typographical errors than with infrequent ones. Because of this emphasis on typo- graphical errors, its performance on orthographical errors will be poor. A specific disadvantage is its dependence on the correctness of initial characters. Even when the omission key is used, word-initial errors involving e.g. j or k do not lead to an appropriate correction. 2.3 Trigram analysis: Fuzzie and Acute Trigram analysis, as used in FUZZIE (De Heer, 1982) and ACUTE (Angell, 1983), uses a more general similarity measure. The idea behind this method is that a word can be divided in a set of small overlapping substrings, called n-grams, which each carry some information about the identity of a word. When a misspelling has at least one undisturbed n- gram, the correct spelling spelling can still be traced. For natural languages, trigrams seem to have the most suitable length. E.g., counting one surrounding space, the word trigram is represented by the trigrams #tr, tri, rig, igr, gra, ram, and am#. Bigrams are in general too short to contain any useful identifying information while tetragrams and larger n-grams are already close to average word length. Correction using trigrams proceeds as follows. The trigrams in a misspelling are looked up in an inverted file consisting of all trigrams extracted from the dictionary. With each trigram in this inverted file, a list of all words containing the trigram is associated. The words retrieved by means of the trigrams in the misspelling are probable corrections. The difference between FUZZIE and ACUTE is mainly in the criteria which are used to restrict the number of possible corrections. FUZZIE emphasizes frequency as a selection criterium whereas ACUTE also uses word length. Low frequency trigrams are assumed to have a higher identifying value than high frequency trigrams. In FUZZIE, only the correction candidates associated with the n least frequent trigrams, which are called selective trigrams, are considered. ACUTE offers the choice between giving low frequency trigrams a higher value and giving all trigrams the same value. Taking trigram frequency into account has advantages as well as disadvantages. On the one hand, there is a favorable distribution of trigrams in natural languages in the sense that there is a large number of low frequency trigrams. Also, the majority of words contain at least one selective trigram. On the other hand, typographical errors may yield very low frequency trigrams which inevitably get a high information value. In general, trigram analysis works better for long words than for short ones, because a single error may disturb all or virtually all trigrams in a short word. Some advantages of this method are that the error position is not important and that complex errors (e.g. differenent), and, to a certain extent, orthographical errors, can often be corrected. A disadvantage which is specific to this method is that transpositions disturb more trigrams than other types of errors and will thus be more difficult to correct. Trigram analysis lends itself well to extensions. By first selecting a large group of intermediate solutions, i.e. all words which share at least one selective trigram with the misspelling, there is a lot of room for other factors to decide which words will eventually be chosen as probable corrections. ACUTE for example uses word length as an important criterium. 2.4 The PF-474 chip The PF-474 chip is a special-purpose VLSI circuit designed for very fast comparison of a string with every entry in a dictionary (Yianilos, 1983). It consists of a DMA controller for handling input from a data base (the dictionary), a proximity computer for computing the proximity (similarity) of two strings, and a ranker for ranking the 16 best solutions according to their proximity values. The proximity value (PV) of two strings is a function of the number of corresponding characters of both strings counted in forward and backward direc- tions. It is basically expressed as the following ratio: 2*(ABforward + ABbackward) PV= AAforward+AAbackward+BBforward+BBbackward This value can be influenced by manipulating the parameters weight, bias and compensation. The para- meter weight makes some characters more important than others. This parameter can e.g. be manipulated to reflect the fact that consonants carry more information than vowels. The parameter bias may correct the weight of a character in either word-initial or word-final position. The parameter compensation determines the importance of an occurrence of a certain character within the word. By using a high compensation/weight ratio, for example, substitution of characters will be less severe than omission. One may force two characters to be considered identical by equalizing their compensation and weight values. An advantage of the PF-474 chip, apart from its high speed, is that it is a general string comparison technique which is not biased to a particular kind of errors. By carefully manipulating the parameters, many orthographical errors may be corrected in addition to typographical errors. 2.5 Spell Therapist SPELL THERAPIST (Van Berkel, 1986) is a linguistic method for the correction of orthographical errors. The misspelling is transcribed into a phonological code which is subsequently looked up in a dictionary consisting of phonological codes with associated spellings. The phonemic transcription, based on the GRAFON system (Daelemans, 1987), is performed in three steps. First the character string is split into syllables. Then a rule-based system con- verts each syllable into a phoneme string by means of transliteration rules. These syllabic phoneme strings are further processed by phonological rules which take the surrounding syllable context into account and are finally concatenated. The transliteration rules in SPELL THERAPIST are grouped into three ordered lists: one for the onset of the syllable, one for the nucleus, and one for the coda. Each rule consists of a graphemic selection pattern, a graphemic conversion pattern, and a phoneme string. The following rules are some examples for Dutch onsets: ((sc (hiey)) c/k/) ((qu) qu (/k//kw/)) ((a (consonantp)) a /a/) The first rule indicates that in a graphemic pattern consisting of sc which is not followed by either h, i, e or y, the grapheme c is to be transcribed as the phoneme /k/. The transcription proceeds as follows. The onset of a syllable is matched with the graphemic selection patterns in the onset rule list. The first rule which matches is selected. Then the characters which match with the conversion pattern are converted into the phoneme string. The same procedure is then per- formed for the nucleus and coda of the syllable. The result of the transcription is then processed by means of phonological rules, which convert a sequence of phonemes into another sequence of phonemes in a certain phonological context on the level of the word. An example for Dutch is the cluster reduction rule which deletes a /t/ in certain consonant clusters: (((obstruent-p) /t/ (obstruent-p)) /t/ //) Such rules account for much of the power of SPELL THERAPIST because many homophonous orthographic errors seem to be related to rules such as assimilation (e.g. inplementation) or cluster reduction and degemination (e.g. Dutch kunstof instead of kunststof). This method is further enhanced by the following refinements. First, a spelling may be transcribed into more than one phonological code in order to account for possible pronunciation variants, especially those due to several possible stress patterns. Second, the phonological code itself is designed to intentionally blur some finer phonological distinctions. E.g. in order to account for the fact that short vowels in unstressed syllables are prone to misspellings (e.g. optomization, incoded) such vowels are always re- duced to a schwa /d/. As a result, misspellings of this type will collocate. It is clear that this method is suited only for errors which result in completely homophonous spellings (e.g. issuing, inplementation). A somewhat less stringent similarity measure is created by using a coarse phonological coding, as mentioned above. Still, this method is not suitable for most typo- graphical errors. Moreover, orthographical errors involving 'hard' phonological differences (e.g. managable, recommand) fail to lead to correction. 3. AN INTEGRATED METHOD 3.1 Combining methods Of the methods described in the previous chapter, no single method sufficiently covers the whole spectrum of errors. Because each method has its strengths and weaknesses, it is advantageous to combine two methods which supplement each other. Because orthographical errors are the most difficult and persistent, we chose to take a linguistic method as a starting point and added another method to cover its weaknesses. SPELL THERAPIST has two weak points. First, most typographical errors cannot be corrected. Second, even though the phonological codes are somewhat blurred, at least one possible transcription of the misspelling must match exactly with the phonological code of the intended word. A possible solution to both problems consists in applying a general string comparison technique to phonological codes rather than spellings. We decided to combine SPELL THERAPIST with trigram analysis by using sequences of three phonemes instead of three characters. We call such a sequence a triphone and the new strategy triphone analysis. 3.2 Triphone analysis Triphone analysis is a fast and efficient method for correcting orthographical and typographical errors. When carefully implemented, it is not significantly slower than trigram analysis. The new method uses only one dictionary in the form of an inverted file of triphones. Such a file is created by first computing phonological variants for each word, then splitting each code into triphones, and finally adding backpointers from each triphone in the file to each spelling in which it occurs. Also, a frequency value is associated with each triphone. The way this inverted file is used during correction is virtually the same as in FUZZIE, except that first all phonological variants of the misspelling have to be generated. The grapheme-to-phoneme conversion is similar to that of SPELL THERAPIST, except that the phonological code is made even coarser by means of various simplifications., e.g. by removing the distinction between tense and lax vowels and by not applying certain phonological rules. The easiest way to select probable corrections from an inverted file is the method used by FUZZIE, because the similarity measure used by ACUTE requires that the number of triphones in the possible correction be known in advance. The problem with this requirement is that phonological variants may have different string lengths and hence a varying number of triphones. Using the FUZZIE method, each phonological variant may select probable corrections by means of the following steps: 1. The phonological code is split into triphones. 2. Each triphone receives an information value depending on its frequency. The sum of all values is 1. 3. The selective triphones (those with a frequency below a certain preset value) are looked up in the inverted file. 4. For all correction candidates found in this way, the similarity with the misspelling is determined by computing the sum of the information values of all triphones shared between the candidate and the misspelling. If a certain candidate for correction is found by more than one phonological variant, only the highest information value for that candidate is retained. After candidates have been selected for all variants, they are ordered by their similarity values. A possible extension could be realized by also taking into account the difference in string length between the misspelling and each candidate. Because processing time increases with each phonological variant, it is important to reduce the number of variants as much as possible. A consid- erable reduction is achieved by not generating a separate variant for each possible stress pattern. The resulting inaccuracy is largely compensated by the fact that a perfect match is no longer required by the new method. Although this method yields very satisfactory results for both orthographical and typographical errors and for combinations of them, it does have some shortcomings for typographical errors in short words. One problem is that certain deletions cause two surrounding letters to be contracted into very different phonemes. Consider the deletion of the r in very: the pronunciation of the vowels in the resulting spelling, vey, changes substantially. Counting one surrounding space, the misspelling does not have a single triphone in common with the original and so it cannot be corrected. A second problem is that a character (or character cluster) leading to several possible phonemes carries more information than a character leading to a single phoneme. Consequently, an error affecting such a character disturbs more triphones. 3.3 An experiment The triphone analysis method presented here has been implemented on a Symbolics LISP Machine and on an APOLLO workstation running Common LISP. After the programs had been completed, we decided to test the new method and compare its qualitative performance with that of the other methods. For a first, preliminary test we chose our domain carefully. The task domain had to be very error-prone, especially with respect to orthographical errors, so that we could elicit errors from human subjects under controlled circumstances. Given these requirements, we decided to choose Dutch surnames as the task domain. In Dutch, many surnames have very different spellings. For example, there are 32 different names with the same pronunciation as Theyse, and even 124 ways to spell Craeybeckx! When such a name is written in a dictation task (e.g. during a telephone conversation) the chance of the right spelling being chosen is quite small. For our experiment, we recorded deviant spellings of Dutch surnames generated by native speakers of Dutch in a writing-to-dictation task. A series of 123 Dutch surnames was randomly chosen from a telephone directory. The names were dictated to 10 subjects via a cassette tape recording. A comparison of the subjects' spelling with the intended spellings showed that on the average, subjects wrote down 37.6% of the names in a deviant way. The set of 463 tokens of misspellings contained 188 different types, which were subsequently given as input to imple- mentations of each of the methods2. The dictionary consisted of 254 names (the 123 names mentioned above plus 131 additional Dutch surnames randomly selected from a different source). The results of the correction are presented in Tables 1 and 2. Table 1. Results of the evaluation study. The numbers refer to percentages of recognized (first, second or third choice) or not recognized surnames (n = 188). 1st choice 2nd or 3rd not found SPELL 58.5 1.1 40.4 SPEEDCOP 53.7 1.1 45.2 FUZZIE 86.2 9.6 4.2 ACUTE 89.9 6.9 3.2 PF-474 84.0 14.9 1.1 SPELL THERAPIST 86.2 1.1 12.8 TRIPHONE ANALYSIS 94.1 5.9 0.0 Table 2. Results of the evaluation study. The numbers refer to percentages of recognized (first, second or third choice) or not recognized surnames multiplied by their frequencies (n = 463). 1st choice 2nd or 3rd not found SPELL 63.7 2.2 34.1 SPEEDCOP 55.7 2.2 42.1 FUZZIE 87.7 8.4 3.9 ACUTE 90.3 6.7 3.0 PF-474 85.5 14.1 0.4 SPELL THERAPIST 90.5 2.2 7.3 TRIPHONE ANALYSIS 95.2 4.8 0.0 2 The PF-474 method was simulated in software instead of using the special hardware. 3.4 Discussion The experiment was designed in order to minimize typographical errors and to maximize orthographical errors. Hence it is not surprising that SPELL and SPEEDCOP, which are very much dependent on the characteristics of typographical errors, do very poorly. What is perhaps most surprising is that SPELL THERAPIST, a method primarily aiming at the correction of orthographical errors, shows worse results than FUZZIE, ACUTE and the PF-474 method, which are general string comparison methods. The reason is that a certain number of orthographical errors turned out to involve real phonological differences. These were probably caused by mishearings rather than misspellings. Poor sound quality of the cassette recorder and dialectal differences between speaker and hearer are possible causes. As expected, triphone analysis yielded the best results: not a single misspelling could not be corrected, and only about one out of twenty failed to be returned as the most likely correction. 4. CONCLUSION We have demonstrated that an integration of complementary correction methods performs better than single methods. With respect to orthographical errors, triphone analysis performs better than either grapheme-to-phoneme conversion or trigram analysis alone. Its capacity to correct typographical errors is still to be evaluated, but it is already clear that it will be better than that of SPELL THERAPIST although somewhat worse than trigram analysis in those cases where a typographical error drastically alters the pronunciation. In practice, however, one always finds both kinds of errors. Therefore, it would be interes- ting to compare the various methods in actual use. Future research will go into a number of variants on the basic ideas presented here. From a linguistic point of view, it is possible to make the pho- nological matching less stringent. One way to do this is to use a comparison at the level of phonological features rather than phonemes. However, greater emphasis on orthographical errors may deteriorate performance on the correction of typing errors. An area of current research is the extension of triphone analysis toward the correction of compounds. In languages like Dutch and German, new compounds such as taaltechnologie (language technology) are normally written as one word. Correction of errors in such compounds is difficult because the constituting words should be corrected separately but there is no [ERROR: Failed to process this page - Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.]"
  },
  {
    "title": "Mostly-Unsupervised Statistical Segmentation of Japanese: Applications to Kanji",
    "abstract": "Given the lack of word delimiters in written Japanese, word segmentation is generally considered a crucial first step in processing Japanese texts. Typical Japanese segmentation algorithms rely either on a lexicon and grammar or on pre-segmented data. In contrast, we introduce a novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyzers over a variety of error metrics.",
    "content": "1 Introduction Because Japanese is written without delimiters be- tween words,¹ accurate word segmentation to re- cover the lexical items is a key step in Japanese text processing. Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) er- rors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Fung, 1998). Typically, Japanese word segmentation is per- formed by morphological analysis based on lexical and grammatical knowledge. This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, al- though using this heuristic alone achieves less than 60% accuracy (Nagata, 1997). Character sequences consisting solely of kanji pose a challenge to morphologically-based seg- menters for several reasons. First and most importantly, kanji sequences often contain domain terms and proper nouns: Fung (1998) notes that 50-85% of the terms in various technical dictio- The analogous situation in English would be if words were written without spaces between them. # of characters % of corpus Sequence length 1-3 kanji 20,405,486 25.6 4 - 6 kanji 12,743,177 16.1 more than 6 kanji 3,966,408 5.1 Total 37,115,071 46.8 Figure 1: Statistics from 1993 Japanese newswire (ΝΙΚΚΕΙ), 79,326,406 characters total. naries are composed at least partly of kanji. Such words tend to be missing from general-purpose lexicons, causing an unknown word problem for morphological analyzers; yet, these terms are quite important for information retrieval, information extraction, and text summarization, making correct segmentation of these terms critical. Second, kanji sequences often consist of compound nouns, so grammatical constraints are not applicable. For instance, the sequence sha-choh|ken|gyoh-mu|bu- choh (president|and|business|general manager = \"a president as well as a general manager of business\") could be incorrectly segmented as: sha- choh|ken-gyoh|mu|bu-choh (president subsidiary business Tsutomu [a name]|general manager); since both alternatives are four-noun sequences, they cannot be distinguished by part-of-speech information alone. Finally, heuristics based on changes in character type obviously do not apply to kanji-only sequences. Although kanji sequences are difficult to seg- ment, they can comprise a significant portion of Japanese text, as shown in Figure 1. Since se- quences of more than 3 kanji generally consist of more than one word, at least 21.2% of 1993 Nikkei newswire consists of kanji sequences requiring seg- mentation. Thus, accuracy on kanji sequences is an important aspect of the total segmentation process. As an alternative to lexico-grammatical and su- pervised approaches, we propose a simple, effi- cient segmentation method which learns mostly from very large amounts of unsegmented training data, thus avoiding the costs of building a lexicon or grammar or hand-segmenting large amounts of training data. Some key advantages of this method are: • No Japanese-specific rules are employed, en- hancing portability to other languages. • A very small number of pre-segmented train- ing examples (as few as 5 in our experiments) are needed for good performance, as long as large amounts of unsegmented data are avail- able. • For long kanji strings, the method produces re- sults rivalling those produced by Juman 3.61 (Kurohashi and Nagao, 1998) and Chasen 1.0 (Matsumoto et al., 1997), two morphological analyzers in widespread use. For instance, we achieve 5% higher word precision and 6% bet- ter morpheme recall. 2 Algorithm Our algorithm employs counts of character n-grams in an unsegmented corpus to make segmentation de- cisions. We illustrate its use with an example (see Figure 2). Let \"ABCDWXYZ\" represent an eight-kanji sequence. To decide whether there should be a word boundary between D and W, we check whether n- grams that are adjacent to the proposed boundary, such as the 4-grams s₁ =\"A B C D\" and s₂ =\"W XYZ\", tend to be more frequent than n-grams that straddle it, such as the 4-gram t₁ = \"BCD W\". If so, we have evidence of a word boundary between D and W, since there seems to be relatively little cohesion between the characters on opposite sides of this gap. The n-gram orders used as evidence in the seg- mentation decision are specified by the set N. For instance, if N = {4} in our example, then we pose the six questions of the form, \"Is #(si) > #(t;)?\", where #(x) denotes the number of occurrences of x in the (unsegmented) training corpus. If N = {2,4}, then two more questions (Is \"#(CD) > #(DW)?\" and \"Is #(WX) > #(DW)?\") are added. More formally, let st and sn be the non- straddling n-grams just to the left and right of lo- cation k, respectively, and let tnj be the straddling n-gram with j characters to the right of location k. S1 S2 ? ABCDWXYZ Figure 2: Collecting evidence for a word boundary are the non-straddling n-grams s1 and s2 more frequent than the straddling n-grams t1, t2, and t3? Let I>(y, z) be an indicator function that is 1 when y > z, and 0 otherwise.2 In order to compensate for the fact that there are more n-gram questions than (n-1)-gram questions, we calculate the fraction of affirmative answers separately for each n in N: vn(k) = 1 2 n-1 2(n - 1) ΣΣ I>(#(s), #(t)) i=1 j=1 Then, we average the contributions of each n-gram order: vN(k) = 1 |N| Σ vn(k) n∈N After vN(k) is computed for every location, bound- aries are placed at all locations l such that either: • vN(l) > vN(l – 1) and vN(l) > vN(l + 1) (that is, l is a local maximum), or • vN(l) ≥ t, a threshold parameter. The second condition is necessary to allow for single-character words (see Figure 3). Note that it also controls the granularity of the segmentation: low thresholds encourage shorter segments. Both the count acquisition and the testing phase are efficient. Computing n-gram statistics for all possible values of n simultaneously can be done in O(m log m) time using suffix arrays, where m is the training corpus size (Manber and Myers, 1993; Nagao and Mori, 1994). However, if the set N of n-gram orders is known in advance, conceptually simpler algorithms suffice. Memory allocation for 2Note that we do not take into account the magnitude of the difference between the two frequencies; see section 5 for discussion. --- v(k) A B C D|WX|Y|Z t Figure 3: Determining word boundaries. The X- Y boundary is created by the threshold criterion, the other three by the local maximum condition. count tables can be significantly reduced by omit- ting n-grams occurring only once and assuming the count of unseen n-grams to be one. In the applica- tion phase, the algorithm is clearly linear in the test corpus size if N❘ is treated as a constant. Finally, we note that some pre-segmented data is necessary in order to set the parameters N and t. However, as described below, very little such data was required to get good performance; we therefore deem our algorithm to be \"mostly unsupervised\". 3 Experimental Framework Our experimental data was drawn from 150 megabytes of 1993 Nikkei newswire (see Figure 1). Five 500-sequence held-out subsets were ob- tained from this corpus, the rest of the data serv- ing as the unsegmented corpus from which to derive character n-gram counts. Each held-out subset was hand-segmented and then split into a 50-sequence parameter-training set and a 450-sequence test set. Finally, any sequences occurring in both a test set and its corresponding parameter-training set were discarded from the parameter-training set, so that these sets were disjoint. (Typically no more than five sequences were removed.) 3.1 Held-out set annotation Each held-out set contained 500 randomly-extracted kanji sequences at least ten characters long (about twelve on average), lengthy sequences being the most difficult to segment (Takeda and Fujisaki, 1987). To obtain the gold-standard annotations, we segmented the sequences by hand, using an observa- tion of Takeda and Fujisaki (1987) that many kanji compound words consist of two-character stem words together with one-character prefixes and suf- fixes. Using this terminology, our two-level bracket- ing annotation may be summarized as follows.3 At the word level, a stem and its affixes are bracketed together as a single unit. At the morpheme level, stems are divided from their affixes. For example, although both naga-no (Nagano) and shi (city) can appear as individual words, naga-no-shi (Nagano city) is bracketed as [[naga-no][shi]], since here shi serves as a suffix. Loosely speaking, word-level bracketing demarcates discourse entities, whereas morpheme-level brackets enclose strings that cannot be further segmented without loss of meaning.4 For instance, if one segments naga-no in naga-no-shi into naga (long) and no (field), the intended mean- ing disappears. Here is an example sequence from our datasets: [小学校][屋内] [[運動][場]][建設] Three native Japanese speakers participated in the annotation: one segmented all the held-out data based on the above rules, and the other two reviewed 350 sequences in total. The percentage of agree- ment with the first person's bracketing was 98.42%: only 62 out of 3927 locations were contested by a verifier. Interestingly, all disagreement was at the morpheme level. 3.2 Baseline algorithms We evaluated our segmentation method by com- paring its performance against Chasen 1.05 (Mat- sumoto et al., 1997) and Juman 3.61,6 (Kurohashi and Nagao, 1998), two state-of-the-art, publically- available, user-extensible morphological analyzers. In both cases, the grammars were used as distributed without modification. The sizes of Chasen's and Ju- man's default lexicons are approximately 115,000 and 231,000 words, respectively. Comparison issues An important question that arose in designing our experiments was how to en- able morphological analyzers to make use of the parameter-training data, since they do not have pa- rameters to tune. The only significant way that they can be updated is by changing their grammars or lexicons, which is quite tedious (for instance, we had to add part-of-speech information to new en- tries by hand). We took what we felt to be a rea- sonable, but not too time-consuming, course of cre- ating new lexical entries for all the bracketed words in the parameter-training data. Evidence that this ³A complete description of the annotation policy, including the treatment of numeric expressions, may be found in a tech- nical report (Ando and Lee, 1999). ⁴This level of segmentation is consistent with Wu's (1998) Monotonicity Principle for segmentation. ⁵http://cactus.aist-nara.ac.jp/lab/nlt/chasen.html ⁶http://pine.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html --- performance (%) 90 85 60 75 70 CHASEN JUMAN Word accuracy optimize optimize recall optimize F precision precision recall F-measure Figure 4: Word accuracy. The three rightmost groups represent our algorithm with parameters tuned for different optimization criteria. was appropriate comes from the fact that these ad- ditions never degraded test set performance, and in- deed improved it by one percent in some cases (only small improvements are to be expected because the parameter-training sets were fairly small). It is important to note that in the end, we are com- paring algorithms with access to different sources of knowledge. Juman and Chasen use lexicons and grammars developed by human experts. Our al- gorithm, not having access to such pre-compiled knowledge bases, must of necessity draw on other information sources (in this case, a very large un- segmented corpus and a few pre-segmented exam- ples) to compensate for this lack. Since we are in- terested in whether using simple statistics can match the performance of labor-intensive methods, we do not view these information sources as conveying an unfair advantage, especially since the annotated training sets were small, available to the morpho- logical analyzers, and disjoint from the test sets. 4 Results We report the average results over the five test sets using the optimal parameter settings for the corre- sponding training sets (we tried all nonempty sub- sets of {2, 3, 4, 5, 6} for the set of n-gram orders N and all values in {.05, 1, 15, ..., 1} for the thresh- old t)7. In all performance graphs, the \"error bars\" represent one standard deviation. The results for Chasen and Juman reflect the lexicon additions de- For simplicity, ties were deterministically broken by pre- ferring smaller sizes of N, shorter n-grams in N, and larger threshold values, in that order. scribed in section 3.2. Word and morpheme accuracy The standard metrics in word segmentation are word precision and recall. Treating a proposed segmentation as a non-nested bracketing (e.g., \"|AB|C|\" corresponds to the bracketing \"[AB][C]\"), word precision (P) is defined as the percentage of proposed brackets that exactly match word-level brackets in the annotation; word recall (R) is the percentage of word-level an- notation brackets that are proposed by the algorithm in question; and word F combines precision and re- call: F = 2PR/(P+R). One problem with using word metrics is that morphological analyzers are designed to produce morpheme-level segments. To compensate, we al- tered the segmentations produced by Juman and Chasen by concatenating stems and affixes, as iden- tified by the part-of-speech information the analyz- ers provided. (We also measured morpheme accu- racy, as described below.) Figures 4 and 8 show word accuracy for Chasen, Juman, and our algorithm for parameter settings optimizing word precision, recall, and F-measure rates. Our algorithm achieves 5.27% higher preci- sion and 0.26% better F-measure accuracy than Ju- man, and does even better (8.8% and 4.22%, respec- tively) with respect to Chasen. The recall perfor- mance falls (barely) between that of Juman and that of Chasen. As noted above, Juman and Chasen were de- signed to produce morpheme-level segmentations. We therefore also measured morpheme precision, recall, and F measure, all defined analogously to their word counterparts. Figure 5 shows our morpheme accuracy results. We see that our algorithm can achieve better recall (by 6.51%) and F-measure (by 1.38%) than Juman, and does better than Chasen by an even wider mar- gin (11.18% and 5.39%, respectively). Precision was generally worse than the morphological analyz- ers. Compatible Brackets Although word-level accu- racy is a standard performance metric, it is clearly very sensitive to the test annotation. Morpheme ac- curacy suffers the same problem. Indeed, the au- thors of Juman and Chasen may well have con- structed their standard dictionaries using different notions of word and morpheme than the definitions we used in annotating the data. We therefore devel- oped two new, more robust metrics to measure the number of proposed brackets that would be incor- [[data] [base]] [system] (annotation brackets) Proposed segmentation word morpheme compatible-bracket errors errors errors crossing morpheme-dividing [[data] [base]] [system] 2 0 0 0 [[data] [basesystem] 2 1 1 0 [[database] [sys] [tem] 2 3 0 2 Figure 6: Examples of word, morpheme, and compatible-bracket errors. The sequence \"data base\" has been annotated as \"[[data] [base]]\" because \"data base\" and \"database\" are interchangeable. Morpheme accuracy 85 80 performance (%) 75 70 CHASEN JUMAN optimize precision optimize recall optimize F Figure 5: Morpheme accuracy. rect with respect to any reasonable annotation. Our novel metrics account for two types of errors. The first, a crossing bracket, is a proposed bracket that overlaps but is not contained within an annotation bracket (Grishman et al., 1992). Crossing brackets cannot coexist with annotation brackets, and it is unlikely that another human would create such brackets. The second type of error, a morpheme-dividing bracket, subdivides a morpheme-level annotation bracket; by definition, such a bracket results in a loss of meaning. See Figure 6 for some examples. We define a compatible bracket as a proposed bracket that is neither crossing nor morpheme- dividing. The compatible brackets rate is simply the compatible brackets precision. Note that this metric accounts for different levels of segmentation simultaneously, which is beneficial because the gran- ularity of Chasen and Juman's segmentation varies from morpheme level to compound word level (by our definition). For instance, well-known university names are treated as single segments by virtue of being in the default lexicon, whereas other university names are divided into the name and the word \"university\". Using the compatible brackets rate, both segmentations can be counted as correct. We also use the all-compatible brackets rate, which is the fraction of sequences for which all the proposed brackets are compatible. Intuitively, this function measures the ease with which a human could correct the output of the segmentation algorithm: if the all-compatible brackets rate is high, then the errors are concentrated in relatively few sequences; if it is low, then a human doing post- processing would have to correct many sequences. Figure 7 depicts the compatible brackets and all- compatible brackets rates. Our algorithm does better on both metrics (for instance, when F-measure is optimized, by 2.16% and 1.9%, respectively, in comparison to Chasen, and by 3.15% and 4.96%, respectively, in comparison to Juman), regardless of training optimization function (word precision, recall, or F—we cannot directly optimize the compatible brackets rate because \"perfect\" performance is possible simply by making the entire sequence a single segment). Compatible and all-compatible brackets rates 100 95 performance (%) 90 85 80 75 CHASEN JUMAN optimize precision optimize recall optimize F compatible brackets rates all-compatible brackets rates Figure 7: Compatible brackets and all-compatible bracket rates when word accuracy is optimized. precision recall F-measure Juman5 vs. Juman50 Our50 vs Juman50 Our5 vs. Juman5 Our5 vs. Juman50 -1.04 -0.63 -0.84 +5.27 -4.39 +0.26 +6.18 -3.73 +1.14 +5.14 -4.36 +0.30 Figure 8: Relative word accuracy as a function of training set size. \"5\" and \"50\" denote training set size before discarding overlaps with the test sets. 4.1 Discussion Minimal human effort is needed. In contrast to our mostly-unsupervised method, morphological analyzers need a lexicon and grammar rules built using human expertise. The workload in creating dictionaries on the order of hundreds of thousands of words (the size of Chasen's and Juman's de- fault lexicons) is clearly much larger than annotat- ing the small parameter-training sets for our algo- rithm. We also avoid the need to segment a large amount of parameter-training data because our al- gorithm draws almost all its information from an unsegmented corpus. Indeed, the only human effort involved in our algorithm is pre-segmenting the five 50-sequence parameter training sets, which took only 42 minutes. In contrast, previously proposed supervised approaches have used segmented train- ing sets ranging from 1000-5000 sentences (Kash- ioka et al., 1998) to 190,000 sentences (Nagata, 1996a). To test how much annotated training data is actu- ally necessary, we experimented with using minis- cule parameter-training sets: five sets of only five strings each (from which any sequences repeated in the test data were discarded). It took only 4 minutes to perform the hand segmentation in this case. As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better. In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five anno- tated sequences when Juman has access to ten times as many, we still achieve better precision and better F measure. Both the local maximum and threshold condi- tions contribute. In our algorithm, a location k is deemed a word boundary if vn(k) is either (1) a local maximum or (2) at least as big as the thresh- old t. It is natural to ask whether we really need two conditions, or whether just one would suffice. We therefore studied whether optimal perfor- mance could be achieved using only one of the con- ditions. Figure 9 shows that in fact both contribute to producing good segmentations. Indeed, in some cases, both are needed to achieve the best perfor- mance; also, each condition when used in isolation yields suboptimal performance with respect to some performance metrics. accuracy optimize optimize optimize precision F-measure word morpheme M recall M & T M M & T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmenta- tion methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Mat- sumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papa- georgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisaki (1987) propose the short unit model, a type of Hidden Markov Model with linguistically- determined topology, to segment kanji compound words. However, their method does not handle three-character stem words or single-character stem words with affixes, both of which often occur in proper nouns. In our five test datasets, we found that 13.56% of the kanji sequences contain words that cannot be handled by the short unit model. Nagao and Mori (1994) propose using the heuris- tic that high-frequency character n-grams may rep- resent (portions of) new collocations and terms, but the results are not experimentally evaluated, nor is a general segmentation algorithm proposed. The work of Ito and Kohda (1995) similarly relies on high-frequency character n-grams, but again, is more concerned with using these frequent n-grams as pseudo-lexicon entries; a standard segmentation algorithm is then used on the basis of the induced lexicon. Our algorithm, on the hand, is fundamen- tally different in that it incorporates no explicit no- tion of word, but only \"sees\" locations between characters. Chinese According to Sproat et al. (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pub- lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical ap- proach. In a later paper, Palmer (1997) presents a transformation-based algorithm, which requires pre-segmented training data. To our knowledge, the Chinese segmenter most similar to ours is that of Sun et al. (1998). They also avoid using a lexicon, determining whether a given location constitutes a word boundary in part by deciding whether the two characters on either side tend to occur together; also, they use thresholds and several types of local minima and maxima to make segmentation decisions. However, the statis- tics they use (mutual information and t-score) are more complex than the simple n-gram counts that we employ. Our preliminary reimplementation of their method shows that it does not perform as well as the morphological analyzers on our datasets, al- though we do not want to draw definite conclusions because some aspects of Sun et al's method seem incomparable to ours. We do note, however, that their method incorporates numerical differences between statistics, whereas we only use indicator functions; for example, once we know that one trigram is more common than another, we do not take into account the difference between the two frequencies. We conjecture that using absolute differences may have an adverse effect on rare sequences. 6 Conclusion In this paper, we have presented a simple, mostly- unsupervised algorithm that segments Japanese se- quences into words based on statistics drawn from a large unsegmented corpus. We evaluated per- formance on kanji with respect to several metrics, including the novel compatible brackets and all- compatible brackets rates, and found that our al- gorithm could yield performances rivaling that of lexicon-based morphological analyzers. In future work, we plan to experiment on Japanese sentences with mixtures of character types, possibly in combination with morphologi- cal analyzers in order to balance the strengths and weaknesses of the two types of methods. Since our method does not use any Japanese-dependent heuristics, we also hope to test it on Chinese or other languages as well. Acknowledgments We thank Minoru Shindoh and Takashi Ando for reviewing the annotations, and the anonymous re- viewers for their comments. This material was sup- ported in part by a grant from the GE Foundation. References Rie Ando and Lillian Lee. 1999. Unsupervised sta- tistical segmentation of Japanese kanji strings. Technical Report TR99-1756, Cornell University. Takeshi Fuchi and Shinichiro Takagi. 1998. Japanese morphological analyzer using word co- occurrence - JTAG. In Proc. of COLING-ACL '98, pages 409-413. Pascale Fung. 1998. Extracting key terms from Chinese and Japanese texts. Computer Process- ing of Oriental Languages, 12(1). Ralph Grishman, Catherine Macleod, and John Sterling. 1992. Evaluating parsing strategies us- ing standardized parse files. In Proc. of the 3rd ANLP, pages 156-161. Akinori Ito and Kasaki Kohda. 1995. Language modeling by string pattern N-gram for Japanese speech recognition. In Proc. of ICASSP. Hideki Kashioka, Yasuhiro Kawata, Yumiko Kinjo, Andrew Finch, and Ezra W. Black. 1998. Use of mutual information based character clus- ters in dictionary-less morphological analysis of Japanese. In Proc. of COLING-ACL '98, pages 658-662. Sadao Kurohashi and Makoto Nagao. 1998. Japanese morphological analysis system JUMAN version 3.6 manual. In Japanese. Udi Manber and Gene Myers. 1993. Suffix arrays: A new method for on-line string searches. SIAM Journal on Computing, 22(5):935-948. T. Matsukawa, Scott Miller, and Ralph Weischedel. 1993. Example-based correction of word seg- mentation and part of speech labelling. In Proc. of the HLT Workshop, pages 227-32. Yuji Matsumoto and Makoto Nagao. 1994. Im- provements of Japanese morphological analyzer JUMAN. In Proc. of the International Workshop on Sharable Natural Language Resources, pages 22-28. Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Osamu Imaichi, and Tomoaki Imamura. 1997. Japanese morphological anal- ysis system ChaSen manual. Technical Report NAIST-IS-TR97007, Nara Institute of Science and Technology. In Japanese. Shinsuke Mori and Makoto Nagao. 1998. Un- known word extraction from corpora using n- gram statistics. Journal of the Information Pro- cessing Society of Japan, 39(7):2093-2100. In Japanese. Makoto Nagao and Shinsuke Mori. 1994. A new method of N-gram statistics for large number of n and automatic extraction of words and phrases from large text data of Japanese. In Proc. of the 15th COLING, pages 611-615. Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backward-A* n-best search algorithm. In Proc. of the 15th COLING, pages 201-207. Masaaki Nagata. 1996a. Automatic extraction of new words from Japanese texts using generalized forward-backward search. In Proc. of the Confer- ence on Empirical Methods in Natural Language Processing, pages 48-59. Masaaki Nagata. 1996b. Context-based spelling correction for Japanese OCR. In Proc. of the 16th COLING, pages 806-811. Masaaki Nagata. 1997. A self-organizing Japanese word segmenter using heuristic word identifica- tion and re-estimation. In Proc. of the 5th Work- shop on Very Large Corpora, pages 203-215. David Palmer. 1997. A trainable rule-based algo- rithm for word segmentation. In Proc. of the 35th ACL/8th EACL, pages 321-328. Constantine P. Papageorgiou. 1994. Japanese word segmentation by hidden Markov model. In Proc. of the HLT Workshop, pages 283-288. Richard Sproat and Chilin Shih. 1990. A statistical method for finding word boundaries in Chinese text. Computer Processing of Chinese and Ori- ental Languages, 4:336-351. Richard Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-sate word-segmentation algorithm for Chinese. Com- putational Linguistics, 22(3). Maosong Sun, Dayang Shen, and Benjamin K. Tsou. 1998. Chinese word segmentation without using lexicon and hand-crafted training data. In Proc. of COLING-ACL '98, pages 1265-1271. Koichi Takeda and Tetsunosuke Fujisaki. 1987. Automatic decomposition of kanji compound words using stochastic estimation. Journal of the Information Processing Society of Japan, 28(9):952-961. In Japanese. Kouichi Takeuchi and Yuji Matsumoto. 1995. HMM parameter learning for Japanese morpho- logical analyzer. In Proc. of the 10th Pacific Asia Conference on Language, Information and Com- putation (PACLING), pages 163-172. Virginia Teller and Eleanor Olds Batchelder. 1994. A probabilistic algorithm for segmenting non- kanji Japanese strings. In Proc. of the 12th AAAI, pages 742-747. Laura Mayfield Tomokiyo and Klaus Ries. 1997. What makes a word: learning base units in Japanese for speech recognition. In Proc. of the ACL Special Interest Group in Natural Language Learning (CoNLL97), pages 60-69. Zimin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems. Journal of the American Society for Information Science, 44(9):532-542. Dekai Wu. 1998. A position statement on Chinese segmentation. http://www.cs.ust.hk/~dekai/- papers/segmentation.html. Presented at the Chinese Language Processing Workshop, University of Pennsylvania. Mikio Yamamoto. 1996. A re-estimation method for stochastic language modeling from ambigu- ous observations. In Proc. of the 4th Workshop on Very Large Corpora, pages 155-167. J. Yamron, J. Baker, P. Bamberg, H. Chevalier, T. Dietzel, J. Elder, F. Kampmann, M. Mandel, L. Manganaro, T. Margolis, and E. Steele. 1993. LINGSTAT: An interactive, machine-aided trans- lation system. In Proc. of the HLT Workshop, pages 191-195."
  },
  {
    "title": "Localizing Expression of Ambiguity",
    "abstract": "In this paper we describe an implemented program for localizing the expression of many types of syntactic ambiguity in the logical forms of sentences, in a manner convenient for subsequent inferential processing. Among the types of ambiguities handled are prepositional phrases, verb compound nominals, adverbials, relative clauses, and preposed prepositional phrases. The algorithm we use is presented, and several possible shortcomings and extensions of our method are discussed.",
    "content": "1 Introduction Ambiguity is a problem in any natural language processing system. Large grammars tend to produce large numbers of alternative analyses for even relatively simple sentences. Furthermore, as is well known, syntactic information may be insufficient for selecting a best reading. It may take se- mantic knowledge of arbitrary complexity to decide which alternative to choose. In the TACITUS project (Hobbs, 1986; Hobbs and Martin, 1987) we are developing a pragmatics component which, given the logical form of a sentence, uses world knowledge to solve various interpretation problems, the resolution of syntactic ambiguity among them. Sentences are translated into logical form by the DIALOGIC system for syntactic and semantic analysis (Grosz et al., 1982]. In this paper we describe how information about alter- native parses is passed concisely from DIALOGIC to the pragmatics component, and more generally, we discuss a method of localizing the representation of syntactic ambi- guity in the logical form of a sentence. One possible approach to the ambiguity problem would be to produce a set of logical forms for a sentence, one for each parse tree, and to send them one at a time to the pragmatics component. This involves considerable dupli- cation of effort if the logical forms are largely the same and differ only with respect to attachment. A more effi- cient approach is to try to localize the information about the alternate possibilities. Instead of feeding two logical forms, which differ only with respect to an attachment site, to a pragmatics com- ponent, it is worthwhile trying to condense the information of the two logical forms together into one expression with a disjunction inside it representing the attachment ambigu- ity. That one expression may then be given to a pragmat- ics component with the effect that parts of the sentence that would have been processed twice are now processed only once. The savings can be considerably more dramatic when a set of five or ten or twenty logical forms can be re- duced to one, as is often the case. In effect, this approach translates the syntactic ambigu- ity problem into a highly constrained coreference problem. It is as though we translated the sentence in (1) into the two sentences in (2) (1) John drove down the street in a car. (2) John drove down the street. It was in a car. where we knew \"it\" had to refer either to the street or to the driving. Since coreference is one of the phenomena the pragmatics component is designed to cope with (Hobbs and Martin, 1987), such a translation represents progress toward a solution. The rest of this paper describes the procedures we use to produce a reduced set of logical forms from a larger set. The basic strategy hinges on the idea of a neu- tral representation (Hobbs, 1982). This is similar to the idea behind Church's Pseudo-attachment (Church, 1980]. Pereira's Rightmost Normal Form (Pereira, 1983), and what Rich et al. refer to as the Procrastination Approach to parsing (Rich, Barnett, Wittenburg, and Whittemore, 1086). However, by expressing the ambiguity as a disjunc- tion in logical form, we put it into the form most convenient for subsequent inferential processing. 2 Range of Phenomena 2.1 Attachment Possibilities There are three representative classes of attachment ambi- guities, and we have implemented our approach to each of these. For each class, we give representative examples and show the relevant logical form fragments that encode the set of possible attachments. In the first class are those constituents that may attach to either nouns or verbs. (3) John saw the man with the telescope. The prepositional phrase (PP) \"with the telescope\" can be attached either to \"the man\" or to \"saw\". If m stands for the man, t for the telescope, and e for the seeing event, the neutral logical form for the sentence includes ... ^ with(y,t) ^ [y = m V y = e] ^ ... That is, something y is with the telescope, and it is either the man or the seeing event. Gerund modifiers may also modify nouns and verbs, re- sulting in ambiguities like that in the sentence I saw the Grand Canyon, flying to New York. Their treatment is identical to that of PPs. If g is the Grand Canyon, n is New York, and e is the seeing event, the neutral logical form will include ... ^ fly(y,n) ^ [y = g V y = e] ^ ... That is, something y is flying to New York, and it is either the Grand Canyon or the seeing event. In the second class are those constituents that can only attach to verbs, such as adverbials. George said Sam left his wife yesterday. Here \"yesterday\" can modify the saying or the leaving but not \"his wife\". Suppose we take yesterday to be a predi- cate that applies to events and specifies something about their times of occurrence, and suppose e₁ is the leaving event and e₂ the saying event. Then the neutral logical form will include ... ^ yesterday(y) ^ [y = e₁ V y = e₂] ^ ... That is, something y was yesterday and it is either the leaving event or the saying event. Related to this is the case of a relative clause where the preposed constituent is a PP, which could have been extracted from any of several embedded clauses. In That was the week during which George thought Sam told his wife he was leaving, the thinking, the telling, or the leaving could have been during the week. Let w be the week, e₁ the thinking, e₂ the telling, and e₃ the leaving. Then the neutral logical form will include ... ^ during(y, w) ^ [y = e₁ V y = e₂ V y = e₃] ^ ... That is, something y was during the week, and y is either the thinking, the telling, or the leaving. The third class contains those constituents that may only attach to nouns, e.g., relative clauses. This component recycles the oil that flows through the compressor that is still good. The second relative clause, \"that is still good,\" can attach to \"compressor\", or \"oil\", but not to \"flows\" or \"recycles\". Let o be the oil and c the compressor. Then, ignoring \"still\", the neutral logical form will include ... ^ good(y) ^ [y = c V y = o] ^ ... That is, something y is still good, and y is either the com- pressor or the oil. Similar to this are the compound nominal ambiguities, as in He inspected the oil filter element. \"Oil\" could modify either \"filter\" or \"element\". Let o be the oil, f the filter, e the element, and nn the implicit relation that is encoded by the nominal compound con- struction. Then the neutral logical form will include ... ^ nn(f,e) ^ nn(o,y) ^ [y = f V y = e] ^ ... That is, there is some implicit relation nn between the filter and the element, and there is another implicit relation nn between the oil and something y, where y is either the filter or the element. Our treatment of all of these types of ambiguity has been implemented. In fact, the distinction we base the attachment possi- bilities on is not that between nouns and verbs, but that between event variables and entity variables in the logical form. This means that we would generate logical forms encoding the attachment of adverbials to event nominal- izations in those cases where the event nouns are translated with event variables. Thus in I read about Judith's promotion last year. \"last year\" would be taken as modifying either the promo- tion or the reading, if \"promotion\" were represented by an event variable in the logical form. 2.2 Single or Multiple Parse Trees In addition to classifying attachment phenomena in terms of which kind of constituent something may attach to, there is another dimension along which we need to clas- sify the phenomena: does the DIALOGIC parser produce all possible parses, or only one? For some regular struc- tural ambiguities, such as very compound nominals, and the \"during which\" examples, only a single parse is pro- duced. In this case it is straightforward to produce from the parse a neutral representation encoding all the possi- bilities. In the other cases, however, such as (nonpreposed) PPs, adverbials, and relative clauses, DIALOGIC produces an exhaustive (and sometimes exhausting) list of the dif- ferent possible structures. This distinction is an artifact of our working in the DIALOGIC system. It would be preferable if there were only one tree constructed which was somehow neutral with respect to attachment. How- ever, the DIALOGIC grammar is large and complex, and it would have been difficult to implement such an approach. Thus, in these cases, one of the parses, the one correspond- ing to right association (Kimball, 1973], is selected, and the neutral representation is generated from that. This makes it necessary to suppress redundant readings, as described below. (In fact, limited heuristics for suppressing multi- ple parse trees have recently been implemented in DIA- LOGIC.) 2.3 Thematic Role Ambiguities Neutral representations are constructed for one other kind of ambiguity in the TACITUS system-ambiguities in the thematic role or case of the arguments. In the sentence It broke the window. we don't know whether \"it\" is the agent or the instru- ment. Suppose the predicate break takes three arguments, an agent, a patient, and an instrument, and suppose x is whatever is referred to by \"it\" and w is the window. Then the neutral logical form will include ... Λ break(y₁, w, y₂) Λ [y₁ = x V y₂ = x] Λ ... That is, something y₁ breaks the window with something else y₂, and either y₁ or y₂ is whatever is referred to by \"it\".1 2.4 Ambiguities Not Handled There are other types of structural ambiguity about which we have little to say. In They will win one day in Hawaii, one of the obvious readings is that \"one day in Hawaii\" is an adverbial phrase. However, another perfectly rea- sonable reading is that \"one day in Hawaii\" is the direct object of the verb \"win\". This is due to the verb having more than one subcategorization frame that could be filled by the surrounding constituents. It is the existence of this kind of ambiguity that led to the approach of not having DIALOGIC try to build a single neutral representation in all cases. A neutral representation for such sentences, though possible, would be very complicated. Similarly, we do not attempt to produce neutral repre- sentations for fortuitous or unsystematic ambiguities such as those exhibited in sentences like They are flying planes. Time flies like an arrow. Becky saw her duck. ¹The treatment of thematic role ambiguities has been implemented by Paul Martin as part of the interface between DIALOGIC and the pragmatic processes of TACITUS that translates the logical forms of the sentences into a canonical representation. 2.5 Resolving Ambiguities It is beyond the scope of this paper to describe the prag- matics processing that is intended to resolve the ambigu- ities (see Hobbs and Martin, 1987). Nevertheless, we dis- cuss one nontrivial example, just to give the reader a feel for the kind of processing it is. Consider the sentence We retained the filter element for future analysis. Let r be the retaining event, f the filter element, and a the analysis. Then the logical form for the sentence will include ... Λ for(y, a) Λ [y = f V y = r] Λ ... The predicate for, let us say, requires the relation enable(y, a) to obtain between its arguments. That is, if y is for a, then either y or something coercible from y must somehow enable a or something coercible from a. The TACITUS knowledge base contains axioms encoding the fact that having something is a prerequisite for analyzing it and the fact that a retaining is a having. y can thus be equal to r, which is consistent with the constraints on y. On the other hand, any inference that the filter element enables the analysis will be much less direct, and conse- quently will not be chosen. 3 The Algorithm 3.1 Finding Attachment Sites The logical forms (LFs) that are produced from each of the parse trees are given to an attachment-finding program which adds, or makes explicit, information about possible attachment sites. Where this makes some LFs redundant, as in the prepositional phrase case, the redundant LFs are then eliminated. For instance, for the sentence in (4), (4) John saw the man in the park with the telescope. DIALOGIC produces five parse trees, and five correspond- ing logical forms. When the attachment-finding routine is run on an LF, it annotates the LF with information about a set of variables that might be the subject (i.e., the at- tachment site) of each PP. The example below shows the LFs for one of the five readings before and after the attachment-finding routine is run on it. They are somewhat simplified for the purposes of exposition. In this notation, a proposition is a predi- cate followed by one or more arguments. An argument is a variable or a complex term. A complex term is a vari- able followed by a \"such that\" symbol \"|\", followed by a conjunction of one or more propositions. Complex terms ²This notation can be translated into a Russellian notation, with the consequent loss of information about grammatical subordination, by repeated application of the transformation p(x|Q) ⇒ p(x) Λ Q. are enclosed in square brackets for readability. Events are represented by event variables, as in [Hobbs, 1985), so that see'(e1, 11, 12) means e₁ is a seeing event by 21 of 22. One of sentence (4)'s LFs before attachment-finding is past([e1|see'(e1, [11|John(11)], [x2|man(x2) in(12, [13|park(x3) with(13, Λ [24 telescope(#4)])])])]) The same LF after attachment-finding is past([e1 | see'(e1, [21 John(x1)], [x2|man(x2) in([Y1Y1 = 12 V Y1 = e1], [23|park(13) Λ with([Y2Y2-13 VY2=X2 V Y2=1], (x4 telescope(x4)])])])]) A paraphrase of the latter LF in English would be some- thing like this: There is an event e₁ that happened in the past; it is a seeing event by ₁ who is John, of 12 who is the man; something y₁ is in the park, and that something is either the man or the seeing event; something y2 is with a telescope, and that something is the park, the man, or the seeing event. The procedure for finding possible attachment sites in order to modify a logical form is as follows. The program recursively descends an LF, and keeps lists of the event and entity variables that initiate complex terms. Event variables associated with tenses are omitted. When the program arrives at some part of the LF that can have mul- tiple attachment sites, it replaces the explicit argument by an existentially quantified variable y, determines whether it can be an event variable, an entity variable, or either, and then encodes the list of possibilities for what y could equal. 3.2 Eliminating Redundant Logical Forms In those cases where more than one parse tree, and hence more than one logical form, is produced by DIALOGIC, it is necessary to eliminate redundant readings. In order to do this, once the attachment possibilities are registered, the LFs are flattened (thus losing temporarily the gram- matical subordination information), and some simplifying preprocessing is done. Each of the flattened LFs is com- pared with the others. Any LF that is subsumed by an- other is discarded as redundant. One LF subsumes another if the two LFs are the same except that the first has a list of possible attachment sites that includes the correspond- ing list in the second. For example, one LF for sentence (3) says that \"with the telescope\" can modify either \"saw\" or \"the man\", and one says that it modifies \"saw\". The first LF subsumes the second, and the second is discarded and not compared with any other LFs. Thus, although the LFs are compared pairwise, if all of the ambiguity is due to only one attachment indeterminacy, each LF is looked at only once. Frequently, only some of the alternatives may be thrown out. For Andy said he lost yesterday after attachment-finding, one logical form allows \"yester- day\" to be attached to either the saying or the losing, while another attaches it only to the saying. The second is sub- sumed by the first, and thus discarded. However, there is a third reading in which \"yesterday\" is the direct object of \"lost\" and this neither subsumes nor is subsumed by the others and is retained. 4 Lost Information 4.1 Crossing Dependencies Our attachment-finding routine constructs a logical form that describes all of the standard readings of a sentence, but it also describes some nonstandard readings, namely those corresponding to parse trees with crossing branches, or crossing dependencies. An example would be a reading of (4) in which the seeing was in the park and the man was with the telescope. For small numbers of possible attachment sites, this is an acceptable result. If a sentence is two-ways ambiguous (due just to attachment), we get no wrong readings. If it is five-ways ambiguous on the standard analysis, we get six readings. However, in a sentence with a sequence of four PPs, the standard analysis (and the DIALOGIC parser) get 42 readings, whereas our single disjunctive LF stands for 120 different readings. Two things can be said about what to do in these cases where the two approaches diverge widely. We could argue that sentences with such crossing dependencies do exist in English. There are some plausible sounding examples. Specify the length, in bytes, of the word. Kate saw a man on Sunday with a wooden leg. In the first, the phrase \"in bytes\" modifies \"specify\", and \"of the word\" modifies \"the length\". In the second, \"on Sunday\" modifies \"saw\" and \"with a wooden leg\" modifies \"a man\". Stucky (1987) argues that such examples are acceptable and quite frequent. On the other hand, if one feels that these putative ex- amples of crossing dependencies can be explained away and should be ruled out, there is a way to do it within our framework. One can encode in the LFs a crossing- dependencies constraint, and consult that constraint when doing the pragmatic processing. To handle the crossing-dependencies constraint (which we have not yet implemented), the program would need to keep the list of the logical variables it constructs. This list would contain three kinds of variables, event variables, entity variables, and the special variables (the y's in the LFs above) representing attachment ambiguities. The list would keep track of the order in which variables were en- countered in descending the LF. A separate list of just the special y variables also needs to be kept. The strategy would be that in trying to resolve referents, whenever one tries to instantiate a y variable to something, the other y variables need to be checked, in accordance with the fol- lowing constraint: There cannot be y1, y2 in the list of y's such that B(Y1) <B(Y2) < Y1 < Y2, where B(yi) is the proposed variable to which y; will be bound or with which it will be coreferential, and the < operator means \"precedes in the list of vari- ables\". This constraint handles a single phrase that has attach- ment ambiguities. It also works in the case where there is a string of PPs in the subject NP, and then a string of PPs in the object NP, as in The man with the telescope in the park lounged on the bank of a river in the sun. With the appropriate crossing-dependency constraints, the logical form for this would be³ past([e1 lounge'(e1, [x1 man(x1) Λ with([yı | y1 = x1 V y1 = e1], [x2 telescope(x2) Λ in([y2 | y2 = x2 V y2 = x1 V y2 = e1], [x3 | park(x3)])])]) Λ on(e1, [x4 | bank(x4) Λ of([y3 | y3 = x4 V y3 = e1], [x5 | river(x5) Λ in([y4 | y4 = x5 V y4 = x4 V y4 = e1], [x6 | sun(x6)])])]) Λ crossing-info(<e1, x1, y1, x2, y2, x3>, {x1,x2}) Λ crossing-info(<e1, x4, y3, x5, y4, x6>, {y3, y4})) ³We are assuming \"with the telescope\" and \"in the park\" can mod- ify the lounging, which they certainly can if we place commas before and after them. 4.2 Noncoreference Constraints One kind of information that is provided by the DIA- LOGIC system is information about coreference and non- coreference insofar as it can be determined from syntactic structure. Thus, the logical form for John saw him. includes the information that \"John\" and \"him\" cannot be coreferential. This interacts with our localization of attachment ambiguity. Consider the sentence, John returned Bill's gift to him. If we attach \"to him\" to \"gift\", \"him\" can be coreferential with \"John\", but it cannot be coreferential with \"Bill\". If we attach it to \"returned\", \"him\" can be coreferential with \"Bill\" but not with \"John\". It is therefore not enough to say that the \"subject\" of \"to\" is either the gift or the re- turning. Each alternative carries its own noncoreference constraints with it. We do not have an elegant solution to this problem. We mention it because, to our knowledge, this interaction of noncoreference constraints and PP at- tachment has not been noticed by other researchers taking similar approaches. 5 A Note on Literal Meaning There is an objection one could make to our whole ap- proach. If our logical forms are taken to be a represen- tation of the \"literal meaning\" of the sentence, then we would seem to be making the claim that the literal mean- ing of sentence (2) is \"Using a telescope, John saw a man, or John saw a man who had a telescope,\" whereas the real situation is that either the literal meaning is \"Using a tele- scope, John saw a man,\" or the literal meaning is \"John saw a man who had a telescope.\" The disjunction occurs in the metalanguage, whereas we may seem to be claiming it is in the language. The misunderstanding behind this objection is that the logical form is not intended to represent \"literal meaning\". There is no general agreement on precisely what consti- tutes \"literal meaning\", or even whether it is a coherent notion. In any case, few would argue that the meaning of a sentence could be determined on the basis of syntactic information alone. The logical forms produced by the DI- ALOGIC system are simply intended to encode all of the information that syntactic processing can extract about the sentence. Sometimes the best we can come up with in this phase of the processing is disjunctive information about attachment sites, and that is what the LF records. 6 Future Extensions 6.1 Extending the Range of Phenomena The work that has been done demonstrates the feasibility of localizing in logical form information about attachment ambiguities. There is some mundane programming to do to handle the cases similar to those described here, e.g., other forms of postnominal modification. There is also the crossing-dependency constraint to implement. The principal area in which we intend to extend our approach is various kinds of conjunction ambiguities. Our approach to some of these cases is quite similar to what we have presented already. In the sentence, (5) Mary told us John was offended and George left the party early. it is possible for George's leaving to be conjoined with ei- ther John's being offended or Mary's telling. Following Hobbs [1985], conjunction is represented in logical form by the predicate and taking a self argument and two event variables as its arguments. In (5) suppose e₁ stands for the telling, e2 for the being offended, e3 for the leaving, and eo for the conjunction. Then the neutral representation for (5) would include and' (eo, yo, e3) A tell' (e1, M, y1) A((Yo = e1 AY1 = e2) V (Yo = e2 AY1 = eo)) That is, there is a conjunction eo of yo and the leaving e3; there is a telling e₁ by Mary of y₁; and either yo is the telling e₁ and y₁ is the being offended e2, or yo is the being offended e2 and y₁ is the conjunction eo. A different kind of ambiguity occurs in noun phrase con- junction. In (6) Where are the British and American ships? there is a set of British ships and a disjoint set of American ships, whereas in (7) Where are the tall and handsome men? the natural interpretation is that a single set of men is desired, consisting of men who are both tall and handsome. In TACITUS, noun phrase conjunction is encoded with the predicate andn, taking three sets as its arguments. The expression andn(s1, s2, s3) means that the set s₁ is the union of sets s2 and s3.4 Following Hobbs [1983], the representation of plurals involves a set and a typical ele- ment of the set, or a reified universally quantified variable ranging over the elements of the set. Properties like cardi- nality are properties of the set itself, while properties that hold for each of the elements are properties of the typical element. An axiom schema specifies that any properties of ⁴If either s₁ or s₂ is not a set, the singleton set consisting of just that element is used instead. the typical element are inherited by the individual, actual elements. Thus, the phrase \"British and American ships\" is translated into the set s₁ such that andn(s1, s2, s3) A typelt(x1, s1) A ship(x1) Atypelt(x2, s2) A British(x2) Atypelt(x3, s3) A American(x3) That is, the typical element x₁ of the set s₁ is a ship, and s₁ is the union of the sets s2 and s3, where the typical element x2 of s2 is British, and the typical element x3 of s3 is American. The phrase \"tall and handsome men\" can be represented in the same way. andn(s1, s2, s3) A typelt(x1, s1) A man(x1) Atypelt(x2, s2) A tall(x2) Atypelt(x3, s3) A handsome(x3) Then it is a matter for pragmatic processing to discover that the set s₂ of tall men and the set s3 of handsome men are in fact identical. In this representational framework, the treatment given to the kind of ambiguity illustrated in I like intelligent men and women. resembles the treatment given to attachment ambiguities. The neutral logical form would include … A andn(s1, s2, s3) A typelt(x1, s1) Atypelt(x2, s2) A man(x2) Atypelt(x3, s3) A woman(x3) Aintelligent(y) A [y = x1 V y = x2] That is, there is a set s₁, with typical element x₁, which is the union of sets s2 and s3, where the typical element x2 of s₂ is a man and the typical element x3 of s3 is a woman, and something y is intelligent, where y is either the typical element x₁ of s₁ (the typical person) or the typical element x2 of s2 (the typical man). Ambiguities in conjoined compound nominals can be represented similarly. The representation for oil pump and filter would include … A andn(s, p, f) A typelt(x, s) A pump(p) A filter(f) A oil(o) A nn(o, y) A [y = p V y = x] That is, there is a set s, with typical element x, composed of the elements p and f, where p is a pump and f is a filter, and there is some implicit relation nn between some oil o and y, where y is either the pump p or the typical element x or s. (In the latter case, the axiom in the TACITUS system's knowledge base, ⁵The reader may with some justification feel that the term \"typical element\" is ill-chosen. He or she is invited to suggest a better term. (∀w, x, y, z, s)nn(w, x) Λ typelt(x, s) Λ andn(s, y, z) ≡ nn(w, y) Λ nn(w, z) allows the nn relation to be distributed to the two con- juncts.) 6.2 Ordering Heuristics So far we have only been concerned with specifying the set of possible attachment sites. However, it is true, em- pirically, that certain attachment sites can be favored over others, strictly on the basis of syntactic (and simple se- mantic) information alone.<sup>6</sup> For example, for the prepositional phrase attachment problem, an informal study of several hundred examples suggests that a very good heuristic is obtained by using the following three principles: (1) favor right association; (2) override right association if (a) the PP is temporal and the second nearest attachment site is a verb or event nom- inalization, or (b) if the preposition typically signals an argument of the second nearest attachment site (verb or relational noun) and not of the nearest attachment site; (3) override right association if a comma (or comma into- nation) separates the PP from the nearest attachment site. The preposition \"of\" should be treated specially; for \"of\" PPs, right association is correct over 98% of the time. There are two roles such a heuristic ordering of possibil- ities can play. In a system without sophisticated seman- tic or pragmatic processing, the favored attachment could simply be selected. On the other hand, in a system such as TACITUS in which complex inference procedures access world knowledge in interpreting a text, the heuristic order- ing can influence an allocation of computational resources to the various possibilities. Acknowledgements The authors have profited from discussions with Stu Shieber about this work. The research was funded by the Defense Advanced Research Projects Agency under Office of Naval Research contract N00014-85-C-0013. References [1] Dowty, David, Lauri Karttunen, and Arnold Zwicky (1985) Natural Language Parsing, Cambridge University Press. [2] Church, Kenneth (1980) \"On Memory Limitations in Natural Language Processing\", Technical Note, MIT Computer Science Lab, MIT. <sup>6</sup>There is a vast literature on this topic. For a good introduction, see Dowty, Karttunen, and Zwicky [1985]. [3] Church, Kenneth, and Ramesh Patil (1982) \"Coping with Syntactic Ambiguity or How to Put the Block in the Box on the Table\", AJCL, Vol S, No 3-4. [4] Grosz, Barbara, Norman Haas, Gary Hendrix, Jerry Hobbs, Paul Martin, Robert Moore, Jane Robin- son, Stanley Rosenschein (1982) \"DIALOGIC: A Core Natural-Language Processing System\", Technical Note 270, Artificial Intelligence Center, SRI International. [5] Hirst, Graeme (1986) \"Semantic Interpretation and Ambiguity\", to appear in Artificial Intelligence. [6] Hobbs, Jerry (1982) \"Representing Ambiguity\", Pro- ceedings of the First West Coast Conference on For- mal Linguistics, Stanford University Linguistics Depart- ment, pp. 15-28. [7] Hobbs, Jerry (1983) \"An Improper Approach to Quan- tification in Ordinary English\", Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, Cambridge, Massachusetts, pp. 57-63. [8] Hobbs, Jerry (1985) \"Ontological Promiscuity\", Pro- ceedings of the 23rd Annual Meeting of the Association for Computational Linguistics, Chicago, Illinois, pp. 61- 69. [9] Hobbs, Jerry (1986) \"Overview of the TACITUS Project\", CL, Vol. 12, No. 3. [10] Hobbs, Jerry, and Paul Martin (1987) \"Local Prag- matics\", Proceedings of the Tenth International Joint Conference on Artificial Intelligence, Milano, Italy, pp. 520-523. [11] Kimball, John (1973) \"Seven Principles of Surface Structure Parsing\", Cognition, Vol. 2, No. 1, pp. 15-47. [12] Pereira, Fernando (1983) \"Logic for Natural Language Analysis\", Technical Note 275, Artificial Intelligence Center, SRI International. [13] Rich, Elaine, Jim Barnett, Kent Wittenburg, and Greg Whittemore (1986) \"Ambiguity and Procrastina- tion in NL Interfaces\", Technical Note HI-073-86, MCC. [14] Stucky, Susan (1987) \"Configurational Variation in English: A Study of Extraposition and Related Mat- ters\", in Syntax and Semantics: Discontinuous Con- stituency, Vol. 20, edited by G. Huck and A. Ojeda, Academic Press. Appendix John saw the man with the telescope. Logical Form before Attachment-Finding: ((PAST (SELF E11) (SUBJECT (E3 (SEE (SELF E3) (SUBJECT (X1 (JOHN (SELF E2) (SUBJECT X1)))) (OBJECT (X4 (MAN (SELF E5) (SUBJECT X4)) (WITH (SELF E6) (PP-SUBJECT X4) ; <-- (with) modifies (man] (OBJECT (X7 (TELESCOPE (SELF E8) (SUBJECT X7)) (THE (SELF E9) (SUBJECT X7)) (NOT= (NP X7) (ANTES (X4)))))) (THE (SELF E10) (SUBJECT X4)) (NOT= (NP X4) (ANTES (X1)))))))))) Logical Form after Attachment-Finding: ((PAST (SELF E11) (SUBJECT (E3 (SEE (SELF E3) (SUBJECT (X1 (JOHN (SELF E2) (SUBJECT X1)))) (OBJECT (X4 (MAN (SELF E5) (SUBJECT X4)) (WITH (SELF E6) (SUBJECT (Y14 (?= (NP Y14) ; <-- [with] modifies (man) or (saw] (ANTES (X4 E3))))) (OBJECT (X7 (TELESCOPE (SELF E8) (SUBJECT X7)) (THE (SELF E9) (SUBJECT X7)) (NOT= (NP X7) (ANTES (X4)))))) (THE (SELF E10) (SUBJECT X4)) (NOT= (NP X4) (ANTES (X1))))))))))"
  },
  {
    "title": "CSeg&Tag1.0: A Practical Word Segmenter and POS Tagger for Chinese Texts",
    "abstract": "Chinese word segmentation and POS tagging are two key techniques in many applications in Chinese information processing. Great efforts have been paid to the research in the last decade, but unfortunately, no practical system with high performance for unrestricted texts is available up to date. CSeg&Tag1.0, a Chinese word segmenter and POS tagger which unifies these two procedures into one model, is introduced in this paper. The preliminary open tests show that the segmentation precision of CSeg&Tag1.0 is about 98.0% - 99.3%, POS tagging precision about 91.0% - 97.1%, and the recall and precision for unknown words are ranging from 95.0% to 99.0% and from 87.6% to 95.3% respectively. The processing speed is about 100 characters per second on Pentium 133 PC. The work of improving the performance of the system is still ongoing.",
    "content": "1. Background and the Related Issues In Chinese, there do not exist delimiters, such as spacing in English, to explicitly indicate boundaries between words. Chinese word segmentation is therefore proposed as the first step in any Chinese information processing systems. Then we still face the problem of part-of-speech tagging. These two issues have been intensively studied by the Chinese language computing community in the last decade[1-18]. Unfortunately however, no word segmenter and POS tagger for Chinese with satisfactory performance in treating unrestricted texts are available so far. Two main obstacles block the progress of Chinese word segmentation: one is ambiguity, another is unknown word. The sentences in (1) are examples of ambiguity and the sentence (2) and (3) examples of unknown word. (1a)这个研究所很有名, (1b)这项研究所涉及的问题很复杂. At least two explanations are possible for the fragment“研究所”in (1), resulting in two different segmentations: correct segmentation for (1a) 这个| 研究所 |很有名 this CLASSIFIER institute very famous (This institute is very famous.) correct segmentation for (1b) 这 | 项 | 研究 | 所 | this CLASSIFIER research AUX 涉及 | 的 | 问题 | 很 | 复杂 |. involve of problem very complex (The problems involved in this research are very complex.) Two transliterated foreign personal names(TFN), i.e.,“穆巴拉克” and “阿斯马特·阿卜杜勒·马 吉德” are involved in the sentence (2): (2) 随同穆巴拉克总统来访的有总理阿斯马 特·阿卜杜勒·马吉德,… They will be wrongly broken into pieces of isolated characters if not processed: correct segmentation for (2) 随同 | 穆巴拉克 | 总统 | 来访 | 的 | accompany TFNI president visit of 有 | 总理 | 阿斯马特·阿卜杜勒·马吉德… have premier TFN2 (Visitors accompanying the president TFNI include the premier TFN2,…) wrong segmentation for (2) 随同 | 穆巴拉克 | 总统 | 来访 | 的 | 有 | 总理 | 阿 | 斯 | 马 | 特 | · | 阿 | 卜 | 杜 | 勒 | · | 马 | 吉 | 德… The sentence (3) contains a Chinese personal name(CN) “单清”: (3) 单清楚楚动人. We have: correct segmentation for (3) 单清 | 楚楚动人 CN beautiful (CN is beautiful) wrong segmentation for (3) 单 | 清 | 楚 | 动人 only clear ChineseSURNAME touching /* logically ill-formed sentence */ POS tagging for Chinese is similar to that of English, except that an English tagger only need to tag one word sequence for an input sentence, but in the case of Chinese, to get a correct tag sequence for a sentence, a Chinese tagger may be requested to tag more than one word sequences simultaneously due to the presence of segmentation ambiguities. Chinese word segmentation and POS tagging techniques can be found many applications in the real world such as information retrieval, text categorization, text proofreading, OCR, speech recognition and text- to-speech conversion systems. For instance, in information retrieval, the incorrect segmentation for the fragment “研究所”in (la) and (1b) will definitely cause improper access to the texts involving it. Another typical application is in text-to-speech conversion. The over-segmentation of TFNI and TFN2 in the sentence (2) will result in the synthesized speeches choppy. The CN in (3) may make the word segmentation and POS tagging of the whole sentence totally wrong, and further, the pronunciation of the character 单 totally wrong (单 should be pronounced as shan4 if it is referred to a surname, whereas as danl if adjective or adverb). 2. The Complexity of the Task Combinatorial Explosion 1: Word Segmentation Candidate Space The number of possible segmentations for some sentences may be rather large. Observe: (4) 发展中国家用电器材料... totally 76 possible segmentations will be found if we simply match the sentence with a dictionary: (seg1)发展中国家用电器 | 材料 | (seg2)发展 | 中国 | 家用|电器|材料 | (seg75)发展|中|国家|用电器材|料 | (seg76)发 | 展|中|国家用电| 器材|料|... Fig.1 shows the word segmentation candidate space for the sentence (4). The situation will be even complicated as unknown words is under consideration(Fig.2). Generally, segmentation ambiguities can be classified into three categories: (a) ambiguities among common words(refer to all arcs in Fig.1) (b) ambiguities among unknown words(see arcs of representing candidates for Chinese place name and for Chinese personal names in Fig.2) (c) ambiguities among common words and unknown words(see arcs across Chinese personal name candidates“江爱”,“林江爱”,“王林江爱” and the arc across common word “爱”(love, like) in Fig.2) In our experience, ambiguities of type (a) will cause about 3% loss on the precision rate of segmentation in the condition of making use of maximal matching strategy, one of the most popular methods employed in word segmentation systems, and type (b) and (c) about 10.0% loss if the processing of unknown words is ignored (unfortunately, type (b) and (c) have received less attention than type (a) in the literature). 。发展中国家5用。电,器材9料10 Common words Fig.1 The word segmentation candidate space --- 0 王 1 林 2 江 3 爱 4 踢 5 足 6 球 7 Common words Candidates for Chinese Place Names Candidates for Chinese Personal Names Fig.2 The word segmentation candidate space regarding unknown words 0 发 1 展 2 中 3 国 4 家 5 用 6 电 7 器 8 材 9 料 10 0 发 1 展 2 中 3 国 4 家 5 用 6 电 7 器 8 材 9 料 10 Words Tags Fig.3 The POS tagging candidate space Combinatorial Explosion 2: POS Tagging Candidate Space Given that: TAG(发) = {vgm, qnq, ngm} TAG(展) = {vgm, ns} TAG(中) = {j, dm, vgm} TAG(国) = {ngm, ns} TAG(家) = {ngm, k} TAG(用) = {vgm, um, pgm} TAG(电) = {vgm, ngm} TAG(器) = {ngm} TAG(材) = {ngn} TAG(料) = {ngm, vgm, qnq} we will get 1296 possible tag sequences solely for seg(76) in the sentence 4 (Fig.3). Combinatorial Explosion 1 × Combinatorial Explosion 2: An Integrated Model We find out through experiments that the word segmentation and POS tagging are mutually interacted, the performance of the both will increase if they are integrated together[18]. Scholars ever tried to do so. The method reported in [11] is: (a) finding out the N- best segmentation candidates explicitly in terms of word frequency and length; (b) POS tagging each of the N-best segmentation candidates, resulting in the N- best tag sequences accordingly; and (c) using a score with weighted contributions from (a) and (b) to select the best solution. Note that the model used in (a) is just word unigram, and (a) and (b) are being done successively (denoted as \"(a)+(b)\"). It is a kind of pseudo-integration. More truly one, in our point of view, should be: (a) taking all segmentation possibilities into account; (b) expanding every segmentation candidate of the input sentence into a number of tag sequences one by one, deriving a considerable huge segmentation and tagging candidate space; and (c) seeking the optimal path over such space with a bigram model, obtaining then both word segmentation and POS tagging result from the path --- found. In the case, (a) and (b) are being done simultaneously (denoted by \"(a)||(b)\"). We regard this as a basic strategy and testbed for conducting our system. Obviously, a much more serious combinatorial problem is encountered here. 3. CSeg&Tag1.0: System Architecture and Algorithm Design Although great efforts have been paid to the related researches by Chinese information processing community in the last decade, we still have not a practical word segmenter and POS tagger at hand yet. What is the problem? The crucial reason, we believe, lies in the \"knowledge\". As indicated in section 2, we meet a very serious difficulty, without relevant knowledge, even humanbeings will definitely fail to solve it. The focus of the research should be no longer solely on the 'pure' or 'new' formal algorithms — no matter what it will be, instead, what is urgently required is on two issues, i.e., (1) what sorts of and how many knowledges are needed; and (2) how these various konwledges can be represented, extracted, and cooperatively mastered, in a system. This is also the philosophy in designing Cseg&Tag 1.0, an integrated system for Chinese word segmentation and POS tagging, which is being developed at the National Key Lab. of Intelligent Technology and Systems, Tsinghua University. The aim of CSeg&Tag is to be able to process unrestricted running texts. Fig.4 gives its architecture. Roughly speaking, Cseg&Tag1.0 can be viewed as a three-level multi-agent(the concept of \"agent\" means an entity that can make decision independently and communicate with others) system plus some other necessary mechanisms. They are: (1) agents at the low level for treating unknown words; (2) a competition agent at the intermediate level for resolving conflicts among low level agents; (3) a bigram-based agent at the high level for coping with all the remaining ambiguities; (4) mechanisms employing the so-called \"global statistics\" and \"local statistics\" (cache); and (5) a rule base. We will introduce them briefly in turn(the detailed discussion of each part is beyond the scope of this paper). 3.1. Agents at the Low Level for Treating Unknown Words The types of unknown words CSeg&Tag1.0 currently concerns include Chinese personal names(CN), transliterated foreign personal names(TFN) and Chinese place names(CPN). They can not be enumerated in any dictionary even with numerous size. To study unknown words systematically, we build up there relevant banks: · CN Bank(CNB): 200,000 samples · TFN Bank(TFNB): 38,769 samples · CPN Bank(CPNB): 17,637 samples The difficulty of identifying unknown words in Chinese arises from characteristics of them: (a) no any explicit hint such as capitalization in English exists to signal the presence of unknown words, and the character sets used for unknown words are strict subsets of Chinese characters(the size of the complete Chinese character set is 6763), with some degree of decentralized distributions; \\begin{tabular}{|l|c|} \\hline CN (surname) & 729 \\\\ \\hline CN (given name) & 3345 \\\\ \\hline TFN & 501 \\\\ \\hline CPN & 2595 \\\\ \\hline \\end{tabular} (b) the length of unknown words may vary arbitrarily; (c) some characters used in unknown words may also be used as mono-syllabic common words in texts; (d) the mono-syllabic words identified above fall into the syntactic categories not only notional words but also function words; (e) the character sets are mutually intersected to some extent; (f) some multi-syllabic words may occur in unknown words. In our system, three agents, CNAgent, TFNAgent and CPNAgent are set up to be responsible for finding candidates in input texts accordingly. A candidate can be regarded as a \"guess\" with a value of belief. Three steps are involved in all the three agents in general: Step 1: Applying MM(maximal matching) first as a pre-processing, then finding candidates over the resulting fragments of characters There are two strategies for seeking candidates in the input sentence. One is simply viewing it as character string, finding candidates over whole of it in terms of the relevant character set: C CName KB Input Text SentToBeSeg MainDic DomainDic Agents at Low Level CName Agent > LocalStatistics TFName KB TFName Agent Guesses & Their Beliefs CPName CPName KB Competition Agent Agent Proper Noun Bank COCO Corpus TCorpus RuleBase MMSeg Intermediate Level DicManager TempDic V Dicinfo Integration Seg-with-Full Possibilities Expanded DAG V Disambiguation Agent (High Level) Char Bigram POS Bigram Results of CSeg&Tag Fig.4 The system architecture of CSeg&Tag1.0 (5a)王雪芝要来参加中国科学杂志社的庆典 CNI CN2 CN3 Many noises will be unnecessarily introduced, as CN2 and CN3 in (5a). Another way is viewing input as word string, applying MM segmentation as a pre- processing first, then trying to find candidates only over the fragments composed of successive single characters: (5b)王雪|芝|要|来|参加 | 中国 | CNI will come attend China 科学 | 杂志社 | 的 | 庆典 science journal of celebration (CNI will come here and attend the celebration of the journal of \"Science in China\") Step 2: Drawing back some multi-syllabic words into the candidates Look at: (6) 他叫白金汉希尔 (His name is Buckinghamshire) after MM, we get 他|叫|白金|汉|希|尔| obviously,白金(platinum) should be drawn back and added into the TFN candidate. Such multi-syllabic words can be collected from the banks. Step 3: Further determining boundaries of the candidates All of the useful information, usually language- specific and unknown-word-type-specific, are activated to perform this work. internal information (i) statistical information Each candidate will be assigned a belief according to the statistics derived from the banks. (ii) structural information #nature of characters absolute closure characters for CNs They will definitely belong to a Chinese surname once falling into the control domain of it: 李逵郑筱云刘景藜 relative closure characters for CNs In certain conditions, they function as absolute closure characters: (7a)胡戎睿|十分|聪明 CNI very clever (CNI is very clever) (7b)胡戎|睿智|过人 CN2 clever very (CN2 is very clever) open characters for CNs For this sort of characters, possibilities of being included in a name and excluded out of the name must be reserved: (8a)张玉爱|读|小说 CNI read novel (CNI is reading a novel) (8b)张玉|爱读|小说 CN2 like read novel (CN2 likes to read novels) # position in unknown words For instance, “在”always occurs in the first position of given name of CNs, illustrated as“于在 河“王在明”“陈在铁”“金在荣”“谭在树”“白在桥“邓 在军”. The CN candidate “邓军在”in (9) (9) 邓军在唱歌 will be therefore properly filtered out, leaving the correct one:“邓军”. # affix Affix(e.g. suffix of CPNs) will be beneficial to locating the boundaries of some unknown words. # constructions CPNS ==> Chinese surname + 家”+ mono-syllabic CPN suffix external information (i) statistical information Refer to \"global & local statistics\". (ii) structural information # titles # special verbs # special syntactic patterns patten x0: “以<CN or TFN> {title}为<title>\" (10) 以潘杜泉为团长的香港代表团... The fragment“潘杜泉为”in (10) will create four CN candidates“潘杜“潘杜泉“杜泉“杜泉为”,but only “潘杜泉” passes under the constraint of pattern x0. 3.2. The Competition Agent at the Intermediate Level for Resolving Conflicts among Low Level Agents The candidates given independently by three agents may contradict each other on some occasions (see Fig.2). We observe from 497 randomly selected sentences that low level agents generate multiple(>=2) unknown word candidates in 17.7% of them (Fig.5), and, the probability of conflicting is about 88% if candidate number is 2 and 100% if it is greater than 2(Fig.6). A competition agent is established to deal with such conflicts. The evaluation is based on all information from various resources, that is: No. of sentences 350-337 300- 250 200 150- 100- 72 50 50- 18 2 0 0 1 2 3 4 5 6 7 No. of candidates in a sentence Fig.5 The distribution of candidates in sentences Probability of conflicting (%) 100 100 100 100 100 100 80- 60 40 20 0 1 2 3 4 5 6 7 No. of candidates in a sentence Fig.6 The probability of conflicting among unknown word candidates Eval(candidate) = f<sub>Σ</sub>(InterStatisInfo, InterStrucInfo, ExterStatisInfo, ExterStrucInfo) About 77% conflicts can be solved by this agent. The output of it, including correct candidates and some unsolved conflicts, are then sent to a high level agent for further processing. 3.3. The Bigram-based Agent at the High Level for Coping with all the Remaining Ambiguities The conventional POS bigram model and a dynamic programming algorithm are used in this high level agent. The searching space of the algorithm is the complete combination of all possible word and tag sequences, and the complexity of it can be theoretically and experimentally proved still polynomial. 3.4. Global Statistics & Local Statistics Global statistics are referred to statistical data derived from very large corpora, as mutual information and t-test in Cseg&Tag1.0, whereas local statistics to those derived from the article in which the input sentence stands — like a chche. Both of them take characters as basic unit of computation, because any Chinese word is exactly a combination of characters in one way or another. Experiments by us reveal that they(especially the latter) are quite important in the resolution of ambiguities and unknown words. Refer back to “张玉”and“张玉爱”in (8a) and (8b) as an example. The both CN candidates are reasonable given the isolated sentence only, but by cache, it is in fact a collection of ambiguous entities unsolved so far in the current input article, the algorithm will have more evidence to make decision. We will discuss this in depth in another paper. 3.5. Rule Base It contains knowledge in rule form, including almost all word formation rules in Chinese, a number of simple but very reliable syntactic rules, and some heuristic rules. 4. Experimental Results Cseg&Tag1.0 is implemented in Windows environment with Visual C++1.0 programming language. The dictionary supporting it contains 60,133 word entries along with word frequencies, parts of speech, and various types of information necessary for the purpose of segmentation and tagging. The size of manually tagged corpus for training the bigram model is about 0.4 M words, and that of the raw corpus for achieving global statistics is 20M characters. We define: Seg. precision= Tag. precision= # words- correctly- segmented # words- in- input- texts # words- correctly- tagged # words- in - input - texts The preliminary open tests show that for CSeg&Tag1.0, the word segmentation precision is ranging from 98.0% to 99.3%, POS tagging precision from 91.0 to 97.1%, and the recall and precision for unknown words are from 95.0% to 99.0% and from 87.6% to 95.3% respectively. The speed is about 100 characters per second on Pentium 133. A running sample of Cseg&Tag1.0 is demonstrated as follows(tokens underlined in the output are unknown words successfully identified while those in bold are words wrongly tagged): [input text] <全国政协举行新年茶话会江泽民发表讲话>(记者 邹爱国、何平) 政协全国委员会今天上午在政协 礼堂举行新年茶话会,迎接九十年代的到来。江泽 民、李鹏、万里、姚依林、宋平、李瑞环、王震、 田纪云、李锡铭、丁关根、温家宝等国家领导人,... [output] <\\xg 全国\\sd 政协\\j举行\\vgd 新年\\td 茶话会\\ngd 江泽民\\np 发表\\vgd 讲话\\ngd >\\xg (\\xp 记者\\ngd 邹爱国\\np、\\xp何平\\np)\\xp 政协\\j 全国\\sd 委员 会\\ngd 今天\\td 上午\\td 在\\vgm 政协\\j 礼堂\\ngd举 行\\vgd 新年\\td 茶话会\\ngd,\\xp 迎接\\vgd 九十\\mx 年代\\ngd 的\\ed 到来\\vgd。\\xs 江泽民\\np、\\xp李 鹏\\np、\\xp 万里\\np、\\xp姚依林\\np、\\xp宋平 \\np、\\xp李瑞环\\np、\\xp王震\\np、\\xp 田纪云 \\np、\\xp李锡铭\\np、\\xp丁关根\\np、\\xp 温家宝 \\np等\\egm 国家\\ngd 领导人\\ngd,\\xp... It should be pointed out that Cseg&Tag1.0 is just the result of the first round of our investigation. To get our goal, i.e., developing a system with approximately 99% segmentation precision and 95% tagging precision for any running Chinese texts in any cases, quite a lot of work is still waiting there to be done. What we can say now is that we believe it is possible to reach this destination in a not very far future, and we know more than before about how to approach it. The second round work is ongoing currently, with emphasis on two aspects: (1) to promote the algorithm, particularly those associated with agents and cache, carefully; (2) to improve the quality of knowledge base by both enlarging the size of the relevant resources(textual corpora, unknown word banks, etc.) and refining the lexicon, tagged corpus and the rule base. Acknowledgment This research is supported by the National Natural Science Foundation of China and by the Youth Science Foundation of Tsinghua University, Beijing, P.R.China. References [1] N.Y. Liang, \"Automatic Chinese Text Word Segmentation System — CDWS\", Journal of Chinese Information Processing, Vol. 1, No.2, 1987 [2] C.K. Fan, W.H. Tsai, \"Automatic Word Identification in Chinese Sentences by the Relaxation Technique\", Computer Processing of Chinese and Oriental Languages, Vol.1, No.1, 1988 [3] C. Kit, Y. Liu, N. Liang, \"On Methods of Chinese Automatic Word Segmentation\", Journal of Chinese Information Processing, Vol.3, No.1, 1989 [4] J.S. Zhang, Z.D. Chen, S.D. Chen, \"A Method of Word Identification for Chinese by Constraint Satisfaction and Statistical Optimization Techniques\", Proc. of ROCLING-IV, Kenting, 1991 [5] J.S. Chang, S. Chen, Y. Zheng, X.Z. Liu, S.J. Ke, \"A Multiple-Corpus Approach to Identification of Chinese Surname-names\", Proc. of Natural Language Processing Pacific Rim Symposium, Singapore, 1991 [6] B.Y. Lai, S. Lun, C.F. Sun, M.S. Sun, \"A Tagging- Based First Order Markov Model Approach to Chinese Word Identification\", Proc. of ICCPCOL-92, Florida, 1992 [7] K.J. Chan, S.H. Liu, \"Word Identification for Mandarin Chinese Sentences\", Proc. of COLING-92, Nantes, 1992 [8] L.J. Wang, et al. \"Recognizing Unregistered Names for Mandarin Word Identification\", Proc. of COLING- 92, Nantes, 1992 [9] M.S. Sun, B.Y. Lai, S. Lun, C.F. Sun, \"Some Issues on Statistical Approach to Chinese Word Identification\", Proc. of 3rd International Conference on Chinese Information Processing, Beijing, 1992 [10] C.H. Chang, C.D. Chen, \"HMM-based Part-of- Speech Tagging for Chinese Corpora\", Proc. of the Workshop on Very Large Corpora, Ohio, 1993 [11] C.H. Chang, C.D. Chen, \"A Study on Integrating Chinese Word Segmentation and Part-of-Speech Tagging\", Communications of COLIPS, Vol.3, No.2, 1993 [12] M.S. Sun and W.J. Zhang, \"Transliterated English Name Identification in Chinese Texts\", Computational Linguistics: Research & Application, Beijing Language Institute Press, Beijing, 1993 [13] M.S. Sun, C.N. Huang, H.Y. Gao, J. Fang, \"Identifying Chinese Names in Unrestricted Texts\", Communications of COLIPS, Vol.4, No.2, 1994 [14] R. Sproat, C. Shih, W. Gale, N. Chang, \"A Stochastic Finite-State Word Segmentation Algorithm for Chinese\", Proc. of 32nd Annual Meeting of ACL, New Mexico, 1994 [15] D.Y. Shen, M.S. Sun and C.N. Huang, \"Identifying Chinese Place Names in Unrestricted Texts\", Computational Linguistics: Research & Development, Tsinghua University Press, Beijing, 1995 [16] J.Y. Nie, M.L. Hannan, W. Jin, \"Unknown Word Detection and Segmentation of Chinese Using Statistical and Heuristic Knowledge\", Communications of COLIPS, Vol.5, No.1, 1995 [17] M.S. Sun, B.K. T.sou, \"Resolving Ambiguities in Chinese Word Segmentation\", Proc. of PACLIC-10, Hong Kong, 1995 [18] M.S. Sun, C.N. Huang, \"Word Segmentation and Part-of-speech Tagging for Unrestricted Chinese Texts\", A Tutorial on the International Conference on Chinese Computing '96, Singapore, 1996"
  },
  {
    "title": "COMBINATORIAL DISAMBIGUATION",
    "abstract": "The disambiguation of sentences is a combinatorial problem. This paper describes a method for treating it as such, directly, by adapting standard combinatorial search optimizations. Traditional disambiguation heuristics are applied but, instead of being embedded in individual decision procedures for specific types of ambiguities, they contribute to numerical weights that are considered by a single global optimizer. The result is increased power and simpler code. The method is being implemented for a machine translation project, but could be adapted to any natural language system.",
    "content": "1. Introduction The disambiguation of sentences is a combinatorial problem. Identification of one word sense interacts with the identification of other word senses, (1) He addressed the chair and with constituent attachment, (2) He shot some bucks with a rifle Moreover, the attachment of one constituent interacts with the attachment of other constituents: (3) She put the vase on the table in the living room This paper describes a method of addressing the prob- lem directly, by adapting standard search optimization techniques. In the first section we describe the core of the method, which applies a version of best-first search to a uniform representation of the set of possibilities. In the second section we relate the work to other approaches to preference-based disambiguation. The final sections describe how the representation may be obtained from a lexicon. 2. The Search Method In the machine translation project for which this tech- nique is being developed, disambiguation begins after a parser, specifically the PLNLP English Grammar by Jensen (1986), has identified one or more parses based primarily on syntactic considerations (including subcategorization). Words are disambiguated up to part-of-speech, but word senses are not identified. In- dividual parses may indicate alternative attachments of many kinds of constituents including prepositional phrases and relative clauses. Beginning disambiguation only after a general parse has the advantage of making clear what all the possi- bilities are, thus allowing their investigation in an ef- ficient order. Performing a full parse before disambiguation need not consume an inordinate amount of space or time; techniques such as those used by Jensen (default rightmost prepositional phrase attachment), and Tomita (1985) (parse forests) ade- quately control resource requirements. The parser output is first transformed so that it is represented as a set of of semantic choice points. Each choice point generally represents a constituent. It con- tains a group of weighted semantic alternatives that represent the different ways in which word-senses of the constituent head can be associated semantically with word-senses of a higher level head. This allows word-sense and attachment alternatives to be treated uniformly. Combinatorial disambiguation then selects the consis- tent combination of alternatives, one from each choice point, that yields the highest total weight. To illustrate the method, we use the extension of the classic example mentioned above: (2) Ile shot some bucks with a rifle A decomposition of the sentence into choice points cl, c2, and c3 is shown in Figure 1. (The illustration assumes that \"shot\" and \"bucks\" have two meanings each, and ignores determiners.) Choice point \"cl\" gives the alternative syntactic and semantic possibilities for the attachment of the constituent \"he\" to its only possible head \"shot\". Alternative cll is that \"he\" is the subject of \"shootl\", with the semantic function \"agent\", and is given (for reasons which will be dis- cussed later) the weight \"3\". Alternative c12 is similar, but the meaning used for \"shoot\" is \"shoot2\". Similar alternatives are used for the attachment (c2) of the object \"some bucks\" to its only possible head, \"shoot\". Alternative c23 represents the unlikely combinations. Choice point c3 reflects the different possible attach- ments of \"with a rifle\"; the highest weight (3) is given to its attachment as an instrumental modifier of \"shootl\". The other possibilities range from barely plausible to implausible and are weighted accordingly. Having obtained this repesentation (whose construction is described in later sections), the next step is to es- tablish the single-alternative choice points as given and to propagate any associated word-sense constraints to narrow or eliminate other alternatives. (This does not occur in our example.) Then combinations of the remaining choices are searched using a variation of the A* best-first search method. See Nilsson (1980) for a thorough description of A*. Briefly, A* views combinatorial search as a problem of finding an optimal path from the root to a leaf of a tree whose nodes are the weighted alterna- tives to be combined. At any point in the process, the next node n to be added is one for which the potential F(n) = G(n) + II(n) is maximal over all possible additions. G(n) represents the value of the path up to and including n. II(n) represents an estimated upper bound for the value of the path below n (i.e., for the additional value which can be obtained)¹. When a complete path is found by this method, it must be optimal. The efficiency of the method, i.e., the speed of finding an optimal path, depends on the accuracy of the estimates II(n). To apply the method in our context, the search tree is defined in terms of levels, with each level corre- sponding to a choice point. Choice points are assigned to levels so that those which would probably be re- sponsible for the greatest difference between estimated and actual II(n) in an arbitrary assignment are exam- ined first. Looked at in another way, the assignment of choice points to levels is made so that those which will best differentiate among potential path scores are examined first. This is done by (partially) ordering the choice points in descending order of their difference potential Dc, the difference in weight between their highest weighted alternative and the next alternative. If the highest weight is represented by two different alternatives, Dc = 0. Within this ordering the choice points are further ordered by weight differences between the 2nd and 3rd highest weighted alternatives, etc. For our example this results in choice point c3 (\"with a rifle\") being assigned to the highest level in the tree, followed by choice points c2 and then cl. cl.He cll agt (he shootl) 3 c12 agt (he shoot2) 3 c2.some bucks c21 goal (buck1 shootl) 3 c22 goal (buck2 shoot2) 3 c23 goal (buck1 shoot2), (buck2 shootl) 0 c3.with a rifle c31 inst (rifle shootl) 3 c32 inst (rifle shoot2) 2 c33 togw (rifle (buck1,buck2)) 0 c34 accm (rifle (shootl,shoot2)) 0 (i.e., fired-at) (i.e., wasted) (i.e., male deer) (i.e., dollar) (i.e., together-with) (i.e., accompanied-by) Figure 1: Choice points and alternatives We also associate with each level = choice point the value IIc, which is the sum of the maximum weights that can be added by contributions below that choice point. This is needed by the algorithm to estimate potential path scores below a given node. For this example, the IIc values are: HO: top=9 H3: rifle=6 H2: buck=3 H1: he=0 Then the best-first search is carried out. At each point in the search, the next node to be added is that which (a) is consistent in word-sense selection with previous choices, and (b) has the highest potential. The potential is calculated as the accumulated weight down to (and including) that node plus Hc for that level. A diagram of the procedure, as applied to the example, is shown in Figure 2. The first node to be added is \"with rifle shootl\", which has the highest potential. At that point, the highest weighted consistent alternative is c21, etc. While the set of choice points implies that there are (4 x 3 x 2) = 24 paths to be searched, only one is pursued to any distance. Thus while the approach takes a combinatorial view of the problem, it does so without loss of efficiency. When a full path is found, it is examined for semantic consistency (beyond word-sense consistency). The checks made include: (a) ensuring that the interpreta- tion includes all required semantic functions for a word-sense (specified in the lexicon), and (b) ensuring that non-repeatable functions (e.g., the goal of an ac- tion) are not duplicated. Even if the full path is found to be consistent, the search does not terminate immediately, but continues until it is certain that no other path can have an equal score. This will be true whenever the maximum po- tential for open nodes is less than the score for an already-found path. A more precise description of the algorithm is given in the Appendix. When more than one path is found with the same high score, additional tests are applied. These tests include comparisons of surface proximity and, as this work is situated within a multi-target translation sys- tem, user queries in the source language, as outlined by Tomita (1985). An extended version of the method is used in com- paring alternate parses which differ in constituent com- position, and thus are more easily analyzed as different parse trees, each with its own set of choice points. An example is the classic: (4) Time flies like an arrow (where the main verb can be any one of the first three words). In such cases, one set of choice points is constructed per parse tree. In general, the search alternates among trees, with the next node to be added being that with the greatest potential across trees. If such trees always had the same number of choice points, this would be the only revision needed. However, the number of choice points may differ, for one thing because the parser may have detected and con- densed non-compositional compounds (e.g., \"all the same\") in one parse but not in another. For this reason the algorithm changes to evaluate paths not by total scores, but by average scores (i.c., the total scores divided by the number of choice points in the particular parse). \"rifle\" **** * top * *********** * c31 *** * \"buck\" c21 c22 (inconsistent) * * \"he\" cll Figure 2: Reduced Tree Search 1 The basic A* algorithm is usually described as \"expanding\" (i.e., adding all immediate successors of) the most promising node. The variant described here, which is more appropriate to our situation (and also mentioned by Nilsson), adds a single node at each step. 3. Related Work There seems to be little work which directly addresses the combinatorial problem. First, there is considerable work in preference-related disambiguation that as- sumes, at least for purposes of discussion, that indi- vidual disambiguation problems can be addressed in isolation. For example, treatments of prepositional phrase attachment by by Dahlgren and McDowell (1986) and Wilks et. al. (1985) propose methods of finding the \"best\" resolution of a single attachment problem by finding the first preference which is satisfied in some recommended order of rule application. Other types of ambiguity, and other instances of the same type, are assumed to have been resolved. This type of work contributes interesting insights, but cannot be used directly. One type of more realistic treatment, which might be called the deferred decision approach, is exemplified by Hirst (1983). When, in the course of a parse, an immediate decision about a word sense or attachment cannot be made, a set of alternative possibilities is developed. The possibility sets are gradually narrowed by propagating the implications of both decisions and narrowings of other possibility sets. This approach has a number of problems. First, it is susceptible to error in semantic \"garden path\" situa- tions, as early decisions may not be justifiable in the context. For example, in processing (5) He shot a hundred bucks with one rifle. a particular expert might decide on \"dollars\" for bucks, because of the modifier \"hundred\", before the prepo- sitional phrase is processed. Also, it is difficult to see how versions of this method could be extended to deal with comparing alternate parses where the alternatives are not just ones of attaching constituents, but of de- ciding what the constituents are in the first place. A full-scale deferred-decision approach also has the potential of significant design complexity (the Hirst version is explicitly limited), as each type of decision procedure (for words and for different kinds of attach- ments) must be able to understand and process the implications of the results of other kinds of decision procedures. Underlying these problems is the lack of quantification of alternatives, which allows for comparison of com- binations. There are, however, early and more recent approaches which do apply numeric weights to sentence analysis. Early approaches using weights applied them primarily to judge alternative parses. Syntactically-oriented ap- proaches in this vein attached weights to phrase struc- ture grammar rules (Robinson 1975, 1980) or ATN arcs (Bates 1976). Some approaches of this period focussing on semantic expectations were those of Wilks (1975) and Maxwell and Tuggle (1975), which em- ployed scores expressing the number of expected de- pendents present in an interpretation. An ambitious approach combining different kinds of weights and cumulative scores, described by Walker and Paxton et. al. (1977), included heuristics to select the most promising subtrees for carlier development, to avoid running out of space before a \"best\" tree can be found. However, except for this type of provision, none of the carly approaches using weights seem to address the combinatorial problem.2 A contemporary approach for thorough syntactic and semantic disambiguation using weights is described by L. Schubert (1986). During a left-to-right parse, in- dividual attachments are weighted based on a list of considerations including preference, relative sentential position, frames/scripts, and salience in the current context. The multiple evolving parse trees are rated by summing their contained weights, and the combi- natorial problem is controlled by retaining only the two highest scoring parses of any complete phrases. This approach is interesting, although some details are vague³. However, the post-parse application of A* described in this paper obtains the benefits of such a within-parse approach without its deficiences in that: (a) combinatorial computations of weights and word- sense consistencies are avoided except when warranted by total sentence information, and (b) there is no pos- sibility of carly erroneous discarding of alternatives. 2 Heidorn (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisions based on syntactic considerations only. 3 No examples are given, so it is unclear whether a parse for a phrase or part thereof represents only one interpretation, or all interpretations having the same structure, scored by the most likely interpretation. The former is obviously inadequate (c.g., for highly ambiguous subject NPs like \"The stands\"), while the latter seems to require either the calculation of all alternative cumulative scores, or recalculation of scores if an interpretation fails. --- he bucks subj he shoot obj buck shoot with a rifle with rifle shoot with rifle buck Figure 3: Syntactic Choice Points One other parser-based work should be noted, that of Wittenburg (1986), as it is explicitly based on A*. The intent and content of the method is quite different from that described here. It is situated within a chart-parser for a categorial grammar, and the intent is to limit parsing expense by selecting that rule for next appli- cation which has the least likelihood of leading to an incomplete tree. While selectional preference is men- tioned in passing as a possible heuristic, the heuristics discussed in any depth are grammar oriented, and operate on the immediate facts of the situation, rather than on informed estimates of total parse scores. It should be also be mentioned that the representation of alternatives in schemes which combine syntactic and semantic disambiguation is rarely discussed, al- though maintaining a consistent representation of the relationships among word-sense and attachment alter- natives is fundamental to a systematic treatment of the problem. An exception is the discussion by K. Schubert (1986), who describes a representation for alternatives with some affinities to that described here. The information limitations of disambiguation during parsing are not found in spreading-activation ap- proaches, exemplified by Charniak (1986), Cottrell and Small (1984), and Waltz and Pollack (1985). These approaches are still in the experimental stage, and are primarily intended for parallel hardware, while the A* algorithm used in this paper is designed for conventional serial hardware. But, in a sense, these approaches reinforce the main point of this paper: they argue for a single global technique for optimized combinatorial disambiguation based on all available information. 4. Preparing Semantic Choices Having described how the choice points are used, we address their development. Two steps are involved: 4 (1) the development of syntactic choice points, and (2) the development of semantic choice points. The first step transforms the parse-level syntactic functions into a form appropriate to the second step, which is the application of the lexicon to those functions to obtain the semantic alternatives. In our example, the first step is a simple one. Syntactic relationships among constituents are transformed into syntactic relationships among head words, and the syntactic relationships are refined, so that \"ppmod\" is replaced by the actual prepositions used. The result of this step is shown in Figure 3. The development of syntactic choice points for some other types of con- stituents is more complex. Before discussing these situations, we discuss step 2: application of the lexicon to the syntactic choice points to obtain the semantic choice points, i.e., those shown in Figure 1. 4.1 The Lexicon The lexicon contains entries for word stems (distin- guished by part-of-speech), linked to word-sense en- tries, which are the lowest level \"concepts\". Concept entries are linked to superordinate concept entries, forming a lattice. Concept entries include a set of concept features (e.g. a list of superordinate concepts), and one or more rules for each syntactic function associated with the concept. The more relevant parts of the lexicon entries for the concepts used in the ongoing example are shown in Figure 4. The \"classes\" are lists of superordinate concepts. The syntactic func- tion rules have the form: synfun dependent head semfun weight Thus the first rule under \"shootl\" indicates that word- senses falling into the class \"animate\" are its preferred objects, with the weight 3, and the combination is given the semantic function \"goal\". The concept entry However, the weighting scheme is different, and rather interesting. The reference does not discuss the selection of a particular combination of alternatives in any detail, but it appears to be based on the presence in a combination of one (or more?) highly weighted alternative (or alternatives?). --- shootl classes (humanact, transv) (object animate shootl goal 3) (with firearm shootl inst 3) humanact classes (act...) (subj human humanact agt 3) (with human humanact accm 3) buck1 classes (animate...) buck2 classes (money..) shoot2 classes (humanact, transv) (object money shoot2 goal 3) transv (object thing transv goal 0) act (with tool act inst 2) rifle classes (firearm...) firearm classes (tool...) Figure 4: Lexicon Entries \"humanact\" contains other rules applicable to shootl and other verb-senses taking human actors. Weights are actually assigned symbolically, to allow experimentation. Current settings are as follows: • Rules for idioms (e.g., kick the bucket), 4. • Rules for more general selectional preferences, 3. • Rules for acceptable but not preferred alternatives (e.g., locative prepositional phrases attached to ar- bitrary actions), 2. • Very general attachments (e.g., \"nounmod nounl noun2), 0. These allow for uninterpreted metaphoric usage.5 One major objective in assigning weights to ensure that combinations of very high (idiom) weights together with very low weights do not outscore more balanced combinations. Thus, for: (6) He kicked the ugly bucket weights such as: subj he kickedl 3 obj bucketl kickedl 4 adjm ugly bucketl 0 subj he kicked2 3 obj bucket2 kicked2 3 adjm ugly bucket2 2 provide the necessary balance. (Here kickedl is the idiomatic interpretation, and bucketl is a word-sense of bucket used only in that interpretation.) 5 By convention, rules for syntactic functions are as- signed, by class, to entries for specific kinds of concepts. Thus rules for verb-attached arguments or preposi- tional phrases are stored with verbs or verb classes. Adjective-noun rules are generally associated with ad- jectives, and noun-noun rules with the right-hand nouns (the next section discusses the treatment of noun-phrase choice points in somewhat greater detail). Lexicon entries also contain additional information. First, a list of required syntactic functions is generally associated with word-senses. Also, syntactic function rules may contain additional conditions limiting their applicability. For example, a combination \"nounl IN noun2\" might be limited to apply only if the second concept denotes an object larger than the first. 4.2 Lexicon Application Given these lexicon entries, the set of semantic alter- natives corresponding to each syntactic alternative \"synfun wordl word2\" may be derived. The goal of the derivation process is to account for all possible combinations of word-senses for wordl and word2 related by the syntactic function \"synfun\". To do this, all concept entries containing potentially applicable rules are searched. For each satisfied rule found, an alternative is created of the form: semantic-relation sensepairs weight where \"sensepairs\" is a list of pairs. Each pair is in the form ((di,dj....) (hm,hn,...)), where the di's are senses of the dependent participant of the function, Obtaining the necessary lexicon information is of course a major problem. But there is significant contemporary work in the automatic or semi-automatic derivation of that information. For example, the aproached described by Jensen and Binot (1986) obtains attachment preference information by parsing dictionary definitions. and the hi's are senses of the headword. The semantic relationship is stated to apply to all combinations of word-senses in the cross-products of those lists. For the example sentence, this process would obtain es- sentially the alternatives shown in Figure 1, except that alternative c23 would first be expressed as: obj ((buck1,buck2)(shoot1,shoot2)) 0 The last step in the process reduces this result. If some of the word-sense combinations are also found in an alternative of higher weight, the \"dominated\" combinations are deleted. And if all word sense com- binations are so dominated, the alternative is deleted. In this way alternative c23 is reduced to its final form. After the semantic choice point list is completed, the search algorithm is applied as described above. 5. Preparing Syntactic Choices In the example above, the preparation of syntactic choice points from parser output was very simple. Assuming an input choice point for a constituent to be a list of (one or more) parser-provided alternative relationships with an immediately containing constitu- ent, the process consisted of obtaining the headwords of each constituent, and of substituting literal prepo- sitions for the general syntactic function \"ppmod\". However, in other cases this step is a more significant one. In the lexicon, selectional preferences are ex- pressed in terms of the syntactic functions of some basic constituent types. For example, verb preferences are expressed in terms of the syntactic functions of active-voice, declarative main clauses, with dependents in unmarked positions. Adjective preferences are ex- pressed in terms of classes of nouns occurring in the relationship \"adjective noun\". But there are many other syntactic relationships whose disambiguation de- pends on this information. The major function of the syntactic choice identification step is to re-express, or \"normalize\" input syntactic relationsips in terms of the relationships assumed by the lexicon. For example, passive constructions such as: (7) The bucks were shot with a rifle are normalized by replacing the choice \"subj buck shoot\" with \"object buck shoot\". (A lexicon condition barring or lowering preference for the \"gambling\" in- terpretation in the passive voice is also needed here.) In ditransitive cases both indirect and direct object functions are used as alternatives. Thus the sequence of deriving semantic choice points consists of two significant steps, which may be depicted in terms of results as shown in Figure 5. The transformation of input syntactic choice points to normalized syntactic choice points is governed by de- clarative specifications indicating, for each syntactic function, how its choice points are to be transformed. The changes are specified as replacements for one or more positions of the choice triple. For example, some of the \"subj\" rules are: (subj (test (voicepassive) synfun 'obj)) {test (not (voicepassive)) synfun 'subj) ... stating that for the input function \"subj\", if the specified test (voicepassive) succeeds, then \"obj\" is used for the synfun part of the normalized choice. The additional rule is used to ensure that the \"subject\" function is retained only for the active voice. Additional applications of these transformations in- clude those for modifiers of nominalized verbs, attrib- utive clauses, and relative clauses. Input Syn Chptl Normalized Syn Chptl Semantic Chptl Choice C11 Choice C111 Choice C1111 Choice C1112 Choice C112 Choice C1121 Choice C1122 Figure 5: Steps in Semantic Choice Point Derivation Noun phrases whose heads are nominalized verbs are addressed by adding choice points corresponding to verb arguments. Thus for (8) The bucks' shooting .... the alternative \"nounmod bucks shooting\" is expanded to include the alternatives \"subj bucks shooting\" and \"obj bucks shooting\". Then, during lexical processing, rules for word-senses of the noun \"shooting\" having an associated verb are understood as expanded to include the expected verb arguments. Attributive clauses such as: (9) The bucks were handsome. are transformed to allow the application of adjective information. Here \"obj handsome were\" is transformed to \"adjmod handsome bucks\". For relative clauses, the core of the transformation expresses alternative attachments of the relative clause as alternative connections between the head of the relative clause, and the possible fillers of the gap po- sition. (Relative clauses with multiple gaps are gen- erally handled in separate parses.) Thus for: (10) The rifle above the mantle that the bucks were shot with... transformations produce the alternatives \"with shoot rifle\" and \"with shoot mantle\". The handling of relative clauses, however, is more complicated than this, as it is desirable to also use information from the relative pronoun (if present) for the disambiguation. Two initial choice points are in- volved, one attaching the relative clause to its higher level head, and one attaching the gap to its head. The first is expanded to to obtain relationships \"relp that rifle\", \"relp that mantle\", and the other to obtain the \"with ....\" relationships. And an additional consistency check is made during the tree search, beyond word- sense consistency, to keep the choices consistent. It should be noted that the transformation rules for syntactic choice points also include \"fixup specifica- tions\" (not shown above) indicating how result semantic functions and attachments are to be modified if the transformed alternatives are used in the final interpre- tation. For example, to \"fixup\" the results of trans- forming attributive clauses, the noun-modifier semantic role is replaced with one suitable to a direct role in the clause. 6. Concluding Remarks This paper has summarized a three-step method for optimized combinatorial preference based disambiguation: 1. obtaining syntactic choice points, with alternatives stated as syntactic functions relating words. 2. transformation into semantic choice points with alternatives stated as weighted semantic functions relating word-senses, via lexicon search. 3. application of A* to search alternative combina- tions. This method, currently being implemented in the con- text of a multi-target machine translation system, is more powerful and systematic than approaches using isolated or interacting decision procedures, and thus is easier to modify and extend with new heuristics as desired. The method is applicable to any post-parse disambiguation situation, and can be taken as an ar- gument for that approach. To demonstrate this, we first note that aspects of the method are useful for within-parse disambiguation. In any realistic scheme, decisions must often be deferred, making two aspects of the method relevant: (a) the unified way of repre- senting word sense and attachment alternatives and their interdependency, and (b) the explicit, additive weights. Explicit additive weights substitute for elab- orate, case-specific rules, and also make possible a systematic treatment of alternative parses which differ in more than word-senses and attachments. However, using weighted attachments for within-parse disambiguation requires calculating the summed weights of, and examining the consistency of, all com- binations encountered whose elements cannot be dis- carded (assuming some good criteria for discarding can be found). Deferring disambiguation until after the parse allows for optimized searching of alternatives, as described above, to significantly limit the number of combinations examined. Future work in this direction will include refining the weighting criteria, extending the method to deal with anaphoric references (using considerations developed by, for example, Jensen and Zadrozny (1987), and integrating a treatment of non-frozen metaphor. 7.Acknowledgements I thank John Sowa, Maria Fuenmayor, and Shelley Smith for their careful reviews and many helpful sug- gestions. Also, I thank Peter Woon for his patient managerial support of this project. 8. References 1. Bates, Madeleine 1976. \"Syntax in Automatic Speech Understanding\", Am. J. Comp. Ling. Mi- crofiche 45. 2. Charniak, Eugene 1986. \"A Neat Theory of Marker Passing\" Proc. ΑΛΑΙ-86 584-588 3. Cottrell, Garrison W. and Steven L. Small 1984. \"Viewing Parsing as Word Sense Discrimination: A Connectionist Approach\", in B.G. Bara and G. Guida (eds), Computation Models of Natural Lan- guage Processing, Elsevier Science Publishers B.V. 4. Dahlgren, Kathleen and Joyce McDowell 1986. \"Using Commonsense Knowledge to Disambiguate Prepositional Phrase Modifiers\", Proc. AAAI-86, 589-593 5. Heidorn, George 1982. \"Experience with an Easily Computed Metric for Ranking Alternative Parses\" Proc. 20th Annual Meeting of the ACL, June 1982 6. Hirst, Graeme 1983. \"Semantic Interpretation Against Ambiguity\", Technical Report CS-83-25, Brown University, December 1983 7. Jensen, Karen 1986. \"Parsing Strategies in a Broad-coverage Grammar of English\", IBM Re- search Report RC 12147, 1986 8. Jensen, Karen and Jean-Louis Binot 1987. \"Α Semantic Expert Using an Online Standard Dic- tionary\", Proc. IJCAI-87, 709-714 9. Jensen, Karen and Wlodzimierz Zadrozny 1987. \"The Semantics of Paragraphs\", presented at Logic and Linguistics, Stanford, July 1987. 10. Maxwell, B.D and F. D. Tuggle 1975. \"\"Toward a Natural Language Question Answering Facility\", Am. J. Comp. Ling., Microfiche 61. 11. Nilsson, Nils J. 1980. Principles of Artificial In- telligence, Tioga Publishing Co. 12. Robinson, Jane J. 1982. \"DIAGRAM: A Gram- mar for Dialogues\", Comm. ACM Vol 25 No 1, 27-47 13. Schubert, Klaus 1986. \"Linguistic and Extra- Linguistic Knowledge\" Computers and Translation Vol 1, No 3, July-September 1986 14. Schubert, Lenhart K. 1986. \"Are There Preference Trade-offs in Attachment Decisions\", Proc. ΑΛΛΙ-86, 601-605 15. Tomita, Masaru 1984. \"Disambiguating Gram- matically Ambiguous Sentences by Asking\", Proc. COLING 84 16. Tomita, Masaru 1985. \"An Efficient Context-Free Parsing Algorithm for Natural Languages\", Proc. IJCAI-85,756-763 17. Walker, Donald E. and William H. Paxton with Gary G. Hendrix, Ann E. Robinson, Jane J. Rob- inson, Jonathan Slocum 1977. \"Procedures for Integrating Knowledge in a Speech Understanding System\", SRI Technical Note 143. 18. Waltz, David L. and J. B. Pollack 1985. \"Mas- sively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation\", Cog- nitive Science Vol 9,No 1, January-March 1985, 51-74 19. Wilks, Yorick, Xiuming Huang, and Dan Fass 1985. \"Syntax, Preference and Right Attachment\", Proc. IJCAI-85 779-784 20. Wilks, Yorick 1975. \"An Intelligent Analyzer and Understander of English\", Comm. ACM, Vol 18 No 5, 264-274 21. Wittenburg, Kent 1986. \"A Parser for Portable NL Interfaces Using Graph-Unification-Based Grammars\", Proc. AAA1-86, 1053-1058 9. Appendix: Search Algorithm Figure 6 describes the step by step application of A* to searching semantic choices. Assume an \"open list\" containing, for each node n with an unexamined child, the following information: 1. The list of choices made on the path up to and including n. 2. The set of constraints on word senses imposed by nodes on the path. 3. The index of the highest weighted unexamined child (choice at next level) of n, where choices within levels are sorted by descending weight. 4. The potential Fc = An + Wc + Hc for that child, where An is the accumulated weight on the path up to and including n, We is the weight of the child, and Hc is the upper bound on the cumulative potential for paths below the child. Then the following algorithm is used to search the tree. 1. Put an entry for the dummy \"top\" node in the open list, with path=self, index of first child, and its potential fc. 2. Find the node n in the \"open list\" which has the highest potential Fc for a child node. 3. If a full path has already been found, and Fc is lower than the total weight for that path, the search is over. 4. Otherwise check the consistency of the designated child in entry n. If it is consistent, add an open list entry for the child, including a new, more constrained consistency requirement. 5. Whether or not the designated child is consistent, determine if there are any unexamined children of node n. If so, modify entry n accordingly. Otherwise remove entry n from the open list. 6. If there is a new entry, and it represents a completed path, remove it from the open list and perform additional consistency checks. If the checks fail, ignore the new path. If they succeed, record the path and its score as a competing alternative. 7. Return to Step 2. Figure 6: Search Algorithm"
  },
  {
    "title": "A Classification Approach to Word Prediction",
    "abstract": "The eventual goal of a language model is to accurately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistic predicates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction. Second, since the number of words \"competing\" for each prediction is large, there is a need to \"focus the attention\" on a smaller subset of these. We exhibit the contribution of a \"focus of attention\" mechanism to the performance of the word predictor. Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.",
    "content": "1 Introduction The task of predicting the most likely word based on properties of its surrounding context is the archetyp- ical prediction problem in natural language process- ing (NLP). In many NLP tasks it is necessary to de- termine the most likely word, part-of-speech (POS) tag or any other token, given its history or context. Examples include part-of speech tagging, word-sense disambiguation, speech recognition, accent restora- tion, word choice selection in machine translation, context-sensitive spelling correction and identifying discourse markers. Most approaches to these prob- lems are based on n-gram-like modeling. Namely, the learning methods make use of features which are conjunctions of typically (up to) three consecutive words or POS tags in order to derive the predictor. In this paper we show that incorporating addi- tional information into the learning process is very This research is supported by NSF grants IIS-9801638 and SBR-987345. beneficial. In particular, we provide the learner with a rich set of features that combine the information available in the local context along with shallow parsing information. At the same time, we study a learning approach that is specifically tailored for problems in which the potential number of features is very large but only a fairly small number of them actually participates in the decision. Word predic- tion experiments that we perform show significant improvements in error rate relative to the use of the traditional, restricted, set of features. Background The most influential problem in motivating statis- tical learning application in NLP tasks is that of word selection in speech recognition (Jelinek, 1998). There, word classifiers are derived from a probabilis- tic language model which estimates the probability of a sentence s using Bayes rule as the product of conditional probabilities, Pr(s) = Pr(W1, W2, ... wn) = = Π=1Pr(w/W1,... Wi-1) II=1Pr(wilhi) where hi is the relevant history when predicting wi. Thus, in order to predict the most likely word in a given context, a global estimation of the sentence probability is derived which, in turn, is computed by estimating the probability of each word given its local context or history. Estimating terms of the form Pr(w/h) is done by assuming some generative probabilistic model, typically using Markov or other independence assumptions, which gives rise to es- timating conditional probabilities of n-grams type features (in the word or POS space). Machine learn- ing based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (Brill, 1995; Yarowsky, 1994; Ratnaparkhi et al., 1994). It has been argued that the information available in the local context of each word should be aug- mented by global sentence information and even in- formation external to the sentence in order to learn better classifiers and language models. Efforts in this directions consists of (1) directly adding syn- tactic information, as in (Chelba and Jelinek, 1998; Rosenfeld, 1996), and (2) indirectly adding syntac- tic and semantic information, via similarity models; in this case n-gram type features are used when- ever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used (Dagan et al., 1999). Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (Chelba and Jelinek, 1998; Rosenfeld, 1996). We believe that the main reason for that is that in- corporating information sources in NLP needs to be coupled with a learning approach that is suitable for it. Studies have shown that both machine learning and probabilistic learning methods used in NLP make decisions using a linear decision surface over the feature space (Roth, 1998; Roth, 1999). In this view, the feature space consists of simple functions (e.g., n-grams) over the the original data so as to allow for expressive enough representations using a simple functional form (e.g., a linear function). This implies that the number of potential features that the learning stage needs to consider may be very large, and may grow rapidly when increasing the ex- pressivity of the features. Therefore a feasible com- putational approach needs to be feature-efficient. It needs to tolerate a large number of potential features in the sense that the number of examples required for it to converge should depend mostly on the num- ber features relevant to the decision, rather than on the number of potential features. This paper addresses the two issues mentioned above. It presents a rich set of features that is con- structed using information readily available in the sentence along with shallow parsing and dependency information. It then presents a learning approach that can use this expressive (and potentially large) intermediate representation and shows that it yields a significant improvement in word error rate for the task of word prediction. The rest of the paper is organized as follows. In section 2 we formalize the problem, discuss the in- formation sources available to the learning system and how we use those to construct features. In sec- tion 3 we present the learning approach, based on the SNoW learning architecture. Section 4 presents our experimental study and results. In section 4.4 we discuss the issue of deciding on a set of candi- date words for each decision. Section 5 concludes and discusses future work. 2 Information Sources and Features Our goal is to learn a representation for each word in terms of features which characterize the syntactic and semantic context in which the word tends to appear. Our features are defined as simple relations over a collection of predicates that capture (some of) the information available in a sentence. 2.1 Information Sources Definition 1 Let s =< W1, W2, ..., Wn > be a sen- tence in which wi is the i-th word. Let I be a col- lection of predicates over a sentence s. IS(s))¹, the Information source(s) available for the sentence s is a representation of s as a list of predicates I E I, IS(s) = {11(W11, ...W1;), ..., Ik (Wk₁,...Wk₁)}. ji is the arity of the predicate Ij. Example 2 Let's be the sentence < John, X, at, the, clock, to, see, what, time, it, is > Let I={word, pos, subj-verb}, with the interpreta- tion that word is a unary predicate that returns the value of the word in its domain; pos is a unary predicate that returns the value of the pos of the word in its domain, in the context of the sentence; subj verb is a binary predicate that returns the value of the two words in its domain if the second is a verb in the sentence and the first is its subject; it returns & otherwise. Then, IS(s) = {word(w₁) = John, ..., word(w3) = at, ..., word(w11) = is, pos(w₄) = DET, ..., subj - verb(w₁, W2) = {John, X}...}. The IS representation of s consists only of the pred- icates with non-empty values. E.g., pos(ως) = modal is not part of the IS for the sentence above. subj - verb might not exist at all in the IS even if the predicate is available, e.g., in The ball was given to Mary. Clearly the IS representation of s does not contain all the information available to a human reading s; it captures, however, all the input that is available to the computational process discussed in the rest of this paper. The predicates could be generated by any external mechanism, even a learned one. This issue is orthogonal to the current discussion. 2.2 Generating Features Our goal is to learn a representation for each word of interest. Most efficient learning methods known today and, in particular, those used in NLP, make use of a linear decision surface over their feature space (Roth, 1998; Roth, 1999). Therefore, in or- der to learn expressive representations one needs to compose complex features as a function of the in- formation sources available. A linear function ex- pressed directly in terms of those will not be expres- sive enough. We now define a language that allows 1 We denote IS(s) as IS wherever it is obvious what the referred sentence we is, or whenever we want to indicate In- formation Source in general. one to define \"types\" of features in terms of the information sources available to it. Definition 3 (Basic Features) Let I ∈ I be a k-ary predicate with range R. Denote wk = (Wj1,..., Wjk). We define two basic binary relations as follows. For a ∈ R we define: f(I(wk), a) = { 1 iff I(wk) = α 0 otherwise (1) An existential version of the relation is defined by: f(I(wk), x) = { 1 iff ∃α ∈ R s.t I(wk) = α 0 otherwise (2) Features, which are defined as binary relations, can be composed to yield more complex relations in terms of the original predicates available in IS. Definition 4 (Composing features) Let f1, f2 be feature definitions. Then fand(f1, f2) for(f1, f2) fnot(f1) are defined and given the usual semantic: fand(f1, f2) = { 1 if f1 = f2 = 1 0 otherwise for(f1, f2) = { 1 if f1 = 1 or f2 = 1 0 otherwise fnot(f1) = { 1 if f1 = 0 0 otherwise In order to learn with features generated using these definitions as input, it is important that features generated when applying the definitions on different ISs are given the same identification. In this pre- sentation we assume that the composition operator along with the appropriate IS element (e.g., Ex. 2, Ex. 9) are written explicitly as the identification of the features. Some of the subtleties in defining the output representation are addressed in (Cumby and Roth, 2000). 2.3 Structured Features So far we have presented features as relations over IS(s) and allowed for Boolean composition opera- tors. In most cases more information than just a list of active predicates is available. We abstract this using the notion of a structural information source (SIS(s)) defined below. This allows richer class of feature types to be defined. 2We note that we do not define the features will be used in the learning process. These are going to be defined in a data driven way given the definitions discussed here and the input ISs. The importance of formally defining the \"types\" is due to the fact that some of these are quantified. Evaluating them on a given sentence might be computationally intractable and a formal definition would help to flesh out the difficulties and aid in designing the language (Cumby and Roth, 2000). 2.4 Structured Instances Definition 5 (Structural Information Source) Let s =< W1, W2, ..., Wn >. SIS(s)), the Structural Information source(s) available for the sentence s, is a tuple (s, E1,..., Ek) of directed acyclic graphs with s as the set of vertices and Ei's, a set of edges in s. Example 6 (Linear Structure) The simplest SIS is the one corresponding to the linear structure of the sentence. That is, SIS(s) = (s,E) where (wi, wj) ∈ E iff the word wi occurs immediately before wj in the sentence (Figure 1 bottom left part). In a linear structure (s =< W1,W2, ..., Wn >,E), where E = {(Wi, Wi+1); i = 1,... n - 1}, we define the chain C(wj, [l,r]) = {Wj-l,..., Wj, ... Wj+r}∩s. We can now define a new set of features that makes use of the structural information. Structural features are defined using the SIS. When defining a feature, the naming of nodes in s is done relative to a distinguished node, denoted wp, which we call the focus word of the feature. Regardless of the arity of the features we sometimes denote the feature f defined with respect to wp as f(wp). Definition 7 (Proximity) Let SIS(s) = (s, E) be the linear structure and let I ∈ I be a k-ary predicate with range R. Let wp be a focus word and C = C(wp, [l, r]) the chain around it. Then, the proximity features for I with respect to the chain C are defined as: fc(I(w), α) = { 1 if I(w) = α, α ∈ R, w ∈ C 0 otherwise (3) The second type of feature composition defined using the structure is a collocation operator. Definition 8 (Collocation) Let f1.... fk be fea- ture definitions. collocc(f1, f2,... fk) is a restricted conjunctive operator that is evaluated on a chain C of length k in a graph. Specifically, let C = {Wj1, Wj2,..., Wjk} be a chain of length k in SIS(s). Then, the collocation feature for f1,... fk with re- spect to the chain C is defined as collocc(f1,..., fk) = { 1 if Vi = 1,... k, fi(wj.) = 1 0 otherwise (4) The following example defines features that are used in the experiments described in Sec. 4. Example 9 Let's be the sentence in Example 2. We define some of the features with respect to the linear structure of the sentence. The word X is used as the focus word and a chain [-10, 10] is defined with respect to it. The proximity features are defined with respect to the predicate word. We get, for example: fc(word) = John; fc (word) = at; fc(word) = clock. Collocation features are defined with respect to a chain [-2,2] centered at the focus word X. They are defined with respect to two basic features f1, f2 each of which can be either f(word, a) or f(pos, a). The resulting features include, for example: collocc (word, word) colloc(word, word) collocc (word, pos) = = = {John - X}; {X - at}; {at - DET}. 2.5 Non-Linear Structure So far we have described feature definitions which make use of the linear structure of the sentence and yield features which are not too different from stan- dard features used in the literature e.g., n-grams with respect to pos or word can be defined as colloc for the appropriate chain. Consider now that we are given a general directed acyclic graph G = (s, E) on the the sentence s as its nodes. Given a distin- guished focus word wp E s we can define a chain in the graph as we did above for the linear structure of the sentence. Since the definitions given above, Def. 7 and Def. 8, were given for chains they would apply for any chain in any graph. This generaliza- tion becomes interesting if we are given a graph that represents a more involved structure of the sentence. Consider, for example the graph DG(s) in Fig- ure 1. DG(s) described the dependency graph of the sentence s. An edge (wi, wj) in DG(s) repre- sent a dependency between the two words. In our feature generation language we separate the infor- mation provided by the dependency grammar³ to two parts. The structural information, provided in the left side of Figure 1, is used to generate SIS(s). The labels on the edges are used as predicates and are part of IS(s). Notice that some authors (Yuret, 1998; Berger and Printz, 1998) have used the struc- tural information, but have not used the information given by the labels on the edges as we do. The following example defines features that are used in the experiments described in Sec. 4. Example 10 Let s be the sentence in Figure 1 along with its IS that is defined using the predicates word, pos, subj, obj, aux_vrb. A subj-verb 3This information can be produced by a functional de- pendency grammar (FDG), which assigns each word a spe- cific function, and then structures the sentence hierarchically based on it, as we do here (Tapanainen and Jrvinen, 1997), but can also be generated by an external rule-based parser or a learned one. feature, fsubj-verb, can be defined as a collocation over chains constructed with respect to the focus word join. Moreover, we can define fsubj-verb to be active also when there is an aux_vrb between the subj and verb, by defining it as a disjunction of two collocation features, the subj-verb and the subj-aux_vrb-verb. Other features that we use are conjunctions of words that occur before the focus verb (here: join) along all the chains it occurs in (here: will, board, as) and collocations of obj and verb. As a final comment on feature generation, we note that the language presented is used to define \"types\" of features. These are instantiated in a data driven way given input sentences. A large number of fea- tures is created in this way, most of which might not be relevant to the decision at hand; thus, this pro- cess needs to be followed by a learning process that can learn in the presence of these many features. 3 The Learning Approach Our experimental investigation is done using the SNoW learning system (Roth, 1998). Earlier ver- sions of SNoW (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998; Munoz et al., 1999) have been applied successfully to several natural lan- guage related tasks. Here we use SNoW for the task of word prediction; a representation is learned for each word of interest, and these compete at evalua- tion time to determine the prediction. 3.1 The SNOW Architecture The SNoW architecture is a sparse network of linear units over a common pre-defined or incrementally learned feature space. It is specifically tailored for learning in domains in which the potential number of features might be very large but only a small subset of them is actually relevant to the decision made. Nodes in the input layer of the network represent simple relations on the input sentence and are being used as the input features. Target nodes represent words that are of interest; in the case studied here, each of the word candidates for prediction is repre- sented as a target node. An input sentence, along with a designated word of interest in it, is mapped into a set of features which are active in it; this rep- resentation is presented to the input layer of SNOW and propagates to the target nodes. Target nodes are linked via weighted edges to (some of) the input features. Let At = {i1,..., im} be the set of features that are active in an example and are linked to the target node t. Then the linear unit corresponding to t is active iff Σω > θε, iEAt where w is the weight on the edge connecting the ith feature to the target node t, and or is the threshold join ,\" Nov.29. will board as Vinken director a the Pierre Years old non-executive 61 Pierre Vinken, 61 years old, will join the board as a non-executive director 29. - Nov. _subj Vinken attr Pierre mod will years qnt 61 join aux_vrb obj corped board ▼ det the Nov. 29. old as pcomp det director 4 attr non-executive a Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29. Figure 1: A sentence with a linear and a dependency grammar structure for the target node t. In this way, SNoW provides a collection of word representations rather than just discriminators. A given example is treated autonomously by each target subnetwork; an example labeled t may be treated as a positive example by the subnetwork for t and as a negative example by the rest of the target nodes. The learning policy is on-line and mistake-driven; several update rules can be used within SNoW. The most successful update rule is a variant of Littlestone's Winnow update rule (Lit- tlestone, 1988), a multiplicative update rule that is tailored to the situation in which the set of input features is not known a priori, as in the infinite attribute model (Blum, 1992). This mechanism is implemented via the sparse architecture of SNoW. That is, (1) input features are allocated in a data driven way — an input node for the feature i is al- located only if the feature i was active in any input sentence and (2) a link (i.e., a non-zero weight) ex- ists between a target node t and a feature i if and only if i was active in an example labeled t. One of the important properties of the sparse ar- chitecture is that the complexity of processing an example depends only on the number of features ac- tive in it, na, and is independent of the total num- ber of features, nt, observed over the life time of the system. This is important in domains in which the total number of features is very large, but only a small number of them is active in each example. Once target subnetworks have been learned and the network is being evaluated, a decision sup- port mechanism is employed, which selects the dominant active target node in the SNoW unit via a winner-take-all mechanism to produce a fi- nal prediction. SNoW is available publicly at http://L2R.cs.uiuc.edu/~cogcomp.html. 4 Experimental Study 4.1 Task definition The experiments were conducted with four goals in mind: 1. To compare mistake driven algorithms with naive Bayes, trigram with backoff and a simple maximum likelihood estimation (MLE) base- line. 2. To create a set of experiments which is compa- rable with similar experiments that were previ- ously conducted by other researchers. 3. To build a baseline for two types of extensions of the simple use of linear features: (i) Non-Linear features (ii) Automatic focus of attention. 4. To evaluate word prediction as a simple lan- guage model. We chose the verb prediction task which is sim- ilar to other word prediction tasks (e.g., (Golding and Roth, 1999)) and, in particular, follows the paradigm in (Lee and Pereira, 1999; Dagan et al., 1999; Lee, 1999). There, a list of the confusion sets is constructed first, each consists of two different verbs. The verb v₁ is coupled with v2 provided that they occur equally likely in the corpus. In the test set, every occurrence of v₁ or v2 was replaced by a set {v₁, v₂} and the classification task was to predict the correct verb. For example, if a confusion set is cre- ated for the verbs \"make\" and \"sell\", then the data is altered as follows: make the paper → {make,sell} the paper sell sensitive data → {make, sell} sensitive data The evaluated predictor chooses which of the two verbs is more likely to occur in the current sentence. In choosing the prediction task in this way, we make sure the task in difficult by choosing between competing words that have the same prior proba- bilities and have the same part of speech. A fur- ther advantage of this paradigm is that in future experiments we may choose the candidate verbs so that they have the same sub-categorization, pho- netic transcription, etc. in order to imitate the first phase of language modeling used in creating can- didates for the prediction task. Moreover, the pre- transformed data provides the correct answer so that (i) it is easy to generate training data; no supervi- sion is required, and (ii) it is easy to evaluate the results assuming that the most appropriate word is provided in the original text. Results are evaluated using word-error rate (WER). Namely, every time we predict the wrong word it is counted as a mistake. 4.2 Data We used the Wall Street Journal (WSJ) of the years 88-89. The size of our corpus is about 1,000,000 words. The corpus was divided into 80% training and 20% test. The training and the test data were processed by the FDG parser (Tapanainen and Jrvi- nen, 1997). Only verbs that occur at least 50 times in the corpus were chosen. This resulted in 278 verbs that we split into 139 confusion sets as above. Af- ter filtering the examples of verbs which were not in any of the sets we use 73, 184 training examples and 19,852 test examples. 4.3 Results 4.3.1 Features In order to test the advantages of different feature sets we conducted experiments using the following features sets: 1. Linear features: proximity of window size ±10 words, conjunction of size 2 using window size ±2. The conjunction combines words and parts of speech. 2. Linear + Non linear features: using the lin- ear features defined in (1) along with non linear features that use the predicates subj, obj, word, pos, the collocations subj-verb, verb-obj linked to the focus verb via the graph structure and conjunction of 2 linked words. The over all number of features we have generated for all 278 target verbs was around 400,000. In all tables below the NB columns represent results of the naive Bayes algorithm as implemented within SNoW and the SNoW column represents the results of the sparse Winnow algorithm within SNoW. Table 1 summarizes the results of the experiments with the features sets (1), (2) above. The baseline experiment uses MLE, the majority predictor. In addition, we conducted the same experiment using trigram with backoff and the WER is 29.3%. From Linear Bline NB SNoW 49.6 13.54 11.56 Non Linear 49.6 12.25 9.84 Table 1: Word Error Rate results for linear and non-linear features these results we conclude that using more expressive features helps significantly in reducing the WER. However, one can use those types of features only if the learning method handles large number of pos- sible features. This emphasizes the importance of the new learning method. WSJ data Similarity NB SNoW AP news 47.6% 54.6% 59.1% Table 2: Comparison of the improvement achieved using similarity methods (Dagan et al., 1999) and using the methods presented in this paper. Results are shown in percentage of improvement in accuracy over the baseline. Table 2 compares our method to methods that use similarity measures (Dagan et al., 1999; Lee, 1999). Since we could not use the same corpus as in those experiments, we compare the ratio of improvement and not the WER. The baseline in this studies is different, but other than that the experiments are identical. We show an improvement over the best similarity method. Furthermore, we train using only 73, 184 examples while (Dagan et al., 1999) train using 587, 833 examples. Given our experience with our approach on other data sets we conjecture that we could have improved the results further had we used that many training examples. 4.4 Focus of attention SNoW is used in our experiments as a multi-class predictor - a representation is learned for each word in a given set and, at evaluation time, one of these is selected as the prediction. The set of candidate words is called the confusion set (Golding and Roth, 1999). Let C be the set of all target words. In previ- ous experiments we generated artificially subsets of size 2 of C in order to evaluate the performance of our methods. In general, however, the question of determining a good set of candidates is interesting in it own right. In the absence of a good method, one might end up choosing a verb from among a larger set of candidates. We would like to study the effects this issue has on the performance of our method. In principle, instead of working with a single large confusion set C, it might be possible to split C into subsets of smaller size. This process, which we call the focus of attention (FOA) would be beneficial only if we can guarantee that, with high probability, given a prediction task, we know which confusion set to use, so that the true target belongs to it. In fact, the FOA problem can be discussed separately for the training and test stages. 1. Training: Given our training policy (Sec. 3) ev- ery positive example serves as a negative exam- ple to all other targets in its confusion set. For a large set C training might become computa- tionally infeasible. 2. Testing: considering only a small set of words as candidates at evaluation time increases the baseline and might be significant from the point of view of accuracy and efficiency. To evaluate the advantage of reducing the size of the confusion set in the training and test phases, we conducted the following experiments using the same features set (linear features as in Table 1). Train All Test All Train All Test 2 Train 2 Test 2 Bline NB SNOW 87.44 65.22 65.05 49.6 13.54 13.15 49.6 13.54 11.55 Table 3: Evaluating Focus of Attention: Word Error Rate for Training and testing using all the words together against using pairs of words. \"Train All\" means training on all 278 targets to- gether. \"Test all\" means that the confusion set is of size 278 and includes all the targets. The results shown in Table 3 suggest that, in terms of accuracy, the significant factor is the confusion set size in the test stage. The effect of the confusion set size on training is minimal (although it does affect training time). We note that for the naive Bayes algorithm the notion of negative examples does not exist, and therefore regardless of the size of confusion set in training, it learns exactly the same representations. Thus, in the NB column, the confusion set size in training makes no difference. The application in which a word predictor is used might give a partial solution to the FOA problem. For example, given a prediction task in the context of speech recognition the phonemes that constitute the word might be known and thus suggest a way to generate a small confusion set to be used when evaluating the predictors. Tables 4,5 present the results of using artificially simulated speech recognizer using a method of gen- eral phonetic classes. That is, instead of transcrib- ing a word by the phoneme, the word is transcribed by the phoneme classes (Jurafsky and Martin, 200). Specifically, these experiments deviate from the task definition given above. The confusion sets used are of different sizes and they consist of verbs with dif- ferent prior probabilities in the corpus. Two sets of experiments were conducted that use the phonetic transcription of the words to generate confusion sets. Train All Test PC Train PC Test PC Bline NB SNOW 19.84 11.6 12.3 19.84 11.6 11.3 Table 4: Simulating Speech Recognizer: Word Error Rate for Training and testing with confusion sets determined based on phonetic classes (PC) from a simulated speech recog- nizer. In the first experiment (Table 4), the transcription of each word is given by the broad phonetic groups to which the phonemes belong i.e., nasals, fricative, etc.4. For example, the word \"b_u_y\" is transcribed using phonemes as \"b-Y\" and here we transcribe it as \"P_V1\" which stands for \"Plosive_Vowell\". This partition results in a partition of the set of verbs into several confusions sets. A few of these confusion sets consist of a single word and therefore have 100% baseline, which explains the high baseline. Bline NB SNOW Train All Test PC 45.63 26.36 27.54 Train PC Test PC 45.63 26.36 25.55 Table 5: Simulating Speech Recognizer: Word Error Rate for Training and testing with confusion sets determined based on phonetic classes (PC) from a simulated speech recog- nizer. In this case only confusion sets that have less than 98% baseline are used, which explains the overall lower baseline. Table 5 presents the results of a similar exper- iment in which only confusion sets with multiple words were used, resulting in a lower baseline. As before, Train All means that training is done with all 278 targets together while Train PC means that the PC confusion sets were used also in train- ing. We note that for the case of SNoW, used here with the sparse Winnow algorithm, that size of the confusion set in training has some, although small, effect. The reason is that when the training is done with all the target words, each target word repre- sentation with all the examples in which it does not occur are used as negative examples. When a smaller confusion set is used the negative examples are more likely to be \"true\" negative. 5 Conclusion This paper presents a new approach to word predic- tion tasks. For each word of interest, a word repre- sentation is learned as a function of a common, but 4In this experiment, the vowels phonemes were divided into two different groups to account for different sounds. potentially very large set of expressive (relational) features. Given a prediction task (a sentence with a missing word) the word representations are evalu- ated on it and compete for the most likely word to complete the sentence. We have described a language that allows one to define expressive feature types and have exhibited experimentally the advantage of using those on word prediction task. We have argued that the success of this approach hinges on the combination of using a large set of expressive features along with a learning approach that can tolerate it and converges quickly despite the large dimensionality of the data. We believe that this approach would be useful for other disambiguation tasks in NLP. We have also presented a preliminary study of a reduction in the confusion set size and its effects on the prediction performance. In future work we intend to study ways that determine the appropriate confusion set in a way to makes use of the current task properties. Acknowledgments We gratefully acknowledge helpful comments and programming help from Chad Cumby. References A. Berger and H. Printz. 1998. Recognition perfor- mance of a large-scale dependency-grammar lan- guage model. In Int'l Conference on Spoken Lan- guage Processing (ICSLP'98), Sydney, Australia. A. Blum. 1992. Learning boolean functions in an infinite attribute space. Machine Learning, 9(4):373-386. E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565. C. Chelba and F. Jelinek. 1998. Exploiting syntac- tic structure for language modeling. In COLING- ACL'98. C. Cumby and D. Roth. 2000. Relational repre- sentations that facilitate learning. In Proc. of the International Conference on the Principles of Knowledge Representation and Reasoning. To ap- pear. I. Dagan, L. Lee, and F. Pereira. 1999. Similarity- based models of word cooccurrence probabilities. Machine Learning, 34(1-3):43-69. A. R. Golding and D. Roth. 1999. A Winnow based approach to context-sensitive spelling correction. Machine Learning, 34(1-3):107-130. Special Issue on Machine Learning and Natural Language. F. Jelinek. 1998. Statistical Methods for Speech Recognition. MIT Press. D. Jurafsky and J. H. Martin. 200. Speech and Lan- guage Processing. Prentice Hall. L. Lee and F. Pereira. 1999. Distributional similar- ity models: Clustering vs. nearest neighbors. In ACL 99, pages 33-40. L. Lee. 1999. Measure of distributional similarity. In ACL 99, pages 25-32. N. Littlestone. 1988. Learning quickly when irrel- evant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285-318. M. Munoz, V. Punyakanok, D. Roth, and D. Zimak. 1999. A learning approach to shallow parsing. In EMNLP-VLC'99, the Joint SIGDAT Conference on Empirical Methods in Natural Language Pro- cessing and Very Large Corpora, June. A. Ratnaparkhi, J. Reynar, and S. Roukos. 1994. A maximum entropy model for prepositional phrase attachment. In ARPA, Plainsboro, NJ, March. R. Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Com- puter, Speech and Language, 10. D. Roth and D. Zelenko. 1998. Part of speech tagging using a network of linear separators. In COLING-ACL 98, The 17th International Conference on Computational Linguistics, pages 1136-1142. D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proc. Na- tional Conference on Artificial Intelligence, pages 806-813. D. Roth. 1999. Learning in natural language. In Proc. of the International Joint Conference of Ar- tificial Intelligence, pages 898-904. P. Tapanainen and T. Jrvinen. 1997. A non- projective dependency parser. In In Proceedings of the 5th Conference on Applied Natural Lan- guage Processing, Washington DC. D. Yarowsky. 1994. Decision lists for lexical ambi- guity resolution: application to accent restoration in Spanish and French. In Proc. of the Annual Meeting of the ACL, pages 88-95. D. Yuret. 1998. Discovery of Linguistic Relations Using Lexical Attraction. Ph.D. thesis, MIT."
  },
  {
    "title": "An Open Distributed Architecture for Reuse and Integration of Heterogeneous NLP Components",
    "abstract": "The shift from Computational Linguistics to Language Engineering is indicative of new trends in NLP. This paper reviews two NLP engineering problems: reuse and integration, while relating these concerns to the larger context of applied NLP. It presents a software architecture which is geared to support the development of a variety of large-scale NLP applications: Information Retrieval, Corpus Processing, Multilingual MT, and integration of Speech Components.",
    "content": "1 Introduction The shift from Computational Linguistics to Lan- guage Engineering is indicative of new trends in NLP. We believe that it is not simply a new fash- ion but that it is indicative of the growing matura- tion of the field, as also suggested by an emphasis on building large-scale systems, away from toy research systems. There is also an increasing awareness that real-size systems are not mere scaled-up toy systems but that they present an altogether qualitatively dif- ferent set of problems that require new tools and new ideas, as clearly exemplified by recent projects and programs such as Pangloss (Frederking et al. 94), Tipster (ARPA 94), and Verbmobil (Görz et al. 96). Natural language engineering addresses some tra- ditional issues in software engineering: robustness, testing and evaluation, reuse, and development of large-scale applications (see e.g., (Sommerville 96) for an overview). These issues have been and are the topic of a number of NLP projects and pro- grams: TSNLP, DECIDE, Tipster, MUC, TREC, Multext, Multilex, Genelex, Eagles, etc. This paper reviews two domains of problems in natural language To use the name of two well-known NLP journals. engineering: reuse and integration in the context of software architectures for Natural Language Pro- cessing. The emphasis is put on reuse of NLP soft- ware, components and their integration in order to build large-scale applications. Also relevant to this presentation are topics such as integration of hetero- geneous components for building hybrid systems or for integrating speech and other \"higher-level\" NLP components (section 2). Section 3 presents the Corelli Document Process- ing Architecture, a new software architecture for NLP which is designed to support the development of a variety of large-scale NLP applications: Infor- mation Retrieval, Corpus Processing, Multilingual MT, and integration of Speech with other NLP com- ponents. 2 Reuse in NLP There is an increasing amount of shared corpora and lexical resources that are being made available for NLP researchers through managed data repositories such as LDC, CLR, ELRA, etc. (see e.g., (Wilks et al. 92) for an overview of these repositories). These resources constitute the basic raw materials for building NLP software but not all of these re- sources can be readily used: they might be available in formats that require extensive pre-processing to transform them into resources that are tractable by NLP software. This pre-processing cannot usually be fully automated and is therefore costly. Some projects have concentrated on developing lexical resources directly in a format suitable for further use in NLP software (e.g., Genelex, Mul- tilex). These projects go beyond the definition of interchange formats to define a \"neutral\" linguistic representation in which all lexical knowledge is en- coded and from which, by means of specialized com- pilers, application-specific dictionaries can be ex- tracted. The lexical knowledge encoded in these systems can truly be called reusable since neither the format nor the content is application-dependent. The result of these projects is however not available to the research community. Reuse of NLP software components remains much more limited (Cunningham et al. 96) since prob- lems are compounded: the software components of an NLP system need not only to be able to exchange data using the same format (e.g., feature structures) and to share the same interpretation of the infor- mation they exchange (same linguistic theory, e.g., LFG), but they also need to communicate at the process level, either through direct API calls if they are written in the same programming language or through other means if, for example, they have to run on different platforms-a classical software inte- gration problem. Thus, reuse of NLP software com- ponents can be defined as an integration problem. It is not of course the only approach to reuse in NLP (see for example (Biggerstaff & Perlis 89) for an overview of alternative approaches to software reuse) and some previous efforts have, for example, been directed at building Integrated Development Envi- ronments ((Boitet et al. 82; Simkins 94; Alshawi 92; Grover et al. 93) to mention but a few). Although Integrated Development Environments address some of the problems, they do not give a complete solu- tion since one still has to develop rules and lexical entries using these systems. Direct reuse of NLP software components, e.g., using an existing morphological analyzer as a com- ponent of a larger system, is still very limited but is nevertheless increasingly attractive since the de- velopment of large-scale NLP applications, a focus of current NLP research, is prohibitive for many research groups. The Tipster architecture for ex- ample is directed towards the development of infor- mation retrieval and extraction systems (ARPA 94; Grishman 95) and provides a modular approach to component integration. The GATES architecture builds upon the Tipster architecture and provides a graphical development environment to test inte- grated applications (Cunningham et al. 96). Speech machine-translation architectures need also to solve difficult integration problems and original solutions have been developed in the Verbmobil project (Görz et al. 96), and by researchers at ATR (e.g., (Boitet & Seligman 94)) for example. A generic NLP archi- tecture needs to address component communication and integration at three distinct levels: 1. The process or communication layer involves, for example, communication between different components that could be written in different programming languages and could be running as different processes on a distributed network. 2. The data layer involves exchange and transla- tion of data structures between components. 3. At the linguistic level, components need to share the same interpretation of the data they exchange. A particular NLP architecture embodies design choices related to how components can talk to each other. A variety of solutions are possible as illus- trated below. • Each component can talk directly to each other and thus all components need to incorporate some knowledge about each other at all three levels mentioned above. This is the solution adopted in the Verbmobil architecture which makes use of a special communication software package (written in C and imposing the use of C and Unix) at the process level and uses a chart annotated with feature structures at the data- structure level. At the linguistic level, a variant of HPSG is used (Kesseler 94; Amtrup 95; Turk & Geibler 95; Görz et al. 96). • A central coordinator can incorporate knowl- edge about each component but the component themselves don't have any knowledge about each other, or even about the coordinator. Fil- ters are needed to transform data back and forth between the central data-structure managed by the coordinator (a lattice would be appropri- ate) and the data processed by each compo- nent. Communication between the coordinator and the components can be asynchronous and the coordinator needs then to serialize the ac- tions of each component. This solution, a vari- ant of the blackboard architecture (Erman & Lesser 80) is used in the Kasuga speech transla- tion prototype described in (Boitet & Seligman 94). This architecture imposes no constraints on the components (programming language or software architecture) since communication is based on the SMTP protocol. • The Tipster Document Architecture makes no assumption about the solution used either at the process level or at the linguistic level. At the data structure level, NLP components ex- change data by reading and writing \"annota- tions\" associated with some segment of a docu- ment (Grishman 95). This solution also forms the basis of the GATES system (Cunningham et al. 96). Various versions of this architecture have been developed (in C, C++ and Lisp) but no support is defined for integration of hetero- geneous components. However, in the Tipster Phase III program, a CORBA version of the Tipster architecture will be developed to sup- port distributed processing. 3 The Corelli Document Processing Architecture The Corelli Document Processing Architecture is an attempt to address the various problems mentioned above and also some other software-level engineer- ing issues such as robustness, portability, scalability and inter-language communication (for integrating components written in Lisp, C or other languages). Also of interest are some ergonomic issues such as tractability, understandability and ease of use of the architecture (the programmer being the user in this case). The architecture provides support for com- ponent communication and for data exchange. No constraint is placed on the type of linguistic pro- cessing but a small library of data-structures for NLP is provided to ease data-conversion problems. The data layer implements the Tipster Document Architecture and enables the integration of Tipster- compliant components. This architecture is geared to support the development of large-scale NLP appli- cations such as Information Retrieval systems, mul- tilingual MT systems (Vanni & Zajac 96), hybrid or multi-engine MT systems (Wilks et al. 92; Fred- erking et al. 94; Sumita & Iida 95), speech-based systems (Boitet & Seligman 94; Görz et al. 96) and also systems for the exploration and exploitation of large corpora (Ballim 95; Thompson 95). Basic software engineering requirements • A modular and scalable architecture enables the development of small and simple applica- tions using a file-based implementation such as a grammar checker, as well as large and resource-intensive applications (information re- trieval, machine translation) using a database back-end (with two levels of functionality al- lowing for a single-user persistent store and a full-size commercial database). • A portable implementation allows the devel- opment of small stand-alone PC applications as well as large distributed Unix applications. Portability is ensured through the use of the Java programming language. • A simple and small API which can be easily learned and does not make any presupposition 3.1 Data Layer: Document Services The data layer of the Corelli Architecture is de- rived from the Tipster Architecture and implements the requirements listed above. In this architecture, components do not talk directly to each other but communicate through information (so-called 'anno- tations') attached to a document. This model re- duces inter-dependencies between components, pro- moting the design of modular applications (Figure 1) and enabling the development of blackboard-type applications such as the one described in (Boitet & Seligman 94). The architecture provides solutions for • Representing information about a document, • Storing and retrieving this information in an ef- ficient way, • Exchanging this information among all compo- nents of an application. It does not however provide a solution for translat- ing linguistic structures (e.g., mapping a dependency tree to a constituent structure). These problems are application-dependent and need to be resolved on a case-by-case basis; such integration is feasible, as demonstrated by the various Tipster demonstration systems, and use of the architecture reduces signifi- cantly the load of integrating a component into the application. Documents, Annotations and Attributes The data layer of the Corelli Document Processing Architecture follows the Tipster Architecture. The basic data object is the document. Documents can have attributes and annotations, and can be grouped into collections. Annotations are used to store infor- mation about a particular segment of the document (identified by a span, i.e., start-end byte offsets in the document content) while the document itself re- mains unchanged. This contrasts with the SGML solution used in the Multext project where infor- mation about a piece of text is stored as additional SGML mark-up in the document itself (Ballim 95; about the type of application. The API is de- fined using the IDL language and structured ac- cording to CORBA standards and the CORBA services architecture (OMG 95). • A dynamic Plug'n Play architecture enabling easier integration of components written in dif- ferent programming languages (C, C++, Lisp, Java, etc), where components are \"wrapped\" as tools supporting a common interface. Thompson 95). This architecture supports read-only data (e.g., data stored in a CD-ROM) as well as writable data. Annotations are attributed objects that contain application objects. They can be used, for example, to store morphological tags produced by some tagger, to represent the HTML structure of an HTML document or to store partial results of a chart-parser. A B A B Accessing Documents Documents are accessible via a Document Server which maintains persistent collections, documents and their attributes and annotations. An applica- tion can define its own classes for documents and collections. In the basic document class provided in the architecture, a document is identified by its name (URL to the location of the document's con- tent). In this distributed data model, accessing a document via a Document Server gives access to a document's contents and to attributes and annota- tions of a document. Application Component's Common Facilities C D C D Figure 1: Document annotations as a centralized data-structure enable modular architectures and re- duce the number of interfaces from the order of n² to the order of n. Document Annotations Corelli document annotations are essentially the same as Tipster document annotations and a similar generic interface is provided. However, considering the requirements of NLP applications such as parsers or documents browsers, two additional interfaces are provided: • Since a set of annotations can be quite natu- rally interpreted as a chart, a chart interface provides efficient access to annotations viewed as a directed graph following the classical model of the chart first presented in (Kay 73). • An interval-tree interface provides efficient ac- cess for efficient implementation of display func- tionalities. Application Objects An application manipulating only basic data types (strings, numbers,...) need not define application ob- jects. However, some applications may want to store complex data structures as document annotations, for example, trees, graphs, feature structures, etc. The architecture provides a top application-object class that can be sub-classed to define specific ap- plication objects. To support persistency in the file- based version, an application object needs to imple- ment the read-persistent and write-persistent interfaces (this is provided transparently by the per- sistent versions). A small library of application ob- jects is provided with the architecture. Java Door Orb Document Services Figure 2: NLP components access Document Ser- vices and other facilities (e.g., codeset converters) through JavaSoft's Java Door Orb. Services The Corelli Architecture incorporates standards such as CORBA for defining inter-operable inter- faces, and HTTP for data transport. Following the CORBA model, the Architecture is structured as a set of services with well- defined interfaces: • A Document Management Service (DMS) pro- vides functions for manipulating collections, documents, annotations and attributes. • A Life-Cycle Service provides creation, copying, moving and deletion of objects. • A Naming Service provides access to documents and collections via their names. Named collec- tions and documents are persistent. Figure 2 gives an overview of the Corelli Doc- ument Architecture: an NLP component accesses a Document Service provided by a Document Server using the Corelli Document Architecture API. Client-side application component API calls on remote object references (requested from the Orb) are transparently 'transferred' by the Orb to a Docu- ment Services implementation object for invocation. Figure 3 describes the Java IDL compiler and Java Door Orb interaction. The Corelli Document Archi- tecture API is specified using the Interface Definition Language (IDL), a standard defined by the Object Management Group (OMG 95). The IDL-to-Java compiler essentially produces three significant files: one containing a Java interface corresponding to the IDL operational interface itself, a second containing client-side 'stub' methods to invoke on remote object references (along with code to handle Orb communi- cation overhead), and a third containing server-side 'skeleton' methods to handle implementation object references. What remains is for the server code, im- plementing the IDL operational interface to be de- veloped. When the server implementing the IDL specifi- cation is launched, it creates skeleton object ref- erences for implemented services/objects and pub- lishes them on the Orb. A client wishing to invoke methods on those remote objects creates stub ob- ject references and accesses the orb to resolve them with the implementation references on the server side. Any client API call made on a resolved ob- ject reference is then transparently (to the client) invoked on the corresponding server-side object. The Document Management Service, the Life- Cycle Service and the Naming Service are included in the three versions of the architecture which imple- ment increasingly sophisticated support of database functionalities: 1. The basic file-based version of the architecture uses the local file system to store persistent data (collections, attributes and annotations); the contents of a document can however be located anywhere on the Internet. 2. A persistent store version uses a persistent-store back-end for storing and retrieving collections, attributes and annotations: this version sup- ports the Persistent Object Service which pro- vides greater efficiency for storing and accessing persistent objects as well as enhanced support for defining persistent application objects. 3. A database version uses a commercial database management system to store and retrieve collec- tions, attributes and annotations and also docu- ments (through an import/export mechanism). This version provides a Concurrency Control Service and a Transaction Service. Communication Layer To support integration and communication at the process level, the current version of the Corelli Ar- chitecture provides component inter-communication via the Corelli Plug'n Play architecture (see below) and the Java Door Orb. Corelli Document Architecture IDL specification Stub Impl. NLP Component Java IDL Compiler Java Orb Skeleton Impl. Corelli Document Architecture Implementation Figure 3: Java IDL Compiler Java Door Orb Inter- action. 3.2 Plug'n Play Architecture The data layer of the Corelli Document Architec- ture, as described above, provides a static model for component integration through a common data framework. This data model does not provide any support for communication between components, i.e., for executing and controlling the interaction of a set of components, nor for rapid tool integration. The Corelli Plug'n Play layer aims at filling this gap by providing a dynamic model for component in- tegration: this framework provides a high-level of plug-and-play, allowing for component interchange- ability without modification of the application code, thus facilitating the evolution and upgrade of indi- vidual components. In the preliminary version of the Corelli Plug'n Play layer, the choice was made to develop the most general version of the architecture to ensure that any tool can be integrated using this framework. In this model, all components run as servers and the appli- cation code which implements the logic of the appli- cation runs as a client of the component servers. To be integrated, a component needs to support syn- chronous or asynchronous versions of one or several of four basic operations: execute, query, convert and exchange (in addition to standard initialization ad termination operations). Client-server communi- cation is supported by the Java Door Orb. The rationale for this architecture is that many NLP tools are themselves rather large software com- ponents, and embedding them in servers helps to re- duce the computation load. For example, some mor- phological analyzers load their dictionary in the pro- cess memory, and on small documents, simply start- ing the process could take more time than actual ex- ecution. In such cases, it is more efficient to run the morphological analyzer as a server that can be ac- cessed by various client processes. This architecture also allows the processing load of an application to be distributed by running the components on sev- eral machines accessible over the Internet, thereby enabling the integration of components running on widely different architectures. This model also pro- vides adequate support for the integration of static knowledge sources (such as dictionaries) and of an- cillary tools (such as codeset converters). Figure 4 gives a picture of one possible integra- tion solution. In this example, each component of the application is embedded in a server which is ac- cessed through the Corelli Component Integration API as described above. A component server trans- lates an incoming request into a component action. The server also acts as a filter by translating the document data structures stored in the Document Server in a format appropriate as input for the com- ponent and conversely for the component output. Each component server acts as a wrapper and sev- eral solutions are possible: • If the component has a Java API, it can be en- capsulated directly in the server. • If the component has an API written in one of the languages supported by the Java Native Interface (currently C and C++), it can be dy- namically loaded into the server at runtime and accessed via a Java front end. • If the component is an executable, the server must issue a system call for running the pro- gram and data communication usually occurs through files. The Document Server itself is accessed via its AΡΙ and is running as a Java Door Orb supporting re- quests from the component's servers. This framework does not provide a model for con- trolling the interaction between the components of an application: the designer of an NLP application can use a simple sequential model or more sophis- ticated blackboard models: since this distributed model supports both the synchronous and the asyn- chronous types of communication between compo- nents, it supports a large variety of control models. HTTP Application TCPAP SMTP CI API Generator HTTP Server CI API Morphological Analyzzr TCP/IP Server CDS API CI API SMTP Server CDS API TCPAP System Call CDS API Parser TCP/P TCPAP Corelli Document Server Figure 4: Some possible integration paths for het- erogeneous components. 4 Implementation 4.1 Document Server Implementation The Document Server consists of three major mod- ules: Document Management Service, Naming Ser- vice, and Life-Cycle Service. The modules are de- fined in IDL, and implemented in Java. The Sun Java IDL system, with its Door Orb implementation, is used to interface client programs to the Document Server implementation. The Document Management Service module pro- vides methods to access and manipulate the com- ponents of objects (e.g., attributes, annotations and content of a document object). The Life-Cycle Service is responsible for creating and copying objects. The Naming Service binds a name to an object. The Naming Service supports a limited form of per- sistency for storing bindings. For example, to create a new document, the client program creates it through the Life-Cycle Service, bind a name to it using the Naming Service, and add attributes and annotations to it through the Docu- ment Management Service. 4.2 Porting of the Temple Machine Translation System To bootstrap the Corelli Machine Translation Sys- tem and test the implementation of the architecture, we are currently porting the CRL's Temple machine- translation system prototype (Vanni & Zajac 96) to the Corelli architecture. This task will be aided by two features: first, the Temple system already uti- lizes the Tipster Document Architecture for data ex- change between components, and second, the Tem- ple system has a pipelined architecture which will allow modular encapsulation of translation stages (e.g., dictionary lookup) as Corelli Plug'n Play tools. The Temple morphological analyzers and the En- glish morphological generator all function as stand- alone executables and will be easily converted to Corelli Plug'n Play tools. Lexical resources (e.g., dictionaries and glossaries), on the other hand, are currently maintained in a database and are accessed via calls to a C library API. Each lexical resource is wrapped as a Plug'n Play tool implementing the query interface: in order to interface with the databases, the Java Native Interface is used to wrap the C database library. Finally, we will have to re- engineer a portion of the top-level application con- trol code (in C) in Java. 5 Conclusion The Corelli Document Architecture is currently used as the integration layer for the Corelli Machine- Translation System. This multilingual machine- translation system is built out of heterogeneous com- ponents, such as an English generator written in Lisp, a Spanish morphological analyzer written in Prolog, a Glossary-Based Machine-Translation en- gine written in C, etc. This architecture will also be used to support integration of various machine trans- lation systems in a multi-engine machine translation project (building on ideas first developed in the Pan- gloss project, see (Frederking et al. 94)). The Corelli project has started collaborating with the University of Sheffield with the aim to merge the Corelli Document Architecture and the GATE architecture.2 More specifically, the current GATE document manager will be replaced with the Corelli document manager and the Plug'n Play layer will be added to support distributed processing. The file-based version of the Corelli Document Processing Architecture will be made freely avail- able for research purposes. It will also be available as part of the GATE system distribution. Acknowledgments. An initial version of this ar- chitecture has been developed by Vani Mahesh. Research reported in this paper is supported by the DoD, contract MDA904-96-C-1040. References Hiyan Alshawi. 1992. The Core Language Engine. MIT Press. 2The GATE system already uses a previous version (written in C) of a Tipster document manager developed at CRL. ARPA - Advanced Research Projects Agency. 1993. Proceedings of the TIPSTER Text Program - Phase 1. Morgan-Kaufmann. Jan W. Amtrup. 1995. \"Chart-based Incremental Transfer in Machine Translation\". Proceedings of the 6th International Conference on Theoretical and Methodological Issues in Machine Translation TIM'95, 5-7 July 1995, Leuven, Belgium. pp188- 195. A. Ballim. 1995. \"Abstract Data Types for Multext Tool I/O\". LRE 62-05 Deliverable 1.2.1. Ted J. Biggerstaff, Alan J. Perlis, eds. 1989. Software Reusability, 2 volumes. ACM Press, Addison- Wesley. Christian Boitet, Pierre Guillaume, Maurice Quézel- Ambrunaz. 1982. \"Implementation of the conver- sational environment of ARIANE 78.4, an inte- grated system for automated translation and hu- man revision\". Proceedings of the 9th Interna- tional Conference on Computational Linguistics - COLING'82. Christian Boitet and Mark Seligman. 1994. \"The Whiteboard Architecture: a Way to Integrate Heterogeneous Components of NLP Systems\". Proceedings of the 15th International Conference on Computational Linguistics - COLING'94, Au- gust 5-9 1994, Kyoto, Japan. pp426-430. H. Cunningham, M. Freeman, W.J. Black. 1994. \"Software Reuse, Object-Oriented Frameworks and Natural Language Processing\". Proceedings of the 1st Conference on New Methods in Natural Language Processing - NEMLAP-1, Manchester. H. Cunningham, Y. Wilks, R. Gaizauskas. 1996. \"New Methods, Current Trends and Software In- frastructure for NLP\". Proceedings of the 2nd Conference on New Methods in Natural Language Processing - NEMLAP-2, Ankara, Turkey. L.D. Erman, V.R. Lesser. 1980. \"The Hearsay- II speech understanding system\". In W.A. Lea (ed.), Trends in Speech Recognition, Prentice- Hall. pp361-381. Robert Frederking, Sergei Nirenburg, David Farwell, Stephen Helmreich, Eduard Hovy, Kevin Knight, Stephen Beale, Constantine Domashnev, Donalee Attardo, Dean Grannes, Ralf Brown. 1994. \"Inte- grating Translations from Multiple Sources within the Pangloss Mark III Machine Translation Sys- tem\". Proceedings of the 1st Conference of the As- sociation for Machine Translation in the Americas - AMTA '94, 5-8 October 1994, Columbia, Mary- land. pp73-80. Günther Görz, Marcus Kesseler, Jörg Spilker, Hans Weber. 1996. \"Research on Architectures for In- tegrated Speech/ Language Systems in Verbmo- bil\". Verbmobil Report 126, Universität Erlangen- Nürnberg, May 1996. Claire Grover, John Caroll and Ted Briscoe. 1992. The Alvey Natural Language Tools. Computer Laboratory, University of Cambridge, UK. Ralph Grishman, editor. 1995. \"Tipster Phase II Ar- chitecture Design Document\". New-York Univer- sity, NY, July 1995. Bill Janssen, Mike Spreitzer. 1996. \"ILU 2.0 Refer- ence Manual\". Xerox PARC. Martin Kay. 1973. \"The MIND system\". In R. Rustin (ed.), Courant Computer Science Sympo- sium 8: Natural Language Processing. Algorith- mics Press, New-York, NY. pp155-188. Martin Kay. 1996. \"Chart Generation\". Proceedings of the 34th Meeting of the Association for Com- putational Linguistics ACL'96. pp200-204. M. Kesseler. 1994. \"Distributed Control in Verbmo- bil\". Verbmobil Report 24, Universität Erlangen- Nürnberg, August 1994. Sergei Nirenburg. 1994. \"The Workstation Substrate of the Pangloss Project\". Proceedings of the Con- ference on Future Generation of Natural Language Processing Systems - FGNLP-2. Sergei Nirenburg and Robert Frederking. 1994. \"To- wards Multi-Engine Machine Translation\". Pro- ceedings of the ARPA Human Language Technol- ogy Workshop, March 8-11 1994, Plainsboro, NJ. pp147-151. Sergei Nirenburg, David Farwell, Robert Frederking, Yorick Wilks. 1994. \"Two types of adaptative MT environments\". Proceedings of the 15th Interna- tional Conference on Computational Linguistics - COLING'94, August 5-9 1994, Kyoto, Japan. pp125-128. OMG. 1995. \"The Common Object Request Bro- ker: Architecture and Specification, Version 2.0\". OMG Technical Document PTC/96-03-0. N.K. Simkins. 1994. \"An Open Architecture for Lan- guage Engineering\". Proceedings of the 1st Lan- guage Engineering Convention, Paris. Ian Sommerville. 1996. Software Engineering (5th Edition). Addison-Wesley. Eiichiro Sumita and Hitoshi Iida. 1995. \"Hetero- geneous Computing for Example-based Transla- tion of Spoken Language\". Proceedings of the 6th International Conference on Theoretical and Methodological Issues in Machine Translation - TIM'95, 5-7 July 1995, Leuven, Belgium. pp273- 286. Henry Thompson and Graeme Ritchie. 1984. \"Im- plementing Natural Language Parsers\". In T. O'Shea and E. Eisenstadt (eds.), Artificial Intel- ligence. Harper & Row, New-York. pp245-300. Henry Thompson. 1995. \"Multext Workpackage 2, Milestone B, Deliverable Overview\". LRE 62-050 Deliverable 2. Andrea Turk and Stefan Geibler. 1995. \"Integra- tion alternativer Komponenten für die Sparchver- arbeitung im Verbmobil Demonstrator\". Verbmo- bil Report 67, IBM Informationssysteme GmBH, April 1995. Michelle Vanni and Rémi Zajac. 1996. \"Glossary- Based MT Engines in a Multilingual Analyst's Workstation for Information Processing\". To ap- pear in Machine Translation, Special Issue on New Tools for Human Translators. Yorick Wilks, Louise Guthrie, Joe Guthrie and Jim Cowie. 1992. \"Combining Weak Methods in Large-Scale Text Processing\". In Paul S. Jacob (ed.), Text-Based Intelligent Systems, Lawrence Erlbaum Associates, pp35-58. Rémi Zajac. 1992. \"Towards Computer-Aided Lin- guistic Engineering\". Proc. of the 14th Interna- tional Conference on Computational Linguistics - COLING'92, 23-28 August 1992, Nantes, France. pp827-834. Rémi Zajac. 1996. “A Multilingual Translator's Workstation for Information Access\", Proceed- ings of the International Conference on Natural Language Processing and Industrial Applications - NLP+IA 96, Moncton, New-Brunswick, Canada, June 4-6, 1996. Rémi Zajac. 1996. \"Towards a Multilingual An- alyst's Workstation: Temple\". In Expanding MT Horizons - Proceedings of the 2nd Confer- ence of the Association for Machine Translation in the Americas, AMTA-96. 2-5 October 1996, Montréal, Canada. pp280-284. Rémi Zajac and Mark Casper. \"The Temple Web Translator\". Proc. of the 1997 AAAI Spring Sym- posium on Natural Language Processing for the World Wide Web, March 24-26, 1997, Stanford University."
  },
  {
    "title": "Arabic Morphology Generation Using a Concatenative Strategy",
    "abstract": "Arabic inflectional morphology requires infixation, prefixation and suffixation, giving rise to a large space of morphological variation. In this paper we describe an approach to reducing the complexity of Arabic morphology generation using discrimination trees and transformational rules. By decoupling the problem of stem changes from that of prefixes and suffixes, we gain a significant reduction in the number of rules required, as much as a factor of three for certain verb types. We focus on hollow verbs but discuss the wider applicability of the approach.",
    "content": "Introduction Morphologically, Arabic is a non-concatenative language. The basic problem with generating Arabic verbal morphology is the large number of variants that must be generated. Verbal stems are based on triliteral or quadriliteral roots (3- or 4-radicals). Stems are formed by a derivational combination of a root morpheme and a vowel melody; the two are arranged according to canonical patterns. Roots are said to interdigitate with patterns to form stems. For example, the Arabic stem katab (he wrote) is composed of the morpheme ktb (notion of writing) and the vowel melody morpheme 'a-a'. The two are coordinated according to the pattern CVCVC (C=consonant, V=vowel). There are 15 triliteral patterns, of which at least 9 are in common use, and 4 much rarer quadriliteral patterns. All these patterns undergo some stem changes with respect to voweling in the 2 tenses (perfect and imperfect), the 2 voices (active and passive), and the 5 moods (indicative, subjunctive, jussive, imperative and energetic).¹ The stem used in the conjugation of the verb may differ depending on the person, number, gender, tense, mood, and the presence of certain root consonants. Stem changes combine with suffixes in the perfect indicative (e.g., katab-naa 'we wrote', kutib-a 'it was written') and the imperative (e.g. uktub-uu 'write', plural), and with both prefixes and suffixes for the imperfect tense in the indicative, subjunctive, and jussive moods (e.g. ya-ktub-na 'they write, feminine plural') and in the energetic mood (e.g. ya-ktub-unna or ya-ktub-un 'he certainly writes'). There are a total of 13 person-number-gender combinations. Distinct prefixes are used in the active and passive voices in the imperfect, although in most cases this results in a change in the written form only if diacritic marks are used.² Most previous computational treatments of Arabic morphology are based on linguistic models that describe Arabic in a non- concatenative way and focus primarily on analysis. Beesley (1991) describes a system that analyzes Arabic words based on Koskenniemi's ¹ The jussive is used in specific constructions, for example, negation in the past with the negative particle lam (e.g., lam aktub 'I didn't write'). The energetic expresses corroboration of an action taking place. The indicative is common to both perfect and imperfect tenses, but the subjunctive and the jussive are restricted to the imperfect tense. The imperative has a special form, and the energetic can be derived from either the imperfect or the imperative. ² Diacritic marks are used in Arabic language textbooks and occasionally in regular texts to resolve ambiguous words (e.g. to mark a passive verb use). --- (1983) two-level morphology. In Beesley (1996) the system is reworked into a finite-state lexical transducer to perform analysis and generation. In two-level systems, the lexical level includes short vowels that are typically not realized on the the surface level. Kiraz (1994) presents an analysis of Arabic morphology based on the CV-, moraic-, and affixational models. He introduces a multi-tape two-level model and a formalism where three tapes are used for the lexical level (root, pattern, and vocalization) and one tape for the surface level. In this paper, we propose a computational approach that applies a concatenative treatment to Arabic morphology generation by separating the issue of infixation from other inflectional variations. We are developing an Arabic morphological generator using MORPHE (Leavitt, 1994), a tool for modeling morphology based on discrimination trees and regular expressions. MORPHE is part of a suite of tools developed at the Language Technologies Institute, Carnegie Mellon University, for knowledge-based machine translation. Large systems for MT from English to Spanish, French, German, Portuguese and a prototype for Italian have already been developed. Within this framework, we are exploring English to Arabic translation and Arabic generation for pedagogical purposes. We generate Arabic words including short vowels and diacritic marks, since they are pedagogically useful and can always be stripped before display. Our approach seeks to reduce the number of rules for generating morphological variants of Arabic verbs by breaking the problem into two parts. We observe that, with the exception of a few verb types, there is very little interaction between stem changes and the processes of prefixation and suffixation. It is therefore possible to decouple, in large part, the problem of stem changes from that of prefixes and suffixes. The gain is a significant reduction in the size number of transformational rules, as much as a factor of three for certain verb classes. This improves the space efficiency of the system and its maintainability by reducing duplication of rules, and simplifies the rules by isolating different types of changes. To illustrate our approach, we focus on a particular type of verbs, termed hollow verbs, and show how we integrate their treatment with that of more regular verbs. We also discuss how the approach can be extended to other classes of verbs and other parts of speech. 1 Arabic Verbal Morphology Verb roots in Arabic can be classified as shown in Figure 1.3 A primary distinction is made between weak and strong verbs. Weak verbs have a weak consonant ('w' or 'y') as one or more of their radicals; strong verbs do not have any weak radicals. Strong verbs undergo systematic changes in stem voweling from the perfect to the imperfect. The first radical vowel disappears in the imperfect. Verbs whose middle radical vowel in the perfect is 'a' can change it to 'a' (e.g., qaTa`a 'he cut' -> yaqTa`u 'he cuts'),' 'i' (e.g., Daraba 'he hit' -> yaDribu 'he hits'), or 'u' (e.g., kataba 'he wrote' -> yaktubu 'he writes') in the imperfect. Verbs whose middle radical vowel in the perfect is 'i' can only change it to 'a' (e.g., shariba 'he drank' -> yashrabu 'he drinks') or 'i' (e.g., Hasiba 'he supposed' -> yaHsibu 'he supposes'). Verbs with middle radical vowel 'u' in the perfect do not change it in the imperfect (e.g., Hasuna 'he was beautiful' -> yaHsunu 'he is beautiful'). For strong verbs, neither perfect nor imperfect stems change with person, gender, or number. Hollow verbs are those with a weak middle radical. In both perfect and imperfect tenses, the underlying stem is realized by two characteristic allomorphs, one short and one long, whose use depends on the person, number and gender. 3 Grammars of Arabic are not uniform in their classification of \"hamzated\" verbs, verbs containing the glottal stop as one of the radicals (e.g. [sa?al] 'to ask'). Wright (1968) includes them as weak verbs, but Cowan (1964) doesn't. Hamzated verbs change the written 'seat' of the hamza from 'alif' to 'waaw' or 'yaa?', depending on the phonetic context. 4 In the Arabic transcription capital letters indicate emphatic consonants; 'H' is the voiceless pharyngeal fricative; ''' the voiced pharyngeal fricative; '?' is the glottal stop 'hamza'. strong regular hamzated doubled radical triliteral tense preterit (perfect) present (imperfect) participle indicative active passive weak initial radical (assimilated) weak weak middle radical (hollow) weak final radical (defective) mood imperative subjunctive jussive energetic Figure 1: Classification of Arabic Verbal Roots and Mood Tense System Hollow verbs fall into four classes: 1. Verbs of the pattern CawaC or CawuC (e.g. [Tawul] 'to be long'), where the middle radical is 'w'. Their characteristic is a long 'uu' between the first and last radical in the imperfect. E.g., From the underlying root [zawar]: zaara 'he visited' and yazuuru 'he visits' Stem allomorphs: Perfect: -zur- and -zaar- Imperfect: -zur- and -zuur- 2. Verbs of the pattern CawiC, where the middle radical is 'w'. Their characteristic is a long 'aa' between the first and last radical in the imperfect. E.g., From the underlying root [nawim]: naama 'he slept and yanaamu 'he sleeps' Stem allomorphs: Perfect: -nim- and -naam- Imperfect: -nam- and -naam- 3. Verbs of the pattern CayaC, where the middle radical is 'y'. Their characteristic is a long 'ii' before the first and last radical in the imperfect. E.g., From the underlying root [baya`]: baa'a 'he sold' and yabii'u 'he sells' Stem allomorphs : Perfect: -bi- and -baa`- Imperfect: and -bi-and-bii`- 4. Verbs of the pattern CayiC, where middle radical is 'y'. E.g., From the underlying root [hayib]: haaba 'he feared' and yahaabu 'he fears' Stem allomorphs : Perfect: -hib- and -haab- Imperfect: -hab- and -haab- In the relevant literature (e.g., Beesley, 1998; Kiraz, 1994), verbs belonging to the above classes are all assumed to have the pattern CVCVC. The pattern does not show the verb conjugation class and makes it difficult to predict the type of stem allomorph to use. To avoid these problems, we keep information on the middle radical and vowel in the base form of the verb. In generation, classes 2 and 4 of the verb can be handled as one because they have the same perfect and imperfect stems.5 5 The only exception is the passive participle. Verbs of classes 1 and 2 behave the same (e.g. Class 1: [zawar]: mazuwr 'visited'; Class 2 [nawil] → manuwl 'obtained'), as do verbs of classes 3 and 4 (e.g. Class 3: [baya'] → mabii 'sold', Class 4: [hayib] → mahiib 'feared'). We describe our approach to modeling strong and hollow verbs below, following a description of the implementation framework. 2 The MORPHE System MORPHE (Leavitt, 1994) is a tool that compiles morphological transformation rules into either a word parsing program or a word generation program.<sup>6</sup> In this paper we will focus on the use of MORPHE in generation. Input and Output. MORPHE's output is simply a string. Input is a feature structure (FS) which describes the item that MORPHE must transform. A FS is implemented as a recursive Lisp list. Each element of the FS is a feature-value pair (FVP), where the value can be atomic or complex. A complex value is itself a FS. For example, the FS for generating the Arabic zurtu 'I visited' would be: ((ROOT \"zawar\") (CAT V) (PAT CVCVC) (VOW HOL) (TENSE PERF) (MOOD IND) (VOICE ACT) (NUMBER SG) (PERSON 1)) The choice of feature names and values, other than ROOT, which identifies the lexical item to be transformed, is entirely up to the user. The FVPs in a FS come from one of two sources. Static features, such as CAT (part of speech) and ROOT, come from the syntactic lexicon, which, in addition to the base form of words, can contain morphological and syntactic features. Dynamic features, such as TENSE and NUMBER, are set by MORPHE's caller. The Morphological Form Hierarchy. MORPHE is based on the notion of a morphological form hierarchy (MFH) or tree. Each internal node of the tree specifies a piece of the FS that is common to that entire subtree. The root of the tree is a special node that simply binds all subtrees together. The leaf nodes of the tree correspond to distinct <sup>6</sup>MORPHE is written in Common Lisp and the compiled MFH and transformation rules are themselves a set of Common Lisp functions. morphological forms in the language. Each node in the tree below the root is built by specifying the parent of the node and the conjunction or disjunction of FVPs that define the node. Portions of the Arabic MFH are shown in Figures 2-4. Transformational Rules. A rule attached to each leaf node of the MFH effects the desired morphological transformations for that node. A rule consists of one or more mutually exclusive clauses. The 'if part of a clause is a regular expression pattern, which is matched against the value of the feature ROOT (a string). The 'then' part includes one or more operators, applied in the given order. Operators include addition, deletion, and replacement of prefixes, infixes, and suffixes. The output of the transformation is the transformed ROOT string. An example of a rule attached to a node in the MFH is given in Section 3.1 below. Process Logic. In generation, the MFH acts as a discrimination network. The specified FS is matched against the features defining each subtree until a leaf is reached. At that point, MORPHE first checks in the irregular forms lexicon for an entry indexed by the name of the leaf node (i.e., the MF) and the value of the ROOT feature in the FS. If an irregular form is not found, the transformation rule attached to the leaf node is tried. If no rule is found or none of the clauses of the applicable rule match, MORPHE returns the value of ROOT unchanged. 3 Handling Arabic Verbal Morphology in MORPHE Figure 2 sketches the basic MFH and the division of the verb subtree into stem changes and prefix/suffix additions.<sup>7</sup> The inflected verb is generated in two steps. MORPHE is first called with the feature CHG set to STEM. The required stem is returned and temporarily substituted for the value of the ROOT feature. <sup>7</sup>The use of two parts of the same tree for the two problems is a constraint of MORPHE's implementation, which does not permit multiple trees with separate roots. The second call to MORPHE, with feature CHG set to PSFIX, adds the necessary prefix and/or suffix and returns the fully inflected verb. *root* (CAT V) (CAT N) (CAT ADJ) characteristics were described in Section 1. Figure 3 shows the MFH for strong and hollow verbs of pattern CVCVC in the perfect tense, active voice. We use the feature VOW to carry information about the voweling of the verb in the imperfect (discussed below) and overload it to distinguish hollow and other kinds of verbs. (CHG STEM) (CHG PSFIX) (PAT CVCVC) (PAT CVCCVC) other forms (VOICE ACT) (VOICE PAS) (TENSE PERF) (TENSE IMPERF) Figure 2: The Basic Verb Hierarchy Figure 2 also shows some of the features used to traverse the discrimination tree. The feature PAT is used in conjunction with the ROOT feature to select the appropriate affixes. Knowing the underlying root and its voweling is crucial for the determination of hollow verb stems, as described in Section 1. Knowing the pattern is also important in cases where it is unclear. For example, verbs of pattern CtVCVC insert a 't' after the first radical (e.g. ntaqal 'to move, change location', intransitive). With some consonants as first radicals, in order to facilitate pronunciation, the 't' undergoes a process of assimilation whose effects differ depending on the preceding consonant. For example, the pattern CtVCVC verb from zaHam 'to shove' instead of *ztaHam is zdaHam 'to team'. It is also difficult to determine from just the string ntaqal whether this is pattern nCVCVC of the verb *taqal (if it existed) or pattern CtVCVC of naqal 'to transport, move', transitive). 3.1 Handling Strong and Hollow Verb Morphology in MORPHE As a demonstration of our approach, we discuss the case of hollow verbs, whose (TENSE PERF) (VOW HOL) (VOW (*or* aiu)) (PERS (*or* 1 2)) (PERS 3) short stem (NUM (*or* sg dl)) long stem (NUM PL) (GENDER M) long stem (GENDER F) short stem Figure 3: The Perfect Stem Change Subtree for Strong and Hollow Verbs of Pattern CVCVC In the perfect active voice, regular strong verbs do not undergo any stem changes, but doubled radical verbs do. Rules effecting these changes are attached to the node labeled with the FVP (VOW (*or* a i u)). The hollow verbs, on the other hand, use a long stem with a middle 'alif' (e.g. [daam] 'to last') for third person singular and dual (masculine and feminine) and for third person plural masculine. The remaining person-number-gender combinations take a short stem whose voweling depends on the underlying root of the verb, as specified earlier. Transformation rules attached to the leaf nodes perform the conversion of the ROOT feature value to the short and long stem. Inside the stem change rules, the four different classes of hollow verbs are treated as three separate conditions (classes 2 and 4 can be merged, as described in Section 1) by matching on the middle radical and the adjacent vowels and replacing them with the appropriate vowel. 8 Hamzated verbs changes are due to interactions with specific suffixes and are best dealt with in the prefixation and suffixation subtree. An example of such a rule, which changes the perfect stem to a short one for persons 1 and 2 both singular and plural, follows. (morph-rule v-stem-f1-act-perf-12 (\"^%{cons) (awa)%{cons}$\" (ri *1* \"u\")) (\"^%{cons} (a[wy]i)%{cons}$\" (ri *1* \"i\")) (\"^%{cons) (aya)%{cons}$\" (ri *1* \"i\"))) The syntax %{var) is used to indicate variables with a given set of values. Enclosing a string in parenthesis associates it with a numbered register, so the replace infix (ri) operator can access it for substitution. Figure 4 shows the imperfect subtree for strong and hollow verbs. Strong verbs are treated efficiently by three rules branching on the middle radical vowel, given as the value of VOW. The consonant-vowel pattern of the computed stem is shown (e.g. for kataba 'he wrote', the imperfect stem would be -ktub- in the pattern CCuC). As described in Section 1, the possible vowel in the imperfect is restricted but not always determined by the perfect vowel and so must be stored in the syntactic lexicon. Separating stem changes from the addition of prefixes and suffixes significantly reduces the number of transformation rules that must be written by eliminating much repetition of prefix and suffix addition for different stem changes. For strong verbs of pattern CVCVC, there is at least a three-fold reduction in the number of rules for active voice (recall the different kinds of vowel changes for these verbs from perfect to imperfect described in Section 1). Other patterns and the passive of pattern CVCVC verbs show less variation in stem voweling but more variation in prefix and suffix voweling. Since some of the patterns share the same prefix and suffix voweling, once the stem has been determined, the prefixation and suffixation rules can be shared by pattern groups. 9 In the presence of certain second and third radicals, the middle radical vowel is more precisely determined. This information can be incorporated into the syntactic lexicon as it is being built. The hollow verb subtree is not as small for the imperfect as it is for the perfect, since the stem depends not only on the mood but also on the person, gender, and number. It is still advantageous to decouple stem changes from prefixation and suffixation. Suffixes differ in the indicative and subjunctive moods; if the two types of changes were merged, the stem transformations would have to be repeated in each of the two moods and for each person- number-gender combination. The same observation applies to stem changes in the passive voice as well. Significant replication of transformational rules that include stem changes makes the system bigger and harder to maintain in case of changes, particularly because each transformational rule needs to take into consideration the four different classes of hollow verbs. 3.2 An Example of Generation Consider again the example verb form zurtu 'I visited' and the feature structure (FS) given in Section 2. During generation, the feature- value pair (CHG STEM) is added to the FS before the first call to MORPHE. Traversing the MFH shown in Figure 2, MORPHE finds the rule v-stem-f1-act-perf-12 given in Section 3.1 above. The first clause fires, replacing the 'awa' with 'u' and MORPHE returns the stem -zur-. This stem is substituted as the value of the ROOT feature in the FS and the feature-value pair (CHG STEM) is changed to (CHG PSFIX) before the second call to MORPHE. This time MORPHE traverses a different subtree and reaches the rule: (morph-rule v-psfix-perf-1-sg (\"\" (+s \"otu\"))) This rule, currently simply appends \"otu\" to the string, and MORPHE returns the string \"zurotu\", where the 'o' denotes the diacritic \"sukuun\" or absence of vowel. This is the desired form for zurtu 'I visited'. (VOW HOL) (MOOD (*or* IND SUB)) (TENSE IMPERF) (VOW a) (VOW i) CCaC CCIC (VOW u) CCuC (MOOD JUS) 4 (NUM (*or* sg dl)) long stem (PERS 1) long stem (PERS (*or* 23)) (GENDER M) long stem (NUM PL) (NUM SG) (NUM DL) long stem (NUM PL) (PERS (*or* 13)) short stem (PERS 2) (PERS 1) short stem (PERS (*or* 2 3)) (GENDER F) short stem (GENDER M) short stem (GENDER F) long stem (PERS (*or* 23)) (GENDER M) long stem (PERS (*or* 2 3)) (GENDER F) short stem Figure 4: The Imperfect Stem Change Subtree for Strong and Hollow Verbs of Pattern CvCvC Extensions In this paper so far we have focused on regular and hollow verbs of the pattern CVCVC. Here we examine how our approach applies to other verb types and other parts of speech. 4.1 Extending the Approach to Other Verb Types The two-step treatment of verbal inflection described in this paper is easily extended to the passive, to doubled radical and hamzated verbs, and to different patterns of strong and hollow verbs. In fact, since not all higher patterns are affected by the presence of a middle or weak radical (e.g. patterns CVCCV, CaaCVC, taCVCCVC and others), the subtrees for these patterns will be significantly less bushy than for pattern CVCVC. The two-step treatment also covers verbs with a weak first radical, especially the radical 'w', which is normally dropped in the active imperfect (e.g. perfect stem warad 'to come', imperfect stem rid-).<sup>10</sup> Alternatively, it can be placed in the <sup>10</sup> Exceptions to this rule exist (e.g. the verb wajil 'to be afraid'), with imperfect stem - wjal-) but are rare and can be handled in MORPHE by placing the irregular stem in the syntactic lexicon and checking for it prior to calling MORPHE for stem changes irregular lexicon, which MORPHE consults when it reaches a leaf node, prior to applying any of the transformational rules. Verbs with a weak third radical, including doubly or trebly weak verbs, are the most problematic since the stem changes interact heavily with the inflectional suffixes, and less is gained by trying to modify the stem separately. We are currently investigating this issue and the best way to treat it in MORPHE. 4.2 Extending the Approach to Other Parts of Speech The two-step approach to generating verbal morphology also presents advantages for the inflectional morphology of nouns and adjectives. In Arabic, the plural of many nouns, especially masculine nouns, is not formed regularly by suffixation. Instead, the stem itself undergoes changes according to a complex set of patterns (e.g. rajul 'man' pluralizes as rijaal 'men'), giving rise to so- called \"broken plurals\". The inflection of broken plurals according to case (nominative, genitive, accusative) and definiteness, however, is basically the same as the inflection The radical 'v' is largely not dropped or changed. of most masculine or feminine singular nouns. The same holds true for adjectives. Finally we note that our two-step approach can also be used to combine derivational and inflectional morphology for nouns and adjectives. Deverbal nouns and present and past participles can be derived regularly from each verb pattern (with the exception of deverbal nouns from pattern CVCVC). Relational or \"nisba\" adjectives are derived, with small variations, from nouns. Since these parts of speech are inflected as normal nouns and adjectives, we can perform derivational and inflectional morphology in two calls to MORPHE, much as we do stem change and prefix/suffix addition. Conclusion We have presented a computational model that handles Arabic morphology generation concatenatively by separating the infixation changes undergone by an Arabic stem from the processes of prefixation and suffixation. Our approach was motivated by practical concerns. We sought to make efficient use of a morphological generation tool that is part of our standard environment for developing machine translation systems. The two-step approach significantly reduces the number of morphological transformation rules that must be written, allowing the Arabic generator to be smaller, simpler, and easier to maintain. The current implementation has been tested on a subset of verbal morphology including hollow verbs and various types of strong verbs. We are currently working on the other kinds of weak verbs: defective and assimilated verbs. Other categories of words can be handled in a similar manner, and we will turn our attention to them next. References K. Beesley. 1990. Finite-State Description of Arabic Morphology. In Proceedings of the Second Cambridge Conference: Bilingual Computing in Arabic and English. K. Beesley. 1991. Computer Analysis of Arabic: A Two-Level Approach with Detours. In B. Comrie and M. Eid, editors, Perspectives on Arabic Linguistics III: Papers from the Third Annual Symposium on Arabic Linguistics. Benjamins, Amsterdam, pages 155-172. K. Beesley. 1996. Arabic Finite-State Morphological Analysis and Generation. In Proceedings COLING'96, Vol. 1, pages 89-94. K. Beesley. 1998. Consonant Spreading in Arabic Stems. In Proceedings of COLING'98. D. Cowan. 1964. An introduction to modern literary Arabic. Cambridge University Press, Cambridge. G. Hudson. 1986. Arabic Root and Pattern Morphology without Tiers. Journal of Linguistics, 22:85-122. G. Kiraz. 1994. Multi-tape Two-level Morphology: A Case study in Semitic Non-Linear Morphology. In Proceedings of COLING-94, Vol. 1, pages 180-186. K. Koskenniemi. 1983. Two-level morphology: A General Computational Model for Word-Form Recognition and Production. PhD thesis, University of Helsinki. A. Lavie, A. Itai, U. Ornan, and M. Rimon. 1988. On the Applicability of Two Level Morphology to the Inflection of Hebrew Verbs. In Proceedings of the Association of Literary and Linguistic Computing Conference. J.R. Leavitt. 1994. MORPHE: A Morphological Rule Compiler. Technical Report, CMU-CMT- 94-MEMO. J. McCarthy and A. Prince. 1990. Foot and Word in Prosodic Morphology: The Arabic Broken Plural. Natural Language and Linguistics Theory, 8: 209-283. J. McCarthy and A. Prince. 1993. Template in Prosodic Morphology. In Stvan, L. et al., editors, Papers from the Third Annual Formal Linguistics Society of Midamerica Conference,. Bloomington, Indiana. Indiana University Linguistics Club, pages 187-218. G. Ritchie. 1992. Languages Generated by Two- Level Morphological Rules. Computational Linguistics, 18(1), pages 41-59. R. Sproat. 1992. Morphology and Computation. MIT Press, Cambridge, Mass. H. Wehr. 1971. A Dictionary of Modern Written Arabic, J.M. Cowan, editor. Spoken Language Services, Ithaca, NY, fourth edition. W. Wright. 1988. A Grammar of the Arabic Language. Cambridge University Press, Cambridge, third edition."
  },
  {
    "title": "A Simple Approach to Building Ensembles of Naive Bayesian Classifiers for Word Sense Disambiguation",
    "abstract": "This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disambiguating the widely studied nouns line and interest show that such an ensemble achieves accuracy rivaling the best previously published results.",
    "content": "1 Introduction Word sense disambiguation is often cast as a prob- lem in supervised learning, where a disambiguator is induced from a corpus of manually sense-tagged text using methods from statistics or machine learning. These approaches typically represent the context in which each sense-tagged instance of a word occurs with a set of linguistically motivated features. A learning algorithm induces a representative model from these features which is employed as a classifier to perform disambiguation. This paper presents a corpus-based approach that results in high accuracy by combining a number of very simple classifiers into an ensemble that per- forms disambiguation via a majority vote. This is motivated by the observation that enhancing the fea- ture set or learning algorithm used in a corpus-based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning al- gorithm. For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sense- tagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Peder- sen and Bruce, 1997)). These studies represent the context in which an ambiguous word occurs with a tribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexi- cal features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often signifi- cantly greater than that of any of the individual clas- sifiers that make up the ensemble (e.g., (Dietterich, 1997)). In natural language processing, ensemble techniques have been successfully applied to part- of-speech tagging (e.g., (Brill and Wu, 1998)) and parsing (e.g., (Henderson and Brill, 1999)). When combined with a history of disambiguation success using shallow lexical features and Naive Bayesian classifiers, these findings suggest that word sense dis- ambiguation might best be improved by combining the output of a number of such classifiers into an ensemble. This paper begins with an introduction to the Naive Bayesian classifier. The features used to rep- resent the context in which ambiguous words occur are presented, followed by the method for selecting the classifiers to include in the ensemble. Then, the line and interest data is described. Experimental re- sults disambiguating these words with an ensemble of Naive Bayesian classifiers are shown to rival pre- viously published results. This paper closes with a discussion of the choices made in formulating this methodology and plans for future work. 2 Naive Bayesian Classifiers A Naive Bayesian classifier assumes that all the fea- ture variables representing a problem are condition- ally independent given the value of a classification variable. In word sense disambiguation, the context in which an ambiguous word occurs is represented by the feature variables (F1, F2, ..., Fn) and the sense of the ambiguous word is represented by the classi- fication variable (S). In this paper, all feature vari- ables Fi are binary and represent whether or not a particular word occurs within some number of words dow of context. For a Naive Bayesian classifier, the joint probability of observing a certain combination of contextual features with a particular sense is ex- pressed as: n p(F1, F2,..., Fn, S) = p(S) Πp(Fi|S) i=1 The parameters of this model are p(S) and p(Fi|S). The sufficient statistics, i.e., the summaries of the data needed for parameter estimation, are the frequency counts of the events described by the in- terdependent variables (Fi, S). In this paper, these counts are the number of sentences in the sense- tagged text where the word represented by Fi oc- curs within some specified window of context of the ambiguous word when it is used in sense S. Any parameter that has a value of zero indicates that the associated word never occurs with the spec- ified sense value. These zero values are smoothed by assigning them a very small default probability. Once all the parameters have been estimated, the model has been trained and can be used as a clas- sifier to perform disambiguation by determining the most probable sense for an ambiguous word, given the context in which it occurs. 2.1 Representation of Context The contextual features used in this paper are bi- nary and indicate if a given word occurs within some number of words to the left or right of the ambigu- ous word. No additional positional information is contained in these features; they simply indicate if the word occurs within some number of surrounding words. Punctuation and capitalization are removed from the windows of context. All other lexical items are included in their original form; no stemming is per- formed and non-content words remain. This representation of context is a variation on the bag-of-words feature set, where a single window of context includes words that occur to both the left and right of the ambiguous word. An early use of this representation is described in (Gale et al., 1992), where word sense disambiguation is performed with a Naive Bayesian classifier. The work in this pa- per differs in that there are two windows of context, one representing words that occur to the left of the ambiguous word and another for those to the right. 2.2 Ensembles of Naive Bayesian Classifiers The left and right windows of context have nine dif- ferent sizes; 0, 1, 2, 3, 4, 5, 10, 25, and 50 words. The first step in the ensemble approach is to train a separate Naive Bayesian classifier for each of the 81 possible combination of left and right window sizes. Naive_Bayes (1,r) represents a classifier where the quency counts of shallow lexical features from two windows of context; one including I words to the left of the ambiguous word and the other including r words to the right. Note that Naive_Bayes (0,0) in- cludes no words to the left or right; this classifier acts as a majority classifier that assigns every instance of an ambiguous word to the most frequent sense in the training data. Once the individual classifiers are trained they are evaluated using previously held-out test data. The crucial step in building an ensemble is select- ing the classifiers to include as members. The ap- proach here is to group the 81 Naive Bayesian clas- sifiers into general categories representing the sizes of the windows of context. There are three such ranges; narrow corresponds to windows 0, 1 and 2 words wide, medium to windows 3, 4, and 5 words wide, and wide to windows 10, 25, and 50 words wide. There are nine possible range categories since there are separate left and right windows. For exam- ple, Naive_Bayes (1,3) belongs to the range category (narrow, medium) since it is based on a one word window to the left and a three word window to the right. The most accurate classifier in each of the nine range categories is selected for inclusion in the ensemble. Each of the nine member classifiers votes for the most probable sense given the particular con- text represented by that classifier; the ensemble dis- ambiguates by assigning the sense that receives a majority of the votes. 3 Experimental Data The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Print- ing House for the Blind corpus with one of six pos- sible WordNet senses. These senses and their fre- quency distribution are shown in Table 1. This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al., 1998). In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uni- formly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training cor- pus of 2094 sense-tagged sentences. The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contempo- rary English. This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Peder- sen and Bruce, 1997). The previous studies and this paper use the entire 2,368 sense-tagged sentence cor- sense count product 2218 written or spoken text 405 telephone connection 429 formation of people or things; queue 349 an artificial division; boundary 376 a thin, flexible object; cord 371 total 4148 Table 1: Distribution of senses for line — the exper- iments in this paper and previous work use a uni- formly distributed subset of this corpus, where each sense occurs 349 times. sense count money paid for the use of money 1252 a share in a company or business 500 readiness to give attention 361 advantage, advancement or favor 178 activity that one gives attention to 66 causing attention to be given to 11 total 2368 Table 2: Distribution of senses for interest — the ex- periments in this paper and previous work use the entire corpus, where each sense occurs the number of times shown above. quency distribution are shown in Table 2. Unlike line, the sense distribution is skewed; the majority sense occurs in 53% of the sentences, while the small- est minority sense occurs in less than 1%. 4 Experimental Results Eighty-one Naive Bayesian classifiers were trained and tested with the line and interest data. Five- fold cross validation was employed; all of the sense- tagged examples for a word were randomly shuffled and divided into five equal folds. Four folds were used to train the Naive Bayesian classifier while the remaining fold was randomly divided into two equal- sized test sets. The first, devtest, was used to eval- uate the individual classifiers for inclusion in the en- semble. The second, test, was used to evaluate the accuracy of the ensemble. Thus the training data for each word consists of 80% of the available sense- tagged text, while each of the test sets contains 10%. This process is repeated five times so that each fold serves as the source of the test data once. The average accuracy of the individual Naive Bayesian classifiers across the five folds is reported in Tables 3 and 4. The standard deviations were between .01 and .025 and are not shown given their relative con- Each classifier is based upon a distinct representa- tion of context since each employs a different com- bination of right and left window sizes. The size and range of the left window of context is indicated along the horizontal margin in Tables 3 and 4 while the right window size and range is shown along the vertical margin. Thus, the boxes that subdivide each table correspond to a particular range category. The classifier that achieves the highest accuracy in each range category is included as a member of the ensem- ble. In case of a tie, the classifier with the smallest total window of context is included in the ensemble. The most accurate single classifier for line is Naive Bayes (4,25), which attains accuracy of 84% The accuracy of the ensemble created from the most accurate classifier in each of the range categories is 88%. The single most accurate classifier for interest is Naive Bayes (4,1), which attains accuracy of 86% while the ensemble approach reaches 89%. The in- crease in accuracy achieved by both ensembles over the best individual classifier is statistically signifi- cant, as judged by McNemar's test with p = .01. 4.1 Comparison to Previous Results These experiments use the same sense-tagged cor- pora for interest and line as previous studies. Sum- maries of previous results in Tables 5 and 6 show that the accuracy of the Naive Bayesian ensemble is comparable to that of any other approach. How- ever, due to variations in experimental methodolo- gies, it can not be concluded that the differences among the most accurate methods are statistically significant. For example, in this work five-fold cross validation is employed to assess accuracy while (Ng and Lee, 1996) train and test using 100 randomly sampled sets of data. Similar differences in train- ing and testing methodology exist among the other studies. Still, the results in this paper are encourag- ing due to the simplicity of the approach. 4.1.1 Interest The interest data was first studied by (Bruce and Wiebe, 1994). They employ a representation of context that includes the part-of-speech of the two words surrounding interest, a morphological feature indicating whether or not interest is singular or plu- ral, and the three most statistically significant co- occurring words in the sentence with interest, as de- termined by a test of independence. These features are abbreviated as p-o-s, morph, and co-occur in Table 5. A decomposable probabilistic model is in- duced from the sense-tagged corpora using a back- ward sequential search where candidate models are evaluated with the log-likelihood ratio test. The se- lected model was used as a probabilistic classifier on a held-out set of test data and achieved accuracy of 78%. wide medium narrow 50.63.73 .80 25.63.74 .80 10.62.75 .81 5 .61.75.80 4.60.73.80 3.58.73.79 2.53.71 79 1 .42.68.78 0.14 .58 .73 0 1 2 .82.83.83.83.83.83 .82.84 83 .83.83.83 .81.82.82.82.82.83 .82 .82.82.82.82.82 .82.83.83 .82.81.82 .81.82.82.81.81.81 .79.80 .79 .80.81.81 .77 .79.79 .79 .79 .80 3 4 5 10 25 50 wide medium Table 3: Accuracy of Naive Bayesian classifiers for line evaluated with the devtest data. The italicized accuracies are associated with the classifiers included in the ensemble, which attained accuracy of 88% when evaluated with the test data. wide 50 74.80 25.73.80 10 .75.82 medium narrow 5.73.83 .85 .86.85.85 4.72.83 .85 .85.84.84 3.70.84.86.86.86 .85 2.66.83.85.86 .86 .84 1 .63.82.85.85.86.85 0.53.72.77 .78.79.77 .82.83.83.83.82.80.81 .82 .83.83.83 .84.84.84.84 .83.81.81 .83.81.80 .83.81 .80 .83.80.80 .82.81.80 .77 .76.75 0 1 2 3 4 5 10 25 50 narrow medium wide Table 4: Accuracy of Naive Bayesian classifiers for interest evaluated with the devtest data. The italicized accuracies are associated with the classifiers included in the ensemble, which attained accuracy of 89% when evaluated with the test data. and Lee, 1996), who represent the context of an ambiguous word with the part-of-speech of three words to the left and right of interest, a morpho- logical feature indicating if interest is singular or plural, an unordered set of frequently occurring keywords that surround interest, local collocations that include interest, and verb-object syntactic re- lationships. These features are abbreviated p-o-s, morph, co-occur, collocates, and verb-obj in Table 5. A nearest-neighbor classifier was employed and achieved an average accuracy of 87% over repeated trials using randomly drawn training and test sets. (Pedersen et al., 1997) and (Pedersen and Bruce, 1997) present studies that utilize the original Bruce and Wiebe feature set and include the interest data. The first compares a range of probabilistic model selection methodologies and finds that none outper- form the Naive Bayesian classifier, which attains ac- curacy of 74%. The second compares a range of ma- chine learning algorithms and finds that a decision tree learner (78%) and a Naive Bayesian classifier 4.1.2 Line The line data was first studied by (Leacock et al., 1993). They evaluate the disambiguation accuracy of a Naive Bayesian classifier, a content vector, and a neural network. The context of an ambiguous word is represented by a bag-of-words where the window of context is two sentences wide. This fea- ture set is abbreviated as 2 sentence b-o-w in Table 6. When the Naive Bayesian classifier is evaluated words are not stemmed and capitalization remains. However, with the content vector and the neural net- work words are stemmed and words from a stop-list are removed. They report no significant differences in accuracy among the three approaches; the Naive Bayesian classifier achieved 71% accuracy, the con- tent vector 72%, and the neural network 76%. The line data was studied again by (Mooney, 1996), where seven different machine learning methodologies are compared. All learning algo- rithms represent the context of an ambiguous word using the bag-of-words with a two sentence window accuracy method feature set Naive Bayesian Ensemble 89% ensemble of 9 | varying left & right b-o-w Ng & Lee, 1996 87% nearest neighbor p-o-s, morph, co-occur collocates, verb-obj Bruce & Wiebe, 1994 78% model selection p-o-s, morph, co-occur Pedersen & Bruce, 1997 78% decision tree p-o-s, morph, co-occur 74% naive bayes Table 5: Comparison to previous results for interest accuracy method feature set Naive Bayesian Ensemble 88% ensemble of 9 varying left & right b-o-w Towell & Voorhess, 1998 87% neural net local & topical b-o-w, p-o-s Leacock, Chodorow, & Miller, 1998 84% naive bayes local & topical b-o-w, p-o-s Leacock, Towell, & Voorhees, 1993 76% neural net 2 sentence b-o-w 72% content vector 71% naive bayes Mooney, 1996 72% naive bayes 2 sentence b-o-w 71% perceptron Table 6: Comparison to previous results for line list are removed, capitalization is ignored, and words are stemmed. The two most accurate methods in this study proved to be a Naive Bayesian classifier (72%) and a perceptron (71%). The line data was recently revisited by both (Tow- ell and Voorhees, 1998) and (Leacock et al., 1998). The former take an ensemble approach where the output from two neural networks is combined; one network is based on a representation of local con- text while the other represents topical context. The latter utilize a Naive Bayesian classifier. In both cases context is represented by a set of topical and local features. The topical features correspond to the open-class words that occur in a two sentence window of context. The local features occur within a window of context three words to the left and right of the ambiguous word and include co-occurrence features as well as the part-of-speech of words in this window. These features are represented as lo- cal & topical b-o-w and p-o-s in Table 6. (Towell and Voorhees, 1998) report accuracy of 87% while (Leacock et al., 1998) report accuracy of 84%. 5 Discussion The word sense disambiguation ensembles in this pa- per have the following characteristics: • The members of the ensemble are Naive Bayesian classifiers, • the context in which an ambiguous word oc- extracted from varying sized windows of sur- rounding words, • member classifiers are selected for the ensembles based on their performance relative to others with comparable window sizes, and • a majority vote of the member classifiers deter- mines the outcome of the ensemble. Each point is discussed below. 5.1 Naive Bayesian classifiers The Naive Bayesian classifier has emerged as a con- sistently strong performer in a wide range of com- parative studies of machine learning methodologies. A recent survey of such results, as well as pos- sible explanations for its success, is presented in (Domingos and Pazzani, 1997). A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accu- racy than the Naive Bayesian classifier (e.g., (Lea- cock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). In many ensemble approaches the member classi- fiers are learned with different algorithms that are trained with the same data. For example, an en- semble could consist of a decision tree, a neural net- work, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data. This paper takes a different approach, where but the training data is different. This is motivated by the belief that there is more to be gained by vary- ing the representation of context than there is from using many different learning algorithms on the same data. This is especially true in this domain since the Naive Bayesian classifier has a history of success and since there is no generally agreed upon set of features that have been shown to be optimal for word sense disambiguation. 5.2 Co-occurrence features Shallow lexical features such as co-occurrences and collocations are recognized as potent sources of dis- ambiguation information. While many other con- textual features are often employed, it isn't clear that they offer substantial advantages. For exam- ple, (Ng and Lee, 1996) report that local collocations alone achieve 80% accuracy disambiguating interest, while their full set of features result in 87%. Prelim- inary experiments for this paper used feature sets that included collocates, co-occurrences, part-of- speech and grammatical information for surrounding words. However, it was clear that no combination of features resulted in disambiguation accuracy signifi- cantly higher than that achieved with co-occurrence features. 5.3 Selecting ensemble members The most accurate classifier from each of nine pos- sible category ranges is selected as a member of the ensemble. This is based on preliminary experi- ments that showed that member classifiers with sim- ilar sized windows of context often result in little or no overall improvement in disambiguation accuracy. This was expected since slight differences in window sizes lead to roughly equivalent representations of context and classifiers that have little opportunity for collective improvement. For example, an ensem- ble was created for interest using the nine classifiers in the range category (medium, medium). The ac- curacy of this ensemble was 84%, slightly less than the most accurate individual classifiers in that range which achieved accuracy of 86%. Early experiments also revealed that an ensemble based on a majority vote of all 81 classifiers per- formed rather poorly. The accuracy for interest was approximately 81% and line was disambiguated with slightly less than 80% accuracy. The lesson taken from these results was that an ensemble should con- sist of classifiers that represent as differently sized windows of context as possible; this reduces the im- pact of redundant errors made by classifiers that represent very similarly sized windows of context. The ultimate success of an ensemble depends on the ability to select classifiers that make complementary errors. This is discussed in the context of combin- ing part-of-speech taggers in (Brill and Wu, 1998). mentarity of errors between two taggers that could be adapted for use with larger ensembles such as the one discussed here, which has nine members. 5.4 Disambiguation by majority vote In this paper ensemble disambiguation is based on a simple majority vote of the nine member classifiers. An alternative strategy is to weight each vote by the estimated joint probability found by the Naive Bayesian classifier. However, a preliminary study found that the accuracy of a Naive Bayesian ensem- ble using a weighted vote was poor. For interest, it resulted in accuracy of 83% while for line it was 82%. The simple majority vote resulted in accuracy of 89% for interest and 88% for line. 6 Future Work A number of issues have arisen in the course of this work that merit further investigation. The simplicity of the contextual representation can lead to large numbers of parameters in the Naive Bayesian model when using wide windows of con- text. Some combination of stop-lists and stemming could reduce the numbers of parameters and thus improve the overall quality of the parameter esti- mates made from the training data. In addition to simple co-occurrence features, the use of collocation features seems promising. These are distinct from co-occurrences in that they are words that occur in close proximity to the ambiguous word and do so to a degree that is judged statisti- cally significant. One limitation of the majority vote in this paper is that there is no mechanism for dealing with out- comes where no sense gets a majority of the votes. This did not arise in this study but will certainly occur as Naive Bayesian ensembles are applied to larger sets of data. Finally, further experimentation with the size of the windows of context seems warranted. The cur- rent formulation is based on a combination of intu- ition and empirical study. An algorithm to deter- mine optimal windows sizes is currently under de- velopment. 7 Conclusions This paper shows that word sense disambiguation accuracy can be improved by combining a number of simple classifiers into an ensemble. A methodol- ogy for formulating an ensemble of Naive Bayesian classifiers is presented, where each member classifier is based on co-occurrence features extracted from different sized window of context. This approach was evaluated using the widely studied nouns line and interest, which are disambiguated with accuracy of 88% and 89%, which rivals the best previously 8 Acknowledgments This work extends ideas that began in collabora- tion with Rebecca Bruce and Janyce Wiebe. Clau- dia Leacock and Raymond Mooney provided valu- able assistance with the line data. I am indebted to an anonymous reviewer who pointed out the impor- tance of separate test and devtest data sets. A preliminary version of this paper appears in (Pedersen, 2000). References E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, Montreal. R. Bruce and J. Wiebe. 1994. Word-sense disam- biguation using decomposable models. In Proceed- ings of the 32nd Annual Meeting of the Associ- ation for Computational Linguistics, pages 139- 146. T. Dietterich. 1997. Machine-learning research: Four current directions. AI magazine, 18(4):97- 136. P. Domingos and M. Pazzani. 1997. On the optimal- ity of the simple Bayesian classifier under zero-one loss. Machine Learning, 29:103-130. R. Duda and P. Hart. 1973. Pattern Classification and Scene Analysis. Wiley, New York, NY. W. Gale, K. Church, and D. Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415- 439. J. Henderson and E. Brill. 1999. Exploiting diver- sity in natural language processing: Combining parsers. In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Pro- cessing, College Park, MD, June. C. Leacock, G. Towell, and E. Voorhees. 1993. Corpus-based statistical sense resolution. In Pro- ceedings of the ARPA Workshop on Human Lan- guage Technology, pages 260-265, March. C. Leacock, M. Chodorow, and G. Miller. 1998. Us- ing corpus statistics and WordNet relations for sense identification. Computational Linguistics, 24(1):147-165, March. R. Mooney. 1996. Comparative experiments on dis- ambiguating word senses: An illustration of the role of bias in machine learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 82-91, May. H.T. Ng and H.B. Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Society for Com- putational Linguistics, pages 40-47. T. Pedersen, R. Bruce, and J. Wiebe. 1997. Sequen- tial model selection for word sense disambigua- tion. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 388- 395, Washington, DC, April. T. Pedersen. 2000. An ensemble approach to corpus-based word sense disambiguation. In Pro- ceedings of the Conference on Intelligent Text Processing and Computational Linguistics, pages 205-218, Mexico City, February. G. Towell and E. Voorhees. 1998. Disambiguating highly ambiguous words. Computational Linguis- tics, 24(1):125-146, March."
  },
  {
    "title": "Bagging and Boosting a Treebank Parser",
    "abstract": "Bagging and boosting, two effective machine learning techniques, are applied to natural language parsing. Experiments using these techniques with a trainable statistical parser are described. The best resulting system provides roughly as large a gain in F-measure as doubling the corpus size. Error analysis of the result of the boosting technique reveals some inconsistent annotations in the Penn Treebank, suggesting a semi-automatic method for finding inconsistent treebank annotations.",
    "content": "1 Introduction Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy. Finding an ensemble of parsers designed to complement each other is clearly desirable. The parsers would need to be the result of a unified research effort, though, in which the errors made by one parser are targeted with priority by the developer of another parser. A set of five parsers which each achieve only 40% exact sentence accuracy would be extremely valu- able if they made errors in such a way that at least two of the five were correct on any given sentence (and the others abstained or were wrong in different ways). 100% sentence accuracy could be achieved by selecting the hypothesis that was proposed by the two parsers that agreed completely. In this paper, the task of automatically creating complementary parsers is separated from the task of creating a single parser. This facilitates study of the ensemble creation techniques in isolation. The result is a method for increasing parsing performance by creating an ensemble of parsers, each produced from data using the same parser induction algorithm. 2 Bagging and Parsing 2.1 Background The work of Efron and Tibshirani (1993) enabled Breiman's refinement and application of their tech- niques for machine learning (Breiman, 1996). His technique is called bagging, short for \"bootstrap ag- gregating\". In brief, bootstrap techniques and bag- ging in particular reduce the systematic biases many estimation techniques introduce by aggregating es- timates made from randomly drawn representative resamplings of those datasets. Bagging attempts to find a set of classifiers which are consistent with the training data, different from each other, and distributed such that the aggregate sample distribution approaches the distribution of samples in the training set. Algorithm: Bagging Predictors (Breiman, 1996) (1) Given: training set L = {(Yi, xi), i ∈ {1...m}} drawn from the set A of possible training sets where yi is the label for example xi, classification induction algorithm Ψ: A → Ф with classification algorithm ΦΕ Φ and 6: X → Y. 1. Create k bootstrap replicates of L by sampling m items from L with replacement. Call them L1... Lk. 2. For each j∈ {1...k}, Let j = (Lj) be the classifier induced using L; as the training set. 3. If Y is a discrete set, then for each xi observed in the test set, yi = mode($j(Xi)... Фj(Xi)). Yi is the value predicted by the most predictors, the majority vote. 2.2 Bagging for Parsing An algorithm that applies the technique of bagging to parsing is given in Algorithm 2. Previous work on combining independent parsers is leveraged to pro- duce the combined parser. The rest of the algorithm is a straightforward transformation of bagging for classifiers. Exploratory work in this vein was de- scribed by Hajič et al. (1999). (2) Algorithm: Bagging A Parser Given: A corpus (again as a function) C: SXT → N, S is the set of possible sentences, and T is the set of trees, with size m = |C| = ∑s.t C(s,t) and parser induction algorithm g. 1. Draw k bootstrap replicates C1... Ck of C each containing m samples of (s,t) pairs randomly picked from the domain of C according to the distribution D(s,t) = C(s,t)/|C|. Each boot- strap replicate is a bag of samples, where each sample in a bag is drawn randomly with replace- ment from the bag corresponding to C. 2. Create parser fi = g(Ci) for each i. 3. Given a novel sentence Stest ∈ Ctest, combine the collection of hypotheses ti = fi(Stest) us- ing the unweighted constituent voting scheme of Henderson and Brill (1999). 2.3 Experiment The training set for these experiments was sections 01-21 of the Penn Treebank (Marcus et al., 1993). The test set was section 23. The parser induction algorithm used in all of the experiments in this pa- per was a distribution of Collins's model 2 parser (Collins, 1997). All comparisons made below refer to results we obtained using Collins's parser. The results for bagging are shown in Figure 2 and Table 1. The row of figures are (from left-to-right) training set F-measure¹, test set F-measure, percent perfectly parsed sentences in training set, and per- cent perfectly parsed sentences in test set. An en- semble of bags was produced one bag at a time. In the table, the Initial row shows the performance achieved when the ensemble contained only one bag, Final (X) shows the performance when the ensem- ble contained X bags, BestF gives the performance of the ensemble size that gave the best F-measure score. TrainBestF and TestBestF give the test set performance for the ensemble size that performed the best on the training and test sets, respectively. On the training set all of the accuracy measures are improved over the original parser, and on the test set there is clear improvement in precision and recall. The improvement on exact sentence accuracy for the test set is significant, but only marginally so. The overall gain achieved on the test set by bag- ging was 0.8 units of F-measure, but because the entire corpus is not used in each bag the initial per- formance is approximately 0.2 units below the best previously reported result. The net gain using this technique is 0.6 units of F-measure. 3 Boosting 3.1 Background The AdaBoost algorithm was presented by Fre- und and Schapire in 1996 (Freund and Schapire, 1996; Freund and Schapire, 1997) and has become a widely-known successful method in machine learn- ing. The AdaBoost algorithm imposes one con- straint on its underlying learner: it may abstain from making predictions about labels of some samples, 1 This is the balanced version of F-measure, where precision and recall are weighted equally. but it must consistently be able to get more than 50% accuracy on the samples for which it commits to a decision. That accuracy is measured accord- ing to the distribution describing the importance of samples that it is given. The learner must be able to get more correct samples than incorrect samples by mass of importance on those that it labels. This statement of the restriction comes from Schapire and Singer's study (1998). It is called the weak learning criterion. Schapire and Singer (1998) extended AdaBoost by describing how to choose the hypothesis mixing co- efficients in certain circumstances and how to incor- porate a general notion of confidence scores. They also provided a better characterization of its theo- retical performance. The version of AdaBoost used in their work is shown in Algorithm 3, as it is the version that most amenable to parsing. Algorithm: AdaBoost (Freund and Schapire, 1997) (3) Given: Training set Las in bagging, except yi E {-1,1} is the label for example xi. Initial uniform distribution D₁(i) = 1/m. Number of iterations, T. Counter t = 1. Ψ, Φ, and are as in Bagging. 1. Create Lt by randomly choosing with replace- ment m samples from C using distribution Dr. 2. Classifier induction: t← (Lt) 3. Choose at ER. 4. Adjust and normalize the distribution. Zt is a normalization coefficient. 1 Dt+1(i) == Di(i) exp(-αζίφι(τι)) 5. Increment t. Quit if t > T. 6. Repeat from step 1. 7. The final hypothesis is boost(x) = sign Σαιφι(π) The value of at should generally be chosen to min- imize ΣDi(i) exp(-αιιφε(xi)) in order to minimize the expected per-sample train- ing error of the ensemble, which Schapire and Singer show can be concisely expressed by Zt. They also t give several examples for how to pick an appropriate a, and selection generally depends on the possible outputs of the underlying learner. Boosting has been used in a few NLP systems. Haruno et al. (1998) used boosting to produce more accurate classifiers which were embedded as control Set Training Instance Original Parser P R Initial BestF(15) 93.63 93.62 F Gain Exact Gain 96.25 96.31 96.28 NA 64.7 NA 93.61 55.5 0.0 0.00 96.16 95.86 96.01 2.39 62.1 6.6 Final(15) 96.16 95.86 96.01 2.39 62.1 6.6 Test Original Parser 88.73 88.54 88.63 NA 34.9 NA Initial 88.43 88.34 88.38 0.00 33.3 0.0 TrainBestF(15) 89.54 88.80 89.17 0.79 34.6 1.3 Test BestF(13) 89.55 88.84 89.19 0.81 34.7 1.4 Final(15) 34.6 1.3 89.54 88.80 89.17 0.79 Table 1: Bagging the Treebank mechanisms of a parser for Japanese. The creators of AdaBoost used it to perform text classification (Schapire and Singer, 2000). Abney et al. (1999) performed part-of-speech tagging and prepositional phrase attachment using AdaBoost as a core compo- nent. They found they could achieve accuracies on both tasks that were competitive with the state of the art. As a side effect, they found that inspecting the samples that were consistently given the most weight during boosting revealed some faulty anno- tations in the corpus. In all of these systems, Ad- aBoost has been used as a traditional classification system. 3.2 Boosting for Parsing Our goal is to recast boosting for parsing while con- sidering a parsing system as the embedded learner. The formulation is given in Algorithm 4. The in- tuition behind the additive form is that the weight placed on a sentence should be the sum of the weight we would like to place on its constituents. The weight on constituents that are predicted incorrectly are adjusted by a factor of 1 in contrast to a factor of a for those that are predicted incorrectly. S Algorithm: Boosting A Parser (4) Given corpus C with size m = |C| = ∑s. C(s,t) and parser induction algorithm g. Initial uniform distribution D₁(i) = 1/m. Number of iterations, T. Counter t = 1. 1. Create C₁ by randomly choosing with replace- ment m samples from C using distribution Dr. 2. Create parser ft - g(Ct). 3. Choose at ∈ R (described below). 4. Adjust and normalize the distribution. Zt is a normalization coefficient. For all i, let parse tree Tft(si). Let d(r. c) be a function indi- cating that e is in parse tree 7, and 7 is the number of constituents in tree T. T(s) is the set of constituents that are found in the reference or hypothesized annotation for s. Dt+1(i) = 1 Di(2) Σ (α+ (1 -α)δ(т, с) – б (ті, с)) CET(s;) - 5. Increment t. Quit if t > T. 6. Repeat from step 1. 7. The final hypothesis is computed by combin- ing the individual constituents. Each parser ot in the ensemble gets a vote with weight at for the constituents they predict. Precisely those constituents with weight strictly larger than 1 2 Σται are are put into the final hypothesis. A potential constituent can be considered correct if it is predicted in the hypothesis and it exists in the reference, or it is not predicted and it is not in the reference. Potential constituents that do not ap- pear in the hypothesis or the reference should not make a big contribution to the accuracy computa- tion. There are many such potential constituents, and if we were maximizing a function that treated getting them incorrect the same as getting a con- stituent that appears in the reference correct, we would most likely decide not to predict any con- stituents. Our model of constituent accuracy is thus sim- ple. Each prediction correctly made over T(s) will be given equal weight. That is, correctly hypothesizing a constituent in the reference will give us one point, but a precision or recall error will cause us to miss one point. Constituent accuracy is then a/(a+b+c), where a is the number of constituents correctly hy- pothesized, b is the number of precision errors and c is the number of recall errors. In Equation 1, a computation of aca as described is shown. Σ 2 Aca D(i) Σ δ(τι, c) + δб(ті, с) – 2б (ті, с)б (т.с) CET(s;) ΣΣ δ(ті, с)б (т, с) 2 CET(s;) Boosting algorithms were developed that at- tempted to maximize F-measure, precision, and re- call by varying the computation of a, giving results too numerous to include here. The algorithm given here performed the best of the lot, but was only marginally better for some metrics. (1) Set Instance P R F Gain Exact Gain Training Original Parser 96.25 96.31 96.28 NA 64.7 NA Initial 93.54 93.61 93.58 0.00 54.8 0.0 BestF(15) 96.21 95.79 96.00 2.42 57.3 2.5 Final(15) 96.21 95.79 96.00 2.42 57.3 2.5 Test Original Parser 88.73 88.54 88.63 NA 34.9 NA Initial 88.05 88.09 88.07 0.00 33.3 0.0 TrainBestF(15) 89.37 88.32 88.84 0.77 33.0 -0.3 TestBestF(14) 89.39 88.41 88.90 0.83 33.4 0.1 Final(15) 89.37 88.32 88.84 0.77 33.0 -0.3 Table 2: Boosting the Treebank 3.3 Experiment The experimental results for boosting are shown in Figure 3 and Table 2. There is a large plateau in performance from iterations 5 through 12. Because of their low accuracy and high degree of specializa- tion, the parsers produced in these iterations had little weight during voting and had little effect on the cumulative decision making. As in the bagging experiment, it appears that there would be more precision and recall gain to be had by creating a larger ensemble. In both the bagging and boosting experiments time and resource constraints dictated our ensemble size. In the table we see that the boosting algorithm equaled bagging's test set gains in precision and re- call. The Initial performance for boosting was lower, though. We cannot explain this, and expect it is due to unfortunate resampling of the data dur- ing the first iteration of boosting. Exact sentence accuracy, though, was not significantly improved on the test set. Overall, we prefer bagging to boosting for this problem when raw performance is the goal. There are side effects of boosting that are useful in other respects, though, which we explore in Section 4.2. 3.3.1 Weak Learning Criterion Violations It was hypothesized in the course of investigating the failures of the boosting algorithm that the parser in- duction system did not satisfy the weak learning cri- terion. It was noted that the distribution of boosting weights were more skewed in later iterations. Inspec- tion of the sentences that were getting much mass placed upon them revealed that their weight was be- ing boosted in every iteration. The hypothesis was that the parser was simply unable to learn them. 39832 parsers were built to test this, one for each sentence in the training set. Each of these parsers was trained on only a single sentence2 and evaluated on the same sentence. It was discovered that a full 4764 (11.2%) of these sentences could not be parsed completely correctly by the parsing system. 2The sentence was replicated 10 times to avoid threshold- 3.3.2 Corpus Trimming In order to evaluate how well boosting worked with a learner that better satisfied the weak learning cri- terion, the boosting experiment was run again on the Treebank minus the troublesome sentences de- scribed above. The results are in Table 3. This dataset produces a larger gain in comparison to the results using the entire Treebank. The initial ac- curacy, however, is lower. We hypothesize that the boosting algorithm did perform better here, but the parser induction system was learning useful informa- tion in those sentences that it could not memorize (e.g. lexical information) that was successfully ap- plied to the test set. In this manner we managed to clean our dataset to the point that the parser could learn each sentence in isolation. The corpus-makers cannot necessarily be blamed for the sentences that could not be mem- orized. All that can be said about those sentences is that for better or worse, the parser's model would not accommodate them. 4 Corpus Analysis 4.1 Noisy Corpus: Empirical Investigation To acquire experimental evidence of noisy data, dis- tributions that were used during boosting the sta- ble corpus were inspected. The distribution was ex- pected to be skewed if there was noise in the data, or be uniform with slight fluctuations if it fit the data well. We see how the boosting weight distribution changes in Figure 1. The individual curves are in- dexed by boosting iteration in the key of the figure. This training run used a corpus of 5000 sentences. The sentences are ranked by the weight they are given in the distribution, and sorted in decreasing or- der by weight along the x-axis. The distribution was smoothed by putting samples into equal weight bins, and reporting the average mass of samples in the bin as the y-coordinate. Each curve on this graph cor- responds to a boosting iteration. We used 1000 bins for this graph, and a log scale on the x-axis. Since there were 5000 samples, all samples initially had a --- Set Instance P R F Gain Exact Gain Training Original Parser 96.25 96.31 96.28 NA 64.7 NA Initial 94.60 94.68 94.64 0.00 62.2 0.0 BestF(8) 97.38 97.00 97.19 2.55 63.1 0.9 Final(15) 97.00 96.17 96.58 1.94 55.0 -7.2 Test Original Parser 88.73 88.54 88.63 NA 34.9 NA Initial 87.43 87.21 87.32 0.00 32.6 0.0 TrainBestF(8) 89.12 87.62 88.36 1.04 32.8 0.2 TestBestF(6) 89.07 87.77 88.42 1.10 32.9 0.4 Final(15) 89.18 87.19 88.18 0.86 31.7 -0.8 Table 3: Boosting the Stable Corpus Set Sentences P R F Exact Training 50 67.57 32.15 43.57 5.4 100 69.03 56.23 61.98 8.5 500 78.12 75.46 76.77 18.2 1000 81.36 80.70 81.03 22.9 5000 87.28 87.09 87.19 34.1 10000 89.74 89.56 89.65 41.0 20000 92.42 92.40 92.41 50.3 39832 96.25 96.31 96.28 64.7 Testing 50 68.13 32.24 43.76 4.7 100 69.90 54.19 61.05 7.8 500 78.72 75.33 76.99 19.1 1000 81.61 80.68 81.14 22.2 5000 86.03 85.43 85.73 28.6 10000 87.29 86.81 87.05 30.8 20000 87.99 87.87 87.93 32.7 39832 88.73 88.54 88.63 34.9 Table 4: Effects of Varying Training Corpus Size --- in the range of interest (between 10000 and 40000 sentences) gives a test set F-measure gain of approx- imately 0.70. Bagging achieved significant gains of approxi- mately 0.60 over the best reported previous F- measure without adding any new data. In this re- spect, these techniques show promise for making performance gains on large corpora without adding more data or new parsers. 6 Conclusion We have shown two methods, bagging and boosting, for automatically creating ensembles of parsers that produce better parses than any individual in the en- semble. Neither of the algorithms exploit any spe- cialized knowledge of the underlying parser induc- tion algorithm, and the data used in creating the ensembles has been restricted to a single common training set to avoid issues of training data quantity affecting the outcome. Our best bagging system performed consistently well on all metrics, including exact sentence accu- racy. It resulted in a statistically significant F- measure gain of 0.6 over the performance of the base- line parser. That baseline system is the best known Treebank parser. This gain compares favorably with a bound on potential gain from increasing the corpus size. Even though it is computationally expensive to create and evaluate a small (15-30) ensemble of parsers, the cost is far outweighed by the opportu- nity cost of hiring humans to annotate 40000 more sentences. The economic basis for using ensemble methods will continue to improve with the increasing value (performance per price) of modern hardware. Our boosting system, although dominated by the bagging system, also performed significantly better than the best previously known individual parsing result. We have shown how to exploit the distri- bution created as a side-effect of the boosting al- gorithm to uncover inconsistencies in the training corpus. A semi-automated technique for doing this as well as examples from the Treebank that are in- consistently annotated were presented. Perhaps the biggest advantage of this technique is that it requires no a priori notion of how the inconsistencies can be characterized. 7 Acknowledgments We would like to thank Michael Collins for enabling all of this research by providing us with his parser and helpful comments. This work was funded by NSF grant IRI-9502312. The views expressed in this paper are those of the authors and do not necessarily reflect the views of the MITRE Corporation. This work was done while both authors were at Johns Hopkins University References Steven Abney, Robert E. Schapire, and Yoram Singer. 1999. Boosting applied to tagging and PP attachment. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Lan- guage Processing and Very Large Corpora, pages 38-45, College Park, Maryland. L. Breiman. 1996. Bagging predictors. In Machine Learning, volume 24, pages 123-140. Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the Annual Meeting of the Association for Com- putational Linguistics, volume 35, Madrid. B. Efron and R. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall. Y. Freund and R.E. Schapire. 1996. Experiments with a new boosting algorithm. In Proceedings of the International Conference on Machine Learn- ing. Y. Freund and R.E. Schapire. 1997. A decision- theoretic generalization of on-line learning and an application to boosting. Journal of Computer and Systems Sciences, 55(1):119-139, Aug. Jan Hajič, E. Brill, M. Collins, B. Hladka, D. Jones, C. Kuo, L. Ramshaw, O. Schwartz, C. Tillmann, and D. Zeman. 1999. Core natural language processing technology applicable to multiple lan- guages. Prague Bulletin of Mathematical Linguis- tics, 70. Masahiko Haruno, Satoshi Shirai, and Yoshifumi Ooyama. 1998. Using decision trees to construct a practical parser. In Proceedings of the 36th Annual Meeting of the Association for Compu- tational Linguistics and 17th International Con- ference on Computational Linguistics, volume 1, pages 505-511, Montreal, Canada. John C. Henderson and Eric Brill. 1999. Exploiting diversity in natural language processing: Combin- ing parsers. In Proceedings of the Fourth Confer- ence on Empirical Methods in Natural Language Processing, College Park, Maryland. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The Penn Treebank. Computational Linguistics, 19(2):313-330. Robert E. Schapire and Yoram Singer. 1998. Im- proved boosting algorithms using confidence-rated predictions. In Proceedings of the Eleventh An- nual Conference on Computational Learning The- ory, pages 80-91. Robert E. Schapire and Yoram Singer. 2000. Boos- texter: A boosting-based system for text catego- rization. Machine Learning, 39(2/3):1-34, May. To appear. - 10000 (KAIST Reali 14 22 2 Precises Recall theasure 14 Recall 2000 Corpus Size 25(xx) 15(XXI 초 두 10 12 14 2 50 Figure 2: Bagging the Treebank 10 12 14 55.5 E Figure 3: Boosting the Treebank SUND 15000 200 25사 S000 Corpus Sue IKAISE 2 2 14 5000 10000 15000 20000 Corpus Size Figure 4: Effects of Varying Training Corpus Size 142 00052 11.25 112 1115 141 = 12.04 10 BOKKI 40000 10000 15000 25003 다 1 Annuatial artage VIỄN Annalized age Annualized av NN NN ית NX SP tutun יין יוין A יויו NNS fot DT אי الا NA NI aftay NP IN NNS יי M NNS ミージ 15 the 一六 CD NNS 230 days CONNS 30 dass N DT NN korerast NNS 1 יויו RB الا DT NN Fortrant NNS fame retir יוי RB IN NN NP foreast NNS NN Anmalized avage rate NX fr Analiza avtage rate יין יויו M NN יו NP DT NN the NNS έχρητικώς VIỄN \" NN ㅏ Antonalized avtag Fale ان Figure 5: An inconsistent set of annotations from the Treebank יויו AN NNS ן DT the Гор N ГОР FIRAG CD NNS Lays SP IN NP NN મળ 1 NP NNS fue TOP SP SNN (10 IN SNN (1) - NN hol DT NN IN 1 יויו NP NNS future us יוי 15 SP NNS future reth SXX future tetartis"
  },
  {
    "title": "The Domain Dependence of Parsing",
    "abstract": "A major concern in corpus-based approaches is that the applicability of the acquired knowledge may be limited by some feature of the corpus, in particular, the notion of text 'domain'. In order to examine the domain dependence of parsing, in this paper, we report 1) Comparison of structure distributions across domains; 2) Examples of domain specific structures; and 3) Parsing experiment using some domain dependent grammars. The observations using the Brown corpus demonstrate domain dependence and idiosyncrasy of syntactic structure. The parsing results show that the best accuracy is obtained using the grammar acquired from the same domain or the same class (fiction or non-fiction). We will also discuss the relationship between parsing accuracy and the size of training corpus.",
    "content": "1 Introduction A major concern in corpus based approaches is that the applicability of the acquired knowledge may be limited by some feature of the corpus. In particular, the notion of text 'domain' has been seen as a ma- jor constraint on the applicability of the knowledge. This is a crucial issue for most application systems, since most systems operate within a specific domain and we are generally limited in the corpora available in that domain. There has been considerable research in this area (Kittredge and Hirschman, 1983) (Grishman and Kittredge, 1986). For example, the domain depen- dence of lexical semantics is widely known. It is easy to observe that usage of the word 'bank' is dif- ferent between the 'economic document' domain and the 'geographic' domain. Also, there are surveys of domain dependencies concerning syntax or syntax- related features (Slocum, 1986) (Biber, 1993) (Karl- gren, 1994). It is intuitively conceivable that there are syntactic differences between 'telegraphic mes- sages' and 'press report', or between 'weather fore- cast sentences' and 'romance and love story'. But, how about the difference between 'press report' and 'romance and love story'? Is there a general and sim- ple method to compare domains? More importantly, shall we prepare different knowledge for these two domain sets? In this paper, we describe two observations and an experiment which suggest an answer to the ques- tions. Among the several types of linguistic knowl- edge, we are interested in parsing, the essential com- ponent of many NLP systems, and hence domain de- pendencies of syntactic knowledge. The observations and an experiment are the following: • Comparison of structure distributions across domains • Examples of domain specific structures • Parsing experiment using some domain depen- dent grammars 2 Data and Tools The definition of domain will dominate the perfor- mance of our experiments, so it is very important to choose a proper corpus. However, for practical rea- sons (availability and time constraint), we decided to use an existing multi-domain corpus which has naturally acceptable domain definition. In order to acquire grammar rules in our experiment, we need a syntactically tagged corpus consisting of different do- mains, and the tagging has to be uniform throughout the corpus. To meet these requirements, the Brown Corpus (Francis and Kucera, 1964) on the distribu- tion of PennTreeBank version 1 (Marcus et.al., 1995) is used in our experiments. The corpus consists of 15 --- domains as shown in Appendix A; in the rest of the paper, we use the letters from the list to represent the domains. Each sample consists of about the same size of text in terms of the number of words (2000 words), although a part of the data is discarded be- cause of erroneous data format. For the parsing experiment, we use 'Apple Pie Parser' (Sekine, 1995) (Sekine, 1996). It is a probabilistic, bottom-up, best-first search, chart parser and its grammar can be obtained from a syntactically-tagged corpus. We acquire two-non- terminal grammars from corpus. Here, 'two-non- terminal grammar' means a grammar which uses only 'S' (sentence) and 'NP' (noun phrase) as ac- tual non-terminals in the grammar and other gram- matical nodes, like 'VP' or 'PP', are embedded into a rule. In other words, all rules can only have either 'S' or 'NP' as their left hand-side symbol. This strat- egy is useful to produce better accuracy compared to all non-terminal grammar. See (Sekine, 1995) for details. In this experiment, grammars are acquired from the corpus of a single domain, or from some combina- tion of domains. In order to avoid the unknown word problem, we used a general dictionary to supplement the dictionary acquired from corpus. Then, we ap- ply each of the grammars to some texts of different domains. We use only 8 domains (A,B,E,J,K,L,N and P) for this experiment, because we want to fix the corpus size for each domain, and we want to have the same number of domains for the non-fiction and the fiction domains. The main objective is to observe the parsing performance based on the gram- mar acquired from the same domain compared with the performance based on grammars of different do- mains, or combined domains. Also, the issue of the size of training corpus will be discussed. 3 Domain Dependence of Structures First, we investigate the syntactic structure of each domain of the Brown corpus and compare these for different domains. In order to represent the syntactic structure of each domain, the distribution of partial trees of syntactic structure is used. A partial tree is a part of syntactic tree with depth of one, and it cor- responds to a production rule. Note that this partial tree definition is not the same as the structure defini- tion used in the parsing experiments described later. We accumulate these partial trees for each domain and compute the distribution of partial trees based on their frequency divided by the total number of partial trees in the domain. For example, Figure 1 shows the five most frequent partial trees (in the format of production rule) in domain A (Press: Re- domain A PP -> IN NP 8.40% NP -> NNPX 5.42% S->S 5.06% S-> NP VP 4.28% NP -> DT NNX 3.81% domain P NP -> PRP 9.52% PP -> IN NP 5.79% S -> NP VP 5.77% S -> S 5.37% NP -> DT NNX 3.90% Figure 1: Partial Trees TM A B E J K L N P A 5.13 5.35 5.41 5.45 5.51 5.52 5.53 5.55 B 5.47 5.19 5.50 5.51 5.55 5.58 5.60 5.60 E 5.50 5.48 5.20 5.48 5.58 5.59 5.58 5.61 J 5.39 5.37 5.35 5.15 5.52 5.57 5.58 5.59 K 5.32 5.25 5.31 5.41 4.95 5.14 5.15 5.17 L 5.32 5.26 5.32 5.45 5.12 4.91 5.09 5.13 N 5.29 5.25 5.28 5.43 5.10 5.06 4.89 5.12 P 5.43 5.36 5.40 5.55 5.23 5.21 5.21 5.00 Figure 2: Cross Entropy of grammar across domains portage) and domain P (Romance and love story). For each domain, we compute the probabilities of partial trees like this. Then, for each pair of domains, cross entropy is computed using the probability data. Figure 2 shows a part of the cross entropy data. For example, 5.41 in column A, row E shows the cross entropy of modeling by domain E and testing on do- main A. From the matrix, we can tell that some pairs of domains have lower cross entropy than others. It means that there are difference in similarity among domains. In particular, the differences among fiction domains are relatively small. In order to make the observation easier, we clus- tered the domains based on the cross entropy data. The distance between two domains is calculated as the average of the two cross-entropies in both direc- tions. We use non-overlapping and average-distance clustering. Figure 3 shows the clustering result based on grammar cross entropy data. From the results, we can clearly see that fiction domains, in particular domains K, L, and N are close which is intuitively understandable. 4 Domain Specific Structures Secondly, in contrast to the global analysis reported in the previous section, we investigate the structural idiosyncrasies of each domain in the Brown corpus. For each domain, the list of partial trees which are relatively frequent in that domain is created. We select the partial trees which satisfy the following --- ADFGKLNPRMJEBCH Text Same domain All non-fiction fiction A 66.62/84.14 64.39/61.45 65.57/62.40 62.23/59.32 5.08 B 67.65/62.55 64.67/61.78 65.73/62.69 63.03/60.36 5.13 E 64.05/60.79 65.25/61.51 65.26/62.18 62.87/59.04 5.17 J 67.80/65.59 65.87/63.90 65.57/64.58 63.04/60.77 5.26 | K 70.99/68.54 71.00/68.04 70.04/66.64 71.79/68.95 5.27 L 67.59/65.02 68.08/66.22 67.32/64.31 68.89/66.55 5.30 N 73.09/71.38 72.97/70.27 70.51/67.90 74.29/72.28 5.30 P 66.44/65.51 64.52/63.95 62.37/61.55 64.69/64.50 5.33 5.33 5.34 5.37 5.38 5.42 5.50 Figure 3: Clustering result two conditions: 1. Frequency of the partial tree in a domain should be 5 times greater than that in the entire corpus 2. It occurs more than 5 times in the domain The second condition is used to delete noise, because low frequency partial trees which satisfy the first con- dition have very low frequency in the entire corpus. The list is too large to show in this paper; a part of the list is shown in Appendix B. It obviously demonstrates that each domain has many idiosyn- cratic structures. Many of them are interesting to see and can be easily explained by our linguistic in- tuition. (Some examples are listed under the cor- responding partial tree) This supports the idea of domain dependent grammar, because these idiosyn- cratic structures are useful only in that domain. 5 Parsing Results In this section, the parsing experiments are de- scribed. There are two subsections. The first is the individual experiment, where texts from 8 domains are parsed with 4 different types of grammars. These are grammars acquired from the same size corpus of the same domain, all domains, non-fiction domains and fiction domains. The other parsing experiment is the intensive ex- periment, where we try to find the best suitable grammar for some particular domain of text and to see the relationship of the size of the training corpus. We use the domains of 'Press Reportage' and 'Ro- mance and Love Story' in this intensive experiment. Figure 4: Parsing accuracy for individual section In order to measure the accuracy of parsing, recall and precision measures are used (Black et.al., 1991). 5.1 Individual Experiment Figure 4 shows the parsing performance for domain A, B, E, J, K, L, N and P with four types of gram- mars. In the table, results are shown in the form of 'recall/precision'. Each grammar is acquired from roughly the same size (24 samples except L with 21 samples) of corpus. For example, the grammar of all domains is created using corpus of 3 samples each from the 8 domains. The grammar of non-fiction and fiction domains are created from corpus of 6 samples each from 4 domains. Then text of each domain is parsed by the four types of grammar. There is no overlap between training corpus and test corpus. We can see that the result is always the best when the grammar acquired from either the same domain or the same class (fiction or non-fiction) is used. We will call the division into fiction and non-fiction as 'class'. It is interesting to see that the grammar ac- quired from all domains is not the best grammar in any tests. In other words, if the size of the training corpus is the same, using a training corpus drawn from a wide variety of domains does not help to achieve better parsing performance. For non-fiction domain texts (A, B, E and J), the performance of the fiction grammar is notably worse than that of the same domain grammar or the same class grammar. In contrast, the performance on some fiction domain texts (K and L) with the non-fiction grammar is not so different from that of the same domain. Here, we can find a relationship between these results and the cross entropy obser- vations. The cross entropies where any of the fic- tion domains are models and any of the non-fiction domains are test are the highest figures in the ta- ble. This means that the fiction domains are not suitable for modeling the syntactic structure of the non-fiction domains. On the other hand, the cross entropies where any of the non-fiction domains are --- models and any of the non-fiction domains (except P) are test have some lower figures. Except for the case of N with the non-fiction grammar, these ob- servations explains the result of parsing very nicely. The higher the cross entropy, the worse the parsing performance. It is not easy to argue why, for some domains, the result is better with the grammar of the same class rather than the same domain. One rationale we can think of is based on the comparison observation de- scribed in section 3. For example, in the cross com- parison experiment, we have seen that domains K, L and N are very close. So it may be plausible to say that the grammar of the fiction domains is mainly representing K, L and N and, because it covers wide syntactic structure, it gives better performance for each of these domains. This could be the explana- tion that the grammar of fiction domains are superior to the own grammar for the three domains. In other words, it is a small sampling problem, which can be seen in the next experiment, too. Because only 24 samples are used, a single domain grammar tends to covers relatively small part of the language phenom- ena. On the other hands, a corpus of similar domains could provide wider coverage for the grammar. The assumption that the fiction domain grammar repre- sents domains of K, L and M may explain that the parsing result of domain P strongly favors the gram- mar of the same domain compared to that of the fiction class domains. 5.2 Intensive Experiments In this section, the parsing experiments on texts of two domains are reported. The texts of the two do- mains are parsed with several grammars, e.g. gram- mars acquired from different domains or classes, and different sizes of the training corpus. The size of the training corpus is an interesting and important issue. We can easily imagine that the smaller the training corpus, the poorer the parsing performance. How- ever, we don't know which of the following two types of grammar produce better performance: a grammar trained on a smaller corpus of the same domain, or a grammar trained on a larger corpus including dif- ferent domains. Figure 5 and Figure 6 shows recall and precision of the parsing result for the Press Reportage text. The same text is parsed with 5 different types of gram- mars of several variations of training corpus size. Be- cause of corpus availability, we can not make single domain grammars of large size training corpus, as you can find it in the figures. Figure 7 and Figure 8 shows recall and precision of the parsing result for the Romance and Love Story recall 70 65 60 55 50 45 0 20 40 60 80 100 Number of Samples Figure 5: Size and Recall (Press Report) precision 70 65 60 55 50 45 0 • ALL • fiction * non-fiction ◇ press report ♡ romance/love 20 40 60 80 100 Number of Samples Figure 6: Size and Precision (Press Report) recall 70 65 60- 55 50 45 0 • ALL • fiction * non-fiction ◇ press report ♡ romance/love 20 40 60 80 100 Number of Samples Figure 7: Size and Recall (Romance/Love) precision 70 65 60 55 50 45 0 20 40 • ALL • fiction * non-fiction ◇ press report romance/love 60 80 100 Number of Samples Figure 8: Size and Precision (Romance/Love) text. This text is also parsed with 5 different types of grammars. The graph between the size of training corpus and accuracy is generally an increasing curve with the slope gradually flattening as the size of the corpus in- creases. Note that the small declines of some graphs at large number of samples are mainly due to the memory limitation for parsing. Parsing is carried out with the same memory size, but when the train- ing corpus grows and the grammar becomes large, some long sentences can't be parsed because of data area limitation. When the data area is exhausted during the parsing, a fitted parsing technique is used to build the most plausible parse tree from the par- tially parsed trees. These are generally worse than the trees completely parsed. It is very interesting to see that the saturation point of any graph is about 10 to 30 samples. That is about 20,000 to 60,000 words, or about 1,000 to 3,000 sentences. In the romance and love story do- main, the precision of the grammar acquired from 8 samples of the same domain is only about 2% lower than the precision of the grammar trained on 26 sam- ples of the same domain. We believe that the reason why the performance in this domain saturates with such a small corpus is that there is relatively little variety in the syntactical structure of this domain. The order of the performance is generally the fol- lowing: the same domain (best), the same class, all domains, the other class and the other domain (worst). The performance of the last two grammars are very close in many cases. In the romance and love story domain, the grammar acquired from the same domain made the solo best performance. The difference of the accuracy of the grammars of the same domain and the other domain is quite large. The results for the press reportage is not so obvious, but the same tendencies can be observed. In terms of the relationship between the size of training corpus and domain dependency, we will compare the performance of the grammar acquired from 24 samples of the same domain (we will call it 'baseline grammar'), and that of the other gram- mars. In the press reportage domain, one needs a three to four times bigger corpus of all domains or non-fiction domains to catch up to the performance of the baseline grammar. It should be noticed that a quarter of the non-fiction domain corpus and one eighth of the all domain corpus consists of the press report domain corpus. In other words, the fact that the performance of the baseline grammar is about the same as that of 92 samples of the non-fiction do- mains means that in the latter grammar, the rest of the corpus does not improve or is not harmful for the parsing performance. In the romance and love story domain, the wide variety grammar, in particu- lar the fiction domain grammar quickly catch up to the performance of the baseline grammar. It needs only less than twice size of fiction domain corpus to achieve the performance of the baseline grammar. These two results and the evidence that fiction do- mains are close in terms of structure indicate that if you have a corpus consisting of similar domains, it is worthwhile to include the corpus in grammar acqui- sition, otherwise not so useful. We need to further quantify these trade-offs in terms of the syntactic di- versity of individual domains and the difference be- tween domains. We also find the small sampling problem in this experiment. In the press reportage experiment, the grammar acquired from the same domain does not make the best performance when the size of the train- ing corpus is small. We observed the same phenom- ena in the previous experiment. 6 Discussion One of our basic claims is the following. When we try to parse a text in a particular domain, we should prepare a grammar which suits that domain. This idea naturally contrasts to the idea of robust broad-coverage parsing (Carroll and Briscoe, 1996), in which a single grammar should be prepared for parsing of any kind of text. Obviously, the latter idea has a great advantage that you do not have to create a number of grammars for different domains and also do not need to consider which grammar should be used for a given text. On the other hand, it is plausible that a domain specific grammar can produce better results than a domain independent grammar. Practically, the increasing availability of corpora provides the possibilities of creating domain dependent grammars. Also, it should be noted that we don't need a very large corpus to achieve a rela- tively good quality of parsing. To summarize our observations and experiments: • There are domain dependencies on syntactic structure distribution. • Fiction domains in the Brown corpus are very similar in terms of syntactic structure. • We found many idiosyncratic structures from each domain by a simple method. • For 8 different domains, domain dependent grammar or the grammar of the same class pro- vide the best performance, if the size of the training corpus is the same. • The parsing performance is saturated at very small size of training corpus. This is the case, in particular, for the romance and love story do- main. • The order of the parsing performance is gener- ally the following; the same domain (best), the same class, all domain, the other class and the other domain (worst). • Sometime, training corpus in similar domains is useful for grammar acquisition. • It may not be so useful to use different domain corpus even if the size of the corpus is relatively large. Undoubtedly these conclusions depend on the parser, the corpus and the evaluation methods. Also our experiments don't cover all domains and possi- ble combinations. However, the observations and the experiment suggest the significance of the notion of domain in parsing. The results would be useful for deciding what strategy should be taken in developing a grammar on a 'domain dependent' NLP application systems. 7 Acknowledgments The work reported here was supported under con- tract 95-F145800-000 from the Office of Research and Development. We would like to thank our colleagues, in particular Prof. Ralph Grishman and Ms.Sarah Taylor for valuable discussions and sug- gestions. References Douglas Biber: 1993. Using Register-Diversified Corpora for General Language Studies. Journal of Computer Linguistics Vol.19, Num 2, pp219- 241. Ezra Black, et.al: 1991. A procedure for Quanti- tatively Comparing the Syntactic Coverage of En- glish Grammars. Proc. of Fourth DARPA Speech and Natural Language Workshop John Carroll and Ted Briscoe: 1996. Apportioning development effort in a probabilistic LR parsing system through evaluation. Proceedings of Confer- ence on Empirical Methods in Natural Language Processing. W. Nelson Francis and Henry Kucera: 1964/1979. Manual of information to accompany A Standard Corpus of Present-Day Edited American English. Brown University, Department of Linguistics Ralph Grishman and Richard Kittredge: 1986. An- alyzing Language in Restricted Domains: Sublan- guage Description and Processing. Lawrence Erl- baum Associates, Publishers Jussi Karlgren and Douglass Cutting: 1994. Rec- ognizing Text Genres with Simple Metrics Us- ing Discriminant Analysis. The 15th Interna- tional Conference on Computational Linguistics, pp1071-1075. Richard Kittredge, Lynette Hirschman: 1983. Sub- language: Studies of Language in Restricted Se- mantic domains. Series of Foundations of Com- munications, Walter de Gruyter, Berlin Mitchell P. Marcus, Beatrice Santorini and Mary A Marcinkiewicz: 1993. Building a Large Anno- tated Corpus of English: The Penn TreeBank. Computational Linguistics, 19.1, pp313-330. Satoshi Sekine: 1996. Apple Pie Parser homepage. http://cs.nyu.edu/cs/projects/proteus/app Satoshi Sekine and Ralph Grishman: 1995. A Corpus-based Probabilistic Grammar with Only Two Non-terminals. International Workshop on Parsing Technologies, pp216-223. Johathan Slocum: 1986. How One Might Automat- ically Identify and Adapt to a Sublanguage: An Initial Exploration. Analyzing Language in Re- stricted Domains, pp195-210. APPENDIX A Categories in Brown corpus I. Informative Prose (374 samples) A. Press: Reportage (44) B. Press: Editorial (27) C. Press: Reviews (17) D. Religion (17) E. Skills and Hobbies (36) F. Popular Lore (48) G. Letters,Bibliography,Memories, (75) H. Miscellaneous (30) J. Learned (80) II. Imaginative Prose (126 Samples) K. General Fiction (29) L. Mystery and Detective Fiction (24) M. Science Fiction (6) N. Adventure and Western Fiction (29) P. Romance and Love Story (29) R. Humor (9) B Sample of Relatively Frequent Partial Trees SYM. DOMAIN (num.of type;total freq. of qualified partial trees) ratio frequency rule (Example) (domain/corpus) A. Press: Reportage (30;507) 9.40 11 / 14 NP -> NNPX NNX NP 9.30 7 / 9 NP -> NP POS JJ NNPX 8.70 8 / 11 NP -> NP VBX VP NP PP 8.44 12 / 17 S -> NP DT $ CD NNX `The $40,000,000 budget´ `a 12,500 payment´ 8.30 77 / 111 NP -> NNPX NP `Vice President L.B. Johnson´ `First Lady Jacqueline Kennedy´ B. Press: Editorial (20;255) 18.57 34 / 34 S -> PP : `To the editor:´ `To the editor of New York Times:´ 11.14 6 / 10 NP -> DT ADJP NNX `an ``autistic´´ child´ `a ``stair-step´´ plan´ C. Press: Reviews (19;267) 26.27 8 / 9 WHADVP -> NNPX 25.33 12 / 14 NP -> NP POS NNPX D. Religion (8;87) 26.83 26 / 28 S -> NP -RRB- S 25.28 14 / 16 NP -> NNPX CD : CD `St. Peter 1:4′ `St. John 3:8′ E. Skills and Hobbies (17;219) 10.58 22 / 22 NP -> CDNNX 10.21 27 / 28 S -> SBAR : `How to feed :´ `What it does :´ F. Popular Lore (12;86) 10.58 8 / 8 NP -> DT NP POS NNPX G. Letters,Bibliography,Memories,etc (12;125) 6.59 8 / 8 WHPP -> TO SBAR `to what they mean by the concept´ `to what may happen next´ 6.04 22 / 24 WHPP -> OOF SBAR `of what it is all about´ `of what he had to show his country´ H. Miscellaneous (69;2607) 16.82 70 / 70 S -> NP . S 16.82 17 / 17 S -> -LRB- VP . -RRB- J. Learned (22;295) 6.51 28 / 28 NP -> CD : CD 6.51 20 / 20 NP -> NNX : 6.22 44 / 46 S -> S -LRB- NP -RRB- Sentence and name and year in bracket Sentence and figure name in bracket K. General Fiction (14;148) 11.58 7 / 10 NP -> PRP S 11.03 6 / 9 S -> ADVP S :: S 10.75 13 / 20 S -> PP S , CC S L. Mystery and Detective Fiction (19;229) 14.28 8 / 11 SQ -> S , SQ Tag questions (6;57) M. Science Fiction 17.89 7 / 32 S -> S , SINV ``Forgive me, Sandalphon´´, said Hal´´ ``sentence´´, remarked Helva´´ 10.22 8 / 64 S -> SBARQ N. Adventure and Western Fiction (24;422) 14.59 45 / 50 VP -> VBX RB 12.97 8 / 10 VP -> VBX RB PP P. Romance and Love Story (31;556) 15.99 7 / 7 S -> CC SBARQ 15.99 6 / 6 S -> , NP VP , NP 12.23 13 / 17 S -> SQ S 11.99 6 / 8 S -> VP , NP R. Humor (3;20) 6.92 6 / 47 NP -> DT ADJP NP 6.78 7 / 56 NP -> PRP ODLQ 5.67 7 / 67 PP -> IN NP `as ``off-Broadway´´´"
  },
  {
    "title": "CN YUR CMPUTR RAED THS?",
    "abstract": "This paper describes strategies for automatic recognition of unknown variants of known words in a natural language processing system. The types of lexical variants which are detectable include inflexional aberrations, ad hoc abbreviations and spelling/typographical errors. This technique is independent of any particular grammar or parsing formalism, and can be implemented as a lexical lookup routine which heuristically prunes and orders the list of possible fixes found in the lexicon, then allowing the parser to treat the list of candidates as a set of multiple meanings for a polysemous word.",
    "content": "1 INTRODUCTION This paper describes a technique for automatic recognition of unknown variants of known words in a natural language processing system. \"Known word\" refers here to a word which is in the lexi- con. The types of lexical variants which are de- tectable include inflexional aberrations, ad hoc abbreviations and spelling/typographical errors. The strategies presented here have been imple- mented fully in an English database query system and play a crucial role in a text-understanding sys- tem which is in the early stages of design. This technique, however, is independent of any par- ticular grammar or parsing formalism, and can be implemented as a lexical lookup routine which heuristically prunes and orders the list of possi- ble fixes found in the lexicon. First, a context-free plausibility assessment is based on a comparison of the structure of each candidate fix with that of the unknown word, and determines the order in which fixes will be considered by the parser. Then, the parsing process can choose among the candidate fixes in the same way that it tests multiple mean- ings of polysemous words for a good syntactic and semantic fit. The use of heuristics to identify the most plausible fixes for a hypothesized ad hoc ab- breviation or spelling error will be the focus of this paper. Unknown words have traditionally been handled by natural language processing systems in the fol- lowing ways: 1. Query the user for a replacement, possibly of- fering a menu of spelling corrections. This strategy will allow correction of misspelled words as well as correctly spelled words which are not in the lexicon, and generally ensures an accurate interpretation by the computer. However, continued interaction of this sort may prove frustrating to a poor typist, and is, of course, unsuitable for a non-interactive natural language processor. 2. Enter into a dialogue with the user to provide a definition for a new word. This strategy requires a lexicon interface based on a met- alanguage which would specify grammatical properties for a word without necessitating an inordinate degree of linguistic sophistication or knowledge of the database on the part of the end user. Although various attempts have been made to design such interfaces¹, many outstanding research issues remain, and this approach too requires an interactive environ- ment. 3. Try to infer syntactic and/or semantic fea- tures of the unknown word from the linguis- tic context, with no user interaction. This strategy can be used to choose a plausible correction for a misspelled word as well as to parse an expression containing an unknown word. Early research in this area attempted to model human reasoning about unknown words in a script-based parser [5], and has since come to encompass a variety of multi- strategy, expectation-based techniques as ex- emplified in the DYPAR [2] and NOMAD [4] systems. This technique shifts the burden of linguistic expertise from the end user to the computer system, but has met so far with only limited success, and accuracy can only be assured by interaction with the user to ¹Two outstanding examples are the TELI [1] and TEAM [6] systems. confirm the hypothesized interpretation. An additional limitation to this approach is that many existing natural language parsers can- not accomodate this sort of analysis. At General Motors Research Laboratories we have encountered two situations in which the tra- ditional interactive strategies are inadequate, and where it is preferable to try to fix an unknown word automatically. The first involves Datalog, our research proto- type of an English database query system [7]. This system was designed with the philosophy that a more humanlike sort of interaction is obtained by letting the system reason about ambiguities of all sorts as well as it can, always informing the user of its interpretation before displaying the response. Automatic lexical correction satisifes the objec- tives of this design principle. A more compelling need for this capability has arisen in a text-understanding project that we have undertaken, which aims to read and sum- marize the content of free-form text records in a diagnostic database. The cases are entered by au- tomotive technicians in the process of solving a va- riety of vehicle failure problems referred to them by service personnel in dealerships. The techni- cians are generally people who are not expert typ- ists, may not have excellent language skills, and have a time constraint imposed by a heavy load of calls. Also, two of the three free-form text fields in the database are abstract lines which are limited to a few words, which imposes a severe space constraint. As a result, the language used in the free-form text tends to be highly ill-formed, with an abundance of ad hoc abbreviations and typographical and spelling errors. And although the technicians' least compelling concern at data- entry time is linguistic in nature, their errors come back to haunt them, as the lexical errors impede the success of subsequent keyword searches done to find analogous cases in the database. So auto- matic lexical correction, in addition to being nec- essary to our text-understanding task in the longer text field, could also be of service in making cor- rections to the abstract lines in the database. The size of the database precludes the feasibility of an interactive system which would read text from existing cases and query a program operator for fixes. Incidentally, all examples used here of spelling errors and ad hoc abbreviations are taken from our free-form text data. 2 FLEXIBLE MORPHOLOGY One common type of spelling error involves spelling changes in base forms when adding an inflexional suffix. Two sorts of errors must be addressed in this area: failure to make a nec- essary spelling change (e.g. plug/pluged), and the occurrence of inappropriate spelling changes (come/comming). Inflected forms containing ei- ther of these errors can be detected by a forgiving morphology algorithm. Our algorithm currently recognizes only one prefix and/or suffix per word. Flexibility regard- ing inflexional spelling changes pertains only to suffixes; although additional suffixes can be spec- ified easily, those currently recognized are: -s (noun, verb) -er (adj) -ed (verb) -est (adj) -ing (verb) -ly (adj, adv) -ment (verb) The flexible morphology algorithm looks for a known suffix at the end of an unknown word. If found, the suffix is stripped off, and the remainder, a postulated base form, is looked up in the lexicon. If not found, spelling change transformations are performed on the base form, and lexical lookup is performed after each transformation. Morphologi- cal analysis succeeds when a postulated base form is found in the lexicon with a syntactic category which is compatible with the suffix. Consider the two ill-formed inflexions men- tioned above. For an unknown word pluged, the -ed suffix is stripped off. The remainder, plug, is found in the lexicon as a verb, so morphology succeeds. For the unknown word comming, the postulated base form after stripping off the -ing is comm. Transformations specified for an -ing suffix are: 1. If base ends in a double consonant, reduce it. 2. If base ends in a single consonant, append an -e. 3. If base ends in -i, change to -y. In the case of comm, morphology succeeds after performing the first two transformations and find- ing the verb come in the lexicon. This algorithm has proven quite effective in rec- ognizing inflected forms which contain common er- rors such as erroneously doubling a word-final con- sonant or failing to double a consonant before an inflected suffix; failure to drop word-final -e before -ing; and failure to change -y to -ie before adding -s or -ed. Morphology is the first fix tried for an unknown word, as it is less computationally intensive than detection of spelling errors and ad hoc abbrevia- tions, and occurs more frequently. 3 AD HOC ABBREVIATION RECOG- NITION When morphology fails to detect a variant of a known word, an ad hoc abbreviation or spelling error is hypothesized. Each possible abbreviation expansion found in the lexicon is assigned a plau- sibility score on the basis of a comparison of the structure of the unknown word and of the poten- tial fix. There are definite identifiable tendencies used by humans to abbreviate words. Although the ten- dencies which will be discussed here have not been tested empirically, our abbreviation-plausibility heuristic which incorporates them performs well in ordering lists of candidate abbreviation expan- sions. Further experimentation with the algorithm will undoubtedly produce even better results, but the heuristics described here have performed sat- isfactorily. Abbreviation occurs as a result of truncation or contraction. To identify candidate abbreviation expansions for a postulated abbreviation, the lex- icon is searched for words whose initial substring coincides with the unknown word (truncation- type fixes) and words which contain all letters of the unknown word in the same relative order (contraction- type fixes). Bear in mind that an- other property of abbreviations is that they gen- erally begin in the same letter as the word from which they are derived², so it is only necessary to search the portion of the lexicon beginning in the same letter as the unknown word, and it is only necessary to consider lexical words whose length exceeds that of the unknown word. Finding possible expansions of an abbreviation is a simple task. ³ Rating their plausibility in the ²The rare exceptions such as zmit for transmit or ztra for extra will not be treated here. ³The only exceptions are those infrequent abbreviations which contain letters not occurring in the expansion, like no. as an abbreviation for number. These tend to be common abbreviations which should be entered into the lexicon; ad hoc abbreviations generally do not have this characteristic. absence of contextual evidence is more difficult. For each possible abbreviation expansion found in the lexicon for an unknown word, a comparison is made between the structure of the unknown word and that of the expansion, and the candidate fix is assigned to one of five categories, which serves as a measure of its plausibility. Distinct heuristics are used to rate contraction-type and truncation-type fixes. The criteria used to classify candidate fixes and the ordering of the five plausibility categories are described below. 3.1 Truncation-type Abbreviations Truncation-type fixes are classified as either plausible or implausible, based on the extension string, i.e. the string which is chopped off to ab- breviate a word. I will refer to the two truncation classifications as trunc-good (plausible) and trunc- bad (implausible). If the extension consists entirely of vowels or en- tirely of consonants, it is classified as trunc-bad, as people generally tend to truncate words by delet- ing at least an entire syllable. If the unknown word ends in a vowel and the extension begins in a consonant, again it gets a trunc-bad rating, as people tend to truncate words by using the entire initial syllable(s), plus the initial consonant(s) of the following syllable if the preceding syllable ends in a vowel. If the entire initial consonant cluster of the expansion does not occur at the start of the unknown word, assign trunc-bad. Any truncation-type fix not classified as trunc- bad is rated as trunc-good (plausible). So the lex- ical word hesitation is classified as trunc-bad for an unknown word hesi, and trunc-good for an un- known word hes. 3.2 Contraction-type Abbreviations Three degrees of plausibility are distinguished for contraction-type fixes. Because the elimination of vowels from a word is a common contraction strategy, and the absence of vowels in an unknown word is a strong indicator of an ad hoc abbrevi- ation as opposed to a spelling error, the highest degree of plausibility is assigned to those lexical words from which only vowels have been excised to derive the unknown word. The classification designated for such fixes is called missing-vowels.⁴ ⁴The missing-vowels criterion requires two qualification. If a doubled consonant in the abbreviation is simplified, the abbreviation can still be classified as missing-vowels. For an unknown word assm, for instance, assume will be classified as a missing-vowels fix. For contraction-type fixes in which consonants as well as vowels have been excised to derive the unknown word, one of two classifications is as- signed: contract-good (plausible) and contract- bad (implausible). The favorable rating is as- signed if none of the following criteria for an im- plausible rating apply: 1. If the abbreviation cannot be derived from the lexical word by removing a single substring, assign contract-bad. This criterion reflects the common tendency to contract a word by using some initial portion plus some final por- tion. For an unknown word ht, some fixes classified as contract-bad for this reason are hatch, hertz and heater. 2. If the abbreviation differs from the lexical word by a single substring, but the substring contains only consonants, assign contract- bad. This is motivated by the fact that at least one entire syllable is generally removed to contract a word. For an unknown word satch, the fix scratch is classified as contract- bad. 3. If the abbreviation differs from the lexical word by a single substring, but a vowel imme- diately precedes or follows the substring in the expansion, assign contract-bad. The implau- sibility here arises from the fact that a vowel in the abbreviation is adjacent to a different consonant than in the lexical word, which is unlikely, or that two adjacent vowels in the abbreviation are not adjacent in the lexical word, which is even less likely. Consider as an example the fix regulate for an unknown word reate. 4. If the entire initial consonant cluster of the lexical word does not occur at the start of the abbreviation, assign contract-bad, e.g. strain as a fix for stn. Consider the classification of fixes from our lex- icon for an unknown word compt as an example of the ordering capability of the contraction heuris- tics: For instance, the abbreviations probbl, probl, prbbl and prbl all have a missing-vowels relationship with the expan- sion probable. Also, for each vowel removed to derive the abbreviation, all adjacent vowels in the expansion must also have been removed. Thus, count is not classified as a missing-vowels fix for an unknown word cont. missing-vowels contract-good contract-bad compute compartment consumption compact compensate component computer complaint composite 3.3 Ordering of Plausibility Categories for Abbreviations The five abbreviation plausibility classifications themselves are ordered in terms of plausibility. From high acceptability to low, the ordering is: 1. missing-vowels 2. trunc-good 3. contract-good 4. trunc-bad 5. contract-bad As a result, missing-vowels fixes are always pre- ferred over truncation or other contraction fixes. Plausible truncation fixes are preferred over plau- sible contraction fixes, but plausible contraction fixes are preferred over implausible truncation fixes. For an unknown word spr, fixes are found in our lexicon for four of the five classifications: missing- vowels spare trunc- good sprocket contract- good speaker contract- bad separate super spring spacer spark spray speedometer sport sprint sputter sulphur spicer suppressor 3.4 Interaction of Morphology and Abbre- viation Detection Abbreviations are easily recognizable when in- flected with suffixes known to the morphological analyzer. If an unknown word ends in a known suffix, the suffix is stripped off, and if the remain- der does not end in a vowel<sup>5</sup>, two comparisons are made with each lexical entry: one with the entire unknown word, and one with the suffix stripped off, checking for syntactic category compatibility between the suffix and the lexical definition. So for an unknown word such as outs, expansions will be found for the entire word (e.g. outside), and also for out, with a syntactic category restriction of noun or verb (e.g. outlet, outline). It is not unusual to encounter an abbreviation with an inflected suffix. It is unusual, however, for <sup>5</sup> Abbreviations rarely end in a vowel (see Section 5), so if the string left after stripping off a suffix ends in a vowel, the unknown word is not likely to be an abbreviation. an ad hoc abbreviation to incorporate the spelling changes required to inflect the expansion. Given the contraction assy for assembly, for instance, the plural form of the abbreviation is likely to be as- sys, not assies. As a result, I don't see a need to perform the usual spelling change transformations on an inflected abbreviation. 4 SPELLING CORRECTION The currently implemented spelling corrector is capable of identifying five types of spelling/typo- graphical errors: wrong letter, missing letter, ex- tra letter, transposed letters, and missing blank (incorrect segmentation in which two words are run together). The first four of the error cate- gories listed above have been found to account for over 80% of the spelling errors found in studies performed with end users [3], [9]. Like the abbre- viation detector, the spelling corrector only treats words which consist entirely of alphabetic charac- ters. 4.1 Spelling Error Plausibility Assessment Unlike the abbreviation plausibility strategies, the spelling correction plausibility heuristics can- not identify implausible fixes; they can only dis- tinguish the most highly plausible fixes from the other fixes, which are considered plausibility- neutral. No discriminators have been established for missing-blank fixes. In the wrong-letter category, the plausibility rating is boosted for fixes in which the correct letter phonologically resembles its erro- neous replacement (e.g. substitutions within sets of sibilants, vowels, or nasals), or when the two letters are adjacent on the keyboard. Missing-letter fixes are rated by comparing the missing letter with the letters that precede and fol- low it in the fix. A fix is assigned a higher score if a missing vowel is preceded or followed by another vowel (reflecting the common spelling error of re- ducing a diphthong, triphthong, vowel digraph or trigraph), or if a missing consonant is preceded or followed by an identical consonant (capturing the propensity to reduce doubled consonants). Extra-letter fixes are also evaluated on the ba- sis of the letters preceding and following the extra letter. A higher rating is assigned to those fixes in which an extra consonant is preceded or followed by an identical consonant, or an extra vowel occurs adjacent to another vowel, capturing the proclivity to unnecessarily double consonants or create diph- thongs or triphthongs. Transposed-letters fixes are considered plausible when the two transposed letters are adjacent (even more so when adjacent transposed letters are both vowels), and when two consonants have been transposed around vowels. Other researchers have expressed doubt about the feasibility of searching a large lexicon for all possible spelling corrections [2], preferring an expectation-based search in which the syntactic and/or semantic features expected by the parser upon encountering an unknown word are used to prune the search space for corrections. For a parser which can accommodate this approach, per- haps a combination of expectation-based pruning and plausibility heuristics would yield the best re- sults. Even in the absence of parser expectations, a couple of strategies can be employed to reduce the search space considerably. First, for the five categories of errors which we can detect, a lexical word need not be considered as a fix unless the length differential with the un- known word is less than 2 (accounting for all er- ror types except missing blank), or unless the un- known word begins in the lexical word (in the case of a missing blank). Secondly, another pruning strategy which mer- its mention is that of searching only the subset of the lexicon beginning in the same letter as the un- known word. An examination of one abstract field in 11,000 cases from our diagnostic database re- vealed only two misspelled words in which the ini- tial letter differs from that of the correction (oight for light and irratic for erratic). There occurred in the same data set 1207 misspelled words begin- ning in the same letter as the correction. Perhaps typists tend to notice and correct a misspelling more readily when the initial letter is in error. At any rate, our data seem to indicate that this prun- ing strategy is not unreasonable, particularly in an interactive application like database query where the fail-safe method of querying the user exists in the event of spelling correction failure. When spelling correction fails to find an accept- able fix beginning in the same letter, other lexical subsets may be searched as a last resort, chosen perhaps on the basis of phonetic similarity or key- board adjacency to the first letter of the unknown word (which would prove successful in our exceр- tional cases of oight and irratic). Using the pruning strategies of a < 2 length differential in a same-first-letter subset of the lexi- con, we have experienced very good response time with a lexicon of over 2600 words, which would be a good indicator of success for applications like database query where lexicons tend to be rather limited. 4.2 Interaction of Morphology and Spelling Correction The interaction of the spelling corrector with the morphological analyzer is more problematic than abbreviation-morphology interaction. The difficulty lies in the occurrence of morphological spelling changes in inflected forms (recall that inflected abbreviations generally do not incorpo- rate the usually requisite spelling changes). For spelling correction as well as abbreviation recogni- tion, two comparisons are made with each entry in the appropriate lexical subset when the unknown word ends in an inflected suffix: one comparison with the entire unknown word, and a second with the postulated base form, checking for syntactic category compatibility with the possible fix. One obvious problem is that while our spelling correction algorithm can identify one and only one spelling error per word, an inflexional spelling change in an already misspelled word results in two deviations from the lexical word. Consider, for instance, seperating as a misspelled inflexion of separate. After stripping off the -ing from the unknown word, a comparison of the postulated misspelled base form seperat with the lexical word separate will fail. One possible solution to this problem would in- volve the use of an inflect-word function which composes its base and suffix arguments into an in- flected form with requisite spelling changes. The lexical search for fixes then would compare the in- flected unknown word with the inflected form of the lexicai word (e.g. compare seperating with sep- arating, computing the latter form from the lexical entry separate inflected with suffix -ing). 5 ADJUDICATION BETWEEN SPELL- ING AND ABBREVIATION FIXES Once the lists of abbreviation fixes and spelling fixes have been ordered internally, priority must be given to one of these categories. Although a numerical score is assigned to each fix, the rela- tivity of the scores holds only within the broader categories of spelling and abbreviation, and not across categories. Various criteria may be applied to determine whether the unknown word is more likely to be an abbreviation or a misspelling. While there occasionally occur such fortuitous signals of abbreviation as a word-final period or an apostrophe before an inflected suffix (e.g. repl'ed for replaced), other subtle discriminators can also be identified. With the exception of standard ab- breviations for U.S. states and a few other com- mon abbreviations (e.g. limo, demo, info), abbre- viations of single words (as opposed to acronymic abbreviations) rarely tend to end in vowels. An unknown word ending in a vowel, then, is much more likely to be a misspelling than an ad hoc abbreviation. Consider too the fact that while the spelling heuristics fail to identify implausible fixes, the abbreviation heuristics do identify classes of im- plausibility for truncations as well as contractions. Generally speaking, abbreviation fixes classified as trunc-bad or contract-bad are implausible to such a high degree that they should not be considered at all when more plausible abbreviation fixes or any spelling fixes have been identified. So the fi- nal ordering of the entire list of fixes may be es- tablished by listing spelling fixes first (or maybe only) when the unknown word ends in a vowel or when no abbreviation fix is classified higher than trunc-bad, and placing abbreviation fixes before spelling fixes otherwise. 6 ROLE OF THE PARSER The Datalog parser has a Cascaded ATN archi- tecture, in which semantic feedback is provided to the working parser during the parse. A suc- cessful parse yields a semantic representation of the input expression as well as a syntactic parse tree. Lexical ambiguity is resolved by the parser through the rejection of unacceptable interpreta- tions by either syntax or semantics. Multiple ab- breviation and/or spelling fixes for an input word are treated by the parser as cases of lexical ambi- guity, differing only in how the user is informed of the fix when the response is displayed. In the orig- inal implementation of the lexical correction algo- rithm in Datalog, even though the list of candi- date fixes found during lexical lookup was not or- dered or pruned before parsing, good results were obtained in the recognition of spelling errors and abbreviations. The incorporation of plausibility heuristics, however, reduces parsing time consid- erably, as they eliminate unlikely candidates from consideration before parsing while also ordering candidates in terms of plausibility, thus obtaining a successful parse earlier. 7 FURTHER RESEARCH ISSUES Although initial experimentation with the lex- ical correction algorithm has yielded fairly good results in our database query system and text- understanding project, many research issues re- main unresolved. These pertain primarily to lex- ical variants which the algorithm cannot identify as possible fixes. 7.1 Robust Spelling Correction Spelling errors which are not handled include extra blank (incorrect segmentation which splits one word into two), more than one letter in error (e.g. droan for drone), and misspellings which co- incide with a lexical word (this would be a rare occurrence for an ad hoc abbreviation). Further work is needed to design a more robust spelling correction algorithm which can account for a greater variety of spelling errors as well as discrim- inate among good and bad correction candidates. The five types of spelling errors found by our pro- gram are often the result of keyboard slips instead of a misconstrual of the correct spelling. An ideal spelling correction algorithm would assess the dif- ferences between an unknown word and a postu- lated fix in terms of keyboard layout and typing habits, as well as phonetic similarity and interfer- ence from other words with a similar pronuncia- tion (e.g. fluxuate for fluctuate). 7.2 Unknown Words Identification of truly unknown words (those which should be added to lexicon) is difficult. We get around this problem with our free-form text by preprocessing all new text to find new words to add to the lexicon. Morphology weeds out in- flected forms of known words. A list of unknown words is then sent to a lexicon building program, which allows a lexicon builder to make decisions about each word and automatically creates a lex- ical definition with features selected by the user. Misspellings, abbreviations and inflexions can be skipped over easily, or renamed as new lexical en- tries. In the lexical lookup stage, an acceptability threshold can be established below which a fix will be rejected from consideration, so if no candidate fixes exceed the threshold, none will be considered by the parser. 7.3 Misspelled Inflexional Suffixes Spelling correction of inflexional suffixes is lack- ing. Morphology cannot recognize a word with a misspelled suffix such as engagemant, and as only the base form engage occurs in the lexicon, spelling correction cannot find the correct fix either. Per- haps spelling correction of inflexional suffixes can be implemented as a last resort measure. I haven't seen much of a need for it, possibly because the suffixes we identify are quite short, although it could be a problem if multiple suffixes were rec- ognized by morphology (e.g. standardization = standard + ize + ation). 7.4 Abbreviation Irregularities Misspelled abbreviations are another difficulty; the algorithm will not recognize accell as a trun- cation of acceleration. Abbreviated inflected suffixes on abbreviations are also beyond our current capabilities; whereas repled can be recognized as an inflected contrac- tion of replaced, repld cannot be recognized (and the latter is probably the more likely contraction). 7.5 Short Words One- and two-letter words are difficult to fix be- cause of the high number of candidate fixes that are found. We do not attempt to fix one-letter words, and generally find an unwieldy number of spelling fixes as well as abbreviation fixes for two- letter words. 7.6 Syllabic Analysis of Abbreviations We have not yet experimented with a syllable- based comparison of the unknown word with can- didate fixes as a method of context-free plausibil- ity assessment. Although this approach may prove to be more effective, it would also be more compu- tationally intensive and may be unnecessary. The strategies described here have been designed to capture many of the same generalities regarding abbreviation plausibility which would be inherent to a syllable-based approach. An excellent source of information on spelling correction algorithms is Peterson's annotated bibliography in [8]. 8 CONCLUSION Several decades of research in natural language processing have resulted in significant advances in our ability to parse well-formed input within a well-specified domain. One challenge which we now face is the ability to forgive linguistic devia- tions which do not obscure meaning. The lexical correction techniques described here- in appear to be promising for natural language ap- plications in which it is necessary to curtail user in- teraction in resolving ambiguities. Even in a more interactive environment, the usefulness of this ca- pability should not be ruled out. It accommodates users who lack good spelling and/or typing skills by forgiving spelling errors and by allowing consid- erable conservation of keystrokes through ad hoc abbreviation recognition. 9 ACKNOWLEDGEMENTS Many thanks to my GMR colleagues Kurt God- den and Sam Uthurusamy for their continued in- terest and feedback on this topic. References [1] Bruce W. Ballard and Douglas E. Stum- berger. Semantic Acquisition in TELI: A Transportable, User-Customized Natural Lan- guage Processor. In Proceedings of the 24th Annual Meeting of the Association for Com- putational Linguistics, pages 20-29, 1986. [2] Jaime G. Carbonell and Philip J. Hayes. Re- covery Strategies for Parsing Extragrammati- cal Language. American Journal of Computa- tional Linguistics, 9(3-4):123-146, 1983. [3] Fred J. Damerau. A Technique for Computer Detection and Correction of Spelling Errors. Communications of the ACM, 7(3):171-176, 1964. [4] Richard H. Granger. The NOMAD System: Expectation-Based Detection and Correction of Syntactically and Semantically Ill-Formed Text. American Journal of Computational Linguistics, 9(3-4):188-196, 1983. [5] Richard H. Granger, Jr. FOUL-UP: A Pro- gram that Figures Out Meanings of Words from Context. In Proceedings of the 5th In- ternational Joint Conference on Artificial In- telligence, pages 172-178, 1977. [6] Barbara J. Grosz, Douglas E. Appelt, Paul A. Martin, and Fernando C.N. Pereira. TEAM: An Experiment in the Design of Transportable Natural-Language Interfaces. Artificial Intel- ligence, 32:173-243, 1987. [7] Carole D. Hafner and Kurt S. Godden. Design of Natural Language Interfaces: A Case Study. Research Publication GMR-4567, General Mo- tors Research Laboratories, Warren, MI, 1984. [8] James L. Peterson. Computer Programs for Detecting and Correcting Spelling Errors. Communications of the ACM, 23(12):676-687, 1980. [9] James L. Peterson. A Note on Undetected Typing Errors. Communications of the ACM, 29(7):633-637, 1986."
  },
  {
    "title": "Understanding “Each Other”",
    "abstract": "Although natural language is ambiguous, various linguistic and extra-linguistic factors often help determine a preferred reading. In this paper, we show that model generation can be used to model this process in the case of reciprocal statements. The proposed analysis builds on insights from Dalrymple et al. 98 and is shown to provide an integrated, computational account of the interplay between model theoretic interpretation, knowledge-based reasoning and preferences that characterises the interpretation of reciprocals.",
    "content": "1 Introduction Although there is widespread agreement that inference is an essential component of natural language processing, little work has been done so far on whether existing automated reason- ing systems such as theorem provers and model builders could be fruitfully put to work in the area of natural language interpretation. In this paper, we focus on the inference prob- lems raised by the reciprocal expression each other and show that model generation provides an adequate tool for modeling them. The paper is structured as follows. Section 3 discusses the meaning of reciprocal statements and proposes a formal semantics for each other. Section 2 shows how model generation can be used to provide this semantics with a compu- tational interpretation. Section 4 compares our approach with the account of reciprocals which inspired it in the first place namely, (Dalrymple et al., 1998). Section 5 concludes with pointers for further research. 2 The meaning of reciprocal statements In the linguistic literature, the reciprocal ex- pression each other is often taken to denote a dyadic quantifier over a first-order set, which we will call the antecedent set, and a binary first- order relation, which we will call the scope re- lation. In what follows, we assume an approach of this type and will use the symbol RCP for such reciprocal quantifiers so that the seman- tic representation of e.g. Jon and Bill saw each other will be: (1) RCP({jon, bill})(xyx saw(x,y)) When antecedent sets of just two members are considered, each set member is required to stand in the scope relation to each other mem- ber. For larger sets however, research on recip- rocal statements has uncovered a variety of log- ical contributions that the reciprocal can pro- vide. Here are some examples. (2) The students like each other. Vx (std(x) → Vy (x ≠ y^ std(y) → like(x,y)) (3) The students stare at each other in sur- prise. Vx (std(x) → ∃y (x ≠ y^ std(y) ^ stare_at(x,y)) (4) The students gave each other measles. Vx (std(x) → ∃y (x ≠ y^ std(y) ^ (gave_measles(x, y) V gave_measle(y,x)))) We can accept (2) to be true only if for each pair x and y of two students it holds that x likes y. But an analogous interpretation would be invalid in the case of (3) and (4) where not all pairs in the antecedent set the students can consistently stand in the scope relation (one can only stare at most at one person at a time, and one can only get measles from at most one per- son). More generally, (Langendoen, 1978; Dal- rymple et al., 1998) convincingly argues that different reciprocal statements can have very different truth conditions. The challenge to be addressed is thus the following: How can we determine a (computational) semantics for the reciprocal expressions each other that accounts for these multiplicity of meanings while predict- ing the specific meaning of a particular recipro- cal statement? Clearly knowledge based reasoning plays an important role: only those readings are possible that are consistent with our knowledge about the situation and the world. Specifically, knowl- edge based reasoning constrains the strength of the truth conditions of a reciprocal statement. Thus if we abstract away from the specific scope relations, the truth conditions of examples such as (2),(3) and (4) are ordered through entail- ment as follows (with A the antecedent set and R the scope relation): ∀x (A(x) → ∀y (A(y) → R(xy)) ∀x (A(x) → ∃y (A(y) ∧ R(xy)) ∀x (A(x) → ∃y (A(y) ∧ (R(xy) ∨ R(yx)))) Specifically, example (2), which does not in- volve any strong knowledge based constraint, has the strongest truth-conditions of the three examples. By contrast in (3), the knowledge that one can stare at most at one person, forces a ∀ reading while in (4), a weaker meaning still is imposed by knowledge based constraints: the x gave y measles relation is asymmetric hence the ∀ reading is ruled out; moreover, since one cannot be infected twice, some students in the group will be infected but not pass on the dis- ease to anyone. Hence the strongest truth con- ditions that can be assigned the sentence are the ∨ disjunctive reading indicated in (4). But are there any other constraints on the interpretation process than these knowledge based constraints? And which meaning shall we assign a reciprocal expression? The compu- tational semantics we will propose is inspired from (Dalrymple et al., 1998) and relies on the following observations. First, we note that (Dalrymple et al., 1998) identifies a lower bound for the truth conditions of reciprocal sentences which they dub Inclusive Alternative Ordering (IAO). It is exemplified by sentence (4) above and corresponds to the fol- lowing definition of RCP. (5) RCPIAO = ∀PAR (|P| ≥ 2 ∧ ∀x (P(x) → ∃y P(y) ∧ x ≠ y ∧ (R(x, y) ∨ R(y, x)))) This definition only adequately characterises ex- amples such as (4). It does not cover the stronger meanings of the reciprocal in sentences such as (2) and (3). However, each known form of reciprocity entails RCPIAO's truth conditions, and RCPIAO therefore provides us with a mini- mal semantics for reciprocals. Further, we observe that given a particular reciprocal statement, there seems to be a pref- erence for consistent interpretations where the number of pairs that are in the scope relation is as large as possible. For instance in (3), not every student can stare at every other student (one can stare at at most one person), but intu- itively, the sentence requires that every student stares at some other student. While such an interpretation is much weaker than that of (2), this maximisation of the scope relation yields a reading that is also much stronger than the min- imal IAO interpretation of (4). More generally, while IAO provides us with a lower bound for the interpretation of reciprocal statements, we will see in section 3 that the maximisation of the scope relation that is consistent with contextual knowledge yields the upper bound for the inter- pretation of a particular reciprocal statement i.e., its meaning. Based on these observations, the principle de- termining the actual logical contribution of a reciprocal statement can be stated as follows: Maximise Meaning Hypothesis (MMH): The valid interpretations of a reciprocal sentence S in a context Γ (where Γ includes knowledge about the previous discourse, the discourse situ- ation and the world) are those which (a) are consistent both with the IAO form of reciprocity and the informa- tion provided by Γ, and (b) whose con- tributions to the scope relation are the strongest. The MMH selects from the set of interpreta- tions that are consistent with IAO and contex- tual knowledge, those that maximise the scope relation. Crucially, this view of reciprocals leads to an inference method that can actually com- pute the preferred interpretations of reciprocal sentences. We now turn to this. 3 Interpretation as Model Generation In Discourse Representation Theory (DRT, (Kamp, 1981; Kamp and Reyle, 1993)), a sen- tence with semantic representation is true with respect to a model M iff there is an embed- ding of onto M. Intuitively, this requirement says that a sub-model M' of M must be found which satisfies Φ. So for instance, sentence (6a) is true in M iff there are two individuals bugs and bunny in M such that bugs and bunny stand in the love relation; or in other words, iff the partial model sketched in (6b) is part of M. (6) a. Bugs likes Bunny. b. {love(bugs, bunny)} As shown in (Gardent and Konrad, To ap- pear), model generators (i.e., programs that compute some of the models satisfying a finite set of logical formulas) can be used to provide DRT, and more generally model-theoretic ap- proaches to natural language semantics, with a procedural interpretation: Given the semantic representation of a discourse and the relevant world knowledge (i.e., a finite set of logical formulas), a model generator proves that is satisfiable by generating some of its models. Intuitively, satisfying models explain how dis- courses can be made true. They give an abstract representation of how (part of) the world should be for a discourse to be true. Concretely, satisfying models can be seen as capturing the meaning of discourses: data- bases that can be queried e.g. as part of a query/answer system or to interpret subse- quent discourse. Satisfying models are also remininiscent of Johnson-Laird's mental mod- els (Johnson-Laird and Byrne, 1991) and in essence, mental models are very much like the Herbrand models we are making use of here. Formally, a model is a mathematical struc- ture that describes how the symbols of a logi- cal theory are interpreted. Given a first-order language L, a model is a pair (I, D) with Da non-empty set of entities (the domain of indi- viduals) and I an interpretation function which maps relation symbols in C to relations of ap- propriate arity in D and constant symbols in L to elements of D. Here we identify these mod- els with sets of positive assumptions that unam- biguously define the interpretation of the rela- tion symbols and fix the interpretation of terms to first-order entities that carry a unique name. These are known in the literature as Herbrand models. The set (7c) is such a model for the logical form (7b) which is a semantic representation of the sentence (7a). (7) a. Jon likes his cousin. b. 3x cousin_of(x, jon) like(jon,x) c. M₁ = {cousin_of (c1, jon), like (jon, C1)} The model M₁ defines an interpretation of the predicates cousin and like over the universe of discourse D = {jon, c₁}. It can also be taken as a valid interpretation of (7a). There are, how- ever, infinitely many models for (7b) that do not correspond to such interpretations e.g. (8) M2 = {cousin_of (jon, jon), like (jon, jon)} (9) M3 = {cousin_of (c₁, jon), like (jon, c₁), like(c1, jon)} The model M2 explains the truth of (7a) by declaring Jon as his own cousin. This is a re- sult of the inappropriate semantic representa- tion (7b) which fails to specify that the relation expressed by the noun cousin is irreflexive. In the case of M3, the model contains superfluous information. While it is consistent to assume like(c₁, jon) it is not necessary for explaining the truth of the input. 3.1 Minimality For applications to natural-language, we are in- terested in exactly those models that capture the meaning of a discourse, or at least capture the preferred interpretations that a hearer asso- ciates with it. As discussed in (Gardent and Webber, January 2000), obtaining only these models requires eliminating both models that are \"too small\" (e.g. M2) and models that are \"too big\" (e.g. M3). Models such as M2 can be eliminated simply by using more appropriate truth conditions for NL expressions (e.g. ∃x cousin(x) of (x, jon) x ≠ jonlike(jon, x) for (7a)). In general how- ever, eliminating models that are \"too small\" is a non-trivial task which involves the interac- tion of model-theoretic interpretation not only with world knowledge reasoning but also with syntax, prosody and pragmatics. The issue is discussed at some length (though not solved) in (Gardent and Webber, January 2000). To eliminate models that are \"too big\", some notion of minimality must be resorted to. For instance, (Gardent and Konrad, 1999; Gardent and Konrad, To appear) argues that local min- imality is an adequate form of minimality for interpreting definite descriptions. Local mini- mality is defined as follows. Local Minimality: Let I be a set of first- order formulas and D be the set of Herbrand models of I that use some finite domain D whose size is minimal. Then a model (I,D) ∈ D is locally minimal iff there is no other model (I', D') ∈ D such that I' ⊆ I. Locally minimal models are models that sat- isfy some input I within a minimal domain D of individuals and are subset-minimal with respect to all other domain minimal models. These models are the simplest in the sense of Occam's Razor and often the best explanation for the truth of an observation. In particular, if we as- sume that M2 is ruled out by a more appro- priate semantics for the word cousin, local min- imality rules out M3 as non locally minimal and therefore M₁ is correctly identified as giving the preferred interpretation for example (7). 3.2 The MMH as a Minimality Constraint In the case of reciprocals, local minimality is clearly not a characterisation of preferred interpretations. Our semantic representation RCPIAO will only capture a reciprocal's mean- ing if the reciprocal group has exactly two mem- bers or if the input meets IAO, the weakest form of reciprocity. For instance, the locally minimal model (10c) of formula (10b) is too weak to con- stitute an acceptable interpretation of (10a). In- stead, the model capturing the meaning of (10a) is the model given in (10d). (10) a. Jon, Bill and Dan like each other. b. RCPIAO({jon, bill, dan})(λyλx like(x,y)) c. {like(jon, bill), like(bill, dan)} d. {like(jon, bill), like(jon, dan), like(bill, dan), like(bill, jon), like(dan, bill), like(dan, jon)} Since the MMH maximises rather than min- imises the logical contribution of formulas, it seems at first sight incompatible with local min- imality. However, a simple method to combine the MMH and model minimality is to consider the maximisation of reciprocal relations as a minimisation of their complement sets. After all, the difference in acceptability between (10c) and (10d) as models for (10a) is due to exactly those pairs (x, y) (with x ≠ y) that are not in the like relation. To capture this intuition, we introduce a special predicate $R that indicates assumptions whose truth is considered \"costly\". In our case, these assumptions correspond to the pairs of individuals that are not in the scope re- lation. The semantic representation of recipro- cal each other is then as follows. (11) RCP 三 ∀xy (P(x) $R(x,y))) ∧P∧R (RCPIAO(P)(R) ∧ P(y) ∧ x ≠ y∧¬R(x,y) ↔ The first conjunct says that a reciprocal sen- tence has as weakest possible meaning an IAO reading. Since IAO is entailed by other identi- fied meaning for reciprocal statements, this is compatible with the fact that reciprocal sen- tences can have other, stronger meanings. The second conjunct says that each pair (x, y) (with x ≠ y) that is not in the like relation is in the $R relation. This encoding leads to mod- els like (12b) and (12c) for (12a). We say that model (12b) has a $R-cost of 4 ($R4), while model (12c) has a cost of 0. (12) a. RCP({jon, bill, dan})(λyλx like(x,y)) b. {like(jon, bill), like(jon, dan), $R(bill, dan), $R(bill, jon), $R(dan, bill), $R(dan, jon)} $R4 c. {like(jon, bill), like(jon, dan), like(bill, dan), like(bill, jon), like(dan, bill), like(dan, jon)} $RO We now introduce a new form of minimality whose definition is as follows. Conservative Minimality: Let I be a set of first-order formulas and D be the set of Her- brand models of I with a minimal domain D. Then D has a subset C of models that carry a minimal cost. A model (I,D) ∈ C is con- servative minimal iff there is no other model (I', D') ∈ C such that I' ⊆ I. Conservative minimality is a conservative ex- tension of local minimality: if there are no costs at all, then all local minimal models are also conservative models. Conservative mini- mality is a combination of local minimality and cost minimisation that correctly identifies the preferred interpretation of reciprocal sentences. For instance since (12c) carries a minimal cost, it is a conservative minimal model for (12a) whereas (12b) isn't. Intuitively the approach works as follows: the more pairs there are that do not stand in the scope relation of the re- ciprocal, the bigger the $R predicate and the more costly (i.e. the least preferred) the model. That is, the combined use of a $R-predicate and of conservative minimality allows us to enforce a preference for interpretations (i.e. models) maximising R. 3.3 The System KIMBA (Konrad and Wolfram, 1999) is a finite model generator for first-order and higher-order logical theories that is based on a translation of logics into constraint problems over finite- domain integer variables. KIMBA uses an effi- cient constraint solver to produce solutions that can be translated back into Herbrand models of the input. We have tailored KIMBA such that it enumer- ates the conservative models of its input. In- tuitively, this works as follows. First, KIMBA searches for some arbitrary model of the input that mentions a minimum number of individu- als. Then, it takes the $R-cost of this model as an upper bound for the cost of all successor models and further minimises the cost as far as possible by branch-and-bound search. After KIMBA has determined the lowest cost possi- ble, it restarts the model search and eliminates those models from the search space that have a non-minimal cost. For each model M that it identifies as a cost-minimal one, it proves by refutation that there is no other cost-minimal model M' that uses only a subset of the pos- itive assumptions in M. Each succesful proof yields a conservative minimal model. All the examples discussed in this paper have been tested on Kimba and can be tried out at: http://www.coli.uni-sb.de/cl/ projects/lisa/kimba.html 3.4 A spectrum of possible meanings Let us see in more detail what the predictions of our analysis are. As we saw in section 2, recip- rocal statements can have very different truth conditions. Intuitively, these truth-conditions lie on a spectrum from the weakest IAO inter- pretation (A is the antecedent set and R the scope relation): |A| ≥ 2 ∀x ∈ A(x) ∃y (A(y) ^ x ≠ y ^(R(x,y) VR(y, x)) to the strongest so-called Strong Reciprocity (SR) interpretation namely: |A| ≥ 2∀x A(x)dy A(y)(x ≠y ⇒ R(x,y)) We now see how the MMH allows us to сар- ture this spectrum. Let us start with example (2) whose truth- conditions are the strongest Strong Reciprocity conditions: every distinct x and y in the an- tecedent set are related to each other by the scope relation. In this case, there is no con- straining world knowledge hence the content of the like relation can be fully maximised. For instance if there are five students, the cheapest model is one in which the cardinality of like is twenty (and consequently the cardinatity of $R is zero). (13) {like(s1, s2), like (s1, s3), like (s1, s4), like(s1, s5), like (s2, s1), like (s2, s3), like (s2, s4), like (s2, s5), like (s3, s1), like(s3, s2), like (s3, s4), like (s3, s5), like(s4, s1), like (s4, s3), like (s4, s2), like(s4, s5), like (s5, s1), like (s5, s3), like(s5, s2), like (s5, s4) } $RO By contrast, example (3) has a much weaker meaning. In this case there is a strong world knowledge constraint at work, namely that one can stare at only one other person at some time. The cheapest models compatible with this knowledge are models in which every student stare at exactly one other student. Thus in a universe with five students, the preferred inter- pretations are models in which the cardinality of the scope relation x stares at y in surprise is five. The following are example models. For simplicity we ommit the $R propositions and give the cost of the model instead (i.e. the car- dinality of the complement set of the scope re- lation). (14) {stare_at(s1, s2), stare_at(s2, s3), stare_at(s3, s4), stare_at(s4, s5), stare_at(s5, s3)} $R15 (15) {stare_at(s1, s2), stare_at(s2, s3), stare_at(s3, s4), stare_at(s4, s5), stare_at(s5, s1)} $R15 Sentence (4) illustrates an intermediate case with respect to strength of truth conditions. World knowledge implies that the scope rela- tion x give y measles is assymetric and further that every individual is given measles by at most one other individual. Given a set of five stu- dents, model (16) and (17) are both acceptable interpretations of (4), (16) being the preferred interpretation. (16) {gave_measles(s1, s2), gave_measles(s1, s3), gave_measles(s2, s4), gave_measles(s3, s5)} $R16 (17) {gave_measles(s1, s2), gave_measles(s2, s4), gave_measles(s3, s5)} $R17 In short, these examples show the MMH at work. They show how given a single seman- tic representation for reciprocals, a variety of meanings can be derived as required by each specific reciprocal statement. Two elements are crucial to the account: the use of model build- ing, and that of minimality as an implemen- tation of preferences. Model building allows us to compute all the finite interpretations of a sentence that are consistent with contextual knowledge and with an IAO interpretation of the reciprocal expression. Preferences on the other hand (i.e. the use of the cost predicate $R and the search for conservative mininal mod- els), permits choosing among these interpreta- tions the most likely one(s) namely, the inter- pretation(s) maximising the scope relation. 4 Related Work (Dalrymple et al., 1998) (henceforth DKKMP) proposes the following taxonomy of mean- ings for reciprocal statements (A stands for the antecedent set and R for the scope relation): Strong Reciprocity (SR) Vx,y ∈ A(x ≠ y → xRy). Intermediate reciprocity (IR) Vx,y ∈ A ∃z1,...∃zm ∈ A(x ≠ y → xRz1 Λ... Λ zmRy) One-way Weak Reciprocity (OWR) ∀x ∈ A ∃y ∈ A (xRy) Intermediate Alternative Reciprocity (IAR) Vx,y ∈ A∃z1,...∃zm ∈ A(x ≠ y → (xRz1 V z1Rx) Λ... Λ (zmRy V yRzm)) Inclusive Alternative Ordering (IAO) ∀x ∈ A ∃y ∈ A(xRy V yRx) To predict the meaning of a specific recip- rocal sentence, DKKMP then postulate the Strongest Meaning Hypothesis which says that the meaning of a reciprocal sentence is the log- ically strongest meaning consistent with world and contextual knowledge. The main difference between the DKKMP ap- proach and the present approach lies in how the best reading is determined: it is the logi- cally strongest of the five postulated meanings in DKKMP, whereas in our approach, it is that reading which maximises the scope relation of the reciprocal. This difference has both empiri- cal and computational consequences. Empirically, the predictions are the same in most cases because maximising the scope rela- tion often results in yielding a logically stronger meaning. In particular, as is illustrated by the examples in section 2, the present approach cap- tures the five meanings postulated by DKKMP. Thus model (13) exemplifies an SR reading, model (15) an IR reading and model (14) an OWR reading. Further, model (16) is an IAR interpretation while model (17) shows an IAO reading. But as the examples also show there are cases where the predictions differ. In particu- lar, in the DKKMP approach, sentence (3) is assigned the IR reading represented by model (15). However as they themselves observe, the sentence also has a natural OWR interpretation namely, one as depicted in model (14), in which some pairs of students reciprocally stare at each other. This is predicted by the present approach which says that models (14) and (15) are equally plausible since they both maximise the stare at relation to cardinality five. On the other hand, the DKKMP account is more appropriate for examples such as: (18) The students sat next to each other a. forming a nice cercle. b. filling the bench. c. some to the front and others to the back of the church. An IR interpretation is predicted for (18) which is compatible with both continuation (18a) and continuation (18b). By contrast, the model generation approach predicts that the preferred interpretation is a model in which the students form a circle, an interpretation com- patible with continuation (18a) but not with continuations (18b-c). However, both approaches fail to predict the reading made explicit by continuation (18c) since this corresponds to the weaker OWR in- terpretation under the DKKMP account and to a model which fails to maximise the scope re- lation under the present approach. More gen- erally, both approaches fail to capture the se- mantic vagueness of reciprocal statements illus- trated by the following examples¹: (19) a. The students often help each other with their homework. b. In the closing minutes of the game, the members of the losing team tried to encour- age each other. In both cases, the sentences can be true with- out maximising either the strength of its truth conditions (Strong Reciprocity) or the scope re- lation. This suggests that an empirically more correct analysis of reciprocals should involve prototypical and probabilistic knowledge – as it is essentially a computational approximation of the DKKMP approach, the present account does not integrate such information though it is compatible with it: just as we restrict the set of generated models to the set of conservative minimal models, we could restrict it to the set of models having some minimal probability. Computationally, the difference between the DKKMP and the present approach is as fol- lows. In essence, the DKKMP approach re- quires that each of the five possible readings (together with the relevant world knoweldge) be checked for consistency: some will be con- sistent, others will not. Since the first order consistency and validity problems are not de- cidable, we know that there can be no method ¹I am thankful to an anonymous NAACL referee for these examples. guaranteed to always return a result. In order to implement the DKKMP approach, one must therefore resort to the technique advocated in (Blackburn et al., 1999) and use both a theo- rem prover and a model builder: for each possi- ble meaning Mi, the theorem is asked to prove Mi and the model builder to satisfy Mi. Mi is inconsistent if the theorem prover succeeds, and consistent if the model builder does. Theoreti- cally however, cases may remain where neither theorem proving nor model building will return an answer. If these cases occur in practice, the approach simply is not an option. Further, the approach is linguistically unappealing as it in essence requires the reciprocal each other to be five-way ambiguous. By contrast, the model generation approach assigns a single semantic representation to each other. The approach strengthens the logical contribution of the weak semantic representa- tion as a process based on computational con- straints on a set of effectively enumerable mod- els. As a result, we will never encounter un- decidable logical problems as long as the repre- sented discourse is consistent. The model gener- ator is the only computational tool that we need for determining preferable readings, and our ex- periment shows that for the examples discussed in this paper, it returns preferred readings in a few seconds on standard PCs as long as the background theory and the size of the domain remain managably small. 5 Conclusion We have argued that model building can be used to provide a computational approximation of DKKMP's analysis of reciprocals. One crucial feature of the account is that it permits building, comparing and ranking of natural-language interpretations against each other. In the case of reciprocals, the ranking is given by the size of the scope relation, but other ranking criteria have already been identified in the literature as well. For instance, (Gardent and Konrad, To appear) shows that in the case of definite descriptions, the ranking defined by local minimality permits capturing the prefer- ence of binding over bridging, over accomoda- tion. Similarly (Baumgartner and Kühn, 1999) shows that a predicate minimisation together with a preference for logically consequent reso- lutions can be used to model the interpretation of pronominal anaphora. This suggests that one of the most promising application of model generators is as a device for developing and testing preference systems for the interpretation of natural language. Infer- ence and knowledge based reasoning are needed in NLP not only to check for consistency and informativity (as illustrated in e.g. (Blackburn et al., 1999)), but also to express preferences between, or constraints on, possible interpreta- tions. For this, finite model builders are natural tools. Another area that deserves further investi- gation concerns the use of minimality for dis- ambiguation. In this paper, conservative min- imality is used to choose among the possible interpretations of a particular reciprocal state- ment. On the other hand, (Gardent and Web- ber, January 2000) shows that minimality is also an important tool for disambiguating noun- compounds, logical metonymy and definite de- scriptions. As the paper shows though, many questions remains open about this use of mini- mality for disambiguation which are well worth investigating. In further work, we intend to look at other ambiguous natural language constructs and to identify and model the ranking criteria deter- mining their preferred interpretation. Plurals are a first obvious choice. But more generally, we hope that looking at a wider range of data will unveil a broader picture of what the gen- eral biases are which help determine a preferred reading either in isolation, as here, or in con- text, as in (Gardent and Webber, January 2000) — and of how these biases can be modelled us- ing automated reasoning systems. Acknowledgements We are grateful to audiences from ITRI- Brighton, the Edinburgh School of Cognitive Science, the Paris VI TALANA seminar and the Amsterdam DIP colloquium for helpful com- ments and discussion on the material presented here as well as to the three NAACL anony- mous referrees for constructive feedback. This work was partially supported by the Project C2 (LISA) in SFB-378, grant by the Deutsche Forschungsgemeinschaft to the University of Saarbrücken. References Peter Baumgartner and Michael Kühn. 1999. Abductive coreference by model construction. In ICoS-1 Inference in Computational Se- mantics, Institute for Logic, Language and Computation, University of Amsterdam, Au- gust. P. Blackburn, J. Bos, M. Kohlhase, and H. de Neville. 1999. Inference and Com- putational Semantics. In Third Interna- tional Workshop on Computational Seman- tics (IWCS-3), Tilburg, The Netherlands. Mary Dalrymple, Makoto Kanasawa, Yookyung Kim, Sam Mchombo, and Stanley Peters. 1998. Reciprocal expressions and the con- cept of reciprocity. Linguistics and Philoso- phy, 21(2):159-210, April. Claire Gardent and Karsten Konrad. 1999. Definites and the proper treatment of rabbits. In Proceedings of ICOS. Also CLAUS Report 111, http://www.coli.uni-sb.de/claus/. Claire Gardent and Karsten Konrad. To ap- pear. Interpreting Definites using Model Generation. Journal of Language and Com- putation. Claire Gardent and Bonnie Webber. Jan- uary 2000. Automated deduction and discourse disambiguation. Submitted for Publication. Also CLAUS Report 113, http://www.coli.uni-sb.de/claus/. P.N. Johnson-Laird and Ruth M.J. Byrne. 1991. Deduction. Lawrence Erlbaum Asso- ciates Publishers. Hans Kamp and Uwe Reyle. 1993. From Dis- course to Logic. Kluwer, Dordrecht. Hans Kamp. 1981. A theory of truth and semantic representation. In J. Groenendijk, Th. Janssen, and M. Stokhof, editors, Formal Methods in the Study of Language, pages 277 — 322. Mathematisch Centrum Tracts, Ams- terdam. Karsten Konrad and D. A. Wolfram. 1999. Kimba, a model generator for many-valued first-order logics. In Proc., 16th Interna- tional Conference on Automated Deduction, CADE 99, LNCS, forthcoming, Trento, Italy. Springer. D. Terence Langendoen. 1978. The logic of reci- procity. Linguistic Inquiry, 9(2):177-197."
  },
  {
    "title": "Developing a hybrid NP parser",
    "abstract": "We describe the use of energy function optimisation in very shallow syntactic parsing. The approach can use linguistic rules and corpus-based statistics, so the strengths of both linguistic and statistical approaches to NLP can be combined in a single framework. The rules are contextual constraints for resolving syntactic ambiguities expressed as alternative tags, and the statistical language model consists of corpus-based n-grams of syntactic tags. The success of the hybrid syntactic disambiguator is evaluated against a held-out benchmark corpus. Also the contributions of the linguistic and statistical language models to the hybrid model are estimated.",
    "content": "1 Introduction The language models used by natural language an- alyzers are traditionally based on two approaches. In the linguistic approach, the model is based on hand-crafted rules derived from the linguist's gen- eral and/or corpus-based knowledge about the ob- ject language. In the data-driven approach, the model is automatically generated from annotated text corpora, and the model can be represented e.g. as n-grams (Garside et al., 1987), local rules (Hindle, 1989) or neural nets (Schmid, 1994). Most hybrid approaches combine statistical infor- mation with automatically extracted rule-based in- formation (Brill, 1995; Daelemans et al., 1996). Rel- atively little attention has been paid to models where the statistical approach is combined with a truly lin- guistic model (i.e. one generated by a linguist). This paper reports one such approach: syntactic rules written by a linguist are combined with statistical information using the relaxation labelling algorithm. Our application is very shallow parsing: identifi- cation of verbs, premodifiers, nominal and adverbial heads, and certain kinds of postmodifiers. We call this parser a noun phrase parser. The input is English text morphologically tagged with a rule-based tagger called EngCG (Voutilainen et al., 1992; Karlsson et al., 1995). Syntactic word- tags are added as alternatives (e.g. each adjective gets a premodifier tag, postmodifier tag and a nomi- nal head tag as alternatives). The system should re- move contextually illegitimate tags and leave intact each word's most appropriate tag. In other words, the syntactic language model is applied by a disam- biguator. The parser has a recall of 100% if all words retain the correct morphological and syntactic reading; the system's precision is 100% if the output contains no illegitimate morphological or syntactic readings. In practice, some correct readings are discarded, and some ambiguities remain unresolved (i.e. some words retain two or more alternative analyses). The system can use linguistic rules and corpus- based statistics. Notable about the system is that minimal human effort was needed for creating its language models (the linguistic consisting of syn- tactic disambiguation rules based on the Constraint Grammar framework (Karlsson, 1990; Karlsson et al., 1995); the corpus-based consisting of bigrams and trigrams): • Only one day was spent on writing the 107 syn- tactic disambiguation rules used by the linguis- tic parser. • No human annotators were needed for annotat- ing the training corpus (218,000 words of jour- nalese) used by the data-driven learning mod- ules of this system: the training corpus was an- notated by (i) tagging it with the EngCG mor- phological tagger, (ii) making the tagged text syntactically ambiguous by adding the alterna- tive syntactic tags to the words, and (iii) re- solving most of these syntactic ambiguities by applying the parser with the 107 disambigua- tion rules. The system was tested against a fresh sample of five texts (6,500 words). The system's recall and pre- cision was measured by comparing its output to a manually disambiguated version of the text. To in- crease the objectivity of the evaluation, system out- puts and the benchmark corpus are made publicly accessible (see Section 6). Also the relative contributions of the linguistic and statistical components are evaluated. The lin- guistic rules seldom discard the correct tag, i.e. they have a very high recall, but their problem is remain- ing ambiguity. The problems of the statistical com- ponents are the opposite: their recall is considerably lower, but more (if not all) ambiguities are resolved. When these components are used in a balanced way, the system's overall recall is 97.2% that is, 97.2% of all words get the correct analysis - and its preci- sion is 96.1% that is, of the readings returned by the system, 96.1% are correct. The system architecture is presented in Figure 1. Ambiguous training corpus Ambiguous test corpus Linguistic language model Linguistic parser Statistical language model Hybrid language model Relaxation labeling Partially disambiguated training corpus Statistics collector Disambiguated test corpus Figure 1: Parser architecture. The structure of the paper is the following. First, we describe our general framework, the relaxation labelling algorithm. Then we proceed to the appli- cation by outlining the grammatical representation used in our shallow syntax. After this, the disam- biguation rules and their development are described. Next in turn is a description of how the data-driven language model was generated. The evaluation of the system is then presented: first the preparation of the benchmark corpus is described, then the re- sults of the tests are given. The paper ends with some concluding remarks. 2 The Relaxation Labelling Algorithm Since we are dealing with a set of constraints and want to find a solution which optimally satisfies them all, we can use a standard Constraint Satis- faction algorithm to solve that problem. Constraint Satisfaction Problems are naturally modelled as Consistent Labeling Problems (Larrosa and Meseguer, 1995). An algorithm that solves CLPs is Relaxation Labelling. It has been applied to part-of-speech tagging (Padró, 1996) showing that it can yield as good re- sults as a HMM tagger when using the same in- formation. In addition, it can deal with any kind of constraints, thus the model can be improved by adding any other constraints available, either statistics, hand-written or automatically extracted (Marquez and Rodríguez, 1995; Samuelsson et al., 1996). Relaxation labelling is a generic name for a family of iterative algorithms which perform function opti- misation, based on local information. See (Torras, 1989) for a summary. Given a set of variables, a set of possible labels for each variable, and a set of compatibility constraints between those labels, the algorithm finds a combina- tion of weights for the labels that maximises \"global consistency\" (see below). Let V = {v1, v2,..., vn} be a set of variables. Let t₁ = {t'₁, t'₂, ..., t'm} be the set of possible labels for variable vi. Let CS be a set of constraints between the labels of the variables. Each constraint C ∈ CS states a \"compatibility value\" C, for a combination of pairs variable-label. Any number of variables may be in- volved in a constraint. The aim of the algorithm is to find a weighted labelling¹ such that \"global consistency\" is max- imised. Maximising \"global consistency\" is defined as maximising Σj pj × Sij, Vvi, where pj is the weight for label j in variable vi and Sij the support received by the same combination. The support for the pair variable-label expresses how compatible that pair is with the labels of neighbouring variables, ac- cording to the constraint set. ¹ A weighted labelling is a weight assignment for each label of each variable such that the weights for the labels of the same variable add up to one. The support is defined as the sum of the influence of every constraint on a label. Sij = ∑ Inf(r) reRij where: Rij is the set of constraints on label j for variable i, i.e. the constraints formed by any combination of variable-label pairs that includes the pair (vi, t). Inf(r) = C, xpk (m) x...x pk(m), is the prod- uct of the current weights for the labels appearing in the constraint except (vi,t)) (representing how applicable the constraint is in the current context) multiplied by C, which is the constraint compatibil- ity value (stating how compatible the pair is with the context). Briefly, what the algorithm does is: 1. Start with a random weight assignment. 2. Compute the support value for each label of each variable. (How compatible it is with the current weights for the labels of the other vari- ables.) 3. Increase the weights of the labels more compat- ible with the context (support greater than 0) and decrease those of the less compatible labels (support less than 0)³, using the updating func- tion: ki pj(m) × (1 + Sij) p(m + 1) = Σp(m) × (1 + Sik) k=1 where −1 ≤ Sij ≤ +1 4. If a stopping/convergence criterion is satisfied, stop, otherwise go to to step 2. 3 Grammatical representation The input of our parser is morphologically analyzed and disambiguated text enriched with alternative syntactic tags, e.g. \"<others>\" \"other\" PRON NOM PL Q>N QNH 2p(m) is the weight assigned to label k for variable r at time m. 3Negative values for support indicate incompatibility. The usual criterion is to stop when there are no more changes, although more sophisticated heuristic proce- dures are also used to stop relaxation processes (Eklundh and Rosenfeld, 1978; Richards et al., 1981). \"<moved>\" \"move\" <SV> <SVO> V PAST VFIN QV \"<away>\" \"away\" ADV ADVL QA QAH \"<from>\" \"from\" PREP ODUMMY \"<traditional>\" \"traditional\" A ABS Q>N QN< QNH \"<jazz>\" \"jazz\" <-Indef> N NOM SG Q>N QNH \"<practice>\" \"practice\" N NOM SG Q>N QNH \"practice\" <SVO> V PRES -SG3 VFIN QV Every indented line represents a morphological reading; the sample shows that some morphological ambiguities are not resolved by the rule-based mor- phological disambiguator, known as the EngCG tag- ger (Voutilainen et al., 1992; Karlsson et al., 1995). Our syntactic tags start with the \"@\" sign. A word is syntactically ambiguous if it has more than one syntactic tags (e.g. practice above has three al- ternative syntactic tags). Syntactic tags are added to the morphological analysis with a simple lookup module. The syntactic parser's main task is dis- ambiguating (rather than adding new information to the input sentence): contextually illegitimate al- ternatives should be discarded, while legitimate tags should be retained (note that also morphological am- biguities may be resolved as a side effect). Next we describe the syntactic tags: • @>N represents premodifiers and determiners. • @N< represents a restricted range of postmod- ifiers and the determiner \"enough\" following its nominal head. • @NH represents nominal heads (nouns, adjec- tives, pronouns, numerals, ING-forms and non- finite ED-forms). • @>A represents those adverbs that premodify (intensify) adjectives (including adjectival ING- forms and non-finite ED-forms), adverbs and various kinds of quantifiers (certain determin- ers, pronouns and numerals). • @AH represents adverbs that function as head of an adverbial phrase. • @A< represents the postmodifying adverb \"enough\". • @V represents verbs and auxiliaries (incl. the infinitive marker \"to\"). • @>CC represents words introducing a coordi- nation (\"either\", \"neither\", \"both\"). • @CC represents coordinating conjunctions. • @CS represents subordinating conjunctions. • @DUMMY represents all prepositions, i.e. the parser does not address the attachment of prepositional phrases. 4 Syntactic rules 4.1 Rule formalism The rules follow the Constraint Grammar formal- ism, and they were applied using the recent parser- compiler CG-2 (Tapanainen, 1996). The parser reads a sentence at a time and discards those ambiguity-forming readings that are disallowed by a constraint. Next we describe some basic features of the rule formalism. The rule REMOVE (O>N) (*1C <<< OR (QV) OR (OCS) BARRIER (QNH)); removes the premodifier tag @>N from an ambigu- ous reading if somewhere to the right (*1) there is an unambiguous (C) occurrence of a member of the set <<< (sentence boundary symbols) or the verb tag @V or the subordinating conjunction tag @CS, and there are no intervening tags for nominal heads (@NH). This is a partial rule about coordination: REMOVE (O>N) (NOT O (DET) OR (NUM) OR (A)) (1C (CC)) (2C (DET)); It removes the premodifier tag if all three context- conditions are satisfied: • the word to be disambiguated (0) is not a de- terminer, numeral or adjective, • the first word to the right (1) is an unambiguous coordinating conjunction, and • the second word to the right is an unambiguous determiner. In addition to REMOVing, also SELECTing a read- ing is possible: when all context-conditions are sat- isfied, all readings but the one the rule was expressly about are discarded. The rules can refer to words and tags directly or by means of predefined sets. They can refer not only to any fixed context positions; also reference to con- textual patterns is possible. The rules never discard a last reading, so every word retains at least one analysis. On the other hand, an ambiguity remains unresolved if there are no rules for that particular type of ambiguity. 4.2 Grammar development A day was spent on writing 107 constraints; about 15,000 words of the parser's output were proofread during the process. The routine was the following: 1. The current grammar (containing e.g. 2 rules) is applied to the ambiguous input in a 'trace' mode in which the parser also indicates, which rule discarded which analysis, 2. The grammarian observes remaining ambigui- ties and proposes new rules for disambiguating them, and 3. He also tries to identify misanalyses (cases where the correct tag is discarded) and, using the trace information, corrects the faulty rule This routine is useful if the development time is very restricted, and only the most common ambigu- ity types have to be resolved with reasonable suc- cess. However, if the grammar should be of a very high quality (extremely few mispredictions, high de- gree of ambiguity resolution), a large test corpus, formally similar to the input except for the manually added extra information about the correct analysis, should be used. This kind of test corpus would en- able the automatic identification of mispredictions as well as counting of various performance statistics for the rules. However, manually disambiguating a test corpus of a few hundred thousand words would probably require a human effort of at least a month. 4.3 Sample output The following is genuine output of the linguistic (CG-2) parser using the 107 syntactic disambigua- tion rules. The traces starting with \"S:\" indicate the line on which the applied rule is in the grammar file. One syntactic (and morphological) ambiguity remains unresolved: until remains ambiguous due to preposition and subordinating conjunction readings. \"<aachen>\" S:46 \"aachen\" <*> <Proper> N NOM SG ONH \"<remained>\" \"remain\" <SVC/N> <SVC/A> V PAST VFIN OV \"<a>\" \"a\" <Indef> DET CENTRAL ART SG Q>N \"<free>\" S:316, 49 \"free\" A ABS Q>N \"<imperial>\" S:49, 57 \"imperial\" A ABS Q>N \"<city>\" S:46 \"city\" N NOM SG ONH \"<until>\" \"until\" PREP QDUMMY \"until\" <**CLB> CS OCS \"<occupied>\" S:116, 345, 46 \"occupy\" <SVO> PCP2 QV \"<by>\" \"by\" PREP ODUMMY \"<france>\" S:46 \"france\" <*> <Proper> N NOM SG QNH \"<in>\" \"in\" PREP ODUMMY \"<1794>\" S:121, 49 \"1794\" <1900> NUM CARD ONΗ \"<$.>\" 5 Hybrid language model To solve shallow parsing with the relaxation labelling algorithm we model each word in the sentence as a variable, and each of its possible readings as a label for that variable. We start with a uniform weight distribution. We will use the algorithm to select the right syn- tactic tag for every word. Each iteration will in- crease the weight for the tag which is currently most compatible with the context and decrease the weights for the others. Since constraints are used to decide how compat- ible a tag is with its context, they have to assess the compatibility of a combination of readings. We adapt CG constraints described above. The REMOVE constraints express total incom- patibility and SELECT constraints express total compatibility (actually, they express incompatibility of all other possibilities). The compatibility value for these should be at least as strong as the strongest value for a statisti- cally obtained constraint (see below). This produces a value of about ±10. But because we want the linguistic part of the model to be more important than the statistical part and because a given label will receive the influence of about two bigrams and three trigrams, a sin- gle linguistic constraint might have to override five statistical constraints. So we will make the compat- ibility values six times stronger, that is, ±60. Since in our implementation of the CG parser (Tapanainen, 1996) constraints tend to be applied in a certain order – e.g. SELECT constraints are usually applied before REMOVE constraints – we adjust the compatibility values to get a similar ef- fect: if the value for SELECT constraints is +60, the value for REMOVE constraints will be lower in absolute value, (i.e. -50). With this we ensure that two contradictory constraints (if there are any) do not cancel each other. The SELECT constraint will win, as if it had been applied before. This enables using any Constraint Grammar with this algorithm although we are applying it more flex- ibly: we do not decide whether a constraint is ap- plied or not. It is always applied with an influence (perhaps zero) that depends on the weights of the labels. If the algorithm should apply the constraints in a more strict way, we can introduce an influence threshold under which a constraint does not have enough influence, i.e. is not applied. We can add more information to our model in the form of statistically derived constraints. Here we use bigrams and trigrams as constraints. The 218,000-word corpus of journalese from which these constraints were extracted was analysed using the following modules: • EngCG morphological tagger • Module for introducing syntactic ambiguities • The NP disambiguator using the 107 rules writ- ten in a day No human effort was spent on creating this train- ing corpus. The training corpus is partly ambigu- ous, so the bi/trigram information acquired will be slightly noisy, but accurate enough to provide an al- most supervised statistical model. For instance, the following constraints have been statistically extracted from bi/trigram occurrences in the training corpus. -0.415371 (QV) (1 (Q>N)); 5We model compatibility values using mutual infor- mation (Cover and Thomas, 1991), which enables us to use negative numbers to state incompatibility. See (Padró, 1996) for a performance comparison between M.I. and other measures when applying relaxation la- belling to NLP. The algorithm tends to select one label per variable, so there is always a bi/trigram which is applied more significantly than the others. 4.28089 (0>A) (-1 (0>A)) (1 (CAH)); The compatibility value is the mutual informa- tion, computed from the probabilities estimated from a training corpus. We do not need to assign the compatibility values here, since we can estimate them from the corpus. The compatibility values assigned to the hand- written constraints express the strength of these con- straints compared to the statistical ones. Modifying those values means changing the relative weights of the linguistic and statistical parts of the model. 6 Preparation of the benchmark corpus For evaluating the systems, five roughly equal-sized benchmark corpora not used in the development of our parsers and taggers were prepared. The texts, totaling 6,500 words, were copied from the Guten- berg e-text archive, and they represent present-day American English. One text is from an article about AIDS; another concerns brainwashing techniques; the third describes guerilla warfare tactics; the fourth addresses the assassination of J. F. Kennedy; the last is an extract from a speech by Noam Chom- sky. The texts were first analysed by a recent version of the morphological analyser and rule-based dis- ambiguator EngCG, then the syntactic ambiguities were added with a simple lookup module. The am- biguous text was then manually disambiguated. The disambiguated texts were also proofread afterwards. Usually, this practice resulted in one analysis per word. However, there were two types of exception: 1. The input did not contain the desired alterna- tive (due to a morphological disambiguation er- ror). In these cases, no reading was marked as correct. Two such words were found in the corpora; they detract from the performance fig- ures. 2. The input contained more than one analyses all of which seemed equally legitimate, even when semantic and textual criteria were consulted. In these cases, all the equal alternatives were marked as correct. The benchmark corpus con- tains 18 words (mainly ING-forms and nonfinite ED-forms) with two correct syntactic analyses. The number of multiple analyses could proba- bly be made even smaller by specifying the gram- matical representation (usage principles of the syn- tactic tags) in more detail, in particular incorpo- rating some analysis conventions for certain appar- ent borderline cases (for a discussion of specify- ing a parser's linguistic task, see (Voutilainen and Järvinen, 1995)). To improve the objectivity of the evaluation, the benchmark corpus (as well as parser outputs) have been made available from the following URLs: http://www.ling.helsinki.fi/~avoutila/anlp97.html http://www-lsi.upc.es/~lluisp/anlp97.html 7 Experiments and results We tested linguistic, statistical and hybrid language models, using the CG-2 parser (Tapanainen, 1996) and the relaxation labelling algorithm described in Section 2. The statistical models were obtained from a train- ing corpus of 218,000 words of journalese, syntac- tically annotated using the linguistic parser (see above). Although the linguistic CG-2 parser does not dis- ambiguate completely, it seems to have an almost perfect recall (cf. Table 1 below), and the noise in- troduced by the remaining ambiguity is assumed to be sufficiently lower than the signal, following the idea used in (Yarowsky, 1992). The collected statistics were bigram and trigram occurrences. The algorithms and models were tested against a hand-disambiguated benchmark corpus of over 6,500 words. We measure the performance of the different mod- els in terms of recall and precision. Recall is the percentage of words that get the correct tag among the tags proposed by the system. Precision is the percentage of tags proposed by the system that are correct. CG-2 parser prec. recall Rel. Labelling prec. recall C 90.8%-99.7% 93.3% - 98.4% Table 1: Results obtained with the linguistic model. B T Rel. Labelling prec. recall 87.4%88.0% 87.6%88.4% BT 88.1% -88.8% Table 2: Results obtained with statistical models. BC TC BTC Rel. Labelling prec. - recall 96.0%-97.0% 95.9%-97.0% 96.1% - 97.2% Table 3: Results obtained with hybrid models. Precision and recall results (computed on all words except punctuation marks, which are unam- biguous) are given in tables 1, 2 and 3. Models are coded as follows: B stands for bigrams, T for tri- grams and C for hand-written constraints. All com- binations of information types are tested. Since the CG-2 parser handles only Constraint Grammars, we cannot test this algorithm with statistical models. These results suggest the following conclusions: • Using the same language model (107 rules), the relaxation algorithm disambiguates more than the CG-2 parser. This is due to the weighted rule application, and results in more misanaly- ses and less remaining ambiguity. • The statistical models are clearly worse than the linguistic one. This could be due to the noise in the training corpus, but it is more likely caused by the difficulty of the task: we are dealing here with shallow syntactic parsing, which is prob- ably more difficult to capture in a statistical model than e.g. POS tagging. • The hybrid models produce less ambiguous re- sults than the other models. The number of errors is much lower than was the case with the statistical models, and somewhat higher than was the case with the linguistic model. The gain in precision seems to be enough to compensate for the loss in recall7. • There does not seem to be much difference be- tween BC and TC hybrid models. The reason is probably that the job is mainly done by the lin- guistic part of the model which has a higher relative weight and that the statistical part only helps to disambiguate cases where the lin- guistic model doesn't make a prediction. The BTC hybrid model is slightly better than the other two. • The small difference between the hybrid models suggest that some reasonable statistics provide enough disambiguation, and that not very so- phisticated information is needed. 7This obviously depends on the flexibility of one's requirements. 8 Discussion In this paper we have presented a method for com- bining linguistic hand-crafted rules with statistical information, and we applied it to a shallow parsing task. Results show that adding statistical information results in an increase in the disambiguation ratio, getting a higher precision. The price is a decrease in recall. Nevertheless, the risk can be controlled since more or less statistical information can be used depending on the precision/recall tradeoff one wants to achieve. We also used this technique to build a shallow parser with minimal human effort: • 107 disambiguation rules were written in a day. • These rules were used to analyze a training cor- pus, with a very high recall and a reasonable precision. • This slightly ambiguous training corpus is used for collecting bigram and trigram occurrences. The noise introduced by the remaining ambigu- ity is assumed not to distort the resulting statis- tics too much. • The hand-written constraints and the statistics are combined using a relaxation algorithm to analyze the test corpus, rising the precision to 96.1% and lowering the recall only to 97.2%. Finally, a reservation must be made: what we have not investigated in this paper is how much of the extra work done with the statistical module could have been done equally well or even better by spend- ing e.g. another day writing a further collection of heuristic rules. As suggested e.g. by Tapanainen and Voutilainen (1994) and Chanod and Tapanainen (1995), hand-coded heuristics may be a worthwhile addition to 'strictly' grammar-based rules. Acknowledgements We wish to thank Timo Järvinen, Pasi Tapanainen and two ANLP'97 referees for useful comments on earlier versions of this paper. The first author benefited from the collaboration of Juha Heikkilä in the development of the linguistic description used by the EngCG morphological tag- ger; the two-level compiler for morphological analy- sis in EngCG was written by Kimmo Koskenniemi; the recent version of the Constraint Grammar parser (CG-2) was written by Pasi Tapanainen. The Con- straint Grammar framework was originally proposed by Fred Karlsson. References E. Brill. 1995. Unsupervised Learning of Disam- biguation Rules for Part-of-speech Tagging. In Proceedings of 3rd Workshop on Very Large Cor- pora, Massachusetts. J.-P. Chanod and P. Tapanainen 1995. Tagging French: comparing a statistical and a constraint- based method. In Proc. EACL '95. ACL, Dublin. T.M. Cover and J.A. Thomas (Editors) 1991. Ele- ments of information theory. John Wiley & Sons. J. Eklundh and A. Rosenfeld. 1978. Convergence Properties of Relaxation Labelling. Technical Re- port no. 701. Computer Science Center. Univer- sity of Maryland. W. Daelemans, J. Zavrel, P. Berck and S. Gillis. 1996. MTB: A Memory-Based Part-of-Speech Tagger Generator. In Proceedings of 4th Work- shop on Very Large Corpora. Copenhagen, Den- mark. R. Garside, G. Leech and G. Sampson (Editors) 1987. The Computational Analysis of English. London and New York: Longman. D. Hindle. 1989. Acquiring disambiguation rules from text. In Proc. ACL'89. F. Karlsson 1990. Constraint Grammar as a Frame- work for Parsing Running Text. In H. Karlgren (ed.), Papers presented to the 13th International Conference on Computational Linguistics, Vol. 3. Helsinki. 168-173. F. Karlsson, A. Voutilainen, J. Heikkilä and A. Anttila. (Editors) 1995. Constraint Grammar: A Language-Independent System for Parsing Un- restricted Text. Mouton de Gruyter, Berlin and New York. J. Larrosa and P. Meseguer. 1995. An Optimization- based Heuristic for Maximal Constraint Satisfac- tion. In Proceedings of International Conference on Principles and Practice of Constraint Program- ming. L. Màrquez and H. Rodríguez. 1995. Towards Learning a Constraint Grammar from Annotated Corpora Using Decision Trees. ESPRIT BRA- 7315 Acquilex II, Working Paper. L. Padró. 1996. POS Tagging Using Relaxation Labelling. In Proceedings of 16th International Conference on Computational Linguistics, Copen- hagen, Denmark. J. Richards, D. Landgrebe and P. Swain. 1981. On the accuracy of pixel relaxation labelling. In IEEE Transactions on System, Man and Cybernetics. Vol. SMC-11 C. Samuelsson, P. Tapanainen and A. Voutilainen. 1996. Inducing Constraint Grammars. In Pro- ceedings of the 3rd International Colloquium on Grammatical Inference. H. Schmid 1994. Part-of-speech tagging with neu- ral networks. In Proceedings of 15th International Conference on Computational Linguistics, Kyoto, Japan. P. Tapanainen 1996. The Constraint Grammar Parser CG-2. Department of General Linguistics, University of Helsinki. P. Tapanainen and A. Voutilainen 1994. Tagging accurately – Don't guess if you know. In Pro- ceedings of the 4th Conference on Applied Natural Language Processing, ACL. Stuttgart. C. Torras. 1989. Relaxation and Neural Learning: Points of Convergence and Divergence. Journal of Parallel and Distributed Computing, 6:217-244 A. Voutilainen, J. Heikkilä and A. Anttila 1992. Constraint Grammar of English. A Performance- Oriented Introduction. Publications 21, De- partment of General Linguistics, University of Helsinki. A. Voutilainen and T. Järvinen. 1995. Specifying a shallow grammatical representation for parsing purposes. In Proceedings of the 7th meeting of the European Association for Computational Linguis- tics. 210-214. D. Yarowsky. 1992. Word-sense disambiguations us- ing statistical models of Roget's categories trained on large corpora. In Proceedings of 14th Interna- tional Conference on Computational Linguistics. Nantes, France."
  },
  {
    "title": "Fast Statistical Parsing of Noun Phrases for Document Indexing",
    "abstract": "Information Retrieval (IR) is an important application area of Natural Language Processing (NLP) where one encounters the genuine challenge of processing large quantities of unrestricted natural language text. While much effort has been made to apply NLP techniques to IR, very few NLP techniques have been evaluated on a document collection larger than several megabytes. Many NLP techniques are simply not efficient enough, and not robust enough, to handle a large amount of text. This paper proposes a new probabilistic model for noun phrase parsing, and reports on the application of such a parsing technique to enhance document indexing. The effectiveness of using syntactic phrases provided by the parser to supplement single words for indexing is evaluated with a 250 megabytes document collection. The experiment's results show that supplementing single words with syntactic phrases for indexing consistently and significantly improves retrieval performance.",
    "content": "1 Introduction Information Retrieval (IR) is an increasingly impor- tant application area of Natural Language Process- ing (NLP). An IR task can be described as to find, from a given document collection, a subset of docu- ments whose content is relevant to the information need of a user as expressed by a query. As the doc- uments and query are often natural language texts, an IR task can usually be regarded as a special NLP task, where the document text and the query text need to be processed in order to judge the relevancy. A general strategy followed by most IR systems is to transform documents and the query into certain level of representation. A query representation can then be compared with a document representation to decide if the document is relevant to the query. In practice, the level of representation in an IR system is quite \"shallow\"—often merely a set of word-like strings, or indexing terms. The process to extract in- dexing terms from each document in the collection is called indexing. A query is often subject to simi- lar processing, and the relevancy is judged based on the matching of query terms and document terms. In most systems, weights are assigned to terms to indicate how well they can be used to discriminate relevant documents from irrelevant ones. The challenge in applying NLP to IR is to deal with a large amount of unrestricted natural lan- guage text. The NLP techniques used must be very efficient and robust, since the amount of text in the databases accessed is typically measured in gi- gabytes. In the past, NLP techniques of different levels, including morphological, syntactic/semantic, and discourse processing, were exploited to enhance retrieval (Smeaton 92; Lewis and Sparck Jones 96), but were rarely evaluated using collections of docu- ments larger than several megabytes. Many NLP techniques are simply not efficient enough or are too labor-intensive to successfully handle a large size document set. However, there are some exceptions. Evans et al. used selective NLP techniques, that are especially robust and efficient, for indexing (Evans et al. 91). Strzalkowski reported a fast and robust parser called TTP in (Strzalkowski 92; Strzalkowski and Vauthey 92). These NLP techniques have been successfully used to process quite large collections, as shown in a series of TREC conference reports by the CLARIT™1 system group and the New York University (later GE/NYU) group (cf., for example, (Evans and Lefferts 95; Evans et al. 96), and (Strza- lkowski 95; Strzalkowski et al. 96)) These research efforts demonstrated the feasibility of using selec- tive NLP to handle large collections. A special NLP track emphasizing the evaluation of NLP techniques for IR is currently held in the context of TREC (Har- man 96). In this paper, a fast probabilistic noun phrase parser is described. The parser can be exploited to $^1$CLARIT is a registered trademark of CLARITECH Corporation. automatically extract syntactic phrases from a large amount of documents for indexing. A 250-megabyte document set<sup>2</sup> is used to evaluate the effectiveness of indexing using the phrases extracted by the parser. The experiment's results show that using syntactic phrases to supplement single words for indexing im- proves the retrieval performance significantly. This is quite encouraging compared to earlier experiments on phrase indexing. The noun phrase parser pro- vides the possibility of combining different kinds of phrases with single words. The rest of the paper is organized as follows. Sec- tion 2 discusses document indexing, and argues for the rationality of using syntactic phrases for index- ing; Section 3 describes the fast noun phrase parser that we use to extract candidate phrases; Section 4 describes how we use a commercial IR system to per- form the desired experiments; Section 5 reports and discusses the experiment results; Section 6 summa- rizes the conclusions. 2 Phrases for Document Indexing In most current IR systems, documents are primarily indexed by single words, sometimes supplemented by phrases obtained with statistical approaches, such as frequency counting of adjacent word pairs. However, single words are often ambiguous and not specific enough for accurate discrimination of documents. For example, only using the word \"bank\" and \"ter- minology\" for indexing is not enough to distinguish \"bank terminology\" from \"terminology bank\". More specific indexing units are needed. Syntactic phrases (i.e., phrases with certain syntactic relations) are al- most always more specific than single words and thus are intuitively attractive for indexing. For example, if \"bank terminology\" occurs in the document, then, we can use the phrase \"bank terminology\" as an ad- ditional unit to supplement the single words \"bank\" and \"terminology\" for indexing. In this way, a query with \"terminology bank\" will match better with the document than one with \"bank terminology\", since the indexing phrase \"bank terminology\" provides ex- tra discrimination. Despite the intuitive rationality of using phrases for indexing, syntactic phrases have been reported to show no significant improvement of retrieval per- formance (Lewis 91; Belkin and Croft 87; Fagan 87). Moreover Fagan (Fagan 87) found that syn- tactic phrases are not superior to simple statistical phrases. Lewis discussed why the syntactic phrase indexing has not worked and concluded that the problems with syntactic phrases are for the most part statistical (Lewis 91). Indeed, many (perhaps most) syntactic phrases have very low frequency and tend to be over-weighted by the normal weighting method. However, the size of the collection used in <sup>2</sup>the Wall Street Journal database in Tipster Disk2 (Harman 96) these early experiments is relatively small. We want to see if a much larger size of collection will make a difference. It is possible that a larger document col- lection might increase the frequency of most phrases, and thus alleviate the problem of low frequency. We only consider noun phrases and the sub- phrases derived from them. Specifically, we want to obtain the full modification structure of each noun phrase in the documents and query. From the view- point of NLP, the task is noun phrase parsing (i.e., the analysis of noun phrase structure). When the phrases are used only to supplement, not replace, the single words for indexing, some parsing errors may be tolerable. This means that the penalty for a parsing error may not be significant. The chal- lenge, however, is to be able to parse gigabytes of text in practically feasible time and as accurately as possible. The previous work taking on this chal- lenge includes (Evans et al. 91; Evans et al. 96; Evans and Zhai 96; Strzalkowski and Carballo 94; Strzalkowski et al. 95) among others. Evans et al. exploited the \"attestedness\" of subphrases to partially reveal the structure of long noun phrases (Evans et al. 91; Evans et al. 96). Strzałkowski et al. adopted a fast Tagged Text Parser (TTP) to ex- tract head modifier pairs including those in a noun phrase (Strzalkowski 92; Strzałkowski and Vauthey 92; Strzałkowski and Carballo 94; Strzalkowski et al. 95). In (Strzalkowski et al. 95), the structure of a noun phrase is disambiguated based on certain statistical heuristics, but there seems to be no ef- fort to assign a full structure to every noun phrase. Furthermore, manual effort is needed in constructing grammar rules. Thus, the approach in (Strzalkowski et al. 95) does not address the special need of scal- ability and robustness along with speed. Evans and Zhai explored a hybrid noun phrase analysis method and used a quite rich set of phrases for document in- dexing (Evans and Zhai 96). The indexing method was evaluated using the Associated Press newswire 89 (AP89) database in Tipster Disk1, and a general improvement of retrieval performance over the in- dexing with single words and full noun phrases was reported. However, the phrase extraction system as reported in (Evans and Zhai 96) is still not fast enough to deal with document collections measured by gigabytes.<sup>3</sup> We propose here a probabilistic model of noun phrase parsing. A fast statistical noun phrase parser has been developed based on the probabilistic model. The parser works fast and can be scaled up to parse gigabytes text within acceptable time.<sup>4</sup> Our goal is to generate different kinds of candidate syntactic <sup>3</sup>It was reported to take about 3.5 hours to process 20 MB documents <sup>4</sup>With a 133MH DEC alpha workstation, it is esti- mated to parse at a speed of 4 hours/gigabyte-text or 8 hours/gigabyte-nps, after 20 hours of training with 1 gigabyte text phrases from the structure of a noun phrase so that the effectiveness of different combinations of phrases and single words can be tested. 3 Fast Noun Phrase Parsing A fast and robust noun phrase parser is a key to the exploration of syntactic phrase indexing. Noun phrase parsing, or noun phrase structure analy- sis (also known as compound noun analysis), is itself an important research issue in computa- tional linguistics and natural language processing. Long noun phrases, especially long compound nouns such as \"information retrieval technique\", generally have ambiguous structures. For instance, \"informa- tion retrieval technique\" has two possible structures: \"[[information retrieval] technique)\" and \"[informa- tion [retrieval technique]]. A principal difficulty in noun phrase structure analysis is to resolve such structural ambiguity. When a large corpus is avail- able, which is true for an IR task, statistical prefer- ence of word combination or word modification can be a good clue for such disambiguation. As summa- rized in (Lauer 95), there are two different models for corpus-based parsing of noun phrases: the adja- cency model and the dependency model. The differ- ence between the two models can be illustrated by the example compound noun \"information retrieval technique\". In the adjacency model, the structure would be decided by looking at the adjacency as- sociation of \"information retrieval\" and \"retrieval technique\". \"information retrieval\" will be grouped first, if \"information retrieval\" has a stronger as- sociation than \"retrieval technique\", otherwise, \"re- trieval technique\" will be grouped first. In the de- pendency model, however, the structure would be decided by looking at the dependency between \"in- formation\" and \"retrieval\" (i.e., the tendency for \"information\" to modify \"retrieval\") and the depen- dency between \"information\" and \"technique\". If \"information\" has a stronger dependency associa- tion with \"retrieval\" than with \"technique\", \"infor- mation retrieval\" will be grouped first, otherwise, \"retrieval technique\" will be grouped first. The ad- jacency model dates at least from (Marcus 80) and has been explored recently in (Liberman and Sproat 92; Pustejovsky et al. 93; Resnik and Hearst 93; Lauer 95; Strzałkowski et al. 95; Evans and Zhai 96); The dependency model has mainly been stud- ied in (Lauer 94). Evans and Zhai (Evans and Zhai 96) use primarily the adjacency model, but the as- sociation score also takes into account some degree of dependency. Lauer (Lauer 95) compared the ad- jacency model and the dependency model for com- pound noun disambiguation, and concluded that the ⁵Strictly speaking, however, compound noun analysis is a special case of noun phrase analysis, but the same technique can often be used for both. dependency model provides a substantial advantage over the adjacency model. We now propose a probabilistic model in which the dependency structure, or the modification structure, of a noun phrase is treated as \"hidden\", similar to the tree structure in the probabilistic context-free grammar (Jelinek et al. 90). The basic idea is as follows. A noun phrase can be assumed to be generated from a word modification structure (i.e., a depen- dency structure). Since noun phrases with more than two words are structurally ambiguous, if we only observe the noun phrase, then the actual struc- ture that generates the noun phrase is \"hidden\". We treat the noun phrases with their possible structures as the complete data and the noun phrases occur- ring in the corpus (without the structures) as the observed incomplete data. In the training phase, an Expectation Maximization (EM) algorithm (Demp- ster et al. 77) can be used to estimate the parame- ters of word modification probabilities by iteratively maximizing the conditional expectation of the likeli- hood of the complete data given the observed incom- plete data and a previous estimate of the parameters. In the parsing phase, a noun phrase is assigned the structure that has the maximum conditional proba- bility given the noun phrase. Formally, assume that each noun phrase is gener- ated using a word modification structure. For exam- ple, \"information retrieval technique\" may be gener- ated using either the structure \"[X1[X2X3]]\" or the structure \"[[X1X2]X3]\". The log likelihood of gen- erating a noun phrase, given the set of noun phrases observed in a corpus NP = {np;} can be written as: L($) = ∑ c(npi)log ΣPo(npi, Sj) npiENP SES where, S is the set of all the possible modification structures; c(npi) is the count of the noun phrase npi in the corpus; and Po(npi, sj) gives the probability of deriving the noun phrase np; using the modification structure sj. With the simplification that generating a noun phrase from a modification structure is the same as generating all the corresponding word modification pairs in the noun phrase and with the assumption that each word modification pair in the noun phrase is generated independently, Po(npi, sj) can further be written as Po(npi, Sj) = P(sj) Π (u,v) ΕΜ(npi,sj) P(u, v)(u,v;npi,sj) where, M(npi, s;) is the set of all word pairs (u, v) in npi such that u modifies (i.e., depends on) v ac- cording to s;. c(u, v; npi, sj) is the count of the ⁶ For example, if npi is \"information retrieval tech- nique\", and sj is \"[[X1X2]X3]\", then, M(npi, Sj) = {(information, retrieval), (retrieval, technique)}. modification pairs (u, v) being generated when npi is derived from sj. Po(sj) is the probability of struc- ture sj; while Po(u, v) is the probability of generat- ing the word pair (u, v) given any word modifica- tion relation. Po(sj) and P₁(u, v) are subject to the constraint of summing up to 1 over all modification structures and over all possible word combinations respectively.7 The model is clearly a special case of the class of the algebraic language models, in which the proba- bilities are expressed as polynomials in the param- eters (Lafferty 95). For such models, the M-step in the EM algorithm can be carried out exactly, and the parameter update formulas are: Pn+1(u, v) = λ₁⁻¹ Σ c(npi) Σ Pn(sj|npi)c(u, v; npi, sj) npiENP sjES Pn+1(sk) = λ₂⁻¹ Σ c(npi)Pn(sk|npi) npiENP where, λ₁ and λ₂ are the Lagrange multipliers cor- responding to the two constraints mentioned above, and are given by the following formulas: λ₁ = Σ Σ c(npi) Σ Pn(sj|npi)c(u, v; npi, sj) (u,v)EWP npiENP sjES λ₂ = Σ Σ c(npi)Pn(sk|npi) sk ES npi ENP where, WP is the set of all possible word pairs. Pn(sj|npi) can be computed as: Pn(sj|npi) = Pn(npi, sj) Pn(npi) = Pn(npi, sj) Σsk ES Pn(npi, sk) = Pn(sj) Π(u,v)∈M(npi,sj) Pn(u, v)c(u,v;npi,sj) Σsk ES (Pn(sk) Π(u,v)∈M(npi,sk) Pn(u, v)c(u,v;npi,sk)) ⁷One problem with such simplification is that the model may generate a set of word modification pairs that do not form a noun phrase, although such \"illegal noun phrases\" are never observed. A better model would be to write the probability of each word modification pair as the conditional probability of the modifier (i.e., the modifying word) given the head (i.e., the word being modified). That is, Po(npi, sj) = Po(sj)Po(h(npi)|sj) Π (u,v)∈M(npi,sj) Pq(u|v)c(u,v;npi,sj) where h(npi) is the head (i.e., the last word) of the noun phrase npi (Lafferty 96). The EM algorithm ensures that L(n+1) is greater than L(n). In other words, every step of parameter update increases the likelihood. Thus, at the time of training, the parser can first randomly initialize the parameters, and then, iteratively update the param- eters according to the update formulas until the in- crease of the likelihood is smaller than some pre-set threshold.⁸ In the implementation described here, the maximum length of any noun phrase is limited to six. In practice, this is not a very tight limit, since simple noun phrases with more than six words are quite rare. Summing over all the possible structures for any noun phrase is computed by enumerating all the possible structures with an equal length as the noun phrase. For example, in the case of a three- word noun phrase, only two structures need to be enumerated. At the time of parsing noun phrases, the structure of any noun phrase np (S(np)) is determined by S(np) = argmaxₛ P(s|np) = argmaxₛ P(np|s)P(s) = argmaxₛ Π(u,v)∈M(np,s) P(u, v)P(s) We found that the parameters may easily be bi- ased owing to data sparseness. For example, the modification structure parameters naturally prefer left association to right association in the case of three-word noun phrases, when the data is sparse. Such bias in the parameters of the modification structure probability will be propagated to the word modification parameters when the parameters are iteratively updated using EM algorithm. In the ex- periments reported in this paper, an over-simplified solution is adopted. We simply fixed the modifica- tion structure parameter and assumed every depen- dency structure is equally likely. ⁹Fast training is achieved by reading all the noun phrase instances into memory. This forces us to split the whole noun phrase corpus into small chunks for training. In the experiments reported in this paper, we split the corpus into chunks of a size of around 4 megabytes. Each chunk has about 170,000 (or about 100,000 unique) raw multiple word noun phrases. The parameters estimated on each sub- corpus are then merged (averaged). We do not know how much the merging of parameters affects the pa- rameter estimation, but it seems that a majority of phrases are correctly parsed with the merged param- eter estimation, based on a rough check of the pars- ing results. With this approach, it takes a 133-MHz DEC Alpha workstation about 5 hours to train the parser over the noun phrases from a 250-megabyte corpus. ⁸For the experiments reported in this paper, the threshold is 2. An alternative way would be to keep the corpus in the disk. In this way, it is not necessary to split the corpus, unless it is extremely large. text corpus. Parsing is much faster, taking less than 1 hour to parse all noun phrases in the corpus of a 250-megabyte text. The parsing speed can be scaled up to gigabytes of text, even when the parser needs to be re-trained over the noun phrases in the whole corpus. However, the speed has not taken into account the time required for extracting the noun phrases for training. In the experiments described in the following section, the CLARIT noun phrase extractor is used to extract all the noun phrases from the 250-megabyte text corpus. After the training on each chunk, the estimation of the parameter of word modifications is smoothed to account for the unseen word modification pairs. Smoothing is made by \"dropping\" a certain number of parameters that have the least probabilities, tak- ing out the probabilities of the dropped parameters, and evenly distributing these probabilities among all the unseen word pairs as well as those pairs of the dropped parameters. It is unnecessary to keep the dropped parameters after smoothing, thus this method of smoothing helps reduce the memory over- load when merging parameters. In the experiments reported in the paper, nearly half of the total num- ber of word pairs seen in the training chunk were dropped. Since, word pairs with the least probabil- ities generally occur quite rarely in the corpus and usually represent semantically illegal word combina- tions, dropping such word pairs does not affect the parsing output so significantly as it seems. In fact, it may not affect the parsing decisions for the majority of noun phrases in the corpus at all. The potential parameter space for the probabilis- tic model can be extremely large, when the size of the training corpus is getting larger. One solution to this problem is to use a class-based model similar to the one proposed in (Brown et al. 92) or use pa- rameters of conceptual association rather than word association, as discussed in (Lauer 94) (Lauer 95). 4 Experiment Design We used the CLARIT commercial retrieval system as a retrieval engine to test the effectiveness of differ- ent indexing sets. The CLARIT system uses the vec- tor space retrieval model(Salton and McGill 83), in which documents and the query are all represented by a vector of weighted terms (either single words or phrases), and the relevancy judgment is based on the similarity (measured by the cosine measure) between the query vector and any document vector (Evans et al. 93; Evans and Lefferts 95; Evans et al. 96). The experiment procedure is described by Figure 1. First, the original database is parsed to form dif- ferent sets of indexing terms (say, using different combination of phrases). Then, each indexing set is passed to the CLARIT retrieval engine as a source document set. The CLARIT system is configured to accept the indexing set we passed as is to ensure that Original Document Set CLARIT NP Extractor Raw Noun Phrases Statistical NP Parser Phrase Extractor Indexing Term Set CLARIT Retrieval Engine Figure 1: Phrase indexing experiment procedure the actual indexing terms used inside the CLARIT system are exactly those generated. It is possible to generate three different kinds/levels of indexing units from a noun phrase: (1) single words; (2) head modifier pairs (i.e., any word pair in the noun phrase that has a linguis- tic modification relation); and (3) the full noun phrase. For example, from the phrase structure \"[[[heavy construction]=industry]]=group]\" (a real example from WSJ90), it is possible to generate the following candidate terms: SINGLE WORDS: heavy, construction, industry, group HEAD MODIFIERS: construction industry, industry group, heavy construction FULL NP: heavy construction industry group Different combinations of the three kinds of terms can be selected for indexing. In particular, the in- dexing set formed solely of single words is used as a baseline to test the effect of using phrases. In the ex- periments reported here, we generated four different combinations of phrases: WD-SET: single word only (no phrases, baseline) WD-HM-SET: --- single word + head modifier pair WD-NP-SET: single word + full NP WD-HM-NP-SET: single word + head modifier + full NP The results from these different phrase sets are discussed in the next section. 5 Results analysis We used, as our document set, the Wall Street Jour- nal database in Tipster Disk2 (Harman 96) the size of which is about 250 megabytes. We performed the experiments by using the TREC-5 ad hoc topics (i.e., TREC topics 251-300). Each run involves an automatic feedback with the top 10 documents re- turned from the initial retrieval. The CLARIT au- tomatic feedback is performed by adding terms from a query-specific thesaurus extracted from the top N documents returned from the initial retrieval(Evans and Lefferts 95). The results are evaluated using the standard measures of recall and precision. Re- call measures how many of the relevant documents have actually been retrieved. Precision measures how many of the retrieved documents are indeed rel- evant. They are calculated by the following simple formulas: Recall = number of relevant items retrieved total number of relevant items in collection Precision = number of relevant items retrieved total number of items retrieved We used the standard TREC evaluation package provided by Cornell University and used the judged- relevant documents from the TREC evaluations as the gold standard(Harman 94). In Table 1, we give a summary of the results and compare the three phrase combination runs with the corresponding baseline run. In the table, \"Ret-rel\" means \"retrieved-relevant\" and refers to the total number of relevant documents retrieved. \"Init Prec\" means \"initial precision\" and refers to the highest level of precision over all the points of recall. \"Avg Prec\" means \"average precision\" and is the average of all the precision values computed after each new relevant document is retrieved. It is clear that phrases help both recall and pre- cision when supplementing single words, as can be seen from the improvement of all phrase runs (WD- HM-SET, WD-NP-SET, WD-HM-NP-SET) over the single word run WD-SET. It can also be seen that when only one kind of phrase (either the full NPs or the head modifiers) is used to supplement the single words, each can lead to a great improvement in precision. However, when we combine the two kinds of phrases, the effect is a greater improvement in recall rather than precision. The fact that each kind of phrase can improve pre- cision significantly when used separately shows that Experiments WD-SET WD-HM-SET inc over WD-SET WD-NP-SET inc over WD-SET WD-HM-NP-SET inc over WD-SET Recall (Ret-Rel) Init Prec Avg Prec 0.56(597) 0.60(638) 0.4546 7% 0.5162 14% 0.2208 0.2402 9% 0.58(613) 4% 0.63(666) 13% 0.5373 18% 0.4747 4% 0.2564 16% 0.2285 3% Total relevant documents: 1064 Table 1: Effects of Phrases with feedback and TREC-5 topics these phrases are indeed very useful for indexing. The combination of phrases results in only a smaller precision improvement but causes a much greater increase in recall. This may indicate that more ex- periments are needed to understand how to combine and weight different phrases effectively. The same parsing method has also been used to generate phrases from the same data for the CLARIT NLP track experiments in TREC-5(Zhai et al. 97), and similar results were obtained, al- though the WD-NP-SET was not tested. The results in (Zhai et al. 97) are not identical to the results here, because they are based on two separate train- ing processes. It is possible that different training processes may result in slightly different parameter estimations, because the corpus is arbitrarily seg- mented into chunks of only roughly 4 megabytes for training, and the chunks actually used in different training processes may vary slightly. 6 Conclusions Information retrieval provides a good way to quanti- tatively (although indirectly) evaluate various NLP techniques. We explored the application of a fast statistical noun phrase parser to enhance document indexing in information retrieval. We proposed a new probabilistic model for noun phrase parsing and developed a fast noun phrase parser that can han- dle relatively large amounts of text efficiently. The effectiveness of enhancing document indexing with the syntactic phrases provided by the noun phrase parser was evaluated on the Wall Street Journal database in Tipster Disk2 using 50 TREC-5 ad hoc topics. Experiment results on this 250-megabyte document collection have shown that using differ- ent kinds of syntactic phrases provided by the noun phrase parser to supplement single words for index- ing can significantly improve the retrieval perfor- mance, which is more encouraging than many early experiments on syntactic phrase indexing. Thus, us- ing selective NLP, such as the noun phrase parsing technique we proposed, is not only feasible for use in information retrieval, but also effective in enhancing the retrieval performance. 10 10 Whether such syntactic phrases are more effective than simple statistical phrases (e.g., high frequency word --- There are two lines of future work: First, the results from information retrieval ex- periments often show variances on different kinds of document collections and different sizes of collec- tions. It is thus desirable to test the noun phrase parsing technique in other and larger collections. More experiments and analyses are also needed to better understand how to more effectively combine different phrases with single words. In addition, it is very important to study how such phrase effects interact with other useful IR techniques such as rel- evancy feedback, query expansion, and term weight- ing. Second, it is desirable to study how the parsing quality (e.g., in terms of the ratio of phrases parsed correctly) would affect the retrieval performance. It is very interesting to try the conditional probabil- ity model as mentioned in a footnote in section 3 The improvement of the probabilistic model of noun phrase parsing may result in phrases of higher qual- ity than the phrases produced by the current noun phrase parser. Intuitively, the use of higher qual- ity phrases might enhance document indexing more effectively, but this again needs to be tested. 7 Acknowledgments The author is especially grateful to David A. Evans for his advising and supporting of this work. Thanks are also due to John Lafferty, Nataša Milić-Frayling, Xiang Tong, and two anonymous reviewers for their useful comments. Naturally, the author alone is re- sponsible for all the errors. References Belkin, N., and Croft, B. 1987. Retrieval techniques. In: Williams, Martha E.(Ed.), Annual Review of Information Science Technology, Vol. 22. Amster- dam, NL: Elsevier Science Publishers. 1987. 110- 145. Brown, P. et al. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4), December, 1992. 467-479. Dempster, A. P. et al. 1977. Maximum likelihood from incomplete data via the EM algorithm. Jour- nal of the Royal Statistical Society, 39 B, 1977. 1-38. Evans, D. A., Ginther-Webster, K., Hart, M., Lef- ferts, R., Monarch, I., 1991. Automatic indexing using selective NLP and first-order thesauri. In: A. Lichnerowicz (ed.), Intelligent Text and Im- age Handling. Proceedings of a Conference, RIAO '91. Amsterdam, NL: Elsevier. 1991. pp. 624-644. Evans, D. A., Lefferts, R. G., Grefenstette, G., Han- derson, S. H., Hersh, W. R., and Archbold, A. bigrams) remains to be tested. A. 1993. CLARIT TREC design, experiments, and results. In: Donna K. Harman (ed.), The First Text Retrieval Conference (TREC-1). NIST Special Publication 500-207. Washington, DC: U.S. Government Printing Office, 1993. pp. 251- 286; 494-501. Evans, David A. and Lefferts, Robert G. 1995. CLARIT-TREC experiments, Information Pro- cessing and Management, Vol. 31, No. 3, 1995. 385-395. Evans, D., Milić-Frayling, N., and Lefferts, R. 1996. CLARIT TREC-4 Experiments, in Donna K. Har- man (Ed.), The Fourth Text Retrieval Confer- ence (TREC-4). NIST Special Publication 500- 236. Washington, DC: U.S. Government Printing Office, 1996. pp. 305-321. Evans, D. and Zhai, C. 1996. Noun-phrase analy- sis in unrestricted text for information retrieval. Proceedings of the 34th Annual meeting of Associ- ation for Computational Linguistics, Santa Cruz, University of California, June 24-28, 1996. 17-24. Fagan, Joel L. 1987. Experiments in Automatic Phrase Indexing for Document Retrieval: A Com- parison of Syntactic and Non-syntactic methods, PhD thesis, Dept. of Computer Science, Cornell University, Sept. 1987. Harman, D. 1994. The Second Text Retrieval Con- ference (TREC-2), NIST Special publication 500- 215. National Institute of Standards and Technol- ogy, 1994. Harman, D. 1996. TREC 5 Conference Notes, Nov. 20-22, 1996. Jelinek, F., Lafferty, J.D., and Mercer, R. L. 1990. Basic methods of probabilistic context free gram- mars. Yorktown Heights, N.Y.: IBM T.J. Wat- son Research Center, 1990. Research report RC. 16374. Lafferty, J. 1995. Notes on the EM Algorithm, In- formation Theory course notes, Carnegie Mellon University. Lafferty, J. 1996. Personal Communications. Lauer, Mark. 1994. Conceptual association for com- pound noun analysis. Proceedings of the 32nd An- nual Meeting of the Association for Computa- tional Linguistics, Student Session, Las Cruces, NM, 1994. 337-339. Lauer, Mark. 1995. Corpus statistics meet with the noun compound: Some empirical results. Proceed- ings of the 33th Annual Meeting of the Association for Computational Linguistics, 1995. Lewis, D. 1991. Representation and Learning in In- formation Retrieval. Ph.D thesis, COINS Techni- cal Report 91-93, Univ. of Massachusetts, 1991. Lewis, D. and Sparck Jones, K. 1996. Applications of natural language processing in information re- trieval. Communications of ACM, Vol. 39, No. 1, 1996, 92-101. Liberman, M. and Sproat, R. 1992. The stress and structure of modified noun phrases in English. In: Sag, I. and Szabolcsi, A. (Eds.), Lexical Matters, CSLI Lecture Notes No. 24. University of Chicago Press, 1992. 131-181. Marcus, Mitchell. 1980. A Theory of Syntactic Rec ognition for Natural Language. MIT Press, Cam- bridge, MA, 1980. Pustejovsky, J., Bergler, S., and Anick, P. 1993. Lex- ical semantic techniques for corpus analysis. In: Computational Linguistics, Vol. 19 (2), Special Is- sue on Using Large Corpora II, 1993. 331-358. Resnik, P. and Hearst, M. 1993. Structural ambi- guity and conceptual relations. In: Proceedings of the Workshop on Very Large Corpora: Academic and Industrial Perspectives, June 22, 1993. Ohio State Un iversity. 58-64. Salton, G. and McGill, M. 1983. Introduction to Modern Information Retrieval, New York, NY: McGraw-Hill, 1983. Smeaton, Alan F. 1992. Progress in application of natural language processing to information re- trieval. The Computer Journal, Vol. 35, No. 3, 1992. 268-278. Strzalkowski, T. 1992. ТТP: A fast and robust parser for natural language processing. Proceed- ings of the 14th International Conference on Com- putational Linguistics (COLING), Nantes, France, July, 1992. 198-204. Strzalkowski, T. and Vauthey, B. 1992. Information retrieval using robust natural language processing. Proceedings of the 30th ACL Meeting, Neward, DE, June-July, 1992. 104-111. Strzalkowski, T. and Carballo, J. 1994. Recent de- velopments in natural language text retrieval. In: Harman, D. (Ed.), The Second Text Retrieval Conference (TREC-2), NIST Special Publication 500-215. 1994. 123-136. Strzalkowski, T. 1995. Natural language informa- tion retrieval. Information Processing and Man- agement. Vol. 31, No. 3, 1995. 397-417. Strzalkowski, T. et al. 1995. Natural language in- formation retrieval: TREC-3 report. In: Har- man, D. (Ed.), The Third Text Retrieval Con- ference (TREC-3), NIST Special Publication 500- 225. 1995. 39-53. Strzałkowski, T. et al. 1996. Natural language in- formation retrieval: TREC-4 report. In: Har- man, D. (Ed.), The Fourth Text Retrieval Con- ference (TREC-4). NIST Special Publication 500- 236. Washington, DC: U.S. Government Printing Office, 1996. pp. 245-258. Zhai, C., Tong, X., Milić-Frayling, N., and Evans D. 1997. Evaluation of syntactic phrase indexing CLARIT TREC5 NLP track report. to appear in The Fifth Text Retrieval Conference (TREC-5), NIST special publication, 1997, forthcoming."
  },
  {
    "title": "Language Generation for Multimedia Healthcare Briefings",
    "abstract": "This paper identifies issues for language generation that arose in developing a multimedia interface to healthcare data that includes coordinated speech, text, and graphics. In order to produce brief speech for time-pressured caregivers, the system both combines related information into a single sentence and uses abbreviated references in speech when an unambiguous textual reference is also used. Finally, due to the temporal nature of the speech, the language generation module needs to communicate information about the ordering and duration of references to other temporal media, such as graphics, in order to allow for coordination between media.",
    "content": "1 Introduction In a hospital setting it can be difficult for care- givers to obtain needed information about patients in a timely fashion. In a Cardiac Intensive Care Unit (ICU), communication regarding patient sta- tus is critical during the hour immediately follow- ing a coronary arterial bypass graft (CABG). It is at this critical point, when care is being trans- ferred from the Operating Room (OR) to the ICU and monitoring is at a minimum, that the pa- tient is most vulnerable to delays in treatment. During this time, there are a number of care- givers who need information about patient status and plans for care, including the ICU nurses who must prepare for patient arrival, the cardiologist who is off-site during the operation, and residents and attendings who will aid in determining post- operative care. The only people who can provide this information are those who were present dur- ing surgery and they are often too busy attending Desmond A. Jordan* Barry A. Allen** Dept. of Anesthesiology* and Medical Informatics Dept.** College of Physicians and Surgeons Columbia University New York, NY 10032 to the patient to communicate much detail. To address this need, we are developing a mul- timedia briefing system, MAGIC (Multimedia Ab- stract Generation for Intensive Care), that takes as input online data collected during the surgical operation as well as information stored in the main databases at Columbia Presbyterian Medical Cen- ter (Roderer and Clayton, 1992). MAGIC gener- ates a multimedia briefing that integrates speech, text, and animated graphics to provide an update on patient status (Dalal et al., 1996a). In this pa- per, we describe the issues that arise for language generation in this context: • Conciseness: The generation process must make coordinated use of speech and text to produce an overview that is short enough for time pressured caregivers to follow, but un- ambiguous in meaning. • Media specific tailoring: Generation must take into account that one output medium is speech, as opposed to the more usual writ- ten language, producing wording and sen- tence structure appropriate for spoken lan- guage. • Coordination with other media: The lan- guage generation process must produce enough information so that speech and text can be coordinated with the accompanying graphics. In the following sections, we first provide an overview of the full MAGIC architecture and then describe the specific language generation issues that we address. We close with a discussion of our current directions. 2 System Overview MAGIC's architecture is shown in Figure 1. MAGIC exploits the extensive online data avail- Data Server Data Filter General Content Planner Media Allocator Medical Databases Graphics Generator pen Coordinator Media Conductor Lexical Chooser SURGE AT&T TTS Semantic Hypotactic Paratactic Referring Component Component Component Speech generator Figure 1: MAGIC system architecture. able through Columbia Presbyterian Medical Cen- ter (CPMC) as its source of content for its brief- ing. Operative events during surgery are moni- tored through the LifeLog database system (Mod- ular Instruments Inc.), which polls medical de- vices (ventilators, pressure monitors and alike) ev- ery minute from the start of the case to the end recording information such as vital signs. In ad- dition, physicians (anesthesiologist and anesthe- sia residents) enter data throughout the course of the patient's surgery, including start of cardiopul- monary bypass and end of bypass as well as sub- jective clinical factors such as heart sounds and breath sounds that cannot be retrieved by med- ical devices. In addition, CPMC main databases provide information from the online patient record (e.g., medical history). From this large body of information, the data filter selects information that is relevant to the bypass surgery and patient care in the ICU. MAGIC's content planner then uses a multimedia plan to select and partially order information for the presentation, taking into account the caregiver the briefing is intended for (nurse or physician). The media allocator allocates content to media, and finally, the media specific generators realize content in their own specific media (see (Zhou and Feiner, 1997) for details on the graphics genera- tor). A media coordinator is responsible for en- suring that spoken output and animated graphics are temporally coordinated. Within this context, the speech generator re- ceives as input a partially ordered conceptual rep- resentation of information to be communicated. Expression The generator includes a micro-planner, which is responsible for ordering and grouping information into sentences. Our approach to micro-planning integrates a variety of different types of operators for aggregation information within a single sen- tence. Aggregation using semantic operators is enabled through access to the underlying domain hierarchy, while aggregation using linguistic op- erators (e.g., hypotactic operators, which add in- formation using modifiers such as adjectives, and paratactic operators which create, for example, conjunctions) is enabled through lookahead to the lexicon used during realization. The speech generator also includes a re- alization component, implemented using the FUF/SURGE sentence generator (Elhadad, 1992; Robin, 1994), which produces the actual language to be spoken as well as textual descriptions that are used as labels in the visual presentation. It performs lexical choice and syntactic realization. Our version of the FUF/SURGE sentence gener- ator produces sentences annotated with prosodic information and pause durations. This output is sent to a speech synthesizer in order to produce final speech. (Currently, we are using AT&T Bell Laboratories' Text To Speech System). Our use of speech as an output medium pro- vides an eyes-free environment that allows care- givers the opportunity to turn away from the dis- play and continue carrying out tasks involving pa- tient care. Speech can also clarify graphical con- ventions without requiring the user to look away from the graphics to read an associated text. Cur- rently, communication between OR caregivers and ICU caregivers is carried out orally in the ICU when the patient is brought in. Thus, the use of speech within MAGIC models current practice. Future planned evaluations will examine caregiver satisfaction with the spoken medium versus text. 3 Issues for Language Generation In the early stages of system development, a pri- mary constraint on the language generation pro- cess was identified during an informal evalua- tion with ICU nurses and residents (Dalal et al., 1996a). Due to time constraints in carrying out tasks, nurses, in particular, noted that speech takes time and therefore, spoken language output should be brief and to the point, while text, which is used to annotate the graphical illustration, may provide unambiguous references to the equipment and drugs being used. In the following sections, we show how we meet this constraint both in the speech content planner, which organizes the con- tent as sentences, and in the speech sentence gen- erator, which produces actual language. In all of the language generation components, the fact that spoken language is the output medium and not written language, influences how generation is carried out. We note this influence on the generation process throughout the section. An example showing the spoken output for a given patient and a screen shot at a single point in the briefing is shown in Figure 3. In actual output, sentences are coordinated with the corresponding part of the graphical illus- tration using highlighting and other graphical ac- tions. In the paper, we show the kinds of modifica- tions that it was necessary to make to the language generator in order to allow the media coordinator to synchronize speech with changing graphics. 3.1 Speech Micro-Planner The speech micro-planner is given as input a set of information that must be conveyed. In order to ensure that speech is brief and yet still conveys the necessary information, the speech micro-planner attempts to fit more information into individual sentences, thereby using fewer words. Out of the set of propositions given as input, the micro-planner selects one proposition to start with. It attempts to include as many other propo- sitions as it can as adjectives or other modifiers of information already included. To do this, from the remaining propositions, it selects a proposition which is related to one of the propositions already selected via its arguments. It then checks whether it can be lexicalized as a modifier by looking ahead S. Jones MRN: 4455667 History: Hypertension Surgeon: Dr. Smith Age: 80 Gender: Female Swan-Ganz with Cordis DRIPS Ventricular Pacemaker Peripheral IV BOLUS Balloon Pump Diabetes Operation: CABG Ventilator SETUP Swan-Gauz: PAD 14 CO. 28 Peripheral IV BLOOD Arterial Line Voice: Ms. Jones is an 80 year old, hypertensive, dia- betic, female patient of Dr. Smith undergoing CABG. Presently, she is 30 minutes post-bypass and will ar- rive in the unit shortly. The existing infusion lines are two IVs, an arterial line, and a Swan-Ganz with Cordis. The patient has received massive vasotonic therapy, massive cardiotonic therapy, and massive- volume blood-replacement therapy. Drips in proto- col concentrations are nitroglycerin, levophed, dobu- tamine, epinephrine, and inocor... Figure 2: Multimedia presentation generated by MAGIC to the lexicon used by the lexical chooser to deter- mine if such a choice exists. The syntactic con- straint is recorded in the intermediate form, but the lexical chooser may later decide to realize the proposition by any word of the same syntactic cat- egory or transform a modifier and a noun into a semantic equivalent noun or noun phrase. The micro-planner uses information from the lexicon to determine how to combine the propo- sitions together while satisfying grammatical and lexical constraints. Semantic aggregation is the first category of operators applied to the set of re- lated propositions in order to produce concise ex- pressions, as shown in lower portion of Fig. 1. Us- ing ontological and lexical information, it can re- duce the number of propositions by replacing them with fewer propositions with equivalent meanings. While carrying out hypotactic aggregation opera- tors, a current central proposition is selected and the system searches through the un-aggregated propositions to find those that can be realized as adjectives, prepositional phrases and relative clauses, and merges them in. After hypotactic ag- gregation, the un-aggregated propositions are then combined using paratactic operators, such as ap- positions or coordinations. X is a patient. X has property last name = Jones. X has property age = 80 years old. X has property history = hypertension property. X has property history = diabetes property. X has property gender = female. X has property surgery = CABG. X has property doctor = Y. Y has property last name = Smith. Figure 3: propositions for the first sentence In the first sentence of the example output, the micro-planner has combined the 9 input proposi- tions shown above in Figure 3 into a single sen- tence: Ms Jones is an 80 year old hypertensive, diabetic female patient of Dr. Smith undergoing CABG. In this example this is possible, in part be- cause the patient's medical history (diabetes and hypertension) can be realized as adjectives. In another example, \"Mr. Smith is a 60 year old male patient of Dr. Jordan undergoing CABG. He has a medical history of transient ischemic attacks, pulmonary hypertension, and peptic ul- cers.\", the medical history can only be realized as noun phrases, thus requiring a second sentence and necessarily, more words. 3.2 Speech Sentence Generator The speech sentence generator also contributes to the goal of keeping spoken output brief, but in- formative. In particular, through its lexical choice component, it selects references to medical con- cepts that are shorter and more colloquial than the text counterpart. As long as the text label on the screen is generated using the full, unam- biguous reference, speech can use an abbreviated expression. For example, when referring to the de- vices which have been implanted, speech can use the term \"pacemaker\" so long as the textual label specifies it as \"ventricular pacemaker\". Similarly, MAGIC uses \"balloon pump\" in speech instead of \"intra-aortic balloon pump\", which is already shown on the screen. In order to do this, lexical choice in both me- dia must be coordinated. Lexical choice for text always selects the full reference, but lexical choice for speech must check what expression the text generator is using. Basically, the speech lexical chooser must check what attributes the text gen- erator includes in its reference and omit those. Finally, we suspect that the syntactic structure of sentences generated for spoken output should be simpler than that generated for written language. This hypothesis is in conflict with our criteria for generating as few sentences as possible, which of- ten results in more complex sentences. This is in part acceptable due to the fact that MAGIC's output is closer to formal speech, such as one might find in a radio show, as opposed to infor- mal conversation. It is, after all, a planned one- way presentation. In order to make the generated sentences more comprehensible, however, we have modified the lexical chooser and syntactic gener- ator to produce pauses at complex constitutions to increase intelligibility of the output. Currently, we are using a pause prediction algorithm which utilizes the sentence's semantic structure, syntac- tic structure as well as the linear phrase length constraint to predict the pause position and rela- tive strength. Our current work involves modify- ing the FUF/SURGE language generation package so that it can produce prosodic and pause infor- mation needed as input to a speech synthesizer, to produce a generic spoken language sentence gen- erator. 3.3 Producing Information for Media Coordination Language generation in MAGIC is also affected by the fact that language is used in the context of other media as well. While there are specific modules in MAGIC whose task is concerned with utilizing multiple media, media coordination af- fects the language generation process also. In par- ticular, in order to produce a coordinated presen- tation, MAGIC must temporally coordinate spo- ken language with animated graphics, both tem- poral media. This means that spoken references must be coordinated with graphical references to the same information. Graphical references may include highlighting of the portion of the illustra- tion which refers to the same information as speech or appearance of new information on the screen. Temporal coordination involves two problems: en- suring that ordering of spoken references to infor- mation is compatible with spatial ordering of the graphical actions and synchronizing the duration of spoken and graphical references (Dalal et al., 1996b). In order to achieve this, language generation must provide a partial ordering of spoken refer- ences at a fairly early point in the generation pro- cess. This ordering indicates its preference for how spoken references are to be ordered in the output linear speech in accordance with both graphical and presentation constraints. For example, in the first sentence of the example shown in Figure 3, the speech components have a preference for med- ical history (i.e., \"hypertensive, diabetic\") to be presented before information about the surgeon, as this allows for more concise output. It would be possible for medical history to be presented after all other information in the sentence by generat- ing a separate sentence (e.g., \"She has a history of hypertension and diabetes.\") but this is less preferable from the language point of view. In our work, we have modified the structure of the lexical chooser so that it can record its decisions about or- dering, using partial ordering for any grammatical variation that may happen later when the final syntactic structure of the sentence is generated. These are then sent to the media coordinator for negotiating with graphics an ordering that is com- patible to both. Details on the implementation of this negotiation are presented in (Dalal et al., 1996b) and (Pan and McKeown, 1996). In order to synchronize duration of the spo- ken and graphical references, the lexical chooser invokes the speech synthesizer to calculate the du- ration of each lexical phrase that it generates. By maintaining a correspondence between the refer- ential string generated and the concepts that those referential actions refer to, negotiation with graph- ics has a common basis for communication. In order to provide for more flexible synchronization, the speech sentence generator includes facilities for modifying pauses if conflicts with graphics dura- tions arise (see (Pan and McKeown, 1996) for de- tails). 4 Related Work There is considerable interest in producing fluent and concise sentences. EPICURE (Dale, 1992), PLANDOC(Kukich et al., 1994; Shaw, 1995), and systems developed by Dalianis and Hovy (Dalia- nis and Hovy, 1993) all use various forms of con- junction and ellipsis to generate more concise sen- tences. In (Horacek, 1992) aggregation is per- formed at text-structure level. In addition to con- joining VP and NPs, FLOWDoc(Passonneau et al., 1996) uses ontological generalization to com- bine descriptions of a set of objects into a more general description. Based on a corpus analy- sis in the basketball domain, (Robin, 1994) cat- alogued a set of revision operators such as adjoin and nominalization in his system STREAK. Un- like STREAK, MAGIC does not use revision to combine information in a sentence. Generating spoken language from meanings or concepts (Meaning to Speech, MTS) is a new topic and only a few such systems were developed in recent years. In (Prevost, 1995) and (Steedman, 1996), they explore a way to generate spoken lan- guage with accurate contrastive stress based on in- formation structure and carefully modeled domain knowledge. In (Davis and Hirschberg, 1988), spo- ken directions are generated with richer intonation features. Both of these systems took advantage of the richer and more precise semantic information that is available during the process of Meaning to Speech production. 5 Conclusions and Current Directions The context of multimedia briefings for access to healthcare data places new demands on the lan- guage generation process. Language generation in MAGIC addresses its user's needs for a brief, yet unambiguous, briefing by coordinating spoken lan- guage with the accompanying textual references in the graphical illustration and by combining infor- mation into fewer sentences. It also must explicitly represent its decisions as it generates a sentence in order to provide information to the media coordi- nator for negotiation with graphics. Our development of MAGIC is very much an ongoing research project. We are continuing to work on improved coordination of media, use of the syntactic and semantic structure of generated language to improve the quality of the synthesized speech, and analysis of a corpus of radio speech to identify characteristics of formal, spoken language. 6 Acknowledgments MAGIC is a joint project which involves the Nat- ural Language Processing group (the authors), the Graphics and User Interface group (Steve Feiner, Michelle Zhou and Tobias Hollerer), the Knowledge Representation group (Mukesh Dalal and Yong Feng) in the Department of Com- puter Science of Columbia University and Dr. Desmond Jordan and Prof. Barry Allen at the Columbia College of Physicians and Surgeons (au- thors). This work is supported by DARPA Con- tract DAAL01-94-K-0119, the Columbia Univer- sity Center for Advanced Technology in High Performance Computing and Communications in Healthcare (funded by the New York State Sci- ence and Technology Foundation) and NSF Grants GER-90-2406. References M. Dalal, S. Feiner, K. McKeown, D. Jordan, B. Allen, and Y. alSafadi. 1996a. Magic: An experimental system for generating multimedia briefings about post-bypass patient status. In Proceedings of American Medical Informatics Association 1996 Fall. M. Dalal, S. Feiner, K. McKeown, S. Pan, M. Zhou, T. Hollerer, J. Shaw, Y. Feng, and J. Fromer. 1996b. Negotiation for automated generation of temporal multimedia presenta- tions. In Proceedings of ACM Multimedia '96. R. Dale. 1992. Generating Referring Expressions: Constructing Descriptions in a Domain of Ob- jects and Processes. MIT Press, Cambridge, ΜΑ. H. Dalianis and E. Hovy. 1993. Aggregation in natural language generation. In Proceedings of the Fourth European Workshop on Natural Language Generation, pages 67-78, Pisa, Italy. J. Davis and J. Hirschberg. 1988. Assigning in- tonational features in synthesized spoken dis- course. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pages 187-193, Buffalo, New York. M. Elhadad. 1992. Using argumentation to con- trol lexical choice: A functional unification- based approach. Ph.D. thesis, Computer Sci- ence Department, Columbia University. H. Horacek. 1992. An integrated view of text planning. In Aspects of Automated Natural Language Generation, pages 29-44. Springer- Verlag. K. Kukich, K. McKeown, and J. Shaw. 1994. Practical issues in automatic documentation generation. In Proceedings of the 4th ACL Conference on Applied Natural Language Pro- cessing, pages 7-14, Stuttgart. S. Pan and K. McKeown. 1996. Spoken language generation in a multimedia system. In Proceed- ings of ICSLP 96, volume 1, pages 374-377, Philadelphia, PA. R. Passonneau, K. Kukich, V. Hatzivassiloglou, L. Lefkowitz, and H. Jing. 1996. Gener- ating summaries of work flow diagrams. In Proceedings of the International Conference on Natural Language Processing and Industrial Applications, pages 204-210, New Brunswick, Canada, June. Univeristy of Moncton. S. Prevost. 1995. A Semantics of Contrast and In- formaiton Structure for Specifying Intonation in Spoken Language Generation. Ph.D. thesis, University of Pennsylvania. J. Robin. 1994. Revision-Based Generation of Natural Language Summaries Providing His- torical Background. Ph.D. thesis, Computer Science Department, Columbia University. N. Roderer and P. Clayton. 1992. Iaims at columbia presbyterian medical center: Accom- plishments and challenges. In Bull. Am. Med. Lib. Assoc., pages 253-262. J. Shaw. 1995. Conciseness through aggregation in text generation. In Proceedings of the 33rd ACL (Student Session), pages 329-331. M. Steedman. 1996. Representing discourse in- formationn for spoken dialogue generation. In Proceedings of ISSD 96, pages 89-92, Philadel- phia, PA. M. Zhou and S. Feiner. 1997. Top-down hier- archical planning of coherent visual discourse. In Proc. IUI '97 (1997 Int. Conf. on Intelligent User Interfaces), Orlando, FL, January 6-9."
  },
  {
    "title": "An Intelligent Multilingual Information Browsing and Retrieval System Using Information Extraction",
    "abstract": "In this paper, we describe our multilingual (or cross-linguistic) information browsing and retrieval system, which is aimed at monolingual users who are interested in information from multiple language sources. The system takes advantage of information extraction (IE) technology in novel ways to improve the accuracy of cross-linguistic retrieval and to provide innovative methods for browsing and exploring multilingual document collections. The system indexes texts in different languages (e.g., English and Japanese) and allows the users to retrieve relevant texts in their native language (e.g., English). The retrieved text is then presented to the users with proper names and specialized domain terms translated and hyperlinked. Moreover, the system allows interactive information discovery from a multilingual document collection.",
    "content": "1 Introduction More and more multilingual information is available on-line every day. The World Wide Web (WWW), for example, is becoming a vast depository of mul- tilingual information. However, monolingual users can currently access information only in their na- tive language. For example, it is not easy for a monolingual English speaker to locate necessary in- formation written in Japanese. The users would not know the query terms in Japanese even if the search engine accepts Japanese queries. In addition, even when the users locate a possibly relevant text in Japanese, they will have little idea about what is in the text. Outputs of off-the-shelf machine trans- lation (MT) systems are often of low-quality, and even \"high-end\" MT systems have problems partic- ularly in translating proper names and specialized domain terms, which often contain the most critical information to the users. In this paper, we describe our multilingual (or cross-linguistic) information browsing and retrieval system, which is aimed at monolingual users who are interested in information from multiple language sources. The system takes advantage of information extraction (IE) technology in novel ways to improve the accuracy of cross-linguistic retrieval and to pro- vide innovative methods for browsing and exploring multilingual document collections. The system in- dexes texts in different languages (e.g., English and Japanese) and allows the users to retrieve relevant texts in their native language (e.g., English). The retrieved text is then presented to the users with proper names and specialized domain terms trans- lated and hyperlinked. The system also allows the user in their native language to browse and discover information buried in the database derived from the entire document collection. 2 System Description The system consists of the Indexing Module, the Client Module, the Term Translation Module, and the Web Crawler. The Indexing Module creates and loads indices into a database while the Client Module allows browsing and retrieval of information in the database through a Web browser-based graphical user interface (GUI). The Term Translation Mod- ule is bi-directional; it dynamically translates user queries into target foreign languages and the indexed terms in retrieved documents into the user's native language. The Web Crawler can be used to add tex- tual information from the WWW; it fetches pages from user-specified Web sites at specified intervals, and queues them up for the Indexing Module to in- gest regularly. For our current application, the system indexes names of people, entities, and locations, and scien- tific and technical (S&T) terms in both English and Japanese texts, and allows the user to query and browse the database in English. When Japanese texts are retrieved, indexed terms are translated into English. This system is designed to expand to other lan- guages besides English and Japanese and other do- mains beyond S&T terms. Moreover, the English- centric browsing and retrieval mode can be switched according to the users' language preference so that, for example, a Japanese user can query and browse English documents in Japanese. 2.1 The Intelligent Indexing Module The Indexing Module indexes names of people, enti- ties, and locations and a list of scientific and techni- cal (S&T) terms using state-of-the-art IE technol- ogy. It uses different configurations of the same fast indexing engine called NameTagTM for differ- ent languages. Two separate configurations (\"index- ing servers\") are used for English and Japanese, and how the English and Japanese indexing servers work is described in (Krupka, 1995; Aone, 1996). In the Sixth Message Understanding Conference (MUC-6), the English system was benchmarked against the Wall Street Journal blind test set for the name tagging task, and achieved a 96% F- measure, which is a combination of recall and preci- sion measures (Adv, 1995),. Our internal testing of the Japanese system against blind test sets of various Japanese newspaper articles indicates that it achieves from high-80 to low-90% accuracy, de- pending on the types of corpora. Indexing names in Japanese texts is usually more challenging than English for two main reasons. First, there is no case distinction in Japanese, whereas English names in newspapers are capitalized, and capitalization is a very strong clue for English name tagging. Sec- ond, Japanese words are not separated by spaces and therefore must be segmented into separate words be- fore the name tagging process. As segmentation is not 100% accurate, segmentation errors can some- times cause name tagging rules not to fire or to mis- fire. Indexing of names is particularly useful in the Japanese case as it can improve overall segmenta- tion and thus indexing accuracy. In English, since words are separated by spaces, there is no issue of in- dexing accuracy for individual words. On the other hand, in languages like Japanese, where word bound- aries are not explicitly marked by spaces, indexing accuracy of individual words depends on accuracy of word segmentation. However, most segmentation algorithms are more likely to make errors on names, as these are less likely to be in the lexicons. Name tagging can reduce such errors by identifying names as single units. Both indexing servers are \"intelligent\" because they identify and disambiguate names with high speed and accuracy. They identify names in texts dynamically rather than relying on finite lists of names. Thus, they can identify names which they have never seen before. In addition, they can dis- ambiguate types of names so that a person named \"Washington\" is distinguished from a place called Washington, and a company \"Apple\" can be dis- tinguished from a common noun \"apple.\" In addi- tion, they can generate aliases of names automat- ically (e.g., \"ANA\" for \"All Nippon Airline\") and link variants of names within a document. As the indexing servers process texts, the in- dexed terms are stored in a relational database with their semantic type information (person, entity, place, S&T term) and alias information along with such meta data as source, date, language, and fre- quency information. The system can use any ODBC (Open DataBase Connectivity)-compliant database, and form-based Boolean queries from the Client Module, similar to those seen in any Web search engine, are translated into standard SQL queries automatically. We have decided to use commercial databases for our applications as we are not only in- dexing strings of terms but also adding much richer information on indexed terms available through the use of IE technology. Furthermore, we plan to apply data-mining algorithms to the resulting databases to conduct advanced data analysis and knowledge discovery. 2.2 The Client Module The Client Module lets the user both retrieve and browse information in the database through the Web browser-based GUI. In the query mode (cf. Fig- ure 1), a form-based Boolean query issued by a user is automatically translated into an SQL query, and the English terms in the query are sent to the Term Translation Module. The Client Module then re- trieves documents which match either the original English query or the translated Japanese query. As the indices are names and terms which may con- sist of multiple words (e.g, \"Bill Clinton,\" \"personal computer\"), the query terms are delimited in sep- arate boxes in the form, making sure no ambiguity occurs in both translation and retrieval. The user has the choice of selecting the sources (e.g, Washing- ton Post, Nikkei Newspaper, Web pages), languages (e.g., English, Japanese, or both), and specific date ranges of documents to constrain queries. In the browsing mode, the Client Module allows the user to browse the information in the database in various ways. As an overview of the database con- tent, the Client Module lets the user browse the top 25 and 50 most frequent entity, person, and loca- tion names and S&T terms in the database (cf. Fig- ure 4). Once the user selects a particular document for viewing, the client sends the document to an ap- propriate (i.e., English or Japanese) indexing server for creating hyperlinks for the indexed terms and in the case of a Japanese document, sends the indexed terms to the Term Translation Module to translate the Japanese terms into English. The result that the user browses is a document each of whose indexed terms are hyperlinked to other documents contain- ing the same indexed terms (cf. Figure 2). Since hy- N Search Specify search terms, an optional boolean expression, and appropriate source, language, date, and result sort options. Click here for example searches. Term Aliases Type Language 1 BILL CLINTON Person Japanese English 2 WASHINGTON Place Sources Washington Post 3 PERSONAL COMPUTER Technology loomi Nikkei 4 Any www 5 Any Dates 01/01/1991 to 02/10/1991 6 Any Sort by Date Boolean expression ex (1 OR 2) AND NOT 3 1 AND 2 AND 3 Clear Form Submit Search Figure 1: The Search Screen Yomiuri 1996-07-31 米国 (U.S.A.)の仲介不可欠あ エジプト (EGYPT) 大統領 【ワシントン(WASHINGTON DC) 30日飯山雅史(MASASHIIYAMA)】訪米 (U.S.A.) 中のムバラク (Mubarak)・エジプト (EGYPT)大領は三十日、ホワイトハウス (WHITE HOUSE)でクリントン (CLINTON) * (U.S.A.)大統領と会談し、イスラエル (ISRAEL)の強硬派ネタニヤフ (Netanyahu) 政権発足で停滞する中東 (THE MIDDLE EAST) 和平交渉やテロ対策について協識した。 会談後の共同記者会見でムバウク (Mubarak) 大統領は、「我々の努力が成功するために は、米国(USA.)の役割が不可欠だ」として、イスラエル (ISRAEL)を和平プロセスに引 き戻すため、米国(U.S.A.)の積極的な仲介を求めたことを明らかにした。 ・これに対し、クリントン (CLINTON)大統領は、「我々は、パレスチナ(PALESTINE) 暫定自治を拡大する現在の中東 (THE MIDDLE EAST) 和平プロセスを堅持する事で合意し た」と述べるとともに、今後、残されたシリア (SYRIA)、レバノン (LEBANON) とイス ・ラエル (ISRAEL)の交渉開始を含め、「我々の可能なことはすべて行う」と決意を示した。 だが、イスラエル (ISRAEL)の政権交代に伴い、「何らかの中断や停滞は免れない」とし、 事態「開への具体的な見通しは示さなかった。 ・一方、両首脳はテロ防止策についても意見交換を行い、ムバラク (Mubarak) 大統領は「 ・テロリズムの問題は、中東(THE MIDDLE EAST) 地域から始まった」として、「中東 (THE MIDDLE EAST)の包括和平が実現すれば、世界のテロ活動の九五%は終結させるこ とができるだろう」と述べた。 (7月31日10:19) TRANSLATE Figure 2: Translated and Hyperlinked Terms perlinking is based on the original or translated En- glish terms, the user can follow the links to both En- glish and Japanese documents transparently. In ad- dition, the Client Module is integrated with a com- mercial MT system for rough translation. A docu- ment which the user is browsing can be translated on the fly by clicking the TRANSLATE button. 2.3 The Term Translation Module The Term Translation Module is used by the Client Module bi-directionally in two different modes. It translates English query terms into Japanese in the query mode and translates Japanese indexed terms into English for viewing of a retrieved Japanese text in the browsing mode. This translation module is sensitive to the seman- tic types of terms it is translating to resolve trans- lation ambiguity. Thus, if a term can be translated in one way for one type and in another way for an- other type, the Term Translation Module can output appropriate translations based on the type informa- tion. For example, in translating Japanese text into English, a single kanji (Chinese) character standing for England can be also a first name of a Japanese personal name, which should be translated to \"Hide\" and not \"England.\" In translating an English query into Japanese, a company \"Apple\" should be trans- lated into a transliteration in katakana and not into a Japanese word meaning a fruit apple. The Term Translation Module uses various re- sources and methods to translate English and Japanese names. We use automated methods as much as possible to reduce the cost of creating a large name lexicon manually. First, this module is unique in that it creates on the fly English translations of hiragana names and personal names. Hiragana names are transliterated into English using the hiragana-to-romaji mapping rules. Japanese personal names are translated by finding a combination of first and last names which spans the input.1 Then, each of the name parts is translated using the Japanese-English first and last name lexicons. In addition, in order to develop a large lexicon of English names and their Japanese translations, which are transliterated into katakana, we have au- tomatically generated katakana names from pho- netic transcriptions of English names. We have written rules which maps phonetic transcriptions to katakana letters, and generated possible Japanese katakana translations for given English names. As transliterations of the same English names may dif- fer, multiple katakana translations may be generated for single English names.2 The remaining terms are currently translated us- ing the English-Japanese translation lexicons, and we are expanding the lexicons by utilizing on-line resources and corpora and a translation aiding tool. 3 Utilizing IE in Multilingual Information Access The system applies information extraction technol- ogy (Adv, 1995) to index names accurately and ro- bustly. In this section, we describe how we have in- corporated this technology to improve multilingual information access in several innovative ways. 3.1 Query Disambiguation As described in Section 2.1, the Indexing Module not only identifies names of people, entities and locations but also disambiguates types among themselves and between names and non-names. Thus, if the user is searching for documents with the location \"Wash- ington (not a person or a company named \"Wash- ington\"), a person \"Clinton\" (not a location), or an entity \"Apple\" (not fruit), the system allows the user to specify, through the GUI, the type of each query term (cf. Figure 1). This ability to disambiguate types of queries not only constrains the search and hence improves retrieval precision but also speeds The Japanese Indexing Module does not specify if an identified name is a first name, a last name, or a combination of first and last name. Since there is no space between first and last names in Japanese, this must be automatically determined. 2This is still an experimental effort, and we have not evaluated the quality of generated translations quantita- tively yet. up the search time considerably especially when the database is very large. 3.2 Translation Disambiguation In developing the system, we have intentionally avoided an approach where we first translate foreign- language documents into English and index the translated English texts (Fluhr, 1995; Kay, 1995; Oard and Dorr, 1996). In (Aone et al., 1994), we have shown that, in an application of extracting in- formation from foreign language texts and present- ing the results in English, the \"MT first, IE second\" approach was less accurate than the approach in the reverse order, i.e., \"IE first, MT second\". In partic- ular, translation quality of names by even the best MT systems is poor. There are two cases where an MT system fails to translate names. First, it fails to recognize where a name starts and ends in a text string. This is a non-trivial problem in languages such as Japanese where words are not segmented by spaces and there is no capitalization convention. Often, an MT sys- tem \"chops up\" names into words and translates each word individually. For example, among the errors we have encountered, an MT system failed to recognize a person name \"Mori Hanae\" in kanji characters, segmented it into three words \"mori,\" \"hana,\" and \"e\" and translated them into \"forest,\" \"England\" and \"blessing,\" respectively. Another common MT system error is where the system fails to make a distinction between names and non-names. This distinction is very important in getting correct translations as names are usu- ally translated very differently from non-names. For example, a personal name \"Dole\" in katakana was translated into a common noun \"doll\" as the two have the same katakana string in Japanese. Abbre- viated country names for Japan and United States in single kanji characters, which often occurs in news- papers, were sometimes translated by an MT system into their literal kanji meanings, \"day\" and \"rice,\" respectively. Our system avoids these common but serious translation errors by taking advantage of the Index- ing Module's ability to identify and disambiguate names. In translating terms from Japanese to En- glish in the browsing mode, the Indexing Module identifies names correctly, avoiding the first type of translation errors. Then, the Term Translation Module utilizes type information obtained by the In- dexing Module to decide which translation strategies to use, thus overcoming the second type of error. 3.3 Intelligent Query Expansion and Hyperlinking As described in Section 2.1, the Indexing Module automatically identifies aliases of names and keeps track of such alias links in the database. For exam- ple, if \"International Business Machine\" and \"IBM\" appears in the same document, the system records in the database that they are aliases. The system uses this information in automatically expanding terms for query expansion and hyper- linking. At the query time, when the user types \"IBM\" and chooses the alias option in the search screen (see Figure 1), the query is automatically ex- panded to include its variant names both in English and Japanese, e.g., \"International Business Ma- chine,\" \"International Business Machine Corp.\" and Japanese translations for \"IBM\" and their aliases in Japanese. This is especially useful in retriev- ing Japanese documents because typically the user would not know various ways to say \"IBM\" in Japanese. The automated query expansion thus improves retrieval recall without manually creating alias lexicons. The same alias capability is also used in hyper- linking indexed terms in browsing a document. For example, when a user follows a hyperlink \"United States,\" it takes the user to a collection of documents which contains the English term \"United States\" and its aliases (e.g., \"US,\" \"U.S.A.\" etc.), and the Japanese translations of \"United States\" and their aliases. The result is a truly transparent multilin- gual document browsing and access capability. 3.4 Information Discovery One of the biggest advantages of introducing IE tech- nology into information access systems is the ability to create rich structured data which can be analyzed for \"buried\" information. Our multilingual capabil- ity enables the merging of possibly complementary data from both English and Japanese sources and enriching the available information. Currently the system offers the user several ways to explore and discover hidden information. Our search capability allows interactive information dis- covery methods. For example, using the query inter- face, the user can in effect ask \"Which company was mentioned along with Intel in regard to micropro- cessors?\" and the system will return all the articles which mentions \"Intel,\" \"microprocessors,\" and one or more company names. The user might see that NexGen and Cyrix often occurs with Intel and find out that they are competitors of Intel in this field. Or the user might ask \"Who is related to \"Shinshin- tou Party,\" a Japanese political party, and the user can find out all the people associated with this party. This type of search capabilities cannot be offered by typical information retrieval systems as they treat words as just strings and do not distinguish their semantic attributes. Furthermore, as we discussed earlier in Sec- tion 2.2, browsing documents by following hyper- links allows a user to discover related information effectively. For example, when the user searches for documents on \"NEC Corp.\", selects one of the re- turned documents, and finds another company name N ? Co-occurrences List of Person names co-occurring with \"PERU\" N ? Name Subtype Co-occurrences HASHIMOTO Unknown. Browse Fujimori, Alberto Unknown CLINTON Unknown 2 Click on the Top 'N' links to son the most frequently occurring names for that type Hashimoto, Ryutaro Unknown 2 TADASHI Unknown 2 Achabui, Grimaldo Unknown 1 AKIKO Unknown: Acki, Morihisa. Unknown 1 Barzinger, Norma Unknown Unknown Person Technology Place [Top 25) [Top 50 Top 25 Top 50 [Top 24) [Top 25 [Top 50] [To 50] Enty Top 25 [Top 50] Bayonet, Julio Cesar Ventura Unknown Next 10 Figure 4: The 25 and 50 Most Frequent Names Figure 3: Person Names Co-occurring with Peru \"Toshiba\" mentioned in this document, the user can establish an immediate connection and follow the link from \"Toshiba\" to other English and Japanese documents which contain that term. In addition, for each indexed term, the user can explore co-occurring persons, entities, places and technology. For example, Figure 3 shows a list of people co-occurring with the place \"Peru.\" It lists the Japanese prime minister and the Peruvian pres- ident at the top (as the Japanese embassy hostage incident occurred recently.) 4 The System Tour In this section, we give a tour of the system. Figure 4 shows the main Browse screen where the user can browse the top 25 or 50 names of people, entities, locations, and S&T terms. This can provide the user with a snapshot of what is in the database and what types of information are likely to be available. By following the top 50 entity name link, the user sees the list of entity names in order of frequency (cf. Figure 5). The Subtype column in the screen indicates more detailed types of the entity (e.g., or- ganization, company, facility, etc.) From this screen, the user can go to a list of all English and Japanese documents which mention, for example, \"Bank of Japan\" by clicking the link (cf. Figure 6). The list provides information on the title, length, source, lan- guage, and date of each article. Browsing by Frequency List of Top 50 Entity names (cont'd.) Name Subtype Documents Democratic Party of Japan Political 14 MINISTRY OF FOREIGN AFFAIRS Government 14 SHINSHINTOU PARTY Organization 14 Army Military 13 NEC Company 12 Associated Press Company 11 BANK OF JAPAN Govertiment 11 Capitol Facility まま Finance Ministry Government 11 FUJITSU Company Next 10 Figure 5: Top 50 Entity Names N N ?Q Browse Results The following 15 documents contain \"BANK OF JAPAN\". Title Length Source Language Date ●横勢判斷、月1回作成へ一「政策委中心型」 522 Jiji Japanese 1997-1-17 業務に移行・日銀 ●東アジア景気拡大テンポが減速ーコンピュー 737 Jiji Japanese 1997-1-17 ・ター関連の輸出強化で・日銀海外経済動向 ●12月の通貨給量、前年比3,2%增-2次 671 Jiji Japanese 1997-1-17 月速来で伸び率化、日銀 ●円藏滿、一時117円25銭~17日の東京 585 Jiji Japanese 1997-1-17 外為 ●円、上停一日銀介入のうわさで・NY外為$ 528 Jiji Japanese 1997-1-17 16日) ●BO: Chief Confident of Independence 2001 Jiji English 1996-11-12 ●Paradise lost 7179 Asahi English 1996-4-10 ●BANK OF JAPAN AUTONOMY CALL 2697 Asahi English 1996-4-10 ●Alliance Oks bank watchdog 2566 Asahi English 1996-12-24 ●日鉄支店長会議で公共投資演の影響を懸念の 661 Asahi Japanese 1997-1-21 Next 5 Co-occurrences with Unknown Person YOMIURI 1996-07-31. <H2> mediation indispensable & Egypt president <H2> of the United States < Washington 30 day - liyama elegance history > rose 7 Egypt president who is in the midst of visiting the United States on the 30th, conversed with American President Clinton at Whitehouse, conferred concerning Middle Eastern peace negotiation and the terrorist measure which are stagnant with strong group news item ニヤフ administration start of Israel. In joint press conference after conversing as for rose 7 president,\" in order for our effort to succeed, American role is indispensable, that doing, in order to pull back Israel to peace process, it made that it sought the positive mediation of the United States clear. Vis-a-vis this, as for President Clinton, \"as for us, as you express, that it agreed by the fact that the present Middle Eastern peace process which expands Palestinian provisional autonomy is firmly maintained\" in the future, it includes the start of Syrian, Lebanon and Israel which are left negotiation, \"does our possible thing entirely,\" that determination was shown. Is, but \"it does not escape some discontinuance and stagnation,\" that attendant upon Israeli administration alternation it did, did not show the concrete prospect to situation break. On one hand, both leaders did opinion exchange concerning terrorist prevention step, as for rose 7 president,\" as for problem of terrorism started from Middle Eastern region \"that doing, \"if inclusive peace of the Middle East actualizes, to conclude it probably is possible 95% of terrorist activity of the world \"that you expressed. (July 31st 10:19) N Figure 6: Documents Containing \"Bank of Japan\" In the main Search screen (cf. Figure 1), the user types in each query term, including multi-words like \"personal computer,\" in each numbered box. The user can formulate a Boolean query using the box numbers and boolean operators. If not specified, the query terms are joined by \"OR\". When the Alias button is on, query terms are expanded to include their aliases. The Type menu allows the user to dis- ambiguate types of query terms. In the Language box, the user has the choice of selecting documents in English, Japanese, or both. In addition, the user can constrain sources and the date range of docu- ments, and also sort the results by date, title, and sources. As discussed in Section 2.2, when the user selects a Japanese article, they can optionally send the article to a commercial MT system for rough translation by pushing the TRANSLATE button (cf. Figure2). Fig- ure 7 shows the translation result for the Japanese document in Figure 2. 5 Summary We have described an advanced multilingual cross- linguistic information browsing and retrieval sys- tem which takes advantage of information extraction technology in unique ways. In addition to its basic capability of allowing a user to send Boolean queries in English against English and Japanese documents and to view the results in semi- and fully translated Figure 7: Translation by a Commercial MT system forms, the system has many innovative capabilities. It can disambiguate query terms to increase preci- sion, expand query terms automatically using aliases to increase recall, and improve translation accuracy significantly by finding and disambiguating names accurately. Moreover, the system allows interactive information discovery from a multilingual document collection by combining IE and MT technologies. The Indexing Module is currently running on a Sun platform and is designed to scale for a multi-user operational environment. The Web browser-based user interface will work in any Web browser sup- porting HTML 3.0 on any platform which the Web browser supports, and this ensures a large user base. The system is customizable in several ways. For our current application, the system indexes names and S&T terms, but for other applications we can cus- tomize the system to index different types of names and terms. For example, the system can be cus- tomized to index product names and financial terms for a business application. Its ODBC-compliance makes porting of databases from one vendor to an- other very easy. Finally, the system does not as- sume any particular language combination or target language. Thus, this system can also be used for Japanese monolingual users who want to query and browse in Japanese a set of documents written in English, Japanese, and Spanish. References Advanced Research Projects Agency. 1995. Proceed- ings of Sixth Message Understanding Conference (MUC-6). Morgan Kaufmann Publishers. Aone, Chinatsu. 1996. NameTag Japanese and Spanish Systems as Used for MET. In Proceedings of Tipster Phase II. Morgan Kaufmann Publish- ers. Aone, Chinatsu, Hatte Blejer, Mary Ellen Okurowski, and Carol Van Ess-Dykema. 1994. A Hybrid Approach to Multilingual Text Processing: Information Extraction and Machine Translation. In Proceedings of the First Conference of the As- sociation for Machine Translation in the Americas (AMTA). Fluhr, Christian. 1995. Multilingual information re- trieval. In Ronald A. Cole, Joseph Mariani, Hans Uszkoreit, Annie Zaenen, and Victor Zue, editors, Survey of the State of the Art in Human Language Technology. Oregon Graduate Institute. Kay, Martin. 1995. Machine translation: The dis- appointing past and present. In Ronald A. Cole, Joseph Mariani, Hans Uszkoreit, Annie Zaenen, and Victor Zue, editors, Survey of the State of the Art in Human Language Technology. Oregon Graduate Institute. Krupka, George. 1995. SRA: Description of the SRA System as Used for MUC-6. In Proceed- ings of Sixth Message Understanding Conference (MUC-6). Oard, Douglas W. and Bonnie J. Dorr, editors. 1996. A Survey of Multilingual Text Retrieval. Techni- cal Report UMIACS-TR-96-19. Institute for Ad- vanced Computer Studies, University of Mary- land."
  },
  {
    "title": "INCREMENTAL FINITE-STATE PARSING",
    "abstract": "This paper describes a new finite-state shallow parser. It merges constructive and reductionist approaches within a highly modular architecture. Syntactic information is added at the sentence level in an incremental way, depending on the contextual information available at a given stage. This approach overcomes the inefficiency of previous fully reductionist constraint-based systems, while maintaining broad coverage and linguistic granularity. The implementation relies on a sequence of networks built with the replace operator. Given the high level of modularity, the core grammar is easily augmented with corpus-specific sub-grammars. The current system is implemented for French and is being expanded to new languages.",
    "content": "1 Background Previous work in finite-state parsing at sentence level falls into two categories: the constructive ap- proach or the reductionist approach. The origins of the constructive approach go back to the parser developed by Joshi (Joshi, 1996). It is based on a lexical description of large collections of syntactic patterns (up to several hundred thousand rules) using subcategorisation frames (verbs + essen- tial arguments) and local grammars (Roche, 1993). It is, however, still unclear whether this heavily lex- icalized method can account for all sentence struc- tures actually found in corpora, especially due to the proliferation of non-argumental complements in corpus analysis. Another constructive line of research concentrates on identifying basic phrases such as in the FASTUS information extraction system (Appelt et al., 1993) or in the chunking approach proposed in (Abney, 1991; Federici et al., 1996). Attempts were made to mark the segments with additional syntactic infor- mation (e.g. subject or object) (Grefenstette, 1996) using simple heuristics, for the purpose of informa- tion retrieval, but not for robust parsing. The reductionist approach starts from a large number of alternative analyses that get reduced through the application of constraints. The con- straints may be expressed by a set of elimi- nation rules applied in a sequence (Voutilainen, Tapanainen, 1993) or by a set of restrictions applied in parallel (Koskenniemi et al., 1992). In a finite- state constraint grammar (Chanod, Tapanainen, 1996), the initial sentence network represents all the combinations of the lexical readings associated with each token. The acceptable readings result from the intersection of the initial sentence network with the constraint networks. This approach led to very broad coverage analyzers, with good linguistic granularity (the information is richer than in typical chunking systems). However, the size of the interme- diate networks resulting from the intersection of the initial sentence network with the sets of constraints raises serious efficiency issues. The new approach proposed in this paper aims at merging the constructive and the reductionist ap- proaches, so as to maintain the coverage and gran- ularity of the constraint-based approach at a much lower computational cost. In particular, segments (chunks) are defined by constraints rather than pat- terns, in order to ensure broader coverage. At the same time, segments are defined in a cautious way, to ensure that clause boundaries and syntactic func- tions (e.g. subject, object, PP-Obj) can be defined with a high degree of accuracy. 2 The incremental parser 2.1 Overview The input to the parser is a tagged text. We cur- rently use a modified version of the Xerox French tagger (Chanod, Tapanainen, 1995). The revisions are meant to reduce the impact of the most frequent errors of the tagger (e.g. errors between adjectives and past participles), and to refine the tagset. Each input token is assigned a single tag, generally representing the part-of-speech and some limited morphological information (e.g the number, but not the gender of nouns). The sentence is initially rep- resented by a sequence of wordform-plus-tag pairs. The incremental parser consists of a sequence of transducers. These transducers are compiled from regular expressions that use finite-state calculus op- erators, mainly the Replace operators (Karttunen, 1996). Each of these transducers adds syntactic in- formation represented by reserved symbols (annota- tions), such as brackets and names for segments and syntactic functions. The application of each trans- ducer composes it with the result of previous appli- cations. If the constraints stipulated in a given transducer are not verified, the string remains unchanged. This ensures that there is always an output string at the end of the sequence, with possibly underspecified segments. Each transducer performs a specific linguistic task. For instance, some networks identify segments for NPs, PPs, APs (adjective phrases) and verbs, while others are dedicated to subject or object. The same task (e.g. subject assignment or verb segmen- tation) may be performed by more than one trans- ducer. The additional information provided at each stage of the sequence is instrumental in the defini- tion of the later stages of the sequence. Networks are ordered in such a way that the easiest tasks are addressed first. 2.2 Non-monotonicity The replace operators allow one not only to add in- formation but also to modify previously computed information. It is thus possible to reassign syntactic markings at a later stage of the sequence. This has two major usages: • assigning some segments with a default marking at some stage of the process in order to provide preliminary information that is essential to the subsequent stages; and correcting the default marking later if the context so requires • assigning some segments with very general marking; and refining the marking later if the context so permits. In that sense, our incremental parser is non- monotonic: earlier decisions may be refined or even revised. However, all the transducers can, in prin- ciple, be composed into a single transducer which produces the final outcome in a single step. 2.3 Cautious segmentation and syntactic marking Each transducer defines syntactic constructions us- ing two major operations: segmentation and syn- tactic marking. Segmentation consists of bracket- ing and labeling adjacent constituents that belong to a same partial construction (e.g. a nominal or a verbal phrase, or a more primitive/partial syntactic chain if necessary). Segmentation also includes the identification of clause boundaries. Syntactic mark- ing annotates segments with syntactic functions (e.g. subject, object, PPObj). The two operations, segmentation and syntactic marking, are performed throughout the sequence in an interrelated fashion. Some segmentations depend on previous syntactic marking and vice versa. If a construction is not recognized at some point of the sequence because the constraints are too strong, it can still be recognized at a later stage, using other linguistic statements and different background infor- mation. This notion of delayed assignment is crucial for robust parsing, and requires that each statement in the sequence be linguistically cautious. Cautious segmentation prevents us from grouping syntacti- cally independent segments. This is why we avoid the use of simplifying ap- proximations that would block the possibility of per- forming delayed assignment. For example, unlike (Abney, 1991), we do not systematically use longest pattern matching for segmentation. Segments are restricted by their underlying linguistic indetermi- nacy (e.g. post-nominal adjectives are not attached to the immediate noun on their left, and coordinated segments are not systematically merged, until strong evidence is established for their linkage). 2.4 Incremental parsing and linguistic description The parsing process is incremental in the sense that the linguistic description attached to a given trans- ducer in the sequence: • relies on the preceding sequence of transducers • covers only some occurrences of a given linguis- tic phenomenon • can be revised at a later stage. This has a strong impact on the linguistic char- acter of the work. The ordering of the linguistic descriptions is in itself a matter of linguistic descrip- tion: i.e. the grammarian must split the description of phenomena into sub-descriptions, depending on the available amount of linguistic knowledge at a given stage of the sequence. This may sound like a severe disadvantage of the approach, as deciding on the order of the transduc- ers relies mostly on the grammarian's intuition. But we argue that this incremental view of parsing is instrumental in achieving robust parsing in a prin- cipled fashion. When it comes to parsing, no state- ment is fully accurate (one may for instance find ex- amples where even the subject and the verb do not agree in perfectly correct French sentences). How- ever, one may construct statements which are true almost everywhere, that is, which are always true in some frequently occuring context. By identifying the classes of such statements, we reduce the overall syntactic ambiguity and we sim- plify the task of handling less frequent phenomena. The less frequent phenomena apply only to segments that are not covered by previous linguistic descrip- tion stages. To some extent, this is reminiscent of the optimal- ity theory, in which: • Constraints are ranked; • Constraints can be violated. Transducers at the top of the sequence are ranked higher, in the sense that they apply first, thus block- ing the application of similar constructions at a later stage in the sequence. If the constraints attached to a given transducer are not fulfilled, the transducer has no effect. The output annotated string is identical to the input string and the construction is bypassed. However, a bypassed construction may be reconsidered at a later stage, using different linguistic statements. In that sense, bypassing allows for the violation of con- straints. 2.5 An example of incremental description: French Subjects As French is typically SVO, the first transducer in the sequence to mark subjects checks for NPs on the left side of finite verbs. Later in the sequence, other transducers allow for subject inversion (thus violating the constraint on subject-verb order), especially in some specific contexts where inversion is likely to occur, e.g. within relative or subordinate clauses, or with mo- tion verbs. Whenever a transducer defines a verb- subject construction, it is implicitly known at this stage that the initial subject-verb construction was not recognized for that particular clause (other- wise, the application of the verb-subject construc- tion would be blocked). Further down in the sequence, transducers may allow for verb-subject constructions outside the previously considered contexts. If none of these subject-pickup constructions applies, the final sen- tence string remains underspecified: the output does not specify where the subject stands. It should be observed that in real texts, not only may one find subjects that do not agree with the verb (and even in correct sentences), but one may also find finite verbs without a subject. This is the case for instance in elliptic technical reports (esp. failure reports) or on cigarette packs with inscrip- tions like Nuit gravement à la santé. This is a major feature of shallow and robust parsers (Jensen et al., 1993; Ejerhed, 1993): they may provide partial and underspecified parses when full analyses cannot be performed; the issue of gram- maticality is independent from the parsing process; the parser identifies the most likely interpretations for any given input. An additional feature of the incremental parser derives from its modular architecture: one may han- dle underspecified elements in a tractable fashion, by adding optional transducers to the sequence. For in- stance, one may use corpus specific transducers (e.g. sub-grammars for technical manuals are specially useful to block analyses that are linguistically ac- ceptable, but unlikely in technical manuals: a good example in French is to forbid second person sin- gular imperatives in technical manuals as they are often ambiguous with nouns in a syntactically unde- cidable fashion). One may also use heuristics which go beyond the cautious statements of the core gram- mar (to get back to the example of French subjects, heuristics can identify any underspecified NP as the subject of a finite verb if the slot is available at the end of the sequence). How specific grammars and heuristics can be used is obviously application de- pendent. 3 Architecture The parser has four main linguistic modules, each of them consisting of one or several sequenced trans- ducers: 1 Seriously endangers your health. This example rep- resents an interesting case of deixis and at the same time a challenge for the POS tagger as Nuit is more likely to be recognized as a noun (Night) than as a verb (Endan- gers) in this particular context. • Primary segmentation • Subject tagging • Segment expansion (Optional) • Other syntactic functions tagging The input text is first tagged with part-of-speech information using the Xerox tagger. The tagger uses 44 morphosyntactic tags such as NOUN-SG for sin- gular nouns and VERB-P3SG for verb 3rd person singular. The morphosyntactic tags are used to mark AP, NP, PP and VP segments. We then use the segmen- tation tags and some additional information (includ- ing typography) to mark subjects which, in turn, determine to what extent VCs (Verb Chunks) can be expanded. Finally, other syntactic functions are tagged within the segments. Marking transducers are compiled from regular expressions of the form A -> T1 T2 that con- tains the left-to-right longest match replace opera- tor -> Such a transducer marks in a left-to-right fashion the maximal instances of A by adding the bracketing strings T1 and T2. 4 Primary Segmentation A segment is a continuous sequence of words that are syntactically linked to each other or to a main word (the Head). In the primary segmentation step, we mark segment boundaries within sentences as shown below where NP stands for Noun Phrase, PP for Preposition Phrase and VC for Verb Chunk (a VC contains at least one verb and possibly some of its arguments and modifiers). Example: [VC [VC Lorsqu' [NP on NP] tourne VC] [NP le commutateur NP] [PP de démarrage PP] [PP sur la position PP] [AP auxiliaire AP] [NP l' aiguille NP] retourne alors [PP à zéro PP] VC] ./SENT<sup>2</sup> All the words within a segment should be linked to words in the same segment at the same level, ex- cept the head. For instance, in the NP le commu- tateur (the switch), le should be linked to commu- tateur (the head) which, in turn, should be linked to the verb tourne, and not to the verb retourne be- cause the two words are not in the same segment. The main purpose of marking segments is therefore to constrain the particular linguistic space that de- termines the syntactic function of a word. <sup>2</sup>Turning the starter switch to the auxiliary position, the pointer will then return to zero. As one can notice from the example above, seg- mentation is very cautious, and structural ambiguity inherent to modifier attachment (even postnominal adjectives), verb arguments and coordination is not resolved at this stage. In order to get more robust linguistic descriptions and networks that compile faster, segments are not defined by marking sequences that match classical regular expressions of the type [Det (Coord Det) Adj* Noun], except in simple or heavily constrained cases (APs, Infinitives, etc). Rather, we take ad- vantage of the fact that, within a linguistic segment introduced by some grammatical words and termi- nated by the head, there is no attachement ambigu- ity and therefore these words can be safely used as segment delimiters (Bès, 1993). We first mark pos- sible beginnings and endings of a segment and then associate each beginning tag with an ending if some internal constraints are satisfied. Hence, the main steps in segmentation are: • Tag potential beginnings and ends of a segment • Use these temporary tags to mark the segment • Remove the temporary tags. 4.1 AP Segmentation Adjective phrases are marked by a replacement transducer which inserts the [AP and AP] bound- aries around any word sequence that matches the regular expression (RE): [ (ADVP) ADJ (COMMA [ (ADVP) ADJ COMMA]+) ( COORD (ADVP) ADJ ) ] ADVP stands for adverb phrase and is defined as: [ ADV+ [[COORD COMMA] ADV+]* ] 4.2 NP Segmentation Unlike APs, NPs are marked in two steps where the basic idea is the following: we first insert a special mark wherever a beginning of an NP is possible, i.e, on the left of a determiner, a numeral, a pronoun, etc. The mark is called a temporary beginning of NP (TBeginNP). The same is done for all possible ends of NP (TEndNP), i.e. nouns, numerals, pro- nouns, etc. Then, using a replacement transducer, we insert the [NP and NP] boundaries around the longest sequence that contains at least one tempo- rary beginning of NP followed by one temporary end of NP: [TBeginNP $[TEndNP] TEndNP ] -> BeginNP EndNP This way, we implicitly handle complicated NPs such as le ou les responsables (the-SG or the-PL per- son(s) in charge), les trois ou quatre affaires (the three or four cases), etc. 4.3 PP Segmentation Once NP boundaries are marked, we insert on the left of any preposition a temporary PP beginning mark (TBeginPP = <PP): <PP Avec ou <PP sans [NP le premier ministre NP³] Then the longest sequence containing at least one TBeginPP followed by one EndNP is surrounded with the [PP and PP] boundaries using the RE: [TBeginPP ~$ [EndNP TVerb] EndNP] -> BeginPP ... EndPP which eventually leads to: [PP Avec ou sans le premier ministre PP] 4.4 VC Segmentation A VC (Verb Chunk) is a sequence containing at least one verb (the head). It may include words or seg- ments (NPs, PPs, APs or other VCs) that are pos- sibly linked as arguments or adjuncts to the verb. There are three types of VCs: infinitives, present participle phrases and finite verb phrases. We first mark infinitives and present participle segments as they are simpler than finite verb phrases-they are not recursive, they cannot contain other VCs. 4.4.1 Infinitives The infinitive phrases are recognized using the reg- ular expression: [(PREPO) (NEG) (ADVP) PC* INF [(ADVP PastPartV+) | PastPartV*]] e.g.: sans même prévenir (without even warning): [VC [NP Mr NP] [NP Guilhaume NP] supprime VC] [PP des émissions PP] [VC sans même prévenir VC] [NP leurs responsables NP] 4.4.2 Present Participle Segments The present participle phrases are recognized us- ing the regular expression: [(EN) (NEG) PC* PrePart [(ADVP PastPartV+) | PastPartV*]] e.g.: en dénonçant (while denouncing) [VC en dénonçant VC] [NP les provocations NP] [ADJ mensongères ADJ] 3 With or without the prime minister. 4.4.3 Finite Verb Segments Here we use the basic idea described in the NP marking: temporary beginnings (TBeginVC) and ends (TEndVC) of VC are first marked. Temporary beginnings of VCs are usually intro- duced by grammatical words such as qui (relative pronoun), lorsque, et (coordination) etc. However, not all these words are certain VC boundaries: et could be an NP coordinator, while que (tagged as CONJQUE by the HMM tagger) could be used in comparatives (e.g. plus blanc que blanc). Therefore, we use three kinds of TBeginVC to handle differ- ent levels of uncertainty: a certain TBeginVC (ТВе- ginVC1), a possible BeginVC (TBeginVC2) and an initial TBeginVC (TBegin VCS) automatically in- serted at the beginning of every sentence in the input text. With TBegin VCS, we assume that the sen- tence has a main finite verb, as is usually the case, but this is just an assumption that can be corrected later. A temporary end of VC (TEndVC) is then in- serted on the right of any finite verb, and the process of recognizing VCs consists of the following steps: • Step 1: Each certain TBegin VC1 is matched with a TEndVC, and the sequence is marked with [VC and VC]. The matching is applied iteratively on the input text to handle the case of embedded clauses (arbitrarily bound to three iterations in the current implementations). • Step 2: The same is done with the TBegin VCS (inserted at the beginning of a sentence). • Step 3: If there is still a TEndVC that was not matched in (1) or (2), then it is matched with a possible TBegin VC2, if any, and the sequence is marked with [VC and VC]. • Step 4: Any TBeginVC that was not matched in (1), (2) or (3) is removed. Verb Segmentation Example: Initial input Lorsqu' [NP on NP] appuie [PP sur l' interrupteur PP] [PP de feux PP] [PP de détresse PP] [NP tous_les indicateurs NP] [PP de direction PP] clignotent simultanément et [NP un triangle NP] [AP rouge AP] clignote [PP dans l' interrupteur PP] ./SENT⁴ ⁴ When the hazard warning switch is pressed all the direction indicators will flash in unison and the switch will flash a red triangle. Temporary tagging of VC boundaries <VCS <VC1 Lorsqu' [NP on NP] appuie VC> [PP sur l', interrupteur PP] [PP de feux PP] [PP de détresse PP] [NP tous_les indicateurs NP] [PP de direction PP] , clignotent VC> simultanément <VC2 et [NP un triangle NP] [AP rouge AP] clignote VC> [PP dans l', interrupteur PP] ./SENT VC marking [VC [VC Lorsqu' [NP on NP] appuie VC] [PP sur l' interrupteur PP] [PP de feux PP] [PP de détresse PP] [NP tous_les indicateurs NP] [PP de direction PP] , clignotent VC] simultanément [VC et [NP un triangle NP] [AP rouge AP] clignote VC] [PP dans l' interrupteur PP] ./SENT 5 Marking Syntactic Functions The process of tagging words and segments with syntactic functions is a good example of the non-monotonic nature of the parser and its hy- brid constructive-reductionnist approach. Syntac- tic functions within non recursive segments (AP, NP and PP) are addressed first because they are easier to tag. Then other functions within verb segments and at sentence level (subject, direct object, verb modifier, etc.) are considered. Potential subjects are marked first: an NP is a potential subject if and only if it satisfies some ty- pographical conditions (it should not be separated from the verb with only one comma, etc.). This prevents the NP Jacques, for example, from being marked as a subject in the sentence below: [VC [NP le président NP]/SUBJ [PP du CSA PP], [NP Jacques NP] [NP Boutet NP], a décidé VC] [VC de publier VC] [NP la profession NP] [PP de foi PP]./SENT 5 Then constraints are applied to eliminate some of the potential subject candidates. The constraints are mainly syntactic: they are about subject uniqueness (unless there is a coordination), the necessary shar- ing of the subject function among coordinated NPs, etc. The remaining candidates are then considered as real subjects. The other syntactic functions, such as object, PP-Obj, verb modifier, etc. are tagged using similar steps. 5 The CSA president, Jacques Boutet, decided to present his profession of faith. 6 Expanding Verb Segments Because primary segmentation is cautious, verb seg- ments end right after a verb in order to avoid arbi- trary attachment of argument or adjunct segments (NPs, PPs and APs on the right of a verb). How- ever, experiments have shown that in some kinds of texts, mainly in technical manuals written in a \"con- trolled language\", it is worth applying the \"nearest attachment\" principle. We expand VCs to include segments and to consider them as arguments or ad- juncts of the VC head. This reduces structural am- biguity in the parser output with a very small error rate. For instance, expanding VCs in the sentence given in the previous section leads to the following structure: [VC [NP le président NP]/SUBJ [PP du CSA PP], [NP Jacques NP] [NP Boutet NP] , a décidé [VC de publier [NP la profession NP] [PP de foi PP] VC] VC] ./SENT Nevertheless, as this principle leads to a significant number of incorrect attachments in the case of more free-style texts, the VC expansion network is option- ally applied depending on the input text. 7 Performance As mentioned above, the parser is implemented as a sequence of finite state networks. The total size of the 14 networks we currently use is about 500 KBytes of disk space. The speed of analysis is around 150 words per second on a SPARCstation 10 machine running in a development environment that we expect to optimize in the future. As for linguistic performance, we conducted a preliminary evaluation of subject recognition over a technical manual text (2320 words, 157 sentences) and newspaper articles from Le Monde (5872 words, 249 sentences). The precision and recall rates were respectively 99.2% and 97.8% in the first case, 92.6% and 82.6% in the case of the newspaper articles. This difference in performance is due to the fact that, on the one hand, we used the technical manual text to develop the parser and on the other hand, it shows much less rich syntactic structures than the newspaper text. We are currently conducting wider experiments to evaluate the linguistic accuracy of the parser. 8 Parsing Samples Below are some parsing samples, where the output is slightly simplified to make it more readable. In par- ticular, morphosyntactic tags are hidden and only the major functions and the segment boundaries ap- pear. A l'interprétation des sentiments présidentiels s'a- joute l'atmosphère de surenchère politique qui précède tout congrès du Parti socialiste. [VC [PP A/PrepN> 1'/DET> interprétation PP]/PPObj [PP des/PrepN> sentiments PP]/PPObj [AP présidentiels AP]/<NM s'ajoute MV VC] [NP 1'/DET> atmosphère NP]/<SUBJ [PP de/PrepN> surenchère PP]/PPObj [AP politique AP]/<NM [VC [NP qui NP]/SUBJ précède VC] tout/VM [NP congrès NP]/OBJ [PP du/PrepN> Parti PP]/PPObj [AP socialiste AP]/<NM./SENT Les députés azerbaidjanais ont adressé à Moscou un ultimatum exigeant la levée de l'état d'urgence, et le retrait des troupes, faute de quoi ils reconsidéreraient \" l'acte d'union\" intégrant l'Azerbaidjan à l'URSS. [VC [NP Les/DET> députés NP]/SUBJ [AP azerbaidjanais AP]/<NM ont adressé MV VC] [PP à/PrepN> Moscou PP]/PPObj [NP un/DET> ultimatum NP]/OBJ [VC exigeant VC] [NP la/DET> levée NP]/OBJ [PP de/PrepN> 1'/DET> état PP]/PPObj [PP d'/PrepN> urgence PP]/PPObj, et [NP le/DET> retrait NP]/N [PP des/PrepN> troupes PP]/PPObj, [VC faute_de_quoi [NP ils NP]/SUBJ reconsidéreraient VC] [NP 1'/DET> acte NP]/OBJ [PP d'/PrepN> union PP]/PPObj [VC intégrant VC] [NP 1'/DET> Azerbaidjan NP]/OBJ [PP à/PrepN> 1'/DET> URSS PP]/PPObj./SENT A l'heure, vendredi soir, où les troupes soviétiques s'apprêtaient à pénétrer dans Bakou, la minuscule République autonome du Nakhitchevan, territoire azéri enclavé en Arménie à la frontière de l'Iran, proclamait unilatéralement son indépendance, par décision de son propre Soviet suprême. [VC [PP A/PrepN> 1'/DET> heure PP]/PPObj [NP vendredi NP]/N [NP soir NP]/<NM [VC où [NP les/DET> troupes NP]/SUBJ [AP soviétiques AP]/<NM s' apprêtaient VC] [VC à pénétrer VC] [PP dans/PrepN> Bakou PP]/PPObj, [NP la/DET> [AP minuscule AP]/NM> République NP]/SUBJ [AP autonome AP]/<NM [PP du/PrepN> Nakhitchevan PP]/PPObj, [NP territoire NP]/N [AP azéri AP]/<NM [AP enclavé AP]/<NM [PP en/PREP Arménie PP]/PPObj [PP à/PrepN> la/DET> frontière PP]/PPObj [PP de/PrepN> 1'/DET> Iran PP]/PPObj, proclamait MV VC] unilatéralement/VM [NP son/DET> indépendance NP]/OBJ [PP par/PrepN> décision PP]/PPObj [PP de/PrepN> son/DET> [AP propre AP]/NM> Soviet PP]/PPObj [AP suprême AP]/<NM./SENT 9 Conclusion The incremental finite-state parser presented here merges both constructive and reductionist ap- proaches. As a whole, the parser is constructive: it makes incremental decisions throughout the pars- ing process. However, at each step, linguistic con- traints may eliminate or correct some of the previ- ously added information. Therefore, the analysis is non-monotonic and handles uncertainty. The linguistic modularity of the system makes it tractable and easy to adapt for specific texts (e.g. technical manuals or newspaper texts). This is done by adding specialized modules into the parsing se- quence. This way, the core grammar is clearly separated from optional linguistic descriptions and heuristics. Ongoing work includes expansion of the French grammar, a wider evaluation, and grammar devel- opment for new languages. We will also experiment with our primary target applications, information retrieval and translation assistance. Acknowledgements We would like to thank Kenneth R. Beesley and Lauri Karttunen for their editorial advice and Gre- gory Grefenstette for the valuable discussions we had about finite-state parsing and filtering. References Steven P. Abney, 'Parsing by chunks', in Principled- Based Parsing, eds., R. Berwick, S. Abney, and C. Tenny, Kluwer Academic Publishers, Dor- drecht, (1991). Douglas E. Appelt, Jerry R. Hobbs, John Bear, David Israel, and Mabry Tyson 'FASTUS: A Finite-State Processor for Information Extraction from Real-World Text', in Proceedings IJCAI-93, Chambery, France, August 1993. Gabriel G. Bès, 'Axiomas y algoritmos en la de- scripción de las lenguas naturales', V Congreso Argentino de Lingüistica, Mendoza, 1993. Jean-Pierre Chanod and Pasi Tapanainen, 'Tagging French - comparing a statistical and a constraint- based method', in Proceedings of the Seventh Con- ference of the European Chapter of the Associa- tion for Computational Linguistics, pp. 149-156, Dublin, (1995). Jean-Pierre Chanod and Pasi Tapanainen. 'A Ro- bust Finite-State Parser for French', in ESSLLI'96 Workshop on Robust Parsing, August 1996 12-16, Prague, Czech Republic. Eva Ejerhed, 'Nouveaux courants en analyse syntax- ique', Traitement automatique des langues, 34(1), (1993). Stefano Federici, Simonetta Montemagni and Vito Pirrelli 'Shallow Parsing and Text Chunking: a View on Underspecification in Syntax', in ESS- LLI'96 Workshop on Robust Parsing, August 1996 12-16, Prague, Czech Republic. Gregory Grefenstette, 'Light Parsing as Finite-State Filtering', in Proceedings ECAI '96 workshop on \"Extended finite state models of language\" Aug. 11-12, 1996, Budapest. Karen Jensen, George E. Heidorn, and Stephen D. Richardson, eds., Natural language processing: the PLNLP approach, number 196 in The Kluwer international series in engineering and computer science, Kluwer Academic Publishers, Boston/Dordrecht/London, 1993. Aravind Joshi. 'A Parser from Antiquity: An Early Application of Finite State Transducers to Natu- ral Language Parsing', in Proceedings ECAI '96 workshop on \"Extended finite state models of lan- guage\", Budapest, August 11-12, 1996, Budapest. Lauri Karttunen, 'Directed replacement', in Proceed- ings of the 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, USA, (June 1996). Association for Computational Lin- guistics. Kimmo Koskenniemi, Pasi Tapanainen, and Atro Voutilainen, 'Compiling and using finite-state syn- tactic rules', in Proceedings of the Fourteenth International Conference on Computational Lin- guistics COLING-92 vol. I, pp. 156-162. Nantes, (1992). Emmanuel Roche, Analyse syntaxique transforma- tionnelle du français par transducteurs et lexique- grammaire, Ph.D. dissertation, Université de Paris 7, 1993. Atro Voutilainen and Pasi Tapanainen, 'Ambigu- ity resolution in a reductionistic parser', in Pro- ceedings of the Sixth Conference of the Euro- pean Chapter of the Association for Computa- tional Linguistics, pp. 394-403, Utrecht, (1993)."
  },
  {
    "title": "Three Heads are Better than One",
    "abstract": "Machine translation (MT) systems do not currently achieve optimal quality translation on free text, whatever translation method they employ. Our hypothesis is that the quality of MT will improve if an MT environment uses output from a variety of MT systems working on the same text. In the latest version of the Pangloss MT project, we collect the results of three translation engines — typically, subsentential chunks in a chart data structure. Since the individual MT systems operate completely independently, their results may be incomplete, conflicting, or redundant. We use simple scoring heuristics to estimate the quality of each chunk, and find the highest-score sequence of chunks (the \"best cover\"). This paper describes in detail the combining method, presenting the algorithm and illustrations of its progress on one of many actual translations it has produced. It uses dynamic programming to efficiently compare weighted averages of sets of adjacent scored component translations. The current system operates primarily in a human-aided MT mode. The translation delivery system and its associated post-editing aide are briefly described, as is an initial evaluation of the usefulness of this method. Individual MT engines will be reported separately and are not, therefore, described in detail here.",
    "content": "1 INTRODUCTION Current MT systems, whatever translation method they employ, do not reach an optimal output on free text. In part, this is due to the inherent problems of a particular method — for instance, the inabil- ity of statistics-based MT to take into account long- distance dependencies, the difficulty in achieving ex- tremely broad coverage in knowledge-based MT sys- tems, or the reliance of most transfer-oriented MT systems on similarities in syntactic structures of the source and the target languages. Our hypothesis is that if an MT environment can use the best results from a variety of MT systems working simultaneously on the same text, the overall quality will improve. Using this novel approach to MT in the latest version of the Pangloss MT project, we submit an input text to a battery of machine translation systems (engines), collect their (possibly, incomplete) results in a joint chart data structure and select the overall best translation using a set of simple heuristics. 2 INTEGRATING MULTI-ENGINE OUTPUT In our experiment we used three MT engines: • a knowledge-based MT (KBMT) system, the mainline Pangloss engine (Frederking et al., 1993b); • an example-based MT (EBMT) system (see (Nirenburg et al., 1993; Nirenburg et al., 1994b); the original idea is due to Nagao (Na- gao, 1984)); and • a lexical transfer system, fortified with mor- phological analysis and synthesis modules and relying on a number of databases — a machine-readable dictionary (the Collins Span- ish/English), the lexicons used by the KBMT modules, a large set of user-generated bilingual glossaries as well as a gazetteer and a list of proper and organization names. The outputs from these engines (target language words and phrases) are recorded in a chart whose positions correspond to words in the source language input. As a result of the operation of each of the MT engines, new edges are added to the chart, each labeled with the translation of a region of the input string and indexed by this region's beginning and end positions. We will refer to all of these edges as components (as in \"components of the translation\") for the remainder of this article. The KBMT and EBMT engines also carry a quality score for each output element. The KBMT scores are produced based on whether any questionable heuristics were used in the source analysis or target generation. The EBMT scores are produced using a technique based on human judgements, as described in (Nirenburg et al., 1994a), submitted. User Translator's WorkStation Knowledge-Based MT Example-Based MT Lexical transfer MT Chart Manager Figure 1: Structure of a multi-engine MT system Figure 1 presents a general view of the operation of our multi-engine MT system. The chart manager selects the overall best cover from the collection of candidate partial translations by normalizing each component's quality score (positive, with larger be- ing better), and then selecting the best combination of components with the help of the chart walk algo- rithm. Figure 2 illustrates the result of this process on the example Spanish sentence: Al momento de su venta a Iberia, VIASA contaba con ocho aviones, que tenían en promedio 13 años de vuelo which can be translated into English as At the moment of its sale to Iberia, VIASA had eight airplanes, which had on average thirteen years of flight (time). This is a sentence from one of the 1993 ARPA MT evaluation texts. For each component, the starting and ending po- sitions in the chart, the corresponding source lan- guage words, and alternative translations are shown, as well as the engine and the engine-internal qual- ity scores. Inspection of these translations shows numerous problems; for example, at position 12, \"aviones\" is translated, among other things, as \"air- crafts\". It must be remembered that these were generated automatically from an on-line dictionary, without any lexical feature marking or other human intervention. It is well known that such automatic methods are at the moment less than perfect, to say the least. In our current system, this is not a major problem, since the results go through a mandatory editing step, as described below. 2.1 Normalizing the component scores The chart manager normalizes the internal scores to make them directly comparable. In the case of KBMT and EBMT, the pre-existing scores are mod- ified, while lexical transfer results are scored based on the estimated reliability of individual databases, from 0.5 up to 15. Currently the KBMT scores are reduced by a constant, except for known erroneous output, which has its score set to zero. The internal EBMT scores range from 0 being perfect to 10,000 being worthless; but the scores are nonlinear. So a region selected by a threshold is converted linearly into scores ranging from zero to a normalized max- imum EBMT score. The normalization levels were empirically determined in the initial experiment by having several individuals judge the comparative av- erage quality of the outputs in an actual translation run. In every case, the base score produced by the scor- ing functions is currently multiplied by the length of the candidate in words, on the assumption that longer items are better. We intend to test a variety of functions in order to find the right contribution of the length factor. 2.2 The chart walk algorithm Figure 3 presents the chart walk algorithm used to produce a single, best, non-overlapping, contigu- ous combination (cover) of the available component translations, assuming correct component quality scores. The code is organized as a recursive divide- and-conquer procedure: to calculate the cover of a region of the input, it is repeatedly split into two parts, at each possible position. Each time, the best possible cover for each part is recursively found, and the two scores are combined to give a score for the chart walk containing the two best subwalks. These different splits are then compared with each other and with components from the chart spanning the whole region (if any), and the overall best result is Position Input L R (Spanish) Output (English) E Q 0 1 Al \"In a minute\" G 10 momento \"At once\" \"A moment\" 2 2 de of from about D 2 for by 3 3 su his her its G 5 one's your their 4 4 venta inn sale selling G 5 marketing \"country inn\" \"small shop\" stall booth 5 5 a to a of D 2 6 6 Iberia Iberia G 5 7 7 G 5 8 8 VIASA VIASA D 2 9 10 contaba \"was rely on\" G 10 con \"rely on\" \"was count on\" \"count on\" \"was depending on\" \"depended on\" have 11 11 ocho eight eighth D 2 12 12 aviones airplane L 2.5 aeroplanes planes aircrafts airplanes martins hopscotches 13 13 G 5 14 14 que who that D 2 whom which 15 15 tenían \"were have\" G 5 \"have\" \"were hold\" hold \"were thinking\" thought \"were considering\" considered \"were deeming\" deemed \"were coming\" came 16 16 en in on onto D 2 at by 17 17 promedio average mean G 5 middle midpoint mid-point 18 18 13 13 L 15 19 21 años de \"years of E 8.8 vuelo experience with space flight\" \"flight activities\" \"of years\" 22 22 . D 2 Figure 2: Chart walk results used. The terminating step of this recursion is thus getting components from the chart. To find best walk on a region: if there is a stored result for this region then return it else begin get all primitive components for the region for each position p within the region begin split region into two parts at p find best walk for first part find best walk for second part combine into a component end find maximum score over all primitive and combined components store and return it end Figure 3: Chart walk algorithm Without dynamic programming, this would have a combinatorial time complexity. Dynamic program- ming utilizes a large array to store partial results, so that the best cover of any given subsequence is only computed once; the second time that a recursive call would compute the same result, it is retrieved from the array instead. This reduces the time complexity to O(n³), and in practice it uses an insignificant part of total processing time. All possible combinations of components are com- pared: this is not a heuristic method, but an efficient exhaustive one. This is what assures that the cho- sen cover is optimal. This assumes, in addition to the scores actually being correct, that the scores are compositional, in the sense that the combined score for a set of components really represents their quality as a group. This might not be the case, for example, if gaps or overlaps are allowed in some cases (per- haps where they contain the same words in the same positions). We calculate the combined score for a sequence of components as the weighted average of their individ- ual scores. Weighting by length is necessary so that the same components, when combined in a different order, produce the same combined scores. Otherwise the algorithm can produce inconsistent results. The chart walk algorithm can also be thought of as filling in the two-dimensional dynamic-programming array¹. Figure 4 shows an intermediate point in the filling of the array. In this figure, each element (i,j) is initially the best score of any single chart compo- nent covering the input region from word i to word j. Dashes indicate that no one component covers ex- 1 Note that this array is a different data structure from the chart. --- 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 0 5 10 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 2.5 2 -- .83 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 2 2 3 5 5 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 4 5 2 .25 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 6 5 7 5 8 2 3.5 7.3 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 9 5 10 7.3 6.1 5.9 5.2 5.2 4.8 4.8 5.8 5.7 5.4 6.5 6.2 10 2 2.0 2.1 2.8 2.7 3.0 2.9 3.1 4.5 4.5 4.3 5.5 5.3 11 2 12 2.2 3.1 2.8 3.3 3.0 3.3 3.5 4.8 4.8 4.5 5.9 5.5 13 2.5 3.7 3.1 3.6 3.3 3.5 5.2 5.1 4.8 6.3 5.9 14 5 3.5 4.0 3.5 3.8 5.6 5.5 5.1 6.7 6.2 15 2 16 5 17 2 18 3.5 3.0 3.5 5.8 5.6 5.1 6.9 6.3 19 3.5 4.0 6.7 6.4 5.6 7.6 6.9 20 3.5 7.3 6.7 5.8 8.0 7.2 21 10. 8.3 6.7 9.3 8.0 22 15 10. 7.3 10. 8.7 5 3.5 8.8 7.1 2 3.5 3.0 5 3.5 2 Figure 4: Triangular array filled in through (8,10) by chart walk --- 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 0 5 10 7.3 6.7 6.4 5.6 5.5 5.5 5.1 5.1 6.0 5.6 5.4 5.3 5.1 5.1 4.9 4.9 5.5 5.4 5.3 5.9 5.7 1 2.5 2.2 3.1 3.6 3.3 3.5 3.7 3.5 3.7 4.8 4.5 4.4 4.4 4.2 4.3 4.1 4.2 4.8 4.8 4.7 5.4 5.2 2 3.5 4.0 3.5 3.8 4.0 3.7 3.8 5.1 4.8 4.5 4.6 4.4 4.4 4.3 4.3 4.9 4.9 4.8 5.5 5.3 3 5 5.0 4.0 4.2 4.4 4.0 4.1 5.5 5.1 4.8 4.8 4.6 4.6 4.4 4.4 4.4 5.1 5.1 4.9 5.7 5.5 4 5 3.5 4.0 4.2 3.8 4.0 5.5 5.1 4.8 4.8 4.5 4.6 4.4 4.4 4.4 5.1 5.1 4.9 5.7 5.5 5 2 3.5 4.0 3.5 3.8 5.6 5.1 4.8 4.8 4.5 4.5 4.3 4.4 5.1 5.1 4.9 5.8 5.6 6 5 5.0 4.0 4.2 6.4 5.6 5.2 5.1 4.8 4.8 4.5 4.6 5.4 5.3 5.1 6.0 5.8 7 5 3.5 4.0 6.7 5.8 5.2 5.2 4.8 4.8 4.5 4.5 5.4 5.4 5.1 6.1 5.8 8 2 3.5 7.3 6.0 5.3 5.2 4.7 4.8 4.5 4.5 5.5 5.4 5.1 6.2 5.9 9 5 10 7.3 6.1 5.9 5.2 5.2 4.8 4.8 5.8 5.7 5.4 6.5 6.2 10 2 2.0 2.1 2.8 2.7 3.0 2.9 3.1 4.5 4.5 4.3 5.5 5.3 11 2 2.2 3.1 2.8 3.3 3.0 3.3 4.8 4.8 4.5 5.9 5.5 12 2.5 3.7 3.1 3.6 3.3 3.5 5.2 5.1 4.8 6.3 5.9 13 5 3.5 4.0 3.5 3.8 5.6 5.5 5.1 6.7 6.2 14 2 15 3.5 3.0 3.5 5.8 5.6 5.1 6.9 6.3 16 5 17 2 18 3.5 4.0 6.7 6.4 5.6 7.6 6.9 19 3.5 7.3 6.7 5.8 8.0 7.2 20 10. 8.3 6.7 9.3 8.0 21 15 10. 7.3 10. 8.7 22 5 3.5 8.8 7.1 3.5 3.0 5 3.5 2 Figure 5: Final array produced by chart walk actly that region. (In rows 1 through 7, the array has not yet been operated on, so it still shows its ini- tial state.) After processing (see rows 9 through 22), each element is the score for the best set of compo- nents covering the input from word i to word j (the best cover for this substring)<sup>2</sup>. (Only a truncated score is shown for each element in the figure, for readability. There is also a list of best components associated with each element.) The array is upper triangular since the starting position of a component i must be less than or equal to its ending position j. For any position, the score is calculated based on a combination of scores in the row to its left and in the column below it, versus the previous contents of the array cell for its position. So the array must be filled from the bottom-up, and left to right. Intuitively, this is because larger regions must be built up from smaller regions within them. For example, to calculate element (8,10), we com- pute the length-weighted averages of the scores of the best walks over the pair of elements (8,8) and (9,10) versus the pair (8,9) and (10,10), and compare them with the scores of any single chart components going from 8 to 10 (there were none), and take the maximum. Referring to Figure 2 again, this corre- sponds to a choice between combining the transla- tions of (8,8) VIASA and (9,10) contaba con versus combining the (not shown) translations of (8,9) VI- ASA contaba and (10,10) con. (This (8,9) element was itself previously built up from single word com- ponents.) Thus, we compare (2*1+10*2)/3 = 7.33 with (3.5*2+2*1)/3 = 3.0 and select the first, 7.33. The first wins because contaba con has a high score as an idiom from the glossary. Figure 5 shows the final array. When the element in the top-right corner is produced (5.78), the algo- rithm is finished, and the associated set of compo- nents is the final chart walk result shown in Figure 2. It may seem that the scores should increase to- wards the top-right corner. This has not generally been the case. While the system produces a num- ber of high-scoring short components, many low- scoring components have to be included to span the entire input. Since the score is a weighted aver- age, these low-scoring components pull the combined score down. A clear example can be seen at position (18,18), which has a score of 15. The scores above and to its right each average this 15 with a 5, for total values of 10.0 (all the lengths happen to be 1), and the score continues to decrease with distance from this point as one moves towards the final score, which does include the component for (18,18) in the cover. <sup>2</sup>In the actual implementation, the initial components are not present yet in the array, since the presence of an element indicates that the computation has been carried out for this position. They are accessed from the chart data structure as needed, but are shown here as an aid to understanding 2.3 Reordering components The chart-oriented integration of MT engines does not easily support deviations from the linear order of the source text elements, as when discontinuous con- stituents translate contiguous strings or in the case of cross-component substring order differences. We use a language pair-dependent set of postprocess- ing rules to alleviate this (for example, by switching the order of adjacent single-word adjective and noun components). 3 TRANSLATION DELIVERY SYSTEM Results of multi-engine MT were fed in our exper- iment into a translator's workstation (TWS) (Co- hen et al., 1993), through which a translator ei- ther approved the system's output or modified it. The main option for human interaction in TWS cur- rently is the Component Machine-Aided Translation (CMAT) editor (Frederking et al., 1993a). The user sees the original source language text in one editor window, and phrases marked by double angle brack- ets in another, each of which is the first translation from a candidate chosen by the chart walk. Menus, function keys and mouse clicks are used to perform both regular and enhanced editing actions. The most important enhancement provided is the ability to select an alternate translation with a popup menu, and instantly replace the system's initially chosen candidate translation string, which becomes the first alternative in this menu if it is used again. The alternate translations are the other translations from the chosen component<sup>3</sup>. As mentioned above, Figure 2 shows the sets of candidates in the best chart walk that are presented as choices to the human user through the CMAT editor in our example. 4 TESTING AND EVALUATING MULTI-ENGINE PERFORMANCE Automatically assessing the utility of the multi- engine system relative to the engines taken sepa- rately would be a useful development tool. The best method we could find was counting the number of keystrokes in the TWS to convert the outputs of in- dividual engines and the multi-engine configuration to a \"canonical\" human translation. A sample test on a passage of 2060 characters from the June 1993 evaluation of Pangloss is shown in figure 6. The difference in keystrokes was calculated as fol- lows: one keystroke for deleting a character; two <sup>3</sup>The CMAT editor may also include translations from other candidates, lower in the menu, if they have the same boundaries as the chosen candidate and the menu is not too long Type of translation Keystroke difference human tester (US Government 1542 Level 2 translator) word-for-word lookup in MRDS 1829 lookup in phrasal glossaries 1973 KBMT 1883 Example-Based MT 1876 Multi-engine configuration 1716 Figure 6: Results of keystroke test keystrokes for inserting a character; three keystrokes for deleting a word (in an editor with mouse action); three keystrokes plus the number of characters in the word being inserted for inserting a word. It is clear from the above table that the multi-engine config- uration works better than any of our available in- dividual engines, though it still does not reach the quality of a Level 2 translator. It is also clear that using keystrokes as a measure is not very satisfactory. It would be much better to make the comparison against the closest member of a set of equivalent paraphrastic translations, since there are many \"correct\" ways of translating a given input. However, this is predicated on the availability of a \"paraphraser\" system, developing which is not a trivial task. 5 CURRENT AND FUTURE WORK Ultimately, a multi-engine system depends on the quality of each particular engine. We expect the performance of KBMT and EBMT to grow. We plan to use a standard regression mechanism to modify the scoring system based on feedback from having humans select the best covers for test texts. The current system is human-aided. We have be- gun an experiment with a fully-automated mode, with the understanding that the quality will drop. The most important effect of this change is that accurate quality scores become much more impor- tant, since the first choice becomes the only choice. Besides improving the KBMT and EBMT scoring mechanisms, we need to provide finer distinctions for the lexical transfer engine's output. As the databases for this are quite large (all together, over 400,000 entries), adding scores to individual entries is, in the short run, prohibitive. We have not as yet discovered any feasible automatic technique for gen- erating such scores. Instead, we are planning to use an English language model on the output, in a man- ner similar to that done by speech and statistical translation systems (Brown et al., 1990). Statisti- cally generating such a model is feasible, since it does not rely on knowing correspondences between source and target languages. It is a weaker approach, but should go some distance in selecting between other- wise indistinguishable outputs. Another possible direction for future development would be to employ ideas from the area of heuristic search, and only run the highest-quality-score en- gine on each unit of source text. This assumes that we can reliably estimate scores in advance (not cur- rently true for the expensive engines), and that the engines can be run on fragments. A less ambitious version of this idea would be to run the low-scoring engines only where there are gaps in the normally high-scoring engines. References Brown, P., K. Cocke, S. Della Pietra, V.J. Della Pietra, F. Jelinek, J.D. Lafferty, R.L. Mercer and P.S. Roossin. \"A statistical approach to Ma- chine Translation\", Computational Linguistics 16, pp.79-85, 1990. Cohen, A., Cousseau, P., Frederking, R., Grannes, D., Khanna, S., McNeilly, C., Nirenburg, S., Shell, P., Waeltermann, D. Translator's WorkStation User Document, Center for Machine Translation, Carnegie Mellon University, 1993. Frederking, R., Grannes, D., Cousseau, P., and Nirenburg, S. \"An MAT Tool and Its Effective- ness.\" In Proceedings of the DARPA Human Lan- guage Technology Workshop, Princeton, NJ, 1993. Frederking, R., A. Cohen, P. Cousseau, D. Grannes and S. Nirenburg. \"The Pangloss Mark I MAT System.\" Proceedings of EACL-93, Utrecht, The Netherlands, 1993. Nagao, M. \"A framework of a mechanical translation between Japanese and English by analogy princi- ple.\" In: A. Elithorn and R. Banerji (eds.) Artifi- cial and Human Intelligence. NATO Publications, 1984. Nirenburg, S., C. Domashnev and D.J. Grannes. \"Two Approaches to Matching in Example-Based Machine Translation.\" Proceedings of TMI-93, Kyoto, 1993. Nirenburg, S., S. Beale and C. Domashnev. \"A Full-Text Experiment in Example-Based Machine Translation.\" Submitted to the International Con- ference on New Methods in Language Processing, Manchester, September 1994. Nirenburg, S., S. Beale, C. Domashnev and P. Sheridan. \"Example-Based Machine Translation of Running Text.\" In preparation."
  },
  {
    "title": "High Performance Segmentation of Spontaneous Speech Using Part of Speech and Trigger Word Information",
    "abstract": "We describe and experimentally evaluate an efficient method for automatically determining small clause boundaries in spontaneous speech. Our method applies an artificial neural network to information about part of speech and trigger words. We find that with a limited amount of data (less than 2500 words for the training set), a small sliding context window (+/-3 tokens) and only two hidden units, the neural net performs extremely well on this task: less than 5% error rate and F-score (combined precision and recall) of over .85 on unseen data. These results prove to be better than those reported earlier using different approaches.",
    "content": "1 Introduction In the area of machine translation, one important in- terface is that between the speech recognizer and the parser. In the case of human-to-human dialogues, the speech recognizer's output is a sequence of turns (a contiguous segment of a single speaker's utter- ance) which in turn can consist of multiple clauses. Lavie et al. (1996) discuss that using smaller units rather than whole turns can greatly facilitate the task of the parser since it reduces the complexity of its input. The problem is thus how to correctly segment an utterance into clauses. The segmentation procedure described in Lavie et al. (1996) uses a combination of acoustic infor- mation, statistical calculation of boundary-trigrams, some highly indicative keywords and also some heuristics from the parser itself. Stolcke and Shriberg (1996) studied the relevance of several word-level features for segmentation per- formance on the Switchboard corpus (see Godfrey et al. (1992)). Their best results were achieved by using part of speech n-grams, enhanced by a couple of trigger words and biases. Another, more acoustics-based approach for turn segmentation is reported in Takagi and Itahashi (1996). Palmer and Hearst (1994) used a neural network to find sentence boundaries in running text, i.e. to determine whether a period indicates end of sentence or end of abbreviation. The input to their network is a window of words centered around a period, where each word is encoded as a vector of 20 reals: 18 val- ues corresponding to the word's probabilistic mem- bership to each of 18 classes and 2 values represent- ing whether the word is capitalized and whether it follows a punctuation mark. Their best result of 98.5% accuracy was achieved with a context of 6 words and 2 hidden units. In this paper we bring their idea to the realm of speech and investigate the performance of a neural network on the task of turn segmentation using parts of speech, indicative keywords, or both of these fea- tures to hypothesize segment boundaries. 2 Data preparation For our experiments we took as data the first 1000 turns (roughly 12000 words or 12 full dialogues) of transcripts from the Switchboard corpus in a version that is already annotated for parts of speech (e.g. noun, adjective, personal pronoun, etc.). The definition of a small clause which we wanted the neural network to learn the boundaries of is as follows: Any finite clause that contains an in- flected verbal form and a subject (or at least either of them, if not possible otherwise). However, com- mon phrases such as good bye, and stuff like that, etc. are also considered small clauses. Preprocessing the data involved (i) expansion of some contracted forms (e.g. I'm → I am), (ii) correc- tion of frequent tagging errors, and (iii) generation of segment boundary candidates using some simple heuristics to speed up manual editing. Thus we obtained a total of 1669 segment bound- aries, which means that on average approximately after every seventh token (i.e. 14% of the text) there is a segment boundary. 3 Features and input encoding 3.1 Features The transcripts are tagged with part of speech (POS) data from a set of 39 tags¹ and were pro- cessed to extract trigger words, i.e. words that are frequently near small clause boundaries (<b>). Two scores were assigned to each word w in the transcript according to the following formulae: scorepre(w) = C(w<b>) P(w<b>/w) scorepost (w) = C(<b>w) P(<b>w/w) where C is the number of times w occurred as the word (before/after) a boundary, and P is the Bayesian estimate for the probability that a bound- ary occurs (after/before) w. This score is thus high for words that are likely (based on P) and reliable (based on C) predictors of small clause boundaries. The pre- and post-boundary trigger words were then merged and the top 30 selected to be used as features for the neural network. 3.2 Input encoding The information generated for each word consisted of a data label (a unique tracking number, the actual word, and its part of speech), a vector of real values 21,..., xe and a label ('+' or '-') indicating whether a segment boundary had preceded the word in the original segmented corpus. The real numbers 21,..., xe are the values given as input to the first layer of the network. We tested three different encodings: 1. Boolean encoding of POS: xi (1 ≤ i ≤ c = 39) is set to 0.9 if the word's part of speech is the ith part of speech, and to 0.1 otherwise. 2. Boolean encoding of triggers: xi (1 ≤ i ≤ c = 30) is set to 0.9 if the word is the ith trigger, and to 0.1 otherwise. 3. Concatenation of boolean POS and trigger en- codings (c = 39 + 30 = 69). 4 The neural network We use a fully connected feed-forward three-layer (input, hidden, and output) artificial neural net- work and the standard backpropagation algorithm to train it (with learning rate n = 0.3 and momen- tum a = 0.3). Given a window size of W and c features per en- coded word, the input layer is dimensioned to c x W units, that is W blocks of c units. The number of hidden units (h) ranged in our ex- periments from 1 to 25. The tagset is based on the standard tagsets of the Penn Treebank and the Brown Corpus. f-score 1 0.8 0.6 0.4 0.2 training validation test 0 0 5 10 15 epochs 20 25 30 35 Figure 1: Training the neural network. (Net with POS and trigger encoding, W = 6, h = 2,0 = 0.7) As for the output layer, in all the experiments it was fixed to a single output unit which indicates the presence or absence of a segment boundary just before the word currently at the middle of the win- dow. The actual threshold to decide between seg- ment boundary and no segment boundary is the pa- rameter & which we varied from 0.1 to 0.9. The data was presented to the network by sim- ulating a sliding window over the sequence of en- coded words, that is by feeding the input layer with the cx W encodings of, say, words wi...wi+w-1 and then, as the next input to the network, shifting the values one block (c units) to the left, thereby admit- ting from the right the c values corresponding to the encoding of witw. Note that at the beginning of each speaker turn or utterance the first c× (-1) input units need be padded with a \"dummy\" value, so that the first word can be placed just before the middle of the window. Symmetrically, at the end of each turn, the last cx (1) input units are also padded. 5 Results and discussion We created two data sets for our experiments, all from randomly chosen turns from the original data: (i) the \"small\" data set (a 20:20:60(%) split be- tween training, validation, and test sets), and (ii) the \"large\" data set (a 60:20:20(%) split). First, we ran 180 experiments on the \"small\" data set, exhaustively exploring the space defined by varying the following parameters: • encoding scheme: POS only, triggers only, POS and triggers. • window size: W∈ {2,4,6,8} • number of hidden units: h∈ {2, 10, 25} • output threshold: 0∈ { 0.1, 0.3, 0.5, 0.7, 0.9 } --- recall 1 0.8 0.6 0.4 0.2 both POS and triggers POS only triggers only 0 0 0.2 0.4 0.6 0.8 1 precision Figure 2: Precision vs. recall tradeoff. (On unseen data, net with W = 6, h = 2, 0.1 ≤ 0 ≤ 0.9) f-score 1 0.8 0.6 0.4 0.2 0 both POS and triggers POS only triggers only 0.1 0.2 0.3 0.4 0.5 0.6 threshold 0.7 0.8 0.9 Figure 3: F-scores as a function of the output unit threshold θ. (On unseen data, net with W = 6, h = 2) Precision (number of correct boundaries found by the neural network divided by total number of boundaries found by the neural network), recall (number of correct boundaries found by the neu- ral network divided by true number of boundaries in the data) and F-score (defined as 2 precision recall) precision+recall were computed for each training, validation and test sets. To be fair, we chose to take the epoch with the maximum F-score on the validation set as the best configuration of the net, and we report results from the test set only. Figure 1 shows a typical train- ing/learning curve of a neural network. The best performance was obtained using a net with 2 hidden units, a window size of 6 and the out- put unit threshold set to 0.7. The following results were achieved. classification rate precision recall F-score 95.8% 0.845 0.860 0.852 Some general trends are observed: • As the window size gets larger, the performance increases, but it seems to peak at around size 6. • Fewer hidden units yield better results; gener- ally we get the best results for just two hidden units. • The global performance as measured by the pro- portion of correct classifications (i.e. both '+' and '-') increases as the F-score increases. • High performance (correct classifications >95%, F-score >0.85) is easily achieved. • The optimal threshold for a high F-score lies in the 0.5 ≤ θ ≤ 0.7 interval. • Varying the threshold leads to a tradeoff of pre- cision vs. recall. To illustrate the last point, we present a graph that shows a comparison between the three encod- ing methods used, for a window size of 6 (Figure 2). The combined method is only slightly better than the POS method, but they both are clearly superior to the trigger-word method. Still it is interesting to note that quite a reasonable performance can be obtained just by looking at the 30 most indicative pre- and post-boundary trigger-words. Noteworthy is also the behavior of the precision-recall curves: with our method a high level of recall can be main- tained even as the output threshold is increased to augment precision. In Figure 3, we plot the F-score against the thresh- old. Whereas for the encodings POS only and POS and triggers, the peaks are in the region between 0.5 and 0.7, for the triggers only encoding, the best F-scores are achieved between 0.3 and 0.5. We also ran another 30 experiments with the \"large\" data set focusing on the region defined by the parameters that achieved the best results in the pre- ceding experiments (i.e. window size 6 or 8, thresh- old between 0.5 and 0.7, number of hidden units be- tween 1 and 10). Under these constraints, F-scores vary slightly, always remaining between .85 and .88 for both validation and test sets. Within this region, therefore, several neural nets yield extremely good performance. While Lavie et al. (1996) just report an im- provement in the end-to-end performance of the JANUS speech-to-speech translation system when us- ing their segmentation method but do not give de- tails the performance of the segmentation method itself, Stolcke and Shriberg (1996) are more explicit and provide precision and recall results. Moreover Lavie et al. (1996) deal with Spanish input whereas Stolcke and Shriberg (1996), like us, drew their data from the Switchboard corpus. --- Type Harmful? Reason Context false positive no trigger word to work <b> and * when I had false positive yes non-clausal and work off * and on false negative yes speech repair <b> but * and they are false positive ? trigger word he you know * gets to a certain false positive yes non-clausal and if you like trip * and fall or something false negative yes speech repair <b> we * that's been false positive no CORRECT <b> and i think * its relevance false negative no CORRECT <b> and she * she was false negative yes embedded relative clause into nursing homes * die very quickly false positive no trigger word wait lists * and all Table 1: Sample of misclassifications (on unseen data, net with encoding of POS and triggers, W = 6, h = 2, θ = 0.7). False positive indicates an instance where the net hypothesizes a boundary where there is none. False negative indicates an instance where the net fails to hypothesize a boundary where there is one. A '<b>' indicates a small clause boundary. A '*' indicates the location of the error. Thus here we compare our approach with that of Stolcke and Shriberg (1996). They trained on 1.4 million words and in their best system, achieved pre- cision .69 and recall .85 (which corresponds to an F-score of .76). We trained on 2400 words (i.e. over 500 times less training data), and we achieved an F-score of .85 (i.e. a 12% improvement). 6 Error analysis Table 1 shows 10 representative errors that one of the best performing neural network made on the test set. 25 randomly selected errors were used to do the error analysis, which consisted of 14 false positives and 11 false negatives. 8 of the errors were errors we considered to be harmful to the parser, 3 were errors of unknown harmfulness, and the remaining 14 were considered harmless. Of the harmful errors, three were due to the word and being used as a conjunction in a non-clausal context, two were due to a failure to detect a speech repair, and one was due to an embedded relative clause (most people that move into nursing homes * die very quickly). The network was also able to correctly identify some mistagged data (marked as CORRECT in Ta- ble 1). These results suggest that adding features rele- vant to speech repairs (such as whether words were repeated) or features relevant to detecting the use of and as a non-clausal conjunct might be useful in achieving better accuracy. 7 Conclusion We have shown that using neural networks for auto- matically segmenting turns in conversational speech into small clauses reaches a level of less than 5% error rate and achieves good precision/recall performance as measured by an F-score of more than .85. These results outperform those obtained by other methods as reported in the literature. Future work on this problem includes issues such as optimizing the set of POS tags, adding acous- tic/prosodic features to the neural network, and us- ing it for pro-drop languages like Spanish to as- sess the relative importance of POS vs. trigger word weights and to examine the performance of the sys- tem for languages where POS tags may not be as informative as they are for English. 8 Acknowledgements The work reported in this paper was funded in part by grants from ATR - Interpreting Telecommunications Re- search Laboratories of Japan, the US Department of De- fense, and the Verbmobil Project of the Federal Republic of Germany. This material is based on work supported under a Na- tional Science Foundation Graduate Fellowship. Any opinions, findings, conclusions or recommendations ex- pressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Sci- ence Foundation. References J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992. SWITCHBOARD: telephone speech corpus for research and development. In Proceedings of the ICASP-92, vol. I, pp. 517-520. A. Lavie, D. Gates, N. Coccaro, and L. Levin. 1996. Input segmentation of spontaneous speech in JANUS: a speech-to-speech translation system. In Proceedings of the ECAI-96. D. D. Palmer and M. A. Hearst. 1994. Adaptive sen- tence boundary disambiguation. In Proceedings of the ANLP-94. A. Stolcke and E. Shriberg. 1996. Automatic linguistic segmentation of conversational speech. In Proceedings of the ICSLP-96, pp. 1005-1008. K. Takagi and S. Itahashi. 1996. Segmentation of spo- ken dialogue by interjections, disfluent utterances and pauses. In Proceedings of the ICSLP-96, pp. 697-700."
  },
  {
    "title": "Building Effective Queries In Natural Language Information Retrieval",
    "abstract": "In this paper we report on our natural language information retrieval (NLIR) project as related to the recently concluded 5th Text Retrieval Conference (TREC-5). The main thrust of this project is to use natural language processing techniques to enhance the effectiveness of full-text document retrieval. One of our goals was to demonstrate that robust if relatively shallow NLP can help to derive a better representation of text documents for statistical search. Recently, we have turned our attention away from text representation issues and more towards query development problems. While our NLIR system still performs extensive natural language processing in order to extract phrasal and other indexing terms, our focus has shifted to the problems of building effective search queries. Specifically, we are interested in query construction that uses words, sentences, and entire passages to expand initial topic specifications in an attempt to cover their various angles, aspects and contexts. Based on our earlier results indicating that NLP is more effective with long, descriptive queries, we allowed for long passages from related documents to be liberally imported into the queries. This method appears to have produced a dramatic improvement in the performance of two different statistical search engines that we tested (Cornell's SMART and NIST's Prise) boosting the average precision by at least 40%. In this paper we discuss both manual and automatic procedures for query expansion within a new stream-based information retrieval model.",
    "content": "1. INTRODUCTION A typical (full-text) information retrieval (IR) task is to select documents from a database in response to a user's query, and rank these documents according to relevance. This has been usually accomplished using statistical methods (often coupled with manual encoding) that (a) select terms (words, phrases, and other units) from doc- uments that are deemed to best represent their content, and (b) create an inverted index file (or files) that pro- vide an easy access to documents containing these terms. A subsequent search process will attempt to match preprocessed user queries against term-based rep- resentations of documents in each case determining a degree of relevance between the two which depends upon the number and types of matching terms. Although many sophisticated search and matching methods are available, the crucial problem remains to be that of an adequate representation of content for both the docu- ments and the queries. In term-based representation, a document (as well as a query) is transformed into a collection of weighted terms, derived directly from the document text or indi- rectly through thesauri or domain maps. The representa- tion is anchored on these terms, and thus their careful selection is critical. Since each unique term can be thought to add a new dimensionality to the representa- tion, it is equally critical to weigh them properly against one another so that the document is placed at the correct position in the N-dimensional term space. Our goal here is to have the documents on the same topic placed close together, while those on different topics placed suffi- ciently apart. Unfortunately, we often do not know how to compute terms weights. The statistical weighting for- mulas, based on terms distribution within the database, such as tf*idf, are far from optimal, and the assumptions of term independence which are routinely made are false in most cases. This situation is even worse when single-word terms are intermixed with phrasal terms and the term independence becomes harder to justify. The simplest word-based representations of content, while relatively better understood, are usually inade- quate since single words are rarely specific enough for accurate discrimination, and their grouping is often acci- dental. A better method is to identify groups of words 299 that create meaningful phrases, especially if these phrases denote important concepts in the database domain. For example, \"joint venture\" is an important term in the Wall Street Journal (WSJ henceforth) data- base, while neither \"joint\" nor \"venture\" are important by themselves. There are a number of ways to obtain \"phrases\" from text. These include generating simple collocations, sta- tistically validated N-grams, part-of-speech tagged sequences, syntactic structures, and even semantic con- cepts. Some of these techniques are aimed primarily at identifying multi-word terms that have come to function like ordinary words, for example \"white collar\" or \"electric car\", and capturing other co-occurrence idio- syncrasies associated with certain types of texts. This simple approach has proven quite effective for some systems, for example the Cornell group reported (Buck- ley et al., 1995) that adding simple bigram collocations to the list of available terms can increase retrieval preci- sion by as much as 10%. Other more advanced techniques of phrase extraction, including extended N-grams and syntactic parsing, attempt to uncover \"concepts\", which would capture underlying semantic uniformity across various surface forms of expression. Syntactic phrases, for example, appear reasonable indicators of content, arguably better than proximity-based phrases, since they can adequately deal with word order changes and other structural varia- tions (e.g., \"college junior\" vs. \"junior in college\" vs. \"junior college\"). A subsequent regularization process, where alternative structures are reduced to a \"normal form\", helps to achieve a desired uniformity, for exam- ple, \"college+junior\" will represent a college for jun- iors, while \"junior+college\" will represent a junior in a college. A more radical normalization would have also \"verb object\", \"noun rel-clause\", etc. converted into col- lections of such ordered pairs. This head+modifier nor- malization has been used in our system, and is further described in this paper. It has to be noted, however, that the use of full-scale syntactic analysis is severely push- ing the limits of practicality of an information retrieval system because of the increased demand for computing power and storage. At the same time, while the gain in recall and precision has not been negligible (we recorded 10-20% increases in precision), no dramatic breakthrough has occurred either.1 1. Currently, the state-of-the art statistical and probabilistic IR sys- tem perform at about 20-40% precision range for arbitrary ad-hoc retrieval tasks. This state of affairs has prompted us take a closer look at the term selection and representation process. Our earlier experiments demonstrated that an improved weighting scheme for compound terms, including phrases and proper names, leads to an overall gain in retrieval accuracy. The fundamental problem, however, remained to be the system's inability to recognize, in the documents searched, the presence or absence of the con- cepts or topics that the query is asking for. The main rea- son for this was, we noted, the limited amount of information that the queries could convey on various aspects of topics they represent. Therefore, we started experimenting with manual and automatic query build- ing techniques. The purpose was to devise a method for full-text query expansion that would allow for creating exhaustive search queries such that: (1) the performance of any system using these queries would be significantly better than when the system is run using the original topics, and (2) the method could be eventually auto- mated or semi-automated so as to be useful to a non- expert user. Our preliminary results from TREC-5 eval- uations show that this approach is indeed very effective. In the rest of this paper we describe the overall organi- zation of our TREC-5 system, and then discuss some experiments that we performed and their results, as well as our future research plans. 2. STREAM-BASED INFORMATION RETRIEVAL MODEL Our NLIR system encompasses several statistical and natural language processing (NLP) techniques for robust text analysis. These has been organized together into a \"stream model\" in which alternative methods of document indexing are strung together to perform in parallel. Stream indexes are built using a mixture of dif- ferent indexing approaches, term extracting and weight- ing strategies, even different search engines. The final results are produced by merging ranked lists of docu- ments obtained from searching all stream indexes with appropriately preprocessed queries, i.e., phrases for phrase stream, names for names stream, etc. The merg- ing process weights contributions from each stream using a combination that was found the most effective in training runs. This allows for an easy combination of alternative retrieval and routing methods, creating a meta-search strategy which maximizes the contribution of each stream. Both Cornell's SMART version 11, and NIST's Prise search engines were used as basic engines.2 Our NLIR system employs a suite of advanced natural language processing techniques in order to assist the sta- tistical retrieval engine in selecting appropriate indexing terms for documents in the database, and to assign them semantically validated weights. The following term extraction methods have been used; they correspond to the indexing streams in our system. 1. Eliminate stopwords: original text words minus cer- tain no-content words are used to index documents. 2. Morphological stemming: we normalize across mor- phological word variants (e.g., \"proliferation\", \"pro- liferate\", \"proliferating\") using a lexicon-based stemmer. 3. Phrase extraction: we use various shallow text pro- cessing techniques, such as part-of-speech tagging, phrase boundary detection, and word co-occurrence metrics to identify stable strings of words, such as \"joint venture”. 4. Phrase normalization: we identify \"head+modifier\" pairs in order to normalize across syntactic variants such as \"weapon proliferation\", \"proliferation of weapons\", \"proliferate weapons\", etc. into \"weapon+proliferate\". 5. Proper names: we identify proper names for index- ing, including people names and titles, location names, organization names, etc. Among the advantages of the stream architecture we may include the following: • stream organization makes it easier to compare the contributions of different indexing features or repre- sentations. For example, it is easier to design exper- iments which allow us to decide if a certain representation adds information which is not contrib- uted by other streams. • it provides a convenient testbed to experiment with algorithms designed to merge the results obtained using different IR engines and/or techniques. • it becomes easier to fine-tune the system in order to obtain optimum performance • it allows us to use any combination of IR engines without having to modify their code at all. While our stream architecture may be unique among IR systems, the idea of combining evidence from multiple sources has been around for some time. Several researchers have noticed in the past that different sys- tems may have similar performance but retrieve differ- ent documents, thus suggesting that they may 2. SMART version 11 is freely available, unlike the more advanced version 12. complement one another. It has been reported that the use of different sources of evidence increases the perfor- mance of a system (see for example, Callan et al., 1995; Fox et al., 1993; Saracevic & Kantor, 1988). 3. STREAMS USED IN NLIR SYSTEM 3.1 Head-Modifier Pairs Stream Our most linguistically advanced stream is the head+modifier pairs stream. In this stream, documents are reduced to collections of word pairs derived via syn- tactic analysis of text followed by a normalization pro- cess intended to capture semantic uniformity across a variety of surface forms, e.g., \"information retrieval\", \"retrieval of information\", \"retrieve more information\", \"information that is retrieved\", etc. are all reduced to \"retrieve+information\" pair, where \"retrieve\" is a head or operator, and \"information\" is a modifier or argu- ment. The pairs stream is derived through a sequence of pro- cessing steps that include: • Part-of-speech tagging • Lexicon-based word normalization (extended \"stem- ming\") • Syntactic analysis with the TTP parser (cf. Strza- lkowski & Scheyen, 1996) • Extraction of head+modifier pairs • Corpus-based decomposition/disambiguation of long noun phrases. Syntactic phrases extracted from TTP parse trees are head-modifier pairs. The head in such a pair is a central element of a phrase (main verb, main noun, etc.), while the modifier is one of the adjunct arguments of the head. It should be noted that the parser's output is a predicate- argument structure centered around main elements of various phrases. The following types of pairs are consid- ered: (1) a head noun and its left adjective or noun adjunct, (2) a head noun and the head of its right adjunct, (3) the main verb of a clause and the head of its object phrase, and (4) the head of the subject phrase and the main verb. These types of pairs account for most of the syntactic variants for relating two words (or simple phrases) into pairs carrying compatible semantic con- tent. This also gives the pair-based representation suffi- cient flexibility to effectively capture content elements even in complex expressions. Long, complex phrases are similarly decomposed into collections of pairs, using corpus statistics to resolve structural ambiguities. 3.2 Linguistic Phrase Stream We used a regular expression pattern matcher on the part-of-speech tagged text to extract noun groups and proper noun sequences. The major rules we used are: 1. a sequence of modifiers (vbn|vbg|jj) followed by at least one noun, such as: \"cryonic suspend\", \"air traf- fic control system\"; 2. proper noun(s) modifying a noun, such as: \"u.s. citi- zen\", \"china trade\"; 3. proper noun(s) (might contain '&'), such as: \"warren commission\", \"national air traffic controller\". In these experiments, the length of phrases was limited to maximum 7 words. 3.3 Name Stream Proper names, of people, places, events, organizations, etc., are often critical in deciding relevance of a docu- ment. Since names are traditionally capitalized in English text, spotting them is relatively easy, most of the time. Many names are composed of more than a single word, in which case all words that make up the name are capitalized, except for prepositions and such, e.g., The United States of America. It is important that all names recognized in text, including those made up of multiple words, e.g., South Africa or Social Security, are repre- sented as tokens, and not broken into single words, e.g., South and Africa, which may turn out to be different names altogether by themselves. On the other hand, we need to make sure that variants of the same name are indeed recognized as such, e.g., U.S. President Bill Clinton and President Clinton, with a degree of confi- dence. One simple method, which we use in our system, is to represent a compound name dually, as a compound token and as a set of single-word terms. This way, if a corresponding full name variant cannot be found in a document, its component words matches can still add to the document score. A more accurate, but arguably more expensive method would be to use a substring compari- son procedure to recognize variants before matching. 3.4 Other Streams used The stems stream is the simplest, yet, it turns out, the most effective of all streams, a backbone in our multi- stream model. It consists of stemmed non-stop single- word tokens (plus hyphenated phrases). Our early experiments with multi-stream indexing using SMART suggested that the most effective weighting of this stream is Inc.ltc, which yields the best average preci- sion, whereas Inc.ntc slightly sacrifices the average pre- cision, but gives better recall (see Buckley, 1993). We used also a plain text stream. This stream was obtained by indexing the text of the documents \"as is\" without stemming or any other processing and running the unprocessed text of the queries against that index. Finally, some experiments involved the fragments stream. This was the result of spliting the documents of the STEM stream into fragments of constant length (1024 characters) and indexing each fragment as if it were a different document. The queries used with this stream were the usual stem queries. For each query, the resulting ranking was filtered to keep, for each docu- ment, the highest score obtained by the fragments of that document. Table 1 shows relative performance of each stream tested for this evaluation. Note that the standard stemmed-word representation (stems stream) is still the most efficient one, but linguistic processing becomes more important in longer queries. In this evaluation, the short queries are one-sentence search directives such as the following: What steps are being taken by governmental or even pri- vate entities world-wide to stop the smuggling of aliens. The long queries, on the other hand, contain substan- tially more text as the result of full-text expansion described in section 5 below. TABLE 1. How different streams perform relative to one another (11-pt avg. Prec) short long STREAM Stems Phrases queries queries 0.1682 0.2626 0.1233 0.2365 H+M Pairs 0.0755 0.2040 Names 0.0844 0.0608 4. STREAM MERGING STRATEGY The results obtained from different streams are list of documents ranked in order of relevance: the higher the rank of a retrieved document, the more relevant it is pre- sumed to be. In order to obtain the final retrieval result, ranking lists obtained from each stream have to be com- bined together by a process known as merging or fusion. The final ranking is derived by calculating the com- bined relevance scores for all retrieved documents. The following are the primary factors affecting this process: 1. document relevancy scores from each stream 2. retrieval precision distribution estimates within ranks from various streams, e.g., projected precision between ranks 10 and 20, etc.; 3. the overall effectiveness of each stream (e.g. mea- sured as average precision on training data) 4. the number of streams that retrieve a particular docu- ment, and 5. the ranks of this document within each stream. Generally, a more effective stream will more effect on shaping the final ranking. A document which is retrieved at a high rank from such a stream is more likely to end up ranked high in the final result. In addi- tion, the performance of each stream within a specific range of ranks is taken into account. For example, if phrases stream tends to pack relevant documents into top 10-20 retrieved documents (but not so much into 1- 10) we would give premium weights to the documents found in this region of phrase-based ranking, etc. Table 2 gives some additional data on the effectiveness of stream merging. Further details are available in a TREC conference article. TABLE 2. Precision improvements over stems- only retrieval short Streams merged queries %change long queries %change All streams +5.4 +20.94 Stems+Phrases+Pairs +6.6 +22.85 Stems+Phrases +7.0 +24.94 Stems+Pairs +2.2 +15.27 Stems+Names +0.6 +2.59 Note that again, long text queries benefit more from lin- guistic processing. 5. QUERY EXPANSION EXPERIMENTS 5.1 Why query expansion? The purpose of query expansion in information retrieval is to make the user query resemble more closely the documents it is expected to retrieve. This includes both content, as well as some other aspects such as composi- tion, style, language type, etc. If the query is indeed made to resemble a \"typical\" relevant document, then suddenly everything about this query becomes a valid search criterion: words, collocations, phrases, various relationships, etc. Unfortunately, an average search query does not look anything like this, most of the time. It is more likely to be a statement specifying the seman- tic criteria of relevance. This means that except for the semantic or conceptual resemblance (which we cannot model very well as yet) much of the appearance of the query (which we can model reasonably well) may be, and often is, quite misleading for search purposes. Where can we get the right queries? In today's information retrieval systems, query expan- sion usually pertains content and typically is limited to adding, deleting or re-weighting of terms. For example, content terms from documents judged relevant are added to the query while weights of all terms are adjusted in order to reflect the relevance information. Thus, terms occurring predominantly in relevant docu- ments will have their weights increased, while those occurring mostly in non-relevant documents will have their weights decreased. This process can be performed automatically using a relevance feedback method, e.g., Roccio's (1971), with the relevance information either supplied manually by the user (Harman, 1988), or other- wise guessed, e.g. by assuming top 10 documents rele- vant, etc. (Buckley, et al., 1995). A serious problem with this content-term expansion is its limited ability to cap- ture and represent many important aspects of what makes some documents relevant to the query, including particular term co-occurrence patterns, and other hard- to-measure text features, such as discourse structure or stylistics. Additionally, relevance-feedback expansion depends on the inherently partial relevance information, which is normally unavailable, or unreliable. Other types of query expansions, including general pur- pose thesauri or lexical databases (e.g., Wordnet) have been found generally unsuccessful in information retrieval (cf. Voorhees & Hou, 1993; Voorhees, 1994) An alternative to term-only expansion is a full-text expansion which we tried for the first time in TREC-5. In our approach, queries are expanded by pasting in entire sentences, paragraphs, and other sequences directly from any text document. To make this process efficient, we first perform a search with the original, un- expanded queries (short queries), and then use top N (10, 20) returned documents for query expansion. These documents are not judged for relevancy, nor assumed relevant; instead, they are scanned for passages that con- tain concepts referred to in the query. Expansion mate- rial can be found in both relevant and non-relevant documents, benefitting the final query all the same. In fact, the presence of such text in otherwise non-relevant documents underscores the inherent limitations of distri- bution-based term reweighting used in relevance feed- back. Subject to some further \"fitness criteria\", these expansion passages are then imported verbatim into the query. The resulting expanded queries undergo the usual text processing steps, before the search is run again. Full-text expansion can be accomplished manually, as we did initially to test feasibility of this approach, or automatically, as we tried in later with promising results. We first describe the manual process focussing on guidelines set forth in such a way as to minimize and streamline human effort, and lay the ground for eventual automation. We then describe our first attempt at auto- mated expansion, and discuss the results from both. The initial evaluations indicate that queries expanded manually following the prescribed guidelines are improving the system's performance (precision and recall) by as much as 40%. This appear to be true not only for our own system, but also for other systems: we asked other groups participating in TREC-5 to run search using our expanded queries, and they reported nearly identical improvements. At this time, automatic text expansion produces less effective queries than manual expansion, primarily due to a relatively unso- phisticated mechanism used to identify and match con- cepts in the queries. 5.2 Guidelines for manual query expansion We have adopted the following guidelines for query expansion. They were constructed to observe realistic limits of the manual process, and to prepare ground for eventual automation. 1. NLIR retrieval is run using the 50 original \"short\" queries. 2. Top 10 documentss retrieved by each query are retained for expansion. We obtain 50 expansion sub-collections, one per query. 3. Each query is manually expanded using phrases, sentences, and entire passages found in any of the documents from this query's expansion subcollec- tion. Text can both added and deleted, but care is taken to assure that the final query has the same for- mat as the original, and that all expressions added are well-formed English strings, though not neces- sarily well-formed sentences. A limit of 30 minutes per query in a single block of time is observed. 4. Expanded queries are sent through all text process- ing steps necessary to run the queries against multi- ple stream indexes. 5. Rankings from all streams are merged into the final result. There are two central decision making points that affect the outcome of the query expansion process following the above guidelines. The first point is how to locate text passages that are worth looking at -- it is impractical, if not downright impossible to read all 10 documents, some quite long, in under 30 minutes. The second point is to actually decide whether to include a given passage, or a portion thereof, in the query. To facilitate passage spotting, we used simple word search, using key con- cepts from the query to scan down document text. Each time a match was found, the text around (usually the paragraph containing it) was read, and if found \"fit\", imported into the query. We experimented also with var- ious \"pruning\" criteria: passages could be either imported verbatim into the query, or they could be \"pruned\" of \"obviously\" undesirable noise terms. In evaluating the expansion effects on query-by-query basis we have later found that the most liberal expan- sion mode with no pruning was in fact the most effec- tive. This would suggest that relatively self-contained text passages, such as paragraphs, provide a balanced representation of content, that cannot be easily approxi- mated by selecting only some words. 5.3 Automatic Query Expansion Queries obtained through the full-text manual expansion proved to be overwhelmingly better than the original search queries, providing as much as 40% precision gain. These results were sufficiently encouraging to motivate us to investigate ways of performing such expansions automatically. One way to approximate the manual text selection pro- cess, we reasoned, was to focus on those text passages that refer to some key concepts identified in the query, for example, \"alien smuggling\" for query 252 below. The key concepts (for now limited to simple noun groups) were identified by either their pivotal location within the query (in the Title field), or by their repeated occurrences within the query Description and Narrative fields. As in the manual process, we run a \"short\" query retrieval, this time retaining 100 top documents retrieved by each query. An automated process then scans these 100 documents for all paragraphs which contain occurrences, including some variants, of any of the key concepts identified in the original query. The paragraphs are subsequently pasted verbatim into the query. The original portion of the query may be saved in a special field to allow differential weighting. Finally, the expanded queries were run to produce the final result. The above, clearly simplistic technique has produced some interesting results. Out of the fifty queries we tested, 34 has undergone the expansion. Among these 34 queries, we noted precision gains in 13, precision loss in 18 queries, with 3 more basically unchanged. However, for these queries where the improvement did occur it was very substantial indeed: the average gain was 754% in 11-pt precision, while the average loss (for the queries that lost precision) was only 140%. Overall, we still can see a 7% improvement on all 50 queries (vs. 40%+ when manual expansion is used). Our experiments show that selecting the right para- graphs from documents to expand the queries can dra- matically improve the performance of a text retrieval system. This process can be automated, however, the challenge is to devise more precise automatic means of \"paragraph picking\". 6. SUMMARY OF RESULTS In this section we summarize the results obtained from query expansion and other related experiments. An automatic run means that there was no human inter- vention in the process at any time. A manual run means that some human processing was done to the queries, and possibly multiple test runs were made to improve the queries. A short query is derived using only one section of a TREC-5 topic, namely the DESCRIPTION field. A full query is derived from any or all fields in the original topic. A long query is obtained through our full-text expansion method (manual, or automatic). An example TREC-5 query is show below; note that the Description field is what one may reasonably expect to be an initial search query, while Narrative provides some further explanation of what relevant material may look like. The Topic field provides a single concept of interest to the searcher; it was not permitted in the short queries. <top> <num> Number: 252 <title> Topic: Combating Alien Smuggling <desc> Description: What steps are being taken by gov- ernmental or even private entities world-wide to stop the smuggling of aliens. <narr> Narrative: To be relevant, a document must describe an effort being made (other than routine border patrols) in any country of the world to prevent the illegal penetration of aliens across borders. </top> Table 3 summarizes selected runs performed with our NLIR system on TREC-5 database using queries 251 through 300. Table 4 gives the performance of Cornell's (now Sabir Inc.) SMART system version 12, using advanced Lnu.ltu term weighting scheme, and query expansion through automatic relevance feedback (rel.fbk), on the same database and with the same que- ries. Sabir used our long queries to obtain long query run. Note the consistently large improvements in retrieval precision attributed to the expanded queries. TABLE 3. Precision improvement in NLIR system PREC. 11pt. avg %change @10 docs %change @100 doc %change Recall %change long long short full queries queries queries 0.1478 queries auto. man. 0.2078 0.2220 0.3176 +41.0 +50.0 +115.0 0.1578 0.2044 0.2089 0.3156 +30.0 +32.0 +100.0 0.0544 0.0696 0.0709 0.0998 +28.0 +30.0 +83.0 0.59 0.65 0.64 +10.0 +8.5 0.77 +31.0 TABLE 4. Results for Cornell's SMART PREC, %change full long short full queries queries queries queries rel.fbk man. 0.2416 0.2983 +43.0 +62.0 +99.0 0.2178 11pt.avg 0.1499 0.2142 @5 docs %change @100 docs %change Recall @change 0.2889 0.2756 0.3600 +33.0 +27.0 +65.0 0.0578 0.0709 0.0771 0.0904 +23.0 +33.0 +56.0 0.58 0.64 0.70 0.73 +10.0 +21.0 +26.0 7. CONCLUSIONS We presented in some detail our natural language infor- mation retrieval system consisting of an advanced NLP module and a `pure' statistical core engine. While many problems remain to be resolved, including the question of adequacy of term-based representation of document content, we attempted to demonstrate that the architec- ture described here is nonetheless viable. In particular, we demonstrated that natural language processing can now be done on a fairly large scale and that its speed and robustness have improved to the point where it can be applied to real IR problems. The main observation to make is that natural language processing is not as effective as we have once hoped to obtain better indexing and better term representations of queries. Using linguistic terms, such as phrases, head- modifier pairs, names, or even simple concepts does help to improve retrieval precision, but the gains remained quite modest. On the other hand, full text query expansion works remarkably well. Our main effort in the immediate future will be to explore ways to achieve at least partial automation of this process. An initial experiment in this direction has been performed as part of NLP Track (genlp3 run), and the results are encouraging. ACKNOWLEDGEMENTS. We would like to thank Donna Harman of NIST for making her PRISE system available to us since the beginning of TREC. Will Rog- ers and Paul Over provided valuable assistance in installing updated versions of PRISE. We would also like to thank Ralph Weischedel for providing the BBN's part of speech tagger. We acknowledge the following members of our TREC-5 team who participated in the query expansion experiments: Louise Guthrie, Jussi Karlgren, Jim Leistensnider, Troy Straszheim, and Jon Wilding. This paper is based upon work supported in part by the Advanced Research Projects Agency under Tipster Phase-2 Contract 94-FI57900-000, Tipster Phase-3 Contract 97-FI56800-000, and the National Sci- ence Foundation under Grant IRI-93-02615. REFERENCES Buckley, Chris. 1993. \"The Importance of Proper Weighting Methods.\" Proc. of ARPA's Human Lan- guage Technology Workshop. pp. 349-352. Buckley, Chris, Amit Singhal, Mandar Mitra, Gerard Salton. 1995. \"New Retrieval Approaches Using SMART: TREC 4\". Proceedings of the Third Text REtrieval Conference (TREC-4), NIST Special Publ. Callan, James, Zhihong Lu, and Bruce Croft. 1995. Searching Distributed Collections with Inference Net- works.\" Proceedings of SIGIR-95. pp. 21-28. Fox, Ed, Prabhakar Kushik, Joseph Shaw, Russell Mod- lin and Durgesh Rao. 1993. \"Combining Evidence from Multiple Searches.\". Proc. of First Text Retrieval Con- ference (TREC-1). NIST Spec. Publ. 500-207. pp. 319- 328. Harman, Donna. 1988. \"Towards interactive query expansion.\" Proceedings of ACM SIGIR-88, pp. 321- 331. Roccio, J. 1971. \"Relevance Feedback in Information Retrieval.\" In G. Salton (ed), \"The SMART Retrieval System. Prentice-Hall, pp. 313-323. Saracevic, T., Kantor, P. 1988. \"A Study of Information Seeking and Retrieving. III. Searchers, Searches, and Overlap\". Journal of the American Society for Informa- tion Science. 39(3):197-216. Strzałkowski, Tomek and Jose Perez-Carballo. 1994. \"Recent Developments in Natural Language Text Retrieval.\" Proceedings of the Second Text Retrieval Conference (TREC-2), NIST Special Publication 500- 215, pp. 123-136. Strzalkowski, Tomek, Jose Perez-Carballo and Mihnea Marinescu. 1995. \"Natural Language Information Retir- ieval: TREC-3 Report.\" Proceedings of the Third Text REtrieval Conference (TREC-3), NIST Special Publica- tion 500-225, pp. 39-53. Strzalkowski, Tomek, Jose Perez-Carballo and Mihnea Marinescu. 1996. \"Natural Language Information Retir- ieval: TREC-4 Report.\" Proceedings of the Third Text REtrieval Conference (TREC-4), NIST Special Publ. Strzalkowski, Tomek. 1995. \"Natural Language Infor- mation Retrieval\" Information Processing and Manage- ment, Vol. 31, No. 3, pp. 397-417. Pergamon/Elsevier. Strzalkowski, Tomek, and Peter Scheyen. 1996. \"An Evaluation of TTP Parser: a preliminary report.\" In H. Bunt, M. Tomita (eds), Recent Advances in Parsing Technology, Kluwer Academic Publishers, pp. 201-220. Voorhees, Ellen. 1994. \"Query Expansion Using Lexi- cal-Semantic Relations.\" Proc. of SIGIR-94, pp. 61-70. Voorhees, Ellen, Yuan-Wang Hou. 1993. \"Vector Expansion in a Large Collection.\" Proc of First Text Retrieval Conference (TREC-1). NIST Spec. Pub. 500- 207. pp. 343-351."
  },
  {
    "title": "Rapid Parser Development: A Machine Learning Approach for Korean",
    "abstract": "This paper demonstrates that machine learning is a suitable approach for rapid parser development. From 1000 newly treebanked Korean sentences we generate a deterministic shift-reduce parser. The quality of the treebank, particularly crucial given its small size, is supported by a consistency checker.",
    "content": "1 Introduction Given the enormous complexity of natural language, parsing is hard enough as it is, but often unforeseen events like the crises in Bosnia or East-Timor create a sudden demand for parsers and machine transla- tion systems for languages that have not benefited from major attention of the computational linguis- tics community up to that point. Good machine translation relies strongly on the context of the words to be translated, a context that often goes well beyond neighboring surface words. Often basic relationships, like that between a verb and its direct object, provide crucial support for translation. Such relationships are usually provided by parsers. The NLP resources for a language of sudden inter- national interest are typically quite limited. There is probably a dictionary, but most likely no treebank. Maybe basic tools for morphological analysis, but probably no semantic ontology. This paper reports on the rapid development of a parser based on very limited resources. We show that by building a small treebank of only a thousand sentences, we could develop a good basic parser us- ing machine learning within only three months. For the language we chose, Korean, a number of research groups have been working on parsing and/or ma- chine translation in recent years (Yoon, 1997; Seo, 1998; Lee, 1997), but advanced resources have not been made publicly available, and we have not used any, thereby so-to-speak at least simulating a low density language scenario. 2 Korean Like Japanese, Korean is a head-final agglutinative language. It is written in a phonetic alphabet called hangul, in which each two-byte character represents one syllable. While our parser operates on the orig- inal Korean hangul, this paper presents examples in a romanized transcription. In sentence (1) for example, the verb is preceded by a number of so- called eojeols (equivalent to bunsetsus in Japanese) like \"chaeg-eul\", which are typically composed of a content part (\"chaeg\" = book) and a postposition, which often corresponds to a preposition in English, but is also used as a marker of topic, subject or ob- ject (\"eul\"). 나는 어제 그 책을 샀다. Na-neun eo-je geu chaeg-eul sass-da. ITOPIC yesterday this bookOBJ bought. I bought this book yesterday. (1) Our parser produces a tree describing the structure of a given sentence, including syntactic and semantic roles, as well as additional information such as tense. For example, the parse tree for sentence (1) is shown below: [1] na-neun eo-je geu chaeg-eul sass-da. [S] (SUBJ) [2] na-neun [NP] (HEAD) [3] na [REG-NOUN] (PARTICLE) [4] neun [DUPLICATE-PRT] (TIME) [5] eo-je [REG-ADVERB] (HEAD) [6] eo-je [REG-ADVERB] (OBJ) [7] geu chaeg-eul [NP] (MOD) [8] geu [DEMONSTR-ADNOMINAL] (HEAD) [9] geu [DEMONSTR-ADNOMINAL] (HEAD) [10] chaeg-eul [NP] (HEAD) [11] chaeg [REG-NOUN] (PARTICLE) [12] eul [OBJ-CASE-PRT] (HEAD) [13] sass-da. [VERB; PAST-TENSE] (HEAD) [14] sa [VERB-STEM] (SUFFIX) [15] eoss [INTERMED-SUF-VERB] (SUFFIX) [16] da [CONNECTIVE-SUF-VERB] (DUMMY) [17] . [PERIOD] Figure 1: Parse tree for sentence 1 (simplified) For preprocessing, we use a segmenter and mor- phological analyzer, KMA, and a tagger, KTAG, both provided by the research group of Prof. Rim of Korea University. KMA, which comes with a built- in Korean lexicon, segments Korean text into eojeols and provides a set of possible sub-segmentations and morphological analyses. KTAG then tries to select the most likely such interpretation. Our parser is initialized with the result of KMA, preserving all interpretations, but marking KTAG's choice as the top alternative. 3 Treebanking Effort The additional resources used to train and test a parser for Korean, which we will describe in more detail in the next section, were (1) a 1187 sentence treebank, (2) a set of 133 context features, and (3) background knowledge in form of an 'is-a' ontology with about 1000 entries. These resources were built by a team consisting of the principal researcher and two graduate students, each contributing about 3 months. 3.1 Treebank The treebank sentences are taken from the Korean newspaper Chosun, two-thirds from 1994 and the re- mainder from 1999. Sentences represent continuous articles with no sentences skipped for length or any other reason. The average sentence length is 21.0 words. 3.2 Feature Set The feature set describes the context of a partially parsed state, including syntactic features like the part of speech of the constituent at the front/top of the input list (as sketched in figure 2) or whether the second constituent on the parse stack ends in a comma, as well as semantic features like whether or not a constituent is a time expression or contains a location particle. The feature set can accommo- date any type of feature as long as it is computable, and can thus easily integrate different types of back- ground knowledge. 3.3 Background Knowledge The features are supported by background knowl- edge in the form of an ontology, which for example has a time-particle concept with nine sub-concepts (accounting for 9 of the 1000 entries mentioned above). Most of the background knowledge groups concepts like particles, suffixes, units (e.g. for lengths or currencies), temporal adverbs - semantic classes that are not covered by part of speech information of the lexicon, yet provide valuable clues for parsing. 3.4 Time Effort The first graduate student, a native Korean and linguistics major, hired for 11 weeks, spent about 2 weeks getting trained, 6 weeks on building two- thirds of the treebank, 2 weeks providing most back- ground knowledge entries and 1 week helping to top of stack -1 front/top of list <input list> 1 \"today\" synt: adv -3 parse stack -2 \"bought\" synt: verb \"a book\" synt: np * \"John\" synt: np (R 2 TO S-VP AS PRED OBJ) \"reduce the 2 top elements of the parse stack to a frame with syntax 'vp' and roles 'pred' and 'obj'\" \"John\" synt: np \"bought a book\" synt: vp sub: (pred) (obj) \"bought\" synt: verb \"a book\" synt: np * \"today\" synt: adv Figure 2: A typical parse action (simplified). Boxes represent frames. The asterisk (*) represents the current parse position. Optionally, parse actions can have additional arguments, like target syntactic or se- mantic classes to overwrite any default. Elements on the input list are identified by positive integers, elements on the parse stack by negative integers. The feature 'Synt of -1' for example refers to the (main) syntactic category of the top stack element. Before the reduce operation, the feature 'Synt of -1' would evaluate to np (for \"a book\"), after the operation to up (for \"bought a book\"). The in- put list is initialized with the morphologically analyzed words, possibly still ambiguous. After a sequence of shift (from input list to parse stack) and reduce (on the parse stack) operations, the parser eventually ends up with a single element on the parse stack, which is then returned as the parse tree. identify useful features. The other graduate student, a native Korean and computer science major, in- stalled Korean tools including a terminal for hangul and the above mentioned KMA and KTAG, wrote a number of scripts tying all tools together, made some tool improvements, built one-third of the treebank and also contributed to the feature set. The prin- cipal researcher, who does not speak Korean, con- tributed about 3 person months, coordinating the project, training the graduate students, writing tree- bank consistency checking rules (see section 6), mak- ing extensions to the tree-to-parse-action-sequence module (see section 4.1) and contributing to the background knowledge and feature set. 4 Learning to Parse We base our training on the machine learning based approach of (Hermjakob & Mooney, 1997), allow- ing however unrestricted text and deriving the parse action sequences required for training from a tree- bank. The basic mechanism for parsing text into a shallow semantic representation is a shift-reduce type parser (Marcus, 1980) that breaks parsing into an ordered sequence of small and manageable parse actions. Figure 2 shows a typical reduce action. The key task of machine learning then is to learn to pre- dict which parse action to perform next. Two key advantages of this type of deterministic parsing are that its linear run-time complexity with respect to sentence length makes the parser very fast, and that the parser is very robust in that it produces a parse tree for every input sentence. Figure 3 shows the overall architecture of parser training. From the treebank, we first automatically generate a parse action sequence. Then, for every step in the parse action sequence, typically several dozens per sentence, we automatically compute the value for every feature in the feature set, add on the parse action as the proper classification of the parse action example, and then feed these examples into a machine learning program, for which we use an ex- tension of decision trees (Quinlan, 1986; Hermjakob & Mooney, 1997). We built our parser incrementally. Starting with a small set of syntactic features that are useful across all languages, early training and testing runs reveal machine learning conflict sets and parsing errors that point to additionally required features and possibly also additional background knowledge. A conflict set is a set of training examples that have identical values for all features, yet differ in their classification (= parse action). Machine learning can therefore not possibly learn how to handle all examples correctly. This is typically resolved by adding an additional feature that differentiates between the examples in a linguistically relevant way. Even treebanking benefits from an incremental ap- proach. Trained on more and more sentences, and at the same time with also more and more features, parser quality improves, so that the parser as a tree- banking tool has to be corrected less and less fre- quently, thereby accelerating the treebanking pro- cess. Knowledge Base (\"ontology\") temporal-concept Treebank N month-of-the-year day-of-the-week N computer science N adverb parse action sequence generator (automatic) mass-noun Parse action sequence: Shift noun Monday ... Sunday syntactic-element T noun verb count-noun Feature set: Synt Synt of -2 of-1 Synt of 1 Shift noun Reduce 2 as mod head Done parse example generator (automatic) Parse action examples: Unavail Unavail Noun Unavail Noun Noun Noun Noun Unavail Unavail Noun Unavail Shift noun Shift noun Reduce 2 as mod head Done decision structure builder (automatic) Parse decision structure: Synt of 1 Noun Shift noun Unavail Unavail Synt of -2 Noun Done Reduce 2 as mod head Figure 3: Derivation of the parser from a treebank and a feature set. The resulting parser has the form of a decision structure, an extension of decision trees. Given a seen or unseen sentence in form of a list of words, the decision structure keeps selecting the next parse action until a single parse tree covering the entire sentence has been built. 4.1 Special Adaptation for Korean The segmenter and morphological analyzer KMA re- turns a list of alternatives for each eojeol. However, the alternatives are not atomic but rather two-level constituents, or mini-trees. Consider for example the following four¹ alternatives for the eojeol '3lil' (the 31st day of a month): 31/NUMERAL+ i/SUFFIX-NOUN + 1/OBJ-CASE-PRT 31/NUMERAL + i/NUMERAL + 1/OBJ-CASE-PRT 31/NUMERAL+ il/UNIT-NOUN 31/NUMERAL+ il/REGULAR-NOUN The analyzer divides '31il' into groups with varying number of sub-components with different parts of speech. When shifting in an element, the parser has to decide which one to pick, the third one in this case, using context of course. The module generating parse action sequences from a tree needs special split and merge operations for cases where the correct segmentation is not of- fered as a choice at all. To make things a little ugly, these splits can not only occur in the middle of a leaf constituent, but even in the middle of a character that might have been contracted from two charac- ters, each with its own meaning. 5 Chosun Newspaper Experiments Table 1 presents evaluation results with the number of training sentences varying from 32 to 1024 and with the remaining 163 sentences of the treebank used for testing. Precision: number of correct constituents in system parse number of constituents in system parse Recall: number of correct constituents in system parse number of constituents in logged parse Crossing brackets: number of constituents which violate constituent boundaries with a con- stituent in the logged parse. Labeled preci- sion/recall measures not only structural correctness, but also the correctness of the syntactic label. Cor- rect operations measures the number of correct operations during a parse that is continuously cor- rected based on the logged sequence; it measures the core machine learning algorithm performance in isolation. A sentence has a correct operating se- quence, if the system fully predicts the logged parse action sequence, and a correct structure and la- beling, if the structure and syntactic labeling of the final system parse of a sentence is 100% correct, re- gardless of the operations leading to it. Figures 4 and 5 plot the learning curves for two key metrics. While both curves are clearly heading ¹KMA actually produces 10 different alternatives in this case, of which only four are shown here. word level constituent labeled precision 87.0%- 86.0%- 85.0%- 84.0%- 32 64 128 256 512 1024 number of training sentences Figure 4: Learning curve for labeled precision corre- sponding to table 1 crossings brackets per sentence 2.1 + 2.0 1.9 1.8 1.7 1.6 1.5 32 64 128 256 512 1024 number of training sentences Figure 5: Learning curve for crossing brackets per sentence corresponding to table 1 in the right direction, up for precision, and down for crossing brackets, their appearance is somewhat jagged. For smaller data sets like in our case, this can often be avoided by running an n-fold cross val- idation test. However, we decided not to do so, because many training sentences were also used for feature set and background knowledge development Training sentences 32 64 128 256 512 1024 Precision 88.6% 88.1% 90.0% 89.6% 90.7% 91.0% Recall 87.3% 87.4% 89.2% 89.1% 89.6% 89.8% Labeled precision 84.1% 83.9% 85.8% 85.6% 86.7% 86.9% Labeled recall 81.2% 81.9% 83.6% 83.6% 84.7% 85.0% Tagging accuracy 94.3% 92.9% 93.9% 93.4% 94.0% 94.2% Crossings/sentence 1.97 2.00 1.72 1.79 1.69 1.63 0 crossings 27.6% 35.0% 38.7% 40.5% 43.6% 42.9% VIVIVIVIC < 1 crossing 56.4% 58.9% 63.2% 59.5% 64.4% 62.6% < 2 crossings 70.6% 72.4% 73.0% 71.8% 73.0% 74.2% < 3 crossings 81.0% 81.6% 82.2% 81.6% 82.2% 83.4% < 4 crossings 88.3% 84.0% 91.4% 89.0% 90.8% 89.6% Correct operations 63.0% 68.3% 71.5% 73.4% 75.0% 76.3% Operation Sequence 2.5% 6.1% 8.0% 8.6% 11.0% 7.4% Structure& Label 5.5% 12.9% 11.7% 16.0% 19.0% 16.0% Table 1: Evaluation results with varying number of training sentences as well as for intermediate inspection, and therefore might have unduly influenced the evaluation. 5.1 Tagging accuracy A particularly striking number is the tagging accu- racy, 94.2%, which is dramatically below the equív- alent 98% to 99% range for a good English or Japanese parser. In a Korean sentence, only larger constituents that typically span several words are separated by spaces, and even then not consistently, so that segmentation errors are a major source for tagging problems (as it is to some degree however also for Japanese²). We found that the segmen- tation part of KMA sometimes still struggles with relatively simple issues like punctuation, proposing for example words that contain a parenthesis in the middle of standard alphabetic characters. We have corrected some of these problems by pre- and post- processing the results of KMA, but believe that there is still a significant potential for further improve- ment. In order to assess the impact of the relatively low tagging accuracy, we conducted experiments that simulated a perfect tagger by initializing the parser with the correctly segmented, morphologically ana- lyzed and tagged sentence according to the treebank. By construction, the tagging accuracy in table 2 rises to 100%. Since the segmenter/tagger returns not just atomic but rather two-level constituents, the precision and recall values benefit particularly strongly, possibly inflating the improvements for these metrics, but other metrics like crossing brack- ets per sentence show substantial gains as well. Thus we believe that refined pre-parsing tools, as they are 2 While Japanese does not use spaces at all, script changes between kanji, hiragana, and katakana provide a lot of seg- mentation guidance. Modern Korean, however, almost exclu- sively uses only a single phonetic script. Segmentation/ Regular Simulating Tagging seg/tag as perfect (\"seg/tag\") implemented seg/tag Labeled precision 86.9% 93.4% Labeled recall 85.0% 92.9% Tagging accuracy Crossings/sentence 94.2% 100.0% 1.63 1.13 0 crossings < 2 crossings Structure&Label 42.9% 48.5% 74.2% 85.3% 16.0% 28.8% Table 2: Impact of segmentation/tagging errors in the process of becoming available for Korean, will greatly improve parsing accuracy. However, for true low density languages, such high quality preprocessors are probably not available so that our experimental scenario might be more re- alistic for those conditions. On the other hand, some low density languages like for example Tetun, the principal indigenous language of East Timor, are based on the Latin alphabet, separate words by spaces and have relatively little inflection, and there- fore make morphological analysis and segmentation relatively simple. 6 Treebank Consistency Checking It is difficult to maintain a high treebank quality. When training on a small treebank, this is particu- larly important, because there is not enough data to allow generous pruning. Treebanking is done by humans and humans err. Even with annotation guidelines there are often ad- ditional inconsistencies when there are several an- notators. In the Penn Treebank (Marcus, 1993) for example, the word ago as in 'two years ago', is tagged 414 times as an adverb and 150 times as a preposi- tion. In many treebanking efforts, basic taggers and parsers suggest parts of speech and tree structures that can be accepted or corrected, typically speed- ing up the treebanking effort considerably. How- ever, incorrect defaults can easily slip through, leav- ing blatant inconsistencies like the one where the constituent 'that' as in 'the dog that bit her' is tree- banked as a noun phrase containing a conjunction (as opposed to a pronoun). From the very beginning of treebanking, we have therefore passed all trees to be added to the tree- bank through a consistency checker that looks for any suspicious patterns in the new tree. For every type of phrase, the consistency checker draws on a list of acceptable patterns in a BNF style notation. While this consistency checking certainly does not guarantee to find all errors, and can produce false alarms when encountering rare but legitimate con- structions, we have found it a very useful tool to maintain treebank quality from the very beginning, easily offsetting the about three man days that it took to adapt the consistency checker to Korean. For a number of typical errors, we extended the checker to automatically correct errors for which this could be done safely, or, alternatively, suggest a likely correction for errors and prompt for confir- mation/correction by the treebanker. 7 Conclusions Comparisons with related work are unfortunately very problematic, because the corpora are differ- ent and are sometimes not even described in other work. In most cases Korean research groups also use other evaluation metrics, particularly dependency accuracy, which is often used in dependency struc- ture approaches. Training on about 40,000 sentences (Collins, 1997) achieves a crossing brackets rate of 1.07, a better value than our 1.63 value for regular parsing or the 1.13 value assuming perfect segmen- tation/tagging, but even for similar text types, com- parisons across languages are of course problematic. It is clear to us that with more training sentences, and with more features and background knowledge to better leverage the increased number of train- ing sentences, accuracy rates can still be improved significantly. But we believe that the reduction of parser development time from two years or more down to three months is in many cases already very valuable, even if the accuracy has not 'maxed out' yet. And given the experience we have gained from this project, we hope this research to be only a first step to an even steeper development time reduction. A particularly promising research direction for this is to harness knowledge and training resources across languages. Acknowledgments I would like to thank Kyoosung Lee for installing, improving and conncecting Korean pre-processing tools like segmenter and tagger as well as starting the treebanking, and Mina Lee, who did most of the treebanking. References M. J. Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. In 35th Proceedings of the ACL, pages 16-23. U. Hermjakob and R. J. Mooney. 1997. Learning Parse and Translation Decisions From Examples With Rich Context. In 35th Proceedings of the ACL, pages 482-489. URL: file://ftp.cs.utexas.edu/pub/mooney/papers /contex-acl-97.ps.Z U. Hermjakob. 1997. Learning Parse and Transla- tion Decisions From Examples With Rich Context. Ph.D. thesis, University of Texas at Austin, Dept. of Computer Sciences TR 97-12. URL: file://ftp.cs.utexas.edu/pub/mooney/papers /hermjakob-dissertation-97.ps.Z Geunbae Lee, Jong-Hyeok Lee, and Hyuncheol Rho. 1997. Natural Language Processing for Session- Based Information Retrieval Interface on the Web. In Proceedings of IJCAI-97 workshop on Al in dig- ital libraries, pages 43-48. M. P. Marcus. 1980. A Theory of Syntactic Recog- nition for Natural Language. MIT Press. M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of En- glish: The Penn Treebank. Computational Lin- guistics 19(2), pages 313-330. J. R. Quinlan. 1993. C4.5 Programs for Machine Learning. Morgan Kaufmann Publishers, San Ma- teo, California. K. J. Seo, K. C. Nam, and K. S. Choi. 1998. A Prob- abilistic Model for Dependency Parsing Consider- ing Ascending Dependencies. Journal of Literary and Linguistic Computing, Vol 13(2). Juntae Yoon, Seonho Kim, and Mansuk Song. 1997. New Parsing Method Using Global Association Table. In Proc. of the International Workshop on Parsing Technology."
  },
  {
    "title": "Multilingual Generation and Summarization of Job Adverts: the TREE Project",
    "abstract": "A multilingual Internet-based employment advertisement system is described. Job ads are submitted as e-mail texts, analysed by an example-based pattern matcher and stored in language-independent schemas in an object-oriented database. Users can search the database in their own language and get customized summaries of the job ads. The query engine uses symbolic case-based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way.",
    "content": "1 Introduction Free movement of labour across national boundaries is an important aim of the European Union.¹ One of the prerequisites for this open labour market is ac- cessibility of information about employment oppor- tunities, both from the point of view of people seek- ing work, and of their potential employers. However, many EU citizens are denied full access to employ- ment opportunities because information may not be readily available, and even where it is, it may not be available in the right language. The TREE project aims to address this problem by providing a system on the Internet where employers can deposit job ads, and which users can browse, each in their own lan- guage. Access to this service will be either through the user's own Internet provider, or at dedicated ter- minals located in employment centres. There are currently very many Internet sites where jobs are advertised, and indeed using information retrieval ¹TREE is Language Engineering project LE 1182 of the European Commission's Fourth Framework Pro- gramme. We would like to express our thanks to other partners on the project: Edy Geerts and Marianne Kamoen (VDAB, Vlaamse Dienst voor Arbeidsbemid- deling en Beroepsopleiding), Mick Riley (Newcastle upon Tyne City Council), and Teresa Paskiewicz and Mark Stairmand (UMIST). The URL for the project's web site is http://www.mari.co.uk/tree/. Joakim Nivre, Torbjörn Lager SSKKII, University of Göteborg, Sweden Jeremy Ellman, Alex Rogers MARI Computer Systems Ltd, Ashington, Northumberland, England techniques next to natural language processing to search job offer databases is not a new application, cf. (Vega, 1990; Caldwell & Korelsky, 1994). But no other application as far as we can discover offers the opportunity of searching and of getting summaries of job ads in languages other than that of the original announcement. TREE therefore offers two significant services: in- telligent search and summarization on the one hand, and these independent of the original language of the job ad on the other. It could be argued that the latter at least could be achieved by hooking a com- mercial Machine Translation (MT) system up to an Internet employment service. Although MT has had some success on the Internet (Flanagan, 1996), this is with largely sympathetic users who understand well the limitations of MT. Its use for a more del- icate task aimed at the general public, especially a public which is not necessarily highly educated, is certainly out of the question, for well known rea- sons which we need not explore here. Suffice to say that an experiment in Canada using an MT system for precisely this application (Murray, 1989) was far from successful. It is also apparent that for many jobs in a loca- tion where a different language is spoken, sufficient linguistic knowledge at least to read an ad for a job in that region would be one of the prerequisites of the job: this is certainly the case for the kind of pro- fessional positions often advertised on the Internet. Nevertheless, our system offers users the possibility of searching in their own language for jobs advertised in a variety of languages. Also, there is a significant workforce for which foreign-language skills are not a prerequisite for working abroad, and which, further- more, has traditionally been one of the most mobile: seasonal semi- and unskilled workers. For this rea- son, the domain we have chosen for the prototype development of the TREE project is the hotel and catering industry. 2 Overall design The TREE system stores job ads in a partly language-independent schematic form, and is ac- cessed by job-seeking users who can specify a num- ber of parameters which are used to search the job database, and who can also customize the way the information retrieved is presented to them. A sec- ond type of user is the potential employer who pro- vides job announcements to the system in the form of free text via an e-mail feed or, it is planned, via a form-filling interface (though we shall not discuss this latter input mode here). The initial prototype system currently imple- mented can store and retrieve job ads in three lan- guages - English, Flemish and French - regardless of which of these three languages the job was originally drafted in. The system has four key components which are the subject of this paper. Telematics, HCI and cer- tain other issues such as maintenance of the system (deleting old ads, user training, legality of texts in different countries) and the information retrieval as- pects of the system will not be discussed in this pa- per. The four components which we discuss here are: (a) the schema data structure for storing the job ads, and the associated terminological and lexical databases; (b) the analysis module for converting job ads received into their schematic form; (c) the query interface to allow users to specify the range of job ads they wish to retrieve; and (d) the gener- ator, which creates a customised selective summary of the job ads retrieved in HTML format. To a great extent, the design of each of these modules is not es- pecially innovative. However, the integration of all these functions is, from a methodological point of view, a good example of how a variety of techniques can be combined into a real application with a real use in the real world. 3 Data Structures 3.1 Job ad representation schema Job ads are stored in the system in a \"schema\", which is a typed feature structure consisting of named slots and fillers. The slots, some of which have a simple internal structure of their own, iden- tify elements of the job ad. Many, though not all of the slots can be specified as part of the search, and all of them can be generated as part of the job summary. The fillers for the slots may be coded language-independent references to the ter- minological database, source-language strings which can nevertheless be translated on demand with ref- erence to the \"lexicon\", or literal strings which will not be translated at all. The stylised partial example of a filled schema in Figure 1 gives an impression of the data structure. The distinction between terms and items in the lexicon is discussed below, but we consider first the design and implementation of the schema database. Figure 1: A partial example of a filled job schema. Slot names are shown in CAPITALS, fillers in quote marks are stored as strings; other fillers are coded. JOBCODE: 92563 WORKTIME: 2 JOB: waiter NUMBER_OF_JOBS: several LOCATION: \"Urmston\" SKILLS: EXPERIENCE:essential APPLICATION: PHONE: 224 8619 CONTACT NAME: \"Andrea\" ORIGINAL_TEXT: \"Urgent!!! P/T Waiters required, Urmston area. Experience essential. Phone Andrea on 224 8619.\" The main aim of the schema is to represent in a consistent way the information which the anal- ysis module extracts from the job ads, which the query module searches, and from which the genera- tion module produces text. Note that the example shown in Figure 1 is rather simplified for the pur- poses of illustration. The schema module provides a database of job schema instances (Onyshkevych, 1990). The analysis and design phases were con- ducted using the OMT (Rumbaugh, 1990) object- oriented methodology. Since the system currently treats three languages (with the prospect of exten- sion to more), we decided to codify in a language- neutral fashion the information extracted from the ads, converting equivalent linguistic terms into codes and vice versa via the analysis and generation mod- ules described below. 3.2 Terminology The terminology module has been designed with the general aim of supporting all the common function- alities shared by the analysis, generation and query modules and of supporting a language-independent term bank to permit multilingual handling of the schema database contents. We have focused on domain-specific terms and classifications, not cover- ing generic language issues nor providing a general lexicon and thesaurus. Different kinds of domain-specific information can be found as slot fillers, depending on the intended meaning of schema slots. The most relevant infor- mation is obviously job types. Existing job classifi- cations have been established for example by the Eu- ropean Commission's Employment Service (EURES, 1989), by the ILO (ILO, 1990) and several individ- ual companies; each provides a hierarchical classifi- cation of jobs, specifying, for each term, a distinct code, a description of the job, one or more generic terms commonly used to refer to the specific job, and possibly a set of synonyms. The description of the job ranges, depending on the classification, from a quite broad one to greatly detailed ones, sometimes highlighting differences existing in different coun- tries (e.g. according to the EURES classification, a \"waiter\" in some EU states is also required to act as a barman while in others is not). Job classifica- tions therefore provide at least three different kinds of information: • Definition of recognized job types, with a (more or less) precise definition of what the job is; chef is a recognized item, as well as pizza chef, while chef specializing in preparing hors d'oeuvres is not; classifications are obviously arbitrary as long the boundary between whether a specific job is a recognized one or simply an \"unrecog- nized\" classification simply depends on the level of granularity the classifier decides to use. • Classification of job types along ISA hierarchies (e.g. a wine waiter ISA type of waiter). • Linguistic information about commonly used terms and synonyms used in a given language (or more than one) to refer to the specific term. Accordingly, job classification terms are classified, coded (i.e. a distinct code identifying the term is associated with each term) and a list of standard \"names\" as well as recognized synonyms is asso- ciated with them. The classification and coding schema of VDAB, one of the end-user partners in the project, is used, but extensions deriving from other schema could obviously be envisaged. Trans- lation tables are provided for each term, containing the names used in the different languages. Align- ments across different languages are kept whenever possible. Problems due to missing equivalent terms in different languages, or to slightly different mean- ings, are handled, at least in the first stage, simply by providing terms nearer in meaning. An example of some job titles is shown in Figure 2: the hier- archical nature of the titles, and also the existence of some synonyms, is suggested by the numbering scheme, and is more or less self-explanatory. Figure 2: Examples of job codes and names in French, Flemish and English. 91200 cuisinier #kok # cook 91202 chef # chef # chef 91205 chef de cuisine # chef-kok # chief cook 91236 cuisinier de regime # dieetkok # diet cook 91237 cuisinier de cantine # kok grootkeuken # canteen cook 91241 commis de cuisine # keukenhulp # kitchen assistant 91241 commis de cuisine # keukenpersoneel # kitchen staff 91241 commis de cuisine # keukenhulp # catering assistant 91241 aide-cuisinier # hulpkok # assistant cook 91260 second cuisinier # hulpkok # second chef Codes are used as slot fillers in the schema database. This makes the schema neutral with re- spect to analysis, query and generation languages. For example, when searching for a job, the classifica- tion hierarchies inherent in the terminology database allow the user to express general search constraints (e.g. looking for a job as a chef), even though indi- vidual jobs are coded for specific types of chef (pas- trycook, pizza chef etc., and of course in different languages (e.g. Bakkersgast). Although the job titles themselves provide an ob- vious area of terminology, we handle various other areas of vocabulary in a similar way. There are two criteria for \"terminological status\" in our system, ei- ther of which is sufficient: (i) hierarchical structure, and (ii) standardization. An example of \"standard- ized vocabulary\" in our domain is words like full- time, part-time, which have an agreed meaning, or adjectives like essential as applied to requirements such as experience, or a driving licence. Of more interest perhaps is vocabulary which can be struc- tured, since this provides us with an opportunity to allow more sophisticated searching of the database. One example is types of establishment, e.g. ho- tel, restaurant, cafe, pub etc. Although such terms do not necessarily figure in recognized terminolog- ical thesauri, it is obvious that some structure can be imposed on these terms, for example to enable a user who is looking for a job in an eating estab- lishment to be presented with jobs in a variety of such places. Some hierarchies are trivially simple, for example full-time/part-time. A more interest- ing example is geographical location. Most job ads express the location of the work either explicitly or implicitly in the contact address. But often, these lo- cations are the names of towns or districts, whereas a user might want to search for jobs in a wider area: a user looking for work in Flanders, for example, should be presented with jobs whose location is iden- tified as Antwerp. This is not as simple as it seems however, since the kind of \"knowledge\" implicated in this kind of search facility is (literally!) \"real- world knowledge\" rather than linguistic knowledge: short of coding an entire gazeteer on the off-chance that some place-name appeared in a job ad, we must rather rely on the user trials envisaged later in our project to identify the extent to which geographical information needs to be included in the system. 3.3 Lexicon Not all the vocabulary that the system needs to rec- ognize and handle can be structured in the way just described, so we recognize a second type of lexical resource which, for want of a better term, we call simply \"the lexicon\". These are words which we of- ten find in job ads, associated with specific slots, which we would like to translate if possible, but which do not have the status of terms, since they are neither structured nor standardized. Examples are adjectives used to describe suitable applicants (e.g. young, energetic, experienced), phrases describ- ing the location (e.g. busy, near the seaside) or the employer (e.g. world-famous) and so on. Job ads that appear in newspapers and journals can be roughly classified according to their length (short, medium, long) with slightly different lex- ical and syntactic features accordingly (Alexa & Bárcena, 1992), the details of which need not con- cern us here. Some of the phrases found in typi- cal job ads serve to signal specific slots (e.g. EM- PLOYER:NAME is seeking JOB-TITLE), but these lin- guistic items do not appear in the lexicon as such. Such elements are regarded as being properly part of the analysis and generation modules, and we de- scribe below how they are handled there. 4 Analysis The system design permits users offering jobs to sub- mit via an e-mail feed job ads more or less without restrictions. The system converts these texts as far as possible into schematic representations which are then stored in the jobs database. The analysis tech- nique that we have chosen to implement falls into the relatively new paradigm of analogy- or example- based processing. In the following paragraphs we ex- plain the analysis process and discuss our reasons for preferring this over a more traditional string match- ing or parsing approach. The input that the TREE system will accept is partially structured, but with much scope for free- text input. One possible way of analysing this would be to employ a straightforward pattern-matching ap- proach, searching for \"trigger phrases\" such as EM- PLOYER:NAME is seeking JOB-TITLE, with special processors for analysing the slot-filler portions of the text. This simple approach has certain advantages over a more complex approach based on traditional phrase-structure parsing, especially since we are not particularly interested in phrase-structure as such. Furthermore, there is a clear requirement that our analysis technique be quite robust: since the input is not controlled in any way, our analysis procedure must be able to extract as much information as pos- sible from the text, but seamlessly ignore — or at least allocate to the appropriate \"unanalysable in- put\" slot — the text which it cannot interpret. However, both these procedures can be identified as essentially \"rule-based\", in the sense that linguis- tic data used to match, whether fixed patterns or syntactic rules, must be explicitly listed in a kind of grammar, which implies a number of disadvantages, which we will mention shortly. An alternative is sug- gested by the paradigm of \"example-based\" process- ing (Jones, 1996), now becoming quite prevalent in MT (Sumita et al., 1990; Somers, 1993), though in fact the techniques are very much like those of the longer established paradigm of case-based reasoning. 4.1 A flexible approach In the example-based approach, the \"patterns\" are listed in the form of model examples. Semi-fixed phrases are not identified as such, nor are there any explicit linguistic rules. Instead, a matcher matches new input against a database of already (correctly) analysed models, and interprets the new input on the basis of a best match (possibly out of several candidates); robustness is inherent in the system, since \"failure\" to analyse is relative. The main advantage of the example-based ap- proach is that we do not need to decide beforehand what the linguistic patterns look like. To see how this works to our advantage, consider the following. Let us assume that our database of already analysed examples contains an ad which includes the follow- ing: Knowledge of Dutch an advantage, and which is linked to a schema with slots filled roughly as fol- lows: SKILLS: LANGUAGE: LANG:nl SKILLS: LANGUAGE: REQ: \"an advantage\" Now suppose we want to process ads containing the following texts: Knowledge of the English language needed. (1) Some knowledge of Spanish would be helpful. (2) Very good knowledge of English. (3) In the rule-based approach, we would probably have to have a \"rule\" which specifies the range of (redun- dant) modifiers (asuming our schema does not store explicitly the level of language skill specified), that fillers for the REQ slots can be a past-participle, a predicative adjective or a noun, and are optional, and so on. Such rules carry with them a lot of bag- gage, such as optional elements, alternatives, restric- tions and so on. The biggest baggage is that some- one has to write them. In the example-based approach, we do not need to be explicit about the structure of the stored example or the inputs. We need to recognize Dutch, English and Spanish as being names of languages, but these words have \"terminological status\" in our system. If the system does not know would be helpful, it will guess that it is a clarification of the language re- quirement, even if it may not be able to translate it. Furthermore, we can extend the \"knowledge\" of the system simply by adding more examples: if they contain \"new\" structures, the knowledge base is ex- tended; if they mirror existing examples, the system still benefits since the evidence for one interpretation or another is thereby strengthened. 4.2 The matching algorithm The matcher, which has been developed from one first used in the MEG project (Somers et al., 1994), processes the new text in a linear fashion, having first divided it into manageable portions, on the ba- sis of punctuation, lay-out, formatting and so on. The input is tagged, using a standard tagger, e.g. (Brill, 1992). There is no need to train the tagger on our text type, because the actual tags do not matter, as long as tagging is consistent. The matching process then involves \"sliding\" one phrase past the other, identifying \"strong\" matches (word and tag) or \"weak\" (tag only) matches, and allowing for gaps in the match, in a method not un- like dynamic programming. The matches are then scored accordingly. The result is a set of possible matches linked to correctly filled schemas, so that even previously unseen words can normally be cor- rectly assigned to the appropriate slot. The approach is not without its problems. For example, some slots and their fillers can be quite ambiguous: cf. moderate German required vs. tall German required (!), while other text portions serve a dual purpose, for example when the name of the employer also indicates the location. However, the possibility of on-line or e-mail feedback to the user submitting the job ad, plus the fact that the matcher is extremely flexible, means that the analysis module can degrade gracefully in the face of such problems. 5 Query engine The query engine takes users' specifications of their employment interests to identify those job ads held in the database that match their specification. In- put is provided from an HTML form consisting of a number of fields which correspond to job-schema object attributes (e.g. job-title, location etc.). Data entered for any given object attribute is then en- coded in the same format used to encode job ad in- formation. Since both (searchable) job ad informa- tion and query data are represented in a language- independent format, matches will be made regard- less of the language in which the data was entered. Symbolic case-based reasoning techniques are used to quantify the extent to which users' queries match database objects, allowing the \"ranking\" of query results. 5.1 Encoding data Input entered by the user must be encoded using the same method adopted by the analysis module. There are two means by which this can be achieved. One method is to restrict the options available to the user for any given field to a number of possi- ble values for a given object attribute (i.e. provide the user with a Boolean choice). The alternative is to allow users to enter a string which is passed to the terminology module to retrieve the appropriate code. If the string does not return a code, it is con- sidered invalid and the user is requested to enter an alternative. 5.2 Applying case-based reasoning User-entered information is used to construct a job- schema object which can be considered as the user's \"ideal\" job. Symbolic case-based reasoning tech- niques are then applied to quantify the difference between the user's ideal job and jobs held within the database in order to identify those jobs most closely resembling the user's ideal job. The purpose of using case-based reasoning tech- niques is to quantify the difference (as a metric value) between any two instances of a job-schema object. That object must be capable of being de- fined by one or more parameters, with the further requirement that comparison operations upon any two parameter values must yield a numeric value re- flecting the semantic difference between the values. Thus, objects can be seen as being located within an n-dimensional parameter space where n is the num- ber of defining parameters of the object. The parameters which are used to define job ads for TREE are given by the job schema definition, described above. The distance between two values for a specific parameter will be dependent upon the method of encoding but any distance function & for a given parameter must define the geometric distance between its two arguments (Salzberg & Cost, 1993). That is: a value must have a distance of zero to it- self (4), a positive distance to all other values (5), distances must be symmetric (6) and must obey the triangle inequality (7). A further proviso is added that the maximum difference between any two pa- rameter values must be 1, which ensures that all pa- rameters have an equivalent maximal difference (8). δ(a, a) = 0 δ(a, b) > 0 if a ≠ b δ(a, b) = δ(b, a) δ(a, b) + δ(b, c) ≥ δ(a, c) δ(a, b) ≤ 1 (4) (5) (6) (7) (8) For example, a distance function for the job-title pa- rameter (as represented by job-title codes illustrated in Figure 2) could be given by (9), δ(a, b) = f(a - b) n (9) where a and b are job codes, f(x) returns the number of digits of its argument, and n is the number of digits in the job codes (i.e. n = 5). δ(a, b) evaluates to 1 if the job code arguments differ on the first digit, 0.8 if they differ on the second digit and so on. The job codes are hierarchically ordered so job- title codes that differ over the first digit will refer to greatly different jobs. As such we can see that this parameter distance function would reflect common- sense judgements on the associated job-titles. The total distance between any two job instances is simply a measure of the distances between indi- vidual parameter distances and is given by (10), N Δ(Α, Β) = ∑ Σ δι(α;, bi) i=1 (10) where A is the instance distance function, d; is the distance function for parameter i, N is the total number of parameters by which A and B are de- fined, and a; and bi are the values of parameter i for instances A and B respectively. Equation (10) provides a measure of the total dis- tance between two instances by summing the dis- tances between all the constituent parameters. Us- ing (10) and a set of parameter distance functions that conform to the properties given as (4)-(8), it is possible to quantify the difference between any job- schema instance held in the database and the \"ideal\" job-schema object specified by the user. Those pa- rameters for which no value has been specified will exactly match every possible parameter value, and as such the database search is only constrained by those values which users enter. Since information on job ads is represented in a language-independent format, a search profile in one language will retrieve job ad information entered in any of languages supported. Database queries are conducted by matching the \"ideal\" job as specified by the user against job-schemas held in the database. The matching process yields a numeric result repre- senting the \"distance\" between two objects. Identi- fied jobs can then be ranked according to how closely they resemble the user's ideal job. The results of a database query are then fed to the generation mod- ule for subsequent presentation in the language spec- ified by the user. Future plans include increasing the number of fields over which the search can be conducted and permitting users to specify the relative importance of each parameter to the search. The query interface will also keep a record of user \"profiles\", so that reg- ular users can repeat a previous search the next time they use the system. 6 Generation The purpose of the TREE generator module is to generate HTML documents in different languages from job database entries (i.e. filled or partially filled schemas), on demand. For several reasons, the approach to generation adopted in the TREE system can be termed \"integrated\". First, it inte- grates canned text, templates, and grammar rules into a single grammar formalism. Second, it inte- grates conditions on the database with other cat- egories in the bodies of grammar rules. Third, it integrates the generation of sentences and the gen- eration of texts and hypertexts in a simple, seamless way. Finally, generation involves just one single, ef- ficient process which is integrated in the sense that no intermediate structures are created during pro- cessing. 6.1 Formalism In our integrated approach to generation, a grammar rule has the format (11), Co/So (11) SS1,..., SSn # Conditions where each SS; has the format C₁, the format Ci/Si, or the format [W1,...,Wm]. Here, Ci denotes a syn- tactic category, Si denotes a semantic value, and Wi a word. The slash symbol \"/\" is used to separate the syntax from the semantics. The symbol \"#\" sepa- rates the grammar body from a set of conditions on the database. If the set of conditions is empty, the symbol \"#\", and what follows it, may simply be omitted. 6.2 Canned text, templates, or grammar? Suppose a system \"knows\" something, on which we want it to report; suppose it knows that both the Cafe Citrus and the Red Herring Restaurant want to hire chefs, facts which could be captured by the following (logical interface to the) job database: item(e1,x1,y1). job(y1,91202). company (x1, 'Cafe Citrus'). item(e2,x2,y2). job(y2,91202). company (x2, 'Red Herring Restaurant'). We can imagine setting up our system in such a way that when the system sees facts of this kind, a rule such as the following - s/E --> ['Cafe Citrus', advertises, as, vacant,a, position, as, chef] # {item(E,X,Y), job (Y, 91202), company (X, 'Cafe Citrus')}. will be triggered, and the system will produce the sentence Cafe Citrus advertises as vacant a position as chef. This is a canned-text approach. It is triv- ial to implement, but the disadvantage is, of course, that we would have to store one rule for each utter- ance that we would like our system to produce. As soon as a sentence must be produced several times with only slight alterations, a template-based approach is more appropriate. Let us modify the above rule as follows: s/E --> pn/X^name (X,C), [advertises, as, vacant, a, position,as, chef] # {item(E,X,Y), job(Y, 91202), company(X,C)}. The following rule is needed to tell the system that it is allowed to realize the value of the feature <company> as the value itself (i.e. the value is the name of the company). pn/X^name (X,Name) --> [Name]. Thus, here too, given the above job database entry, the sentence Cafe Citrus advertises as vacant a po- sition as chef can be generated. Furthermore, Red Herring Restaurant advertises as vacant a position as chef can be generated as well. It is not hard to see that the two rules above form the beginning of a grammar. Such a grammar may be further elaborated as follows: s/E --> np/X, vp/X^E^A # {A}. np/X --> pn/X^A # {A}. np/X --> n/X^Α # {A}. vp/A --> v/X^Α, np/Χ. pn/X^company(X, Name) --> [Name]. n/X^job(X,91202) --> [chef]. v/Y^X^E^item (E,X,Y) --> [advertises, as, vacant, a, position, as]. Now, the above sentences, plus many other sentences, may be generated, given appropriate database entries. Our approach is based on the idea that canned- text approaches, template-based approaches and grammar-based approaches to natural language gen- eration — while they are often contrasted — may in fact be regarded as different points on a scale, from the very specific to the very general. In a sense, tem- plates are just generalized canned texts, and gram- mars are just generalized templates. Indeed, the pos- sibility of combining these different modes of gener- ation has recently been highlighted as one of the keys to efficient use of natural language generation techniques in practical applications (van Noord & Neumann, 1996; Busemann, 1996). 6.3 Processing Let us now indicate how the rules are meant to be used by the generator module. Traditionally, the process of generation is divided into two steps: gen- eration of message structure from database records (what to say), and generation of sentences from mes- sage structures (how to say it). One way of charac- terizing the integrated approach to generation is to say that we go from database records to sentences in just one step. The process of computing what to say, and the process of computing how to say it, are, in the general case, interleaved processes. The process of generating from a set of grammar rules, given a particular job database entry, will simply in- volve picking the rules the conditions of which (best) match the entry, and using them to generate a doc- ument. 6.4 Generating hypertext The TREE system provides its output in the form of hypertext. This approach has several advantages: first, as argued by (Reiter & Mellish, 1993), the gen- eration of hypertext can obviate the need to perform high-level text structuring, such as assembling para- graphs into documents. \"The basic idea is to use hy- pertext mechanisms to enable users to dynamically select the paragraphs they wish to read, and there- fore in essence perform their own high-level text- planning\" (Reiter & Mellish, 1993), p.3. Second, but related to the first point, the hypertext capabil- ities are also a mild form of tailoring to the needs of different users. Users are expected to explore only links containing information that they need. Hypertext is generated by means of rules that are very similar to the grammar rules described above, but are formulated on a meta-level with re- spect to sentence/text rules. HTML code \"wrap- pers\" can be simply generated around the text. It is fairly straightforward to extend the grammar to other HTML constructions, such as headers, styles, lists, and tables. Using such rules in combination with other rules enables us to produce simple HTML documents, or, if required, quite complex and deeply nested documents incorporating links to other ads, or buttons to expand information, or clarify termi- nology (e.g. to get a definition of an unfamiliar job- title). 7 Conclusion The European Union is a loose geo-political organi- zation that has eleven official languages. As such, it is clear that even in a restricted domain such as that of job ads, novel approaches to Language Engineer- ing are required. In this paper we have described an approach that summarizes ads into a base schema, and then gener- ates output in the desired language in a principled, though restricted way. At first glance, this may look like old-fashioned interlingual MT, but there are two important differences. First, our approach is inher- ently \"lossy\", in that not all the information in the input ad may be analysed into the schema. It cannot consequently be included in the generated output. Second, the format of the output can be controlled and customised by the user which means again that the output text is a summary or digest, not nec- essarily presented in the same order as the original text. For both these reasons, our system cannot be descibed as a \"translation system\". Nonetheless we believe this approach is capable of giving consider- able coverage at a far lower cost and higher quality than that usually associated with MT. Our approach is not without some disadvantages however: it is well known that a considerable quan- tity of the semantics of human language is culturally and socially determined. Thus, even though one can map the names of job categories from one language to another, it is not necessarily true that they mean the same thing. So for example, waiters in Spain are expected to serve snacks, whereas in Belgium they do not. There is of course no easy solution to these problems from the Language Engineering point of view: our service must simply advise users to check that the job description in the target country corre- sponds to their understanding. Legal constraints are also a significant issue in the area of job advertising. Thus, whilst most coun- tries in the EU have legislation to prevent race and sex discrimination in job advertising, some do not. Thus a Spanish bar can (or could until recently) ad- vertise for Pretty girls wanted as bar staff, and Men wanted to work in the kitchen. This type of discrim- ination is illegal in the UK where it would violate Sex Equality Legislation. Thus we must generate non-discriminatory text to avoid running foul of UK law. This clearly shows how practical applications of Language Engineering have to conform in unfore- seen ways to the real world. Our future work will continue to extend the prag- matic approach taken so far. In particular, we are being encouraged to broaden the coverage of our sys- tem to include many more employment domains. It remains to be seen what are the consequences of this scaling on what has so far proved to be a simple but effective architecture. References Alexa, Melpomeni & Elena Bárcena. 1992. A cross- linguistic study of the sublanguage of short job advertisements for the design of a multilingual text generation system (MEG). CCL/UMIST Re- port 92/8, Centre for Computational Linguistics, UMIST, Manchester. Brill, Eric. 1992. A simple rule-based part of speech tagger. In Third Conference on Applied Natural Language Processing, Trento, Italy, pp. 153-5. Busemann, Stephan. 1996. Best-first sur- face realization. Computation and Lan- guage E-Print Archive cmp-lg/9605010. URL http://xxx.lanl.gov/cmp-lg/. Caldwell, David E. & Tatiana Korelsky. 1994. Bilin- gual generation of job descriptions from quais- conceptual forms. In Fourth Conference on Applied Natural Language Processing, Stuttgart, Germany, pp. 1-6. EURES. 1989. Communication of the comparison of vocational training qualifications between mem- ber states established in implementation of Com- mission Decision 85/368/EEC of 16th July 1985: Hotel and Catering Industry. Official Journal of the European Communities 32, C166, 3 July 1989, pp. 1-56. Flanagan, Mary. 1996. Two years online: experi- ences, challenges and trends. In Expanding MT Horizons: Proceedings of the Second Conference of the Association for Machine Translation in the Americas. Montreal, Canada, pp. 192-7. ILO. 1990. International Standard Classification of Occupations: ISCO-88, International Labour Office, Geneva. Jones, Daniel. 1996. Analogical Natural Language Processing, UCL Press, London. Murray, Pamela. 1989. A review of MT policy and current commercial systems in Canada with a view to illustrating the importance of sublan- guages in successful MT application. MSc disser- tation, UMIST, Manchester. Onyshkevych, Boyan. 1993. Template Design for Information Extraction. In Proceedings of the Fifth Message Understanding Conference (MUC- 5), Baltimore, Md., pp. 19-23. Reiter, Ehud & Chris Mellish 1993. Optimis- ing the costs and benefits of natural language generation. In Proceedings of the 13th Interna- tional Joint Conference on Artificial Intelligence, Chambéry, France, pp. 1164-71. Rumbaugh, James. 1995. OMT: the Object Model. Journal of Object-Oriented Programming, 7.8:21- 7. Salzberg, Steven & Scott Cost. 1993. A weighted nearest neighbour algoritm for learning with sym- bolic features. Machine Learning 10:57-78. Somers, Harold L. 1993. La traduction automatique basée sur l'exemple ou sur les corpus. In La tra- ductique: Études et recherches de traduction par ordinateur, Pierrette Bouillon & André Clas (eds), Les Presses de l'Université de Montréal, pp. 149- 66. Somers, Harold L., Ian McLean & Daniel Jones. 1994. Experiments in multilingual example-based generation. In CSNLP 1994: 3rd Conference on the Cognitive Science of Natural Language Pro- cessing, Dublin, Ireland. Sumita, Eiichiro, Hitoshi Iida & Hideo Kameyama. 1990. Translating with examples: a new approach to Machine Translation. In The Third Interna- tional Conference on Theoretical and Methodolog- ical Issues in Machine Translation of Natural Lan- guage, Austin, Texas, pp. 203-12. van Noord, Gertjan & Günter Neumann. 1996. Syn- tactic generation. In Survey of the State of the Art in Human Language Technology (Ronald A. Cole, general ed.), Chapter 4 (Hans Uszkoreit, ed.). Available at http://www.cse.ogi.edu/CSLU/ HLTsurvey/ch4node4.html#SECTION42. To be published by Cambridge University Press. Vega, José. 1990. Semantic matching between job offers and job search requests. In COLING-90: Papers presented to the 13th International Confer- ence on Computational Linguistics, Helsinki, Fin- land, Vol.1 pp. 67-9."
  },
  {
    "title": "Who did what to Whom? Language models and humans respond diversely to features affecting argument hierarchy construction",
    "abstract": "Pre-trained transformer-based language models have achieved state-of-the-art performance in many areas of NLP. It is still an open question whether the models are capable of integrating syntax and semantics in language processing like humans. This paper investigates if models and humans construct argument hierarchy similarly with the effects from telicity, agency, and individuation, using the Chinese structure \"NP1+BA/BEI+NP2+VP\". We present both humans and six transformer-based models with prepared sentences and analyze their preference between BA (view NP1 as an agent) and BEI (NP2 as an agent). It is found that the models and humans respond to (non-)agentive features in telic context and atelic feature very similarly. However, the models show insufficient sensitivity to both pragmatic function in expressing undesirable events and different individuation degrees represented by human common nouns vs. proper names. By contrast, humans rely heavily on these cues to establish the thematic relation between two arguments NP1 and NP2. Furthermore, the models tend to interpret the subject as an agent, which is not the case for humans who align agents independently of subject position in Mandarin Chinese.",
    "content": "1 Introduction Pre-trained transformer-based language models (LMs) keep achieving state-of-the-art performance in NLP tasks. Many studies have indicated that pre-trained LMs can learn syntactic knowledge (e.g., Linzen et al. 2016; Gulordava et al. 2018 for subject-verb agreement, Wilcox et al. 2018 for filler-gap dependencies, Futrell et al. 2019 for garden-path effects) and semantic knowledge (e.g., Zhao et al. 2021 for telicity, Kementchedjhieva et al. 2021 for causality bias, Misra et al. 2020 for semantic priming, Misra et al. 2021 for typicality, Dataset for both humans and language models, and analysis code are available at https://github.com/ NLPbelllabs/WhoWhom.git 254 Ettinger 2020 for role reversal and same-category distinctions). However, to what extent LMs can acquire knowledge in the syntax-semantics inter- face is still an open question. To answer this ques- tion, we explore arguments hierarchy construction which identifies the thematic roles of arguments in the semantic domain and aligns arguments and subject/object in the syntactic domain. In this hi- erarchy, the active, controlling agent (prototyp- ical actor) outranks the affected patient (proto- typical undergoer), i.e., who did what to whom? (Van Valin Jr, 1990; Van Valin and LaPolla, 1997; Bornkessel et al., 2005). The mapping between thematic roles (agent/patient) and syntactic struc- ture (subject/object) varies depending on various features. In this paper, we investigate whether pre-trained transformer-based LMs and humans behave simi- larly in the argument hierarchy construction using the Chinese structure \"NP1+BA/BEI+NP2+VP”. This structure provides a unique opportunity to examine the alignment through the occurrence of BA/BEI (Deng et al., 2018), without interference from morphology or word order. For example, hu- man name Zhang-san (NP1) in the subject position of sentence (1a) with BA is interpreted as an agent, and human name Li-si (NP2) in the object position is viewed as a patient. By contrast, if BEI occurs as in (1b), subject Zhang-san is viewed as a patient, and object Li-si is considered an agent. This in- verse interpretation depending on BA/BEI allows us to use word prediction to study LMs without task- specific fine-tuning. It also avoids tokenization issues since both BA and BEI are single characters. (1a) 张三 把 李四 杀 死 了。 zhang-san ba li-si sha si -le Zhangsan BA Lisi kill dead -PERF 'Zhangsan killed Lisi.' (1b) 张三 被 李四 杀 死 了。 zhang-san bei li-si sha si -le Zhangsan BEI Lisi kill dead -PERF 'Zhangsan was killed by Lisi.' The construction of argument hierarchy can be affected by different cues related to telicity, agency, and individuation via notion transitivity (Hopper and Thompson, 1980; De Mattia-Viviès, 2009; Vir- tanen, 2015). For example, a cue emphasizing the agentive property of NP1 (e.g., by adding the adver- bial volitionally) increases the probability of NP1 being viewed as an agent (Cruse, 1973), making BA more natural than BEI. By contrast, a cue denoting the non-agentive property of NP1 (e.g. by adding the clause what happend to NP1 was that...) de- creases the probability of NP2 being viewed as an agent, making BEI more natural than BA. To exam- ine the effects of these cues, we carry out a human acceptability judgment experiment using sentences with BA/BEI and compare the result with the proba- bility of masked token BA/BEI predicted by the six pre-trained transformer-based LMs: BERT-base, ELECTRA-large, RoBERTa-base, ERNIE 1.0, and MacBERT-base/large. The results show that the models and humans construct similar argument hi- erarchy with atelic feature, and both agentive and non-agentive feature in telic context. However, (A) LMs show insufficient sensitivity to the prag- matic function of BEI in forming adversative pas- sives with disposal verbs, but humans depend on it in establishing thematic relation between the argu- ments. (B) LMs and humans present different responses to various degrees of individuation encoded in hu- man common nouns vs. proper names. Humans often perceive proper nouns as agents. However, LMs are inclined to interpret common nouns as agents. (C) Unlike Mandarin Chinese native speakers who do not align the agent role depending on sub- ject position, LMs tend to interpret the subject as an agent in telic context. 2 Materials We prepare a dataset including the sentences highlighting telicity-, agency-, and individuation- related features. To avoid gender effect, we choose frequently used male surnames and first names to form NP1 and NP2 in the structure \"NP1+BA/BEI+NP2+VP\". For each condition, we make a hypothesis about human judgment in BA/BEI-preference based on previous studies about features in the structure. 2.1 Telicity 2.1.1 Atelic-condition We use dynamic atelic verbs and imperfective as- pect -zhe² to build atelic sentences. The dynamic verbs such as la 'pull' in (2a) and xun-chi 'repri- mand' in (2b) with imperfective -zhe represent du- rative events without inherent endpoints (Vendler, 1957; Smith, 2012; Xiao and McEnery, 2004a). BEI with dynamic verbs can collocate with imper- fective aspect -zhe (Cook, 2019; Xiao et al., 2006). But the co-occurrence of BA with dynamic verbs and -zhe is rarely found (Tsung and Gong, 2021). We expect a preference for BEI over BA in the atelic- condition. (2a)郭杰 把/被 张伟 拉 着。 guo-jie ba/bei zhang-wei la -zhe Guojie BA/BEI Zhangwei pull -IMPF 'Guojie is pulling Zhangwei.'/ 'Guojie is being pulled by Zhangwei.' (2b)赵涛 把/被 吴波 训斥 着。 zhao-tao ba/bei wu-bo xun-chi -zhe. Zhaotao BA/BEI Wubo reprimande -IMPF 'Zhaotao is reprimanding Wubo.'/ 'Zhaotao is being reprimanded by Wubo.' 2.1.2 Telic-condition (3a)郭杰 把/被 张伟 拉到了门口。 guo-jie ba/bei zhang-wei la dao -le men-kou Guojie BA/BEI Zhangwei pull arrive -PERF door 'Guojie pulled Zhangwei to the door.'/ 'Guojie was pulled to the door by Zhangwei.' (3b)赵涛 把/被 吴波 训斥 了 一顿。 zhao-tao ba/bei wu-bo xun-chi -le yi-dun. Zhaotao BA/BEI Wubo reprimande -PERF one-CL 'Zhaotao reprimanded Wubo.'/ 'Zhaotao was reprimanded by Wubo once.' A modifier specifying an endpoint can change an atelic verb at the lexical level into a telic situation at clause level (Vendler, 1957; Xiao and McEnery, 2004a). We set up two types of telic modifiers. The first one uses prepositional phrases (PPs) like dao...men-kou 'arrive at the door' denoting a spa- tial endpoint (3a). The second one uses yi-dun 'one+CL' indicating an temporal endpoint, where the specific verbal classifier dun is used to mea- sure the count of a durative event (3b)(McEnery 2 Markers signaling viewpoint aspect, such as perfective marker -le in the examples (1, 3-10) or imperfective marker -zhe in (2), are necessary for the grammatical correctness of Chinese sentences (Li and Thompson, 1989). In atelic- condition, we choose the imperfective marker -zhe to empha- size ongoing, uncompleted events. 255 and Xiao, 2007; Li and Thompson, 1989). We (4) 郭杰故意 把/被 张伟 拉到了门口。 combine one-half of atelic verbs like la 'pull' with PPs to build spatially telic VPs (3a) and the other half verbs with yi-dun to form temporally telic VPs (3b)³. Both telic VPs co-occur with the perfective (5) 郭杰不幸 把/被 张伟 拉到了门口。 marker -le and are used in the following agency- and individuation-related conditions. One crucial distinction between the spatially and temporally telic sentences is that the former with dao 'arrive' denotes an instantaneous, non-durative event, and the latter describes a durative event ap- proaching an endpoint incrementally⁴. Linguistic studies suggest that both BA and BEI are compat- ible with a telic situation (Liu, 1997; Yang, 1995; Xiao and McEnery, 2004b). We examine whether BA and BEI are acceptable in both temporally and spatially context in the telic-condition. 2.2 Agency Adopting cues highlighting agentive or non- agentive feature can modify the thematic roles mapped to NPs. We form three condition groups: (1) a manner adverbial ‘volitionally' vs. 'unfortu- nately', (2) a subordinate clause with 'do' vs. 'hap- pen', and (3) a purpose phrase with 'in order to' (Gruber, 1967; Cruse, 1973) to construct sentences. 2.2.1 Volition and non-volition-condition The Chinese adverbial gu-yi 'volitionally' after NP1 in (4) presents the intention of NP1 to carry out an action (Cruse, 1973) and drives NP1 to be interpreted as an agent. It harmonizes with BA, which indicates NP1 as an agent, but conflicts with BEI, which signals NP1 as a patient. By contrast, the adverbial bu-xing ‘unfortunately' in (5) demon- strates a non-volitional, passive property of NP1. It agrees with BEI but contradicts BA. 2.2.2 Do- and happen-condition The do/happen-clause is another way to test agen- tive and non-agentive property. For example, John in John punched Bill is viewed as an agent, as What guo-jie gu-yi ba/bei zhang-wei da dao -le men-kou. Guojie volitionally BA/BEI Zhangwei pull arrive -PERF door 'Guojie pulled Zhangwei to the door volitionally.'/ 'Guojie was pulled to the door by Zhangwei volitionally.' guo-jie bu-xing ba/bei zhang-wei da dao -le men-kou. Guojie unfortunately BA/BEI Zhangwei pull arrive -PERF door 'Guojie pulled Zhangwei to the door unfortunately.'/ 'Guojie was pulled to the door by Zhangwei unfortunately.' John did was punch Bill is normal and What hap- pened to John was punch Bill is odd (Cruse, 1973). On the contrary, John in John was punched by Bill is viewed as non-agent, as What happened to John was that he was punched by Bill is normal and What John did was that he was punched by Bill is abnor- mal. We place the do/happen-clause as in (6) and (7) to modify agentive/non-agentive feature of NP1. The do-clause emphasizes the agentive feature of NP1 with BA and the happen-clause harmonizes with the patient role of NP1 using BEI. (6) 郭杰昨天 做了一件事, guo-jie zuo-tian zuo-le yi-jian shi Guojie yesterday do-PERF one-CL thing 'Guojie did something yesterday,' 他把/被张伟 拉到了门口。 ta ba/bei zhang-wei la dao -le men-kou he BA/BEI Zhangwei pull arrive -PERF door '(that is,) he pulled Zhangwei to the door.'/ '(that is,) he was pulled by Zhangwei to the door.' (7) 昨天 发生在郭杰身上的 是, zuo-tian fa-sheng zai guo-jie shen-shang de shi yesterday happen at Guojie body-up DE is 'What happened to Guojie yesterday is,' 他把/被张伟 拉到了门口。 ta ba/bei zhang-wei la dao -le men-kou he BA/BEI Zhangwei pull arrive -PERF door '(that) he pulled Zhangwei to the door.' '(that) he was pulled to the door by Zhangwei.' 2.2.3 Aim-condition A third widely discussed test for the agency is the modifiability by a phrase with in order to. For example, John in John looked into the room in order to learn who was there is viewed as a willful agent (Gruber, 1967). Similarly, the purpose phrase wei-le da-dao mu-di 'in order to achieve goal' after the NP1 in (8) emphasizes NP1's purpose, which matches NP1's agent role with BA and contradict NP1's patient role with BEI. In sum, we predict that the tested telic context show consistent BA/BEI-preference under the ef- fect of agency, that is, the volition-, do- and aim- 256 (8)郭杰为了达到目的, guo-jie wei-le da-dao mu-di Guojie in order to achieve goal 'Guojie aiming to achieve his goal,' 他把/被张伟拉到了门口。 ta ba/bei zhang-wei la dao -le men-kou he BA/BEI Zhangwei pull arrive -PERF door '(that) he pulled Zhangwei to the door.'/ '(that) he was pulled by Zhangwei to the door.' condition with agentive cues for NP1 prefer BA, and the non-volition- and happen-condition with non-agentive cues for NP1 prefer BEI. 2.3 Individuation Human common nouns like 'worker' are regarded to be less identifiable and individuated than human proper names like Guo-jie, which are more likely to be perceived as agents in human comprehension (Fraurud, 1996; Yamamoto, 1999; Dixon, 1979; Timberlake, 1977). In NP2com-condition (9), fre- quently used occupation names like \"worker\" are used as common nouns for NP2 and male human names are used as proper names for NP1. NP1com- condition (10) is in reverse. We predict that hu- mans prefer BA for NP2com-condition and BEI for NP1com-condition as the proper names are more likely to be viewed as agents. Human BA/BEI-preference can be attributed to human sensitivity to different ways of referring such as common nouns vs. proper names. It is uncertain whether LMs own this sensitivity. There- fore, we predict that LMs may behave differently. For grammatical correctness, each common noun occurs with a numeral yi 'one' and the general clas- sifier ge (Zhang, 2013). NP2com-condition: (9) 郭杰把/被一个工人拉到了门口。 guo-jie ba/bei yi-ge go-ren la dao -le men-kou Guojie BA/BEI one-CL worker pull arrive -PERF door 'Guojie pulled a worker.'/ 'Guojie was pulled to the door by a worker.' NP1com-condition: (10)一个工人把/被张伟拉到了门口。 yi-ge gong-ren ba/bei zhang-wei la dao -le men-kou one-CL worker BA/BEI Zhangwei pull arrive -PERF door 'A worker pulled Zhangwei to the door.'/ 'A worker was pulled to the door by Zhangwei.' 3 Experiment 3.1 Human Judgment Task We prepare 18 verbs to form 36 sentences either with BA or with BEI for each of the 9 conditions, resulting in 324 sentences in total5. To avoid repeat- ing verbs and NPs, we split these sentences evenly over 18 lists following a Latin-Square design, with 18 sentences in each list. Every list contains each condition twice and each of the 18 verbs once. Ad- ditional 10 sentences which are either semantically or syntactically incorrect were added to each list as fillers. Each of the lists was pseudo-randomized so that two test items from a single condition did not appear sequentially. We conducted an acceptability judgment experi- ment using a four-point-scale questionnaire to ob- tain human ratings. Participants are required to mark the sentences following this instruction: en- tirely acceptable sentences should be marked with 1; sentences containing some expression which is acceptable to some degree, but not fully acceptable, should be marked with 2; sentences containing some expression which is unacceptable to some de- gree, but not fully unacceptable, should be marked with 3; and sentences containing some expression which is fully unacceptable should be marked with 4. A larger score indicates a sentence is less accept- able. This human judgement experiment was admin- istered on the Chinese website of wenjuanxing6. 121 university students from mainland China par- ticipated in this experiment voluntarily. Their ages range from 18 to 25 years old, with a mean age of 20.6 years. Fifty-six of them are female. They all reported a monolingual Mandarin Chinese back- ground except one female. Her and the other 11 participants' data were filtered out because of their low judgment scores (meaning high acceptable) on unacceptable filler items sentences (mean < 3.5). 3.2 LM Prediction We replace BA/BEI in our sentences with a masked token and measure the output at the correspond- ing position for BA and BEI in different conditions for six pre-trained transformer-based LMs: BERT- base (Devlin et al., 2018), RoBERTa-base (Liu et al., 2019), ELECTRA-large (Clark et al., 2020), ERNIE 1.0 (Sun et al., 2019), MacBERT-base and MacBERT-large (Cui et al., 2020), implemented in We publish all the sentences at Github. https://www.wjx.cn 257 the Huggingface Transformers library (Wolf et al., 2019). Even though these LMs have different pre- training tasks and use different databases in dif- ferent sizes (see Table 5 in Appendix), we expect that they show (or tend to show) a consistent rather than inconsistent performance in the prediction of BA/BEI for each condition. 3.3 Measure We define Bhum as BA/BEI-preference bias B for humans based on Accep which is the judgment score for each sentence S. Bhum quantifies the preference of a sentence to occur with BA or BEI. It is negative with BA preferred and positive with BEI preferred. Bhum = Accep(BA|S) – Ассер(BEIS) (1) For LMs, surprisal is defined as the inverse log probability of a word (wi) conditioned on the sur- rounding words in a context C: Surp(wi C) = log- 1 p(wi C) (2) Due to the fact that BA and BEI are not exclusive to each other, we follow Misra et al. (2020) and define BA/BEI-preference bias B for LM BLM as the surprisal difference between BA and BEΙ. BLM = Surp(BA|C) – Surp(BEIC) (3) BLM is negative if BA is preferred and positive if BEI is preferred. BLM has been applied as a linking function between human expectations and LM's output (Hale, 2001). In this paper, we employ BLM and Bhum to test the BA/BEI-preference of humans and LMs under the effects of various features. 4 Results Average BLM and Bhum are visualized in Figure 1. BLM is averaged for every condition within each LM. Bhum is averaged over all the partic- ipants for every condition. We further examine average Accep and average Surp for BA and BEI from ROBERTa-base for each condition in Figure 2 (other LMs present similar results, see Figure 5 in Appendix). The human Accep for all items in each condition show a lower averaged coefficient of variation over all the conditions than Surp of This non-exclusivity is also verified in our study by the result of higher human acceptability for both BA and BEI in telic-condition than in atelic-condition, see Figure 2. 259 average Bhum average BLM 1 T 5 0 -5- → human A atelic telic do happen aim non-volition/ volition NP2.com NP1.com B → BERT-base → ROBERTa-base → MacBERT-base → ERNIE 1.0 → ELECTRA-large → MacBERT-large Figure 1: Average Bhum from human acceptability judg- ment experiment (A) and average BLM for six LMs (B) for each condition. The 9 conditions belong to three groups: telic/atelic-condition is related to telicity (Sec. 2.1), do/happen/aim/non-volition/volition-condition is related to agency (Sec. 2.2) and NP2.com/NP1.com- condition is related to individuation (Sec. 2.3). The zero value is set as a reference line. all LMs (0.42 vs. 0.64, detailed results see Fig- ure 4 in Appendix). Statistically, the temporally telic and spatially telic context in all the conditions except for telic- and NP2com-condition show quite consistent pattern regarding the BA/BEI-preference in both human Accep and Surp of LMs, suggest- ing that the difference between temporally telic and spatially telic context play a limited role in the BA/BEI-preference for these conditions. Thus we compare the results between temporally telic and spatially telic context only for telic- and NP2.com- condition. The human Accep and Surp of each LM for each condition are fitted with a linear mixed-effects model using the Ime4 package in R (Bates et al., 2015). The model treated variable BA/BEI as a fixed effect with a random intercept for each verb (detailed results see Table 3 in Ap- pendix). 4.1 Telicity In atelic-condition, positive Bhum (p <0.001) and BLM (p ≤0.05 for all the LMs), see Figure 1 and Table 3 in Appendix, confirm our prediction of BEI- preference for humans and LMs. In telic-condition, Figure 2 shows that the human acceptability of BA and BEI are relatively high (low judgement scores), which supports our prediction that BA and BEI are condition context Humans BERT-base RoBERTa-base ELECTRA-large ERNIE 1.0 MacBERT-base MacBERT-large telic temporal telic bei*** ba*** ba*** bei*** ba*** ba*** ba*** spatially telic –– ba*** bei** ba*** bei*** ba*** ba*** NP2<sub>com</sub> temporally telic ba*** –– ba*** –– bei*** bei** ba*** spatially telic ba*** –– ba*** –– bei*** ba** ba*** Table 1: Preference comparison between BA and BEI for humans and LMs in the temporally and spatially telic context for telic- and NP2<sub>com</sub>-condition. (ba: statistically significant BA-preference, bei: statistically significant BEI-preference. **: p ≤0.01, ***: p ≤0.001) human 3.0- average Accep ba bei 2.5 2.0 1.5- atelic telic do happen aim non-volition volition NP2<sub>com</sub> NP1<sub>com</sub> 10.0- ba bei 7.5- 5.0 2.5 0.0- average Surp ROBERTa-base A B Figure 2: Average Accep from human acceptability judgment experiment (A) and average Surp for ROBERTa-base (B) for each condition. The 9 conditions belong to three groups: telic/atelic-condition is related to telicity (Sec. 2.1), do/happen/aim/non-volition/volition-condition is related to agency (Sec. 2.2) and NP2<sub>com</sub>/NP1<sub>com</sub>-condition is related to individuation (Sec. 2.3). The values from telic-condition are set as reference lines. both acceptable in the telic context. However, positive B<sub>hum</sub> (p ≤0.01) and negative B<sub>LM</sub> (p ≤0.05 except ELECTRA-large) in Figure 1 reveal distinction between humans and LMs. Results of humans and each LM in both temporally and spatially telic context of telic-condition are further compared at Table 1. While participants preferred BEI (p ≤0.001) for the temporally telic sentences, LMs show inconsistent results. As LMs prefer BA (p ≤0.01) consistently for the spatially telic sentences, no significant preference is found in human judgment. 4.2 Agency Figure 1 shows consistent negative B<sub>LM</sub> and B<sub>hum</sub> for do/aim/volition-condition (all with p ≤0.001) and consistent positive B<sub>LM</sub> and B<sub>hum</sub> (all with p ≤0.001) for non-volition-condition. A small discrepancy is found in happen-condition, where participants preferred BEI (p ≤0.001) but three LMs out of six do not present clear BEI-preference (see Table 3 in Appendix). Mostly-aligned prefer- ences between humans and LMs for agency-related conditions suggest that both rely heavily on the agentive/non-agentive features in the tested telic context to construct argument hierarchy as pre- dicted. We observe an interesting discrepancy between humans and LMs in the responses to the agency- related and telic-condition sentences. Participants scored almost all the agency-related sentences above the reference lines (telic-condition), see Figure 2(A), but the results of the LMs do not present this apparent offset, see Figure 2(B). This discrepancy between human and model results is likely contributed by the differences in the mechanism of human judgment and LM prediction. Masked lan- guage models behave as a classifier which assigns probability to BA and BEI in sentence context de- pending on their relative compatibility to the other tokens in the vocabulary. Therefore, the probability of BA/BEI does not directly reflect the adequacy of the whole sentence. In contrast, participants score the acceptability of each sentence as a whole. Acceptability of other factors inside the sentence such as attached adverbials/subordinate clauses may also play a role in participants' judgment. 4.3 Individuation In NP1<sub>com</sub>-condition, LMs prefer BA (p ≤0.001) but no significant preference is observed in human judgment for telic context. In NP2<sub>com</sub>-condition, humans show BA-preference (p ≤0.001) but three LMs out of six show clear BEI-preference, see Table 3 in Appendix. We compare further between different telic contexts in NP2<sub>com</sub>-condition, see Table 1. In temporally telic NP2<sub>com</sub>-condition, LMs show a mostly consistent BEI-preference (p ≤0.05 except MacBERT-large) but no significant prefer- ence is found in human judgment. In spatially telic NP2<sub>com</sub>-condition, humans prefer BA (p ≤0.001) 250 but inconsistent preference is observed for LMs. These results clearly show that LMs differ from hu- mans in their interpretation of human common NPs like yi-ge gong-ren ‘one-CL worker' and proper names like Zhang-wei. A follow-up study is carried out to confirm the negligible influence from yi-ge 'one-CL' and exam- ine the thematic relation between common nouns (C, like gong-ren ‘worker') and proper names (P, like Zhang-wei) in LMs. We focus on the spatially telic context since LMs show a more consistent performance in this context than that in the tempo- rally telic context in telic-condition, as indicated in Table 1. The telic-, NP1com- and NP2com-condition in spatially telic context is renamed as P/P-, Ccl/P- and P/Ccl-condition, in the format of \"[NP1]/[NP2]- condition\". Ccl represents a common noun phrase composed of a numeral, a classifier and a common noun, e.g., yi-ge gong-ren ‘one-CL worker'. For a comprehensive comparison, we add two more conditions Ccl/Ccl and C/P. Table 2 exemplifies all the five conditions. The BA/BEI-preference of six LMs is obtained for each condition (detailed results see Table 4) and their average BLM is shown in Figure 3. Figure 3 shows consistent negative BLM for P/P-condition (p ≤0.01) and Ccl/Ccl-condition (p ≤0.06 except BERT-base) where subject and object are equal in the degree of individuation (both are P or both are Ccl). This result implies that the spatially telic context is inclined to prefer BA under the condition that both NPs are equal in the individuation degree. Compared to P/P- and Ccl/Ccl-condition, BA- preference increases (larger negative BLM) in Ccl/P-condition and decreases (smaller negative even positive BLM) in P/Ccl-condition. The re- sults suggest that the unequal individuation degree between Ccl and P also imposes an effect on the preference. The agentive interpretation of Ccl over P strengthens the BA-preference in Ccl/P-condition and weakens the BA-preference in P/Ccl-condition. Furthermore, 'one-CL' in common NPs shows no significant effect on preference, as C/P- condition agrees with Ccl/P-condition in the BA- preference (p ≤0.05 for all LMs in both condi- tions). In sum, these results suggest that LMs de- liver a more agentive interpretation of the common nouns than that of the proper names in the spatially telic context. condition NP1 NP2 P/P guo-jie 'Guojie' (P) zhang-wei 'Zhangwei' (P) Ccl/Ccl yi-ge gong-ren 'one-CL worker' (Ccl) yi-ge si-ji 'one-CL driver' (Ccl) Ccl/P yi-ge gong-ren 'one-CL worker' (Ccl) zhang-wei 'Zhangwei' (P) P/Ccl guo-jie 'Guojie' (P) yi-ge gong-ren 'one-CL worker' (Ccl) C/P gong-ren 'worker' (C) zhang-wei 'Zhangwei' (P) Table 2: Examples of NPs for different conditions with a spatially telic context. (P: proper name, C: common noun, Ccl: common noun phrase with a numeral and a classifier) average BLM 0.0 -2.5 -5.0 -7.5 P/P Ccl/Ccl Ccl/P P/Ccl C/P BERT-base ERNIE 1.0 ELECTRA-large MacBERT-base MacBERT-large RoBERTa-base Figure 3: Average BLM of six LMs for items with a spatially telic context. The value of zero is set as a reference line. 5 Discussion This study compares LMs and human behavior in argument hierarchy construction. The results show that LMs and humans perform more similarly with atelic feature than with telic feature. In telic context, LMs and humans show similar behaviour with (non-)agentive features, but differently with individuation-related features. We discuss these (dis)similarities from the following four perspec- tives. LMs rely on non-durative property to con- struct argument hierarchy in a telic context. In telic-condition, spatially telic sentences with adverb dao 'arrive' (like 3a) signal non-durative events and show a consistent preference for all LMs, while temporally telic sentences (like 3b) de- scribe durative events and display an inconsistent preference among the LMs. A previous study has suggested that non-duration plays a crucial role for LMs to make telic interpretation (Zhao et al., 2021). Our results further develop the importance of non- durative property: LMs rely more strongly on the non-durative property (compared to durative prop- erty) to construct a consistent argument hierarchy in a telic context. LMs lack sufficient sensitivity in pragmatic function to make the human-like prediction. BEI has a specific pragmatic function in forming adver- sative passives which express undesirable, unfor- tunate events (Li and Thompson, 1989; Chao and Zhao, 1968; Philipp et al., 2008) and often comes with disposal verbs denoting unfavorable meaning like piping 'criticize' and da 'hit' (Cook, 2019; Wenfang and Susumu, 2013; Loar, 2012). The majority of the temporally telic sentences (7 out of 9) contain disposal verbs whose close connec- tion with BEI may directly contribute to the human BEI-preference in the temporally telic-condition. The pragmatic function of BEI may also increase human BEI-preference for happen-condition. The verb fa-sheng 'happen' has a negative prosody (i.e., is likely to occur in a negative context) (Zhang and Ping, 2006; Xiao and McEnery, 2006; Sinclair and Sinclair, 1991), making BEI natural to occur in happen-condition in our results. However, LMs fail to show sensitivity in this pragmatic function of BEI, as no human-like pref- erence is found for both temporally telic- and hap- pen-condition. Our results are in line with previ- ous study that pre-trained transformer-based LMs have shortage in acquiring pragmatic knowledge (Ettinger, 2020). LMs are inclined to interpret the subject as an agent in a spatially telic context. As both NP1 and NP2 are proper nouns, humans show high ac- ceptability of both BA and BEI in a spatially telic context. It indicates that participants do not inter- pret argument hierarchy based on the linear posi- tion of arguments, at least in Mandarin Chinese (Philipp et al., 2008; Bornkessel and Schlesewsky, 2006), that is, the sequence subject-verb-object does not determine the argument assignment. How- ever, LMs show a clear preference for BA in a spatially telic context where both NPs are com- mon nouns (Ccl/Ccl-condition) or proper names (telic-condition), indicating that LMs intend to in- terpret the subject in the telic context as an agent. This BA-preference in LMs may be explained by 1) unbalanced occurrences between active and pas- sive voice, as more active sentences increase the probability of subjects interpreted as agents, and 2) a higher occurrence frequency of BA over BEI during training. The occurrence frequencies of ac- tive/passive and BA/BEI in the LMs' training corpus worth further investigation. 261 Individuation degree plays a different role be- tween LMs and humans in spatially telic context. Proper names have a higher degree of individuation than common nouns. A proper name is more likely to function as an agent than a common NP (Ya- mamoto, 1999; Dixon, 1979), which agrees with the results in spatially telic context for humans: 1) BA-preference in NP2com-condition and 2) high acceptability of BEI in NP1com-conditions. However, LMs show an opposite tendency in viewing a common NP in spatially telic context as an agent through BA-preference for NP1com- condition for all LMs. The follow-up study in spatially telic context further confirms the agen- tive interpretation of common nouns in LMs. LMs fall short to interpret proper names as agents, which may be attributed to their low occur- rence frequency during training. Moreover, almost each character in proper names has separate seman- tic meanings. We use Zhang-wei as an example. Zhang is usually used as a classifier for flat objects like table and paper and wei forms a number of ad- jectives meaning great and grand. Therefore, LMs may have difficulty in interpreting the combina- tion of these characters as human names (Lake and Murphy, 2021; Yu and Ettinger, 2020). 6 Future work Note that telic predicates in the agency- and individuation-related conditions are neces- sary to build items in the Chinese structure \"NP1+BA/BEI+NP2+VP\" (Xiao et al., 2006), which is also verified by the high acceptability of BA and BEI in telic-condition (low judgment scores in Figure 2(A)) in our experiment. Future work could continue to explore LMs' sensitivity to agency- and individuation-related features isolated from telic context in syntax-semantics-interface. Moreover, as we treat LMs as a whole and pay attention to their final predictions of BA/BEI to compare with human judgment in our study, more probing measures, such as attention probing, could be taken to deepen our understanding about internal performance of LMs. In NP1com-condition, humans show high acceptability for both BA and BEI as indicated in Figure 2(A). The high acceptability of BA for NP1com-condition may be contributed by the tendency of BA-construction with a definite NP2 (Ye et al., 2007). 7 Conclusion This study uses BA/BEI-preference in the Chi- nese structure \"NP1+BA/BEI+NP2+VP\" to exam- ine if pre-trained transformer-based language mod- els construct similar argument hierarchy like hu- mans, i.e., the interpretation of Who did what to Whom, with the effect of telicity-, agency- and individuation-related features. The results show that LMs and humans behave similarly for atelic and non-agentive/agentive features, but differently to telic and individuation-related features in the tested context. Specifically, their discrepancy in the temporally telic context suggests that unlike hu- mans, LMs lack sufficient sensitivity to pragmatic function of BEI describing undesirable events with disposal verbs. The different BA/BEI-preference in the sentences with human common vs. proper nouns between LMs and humans indicates that un- like humans who perceive proper nouns as agents, LMs tend to interpret common nouns as agents. Acknowledgements We sincerely appreciate anonymous reviewers for their valuable comments and suggestions, which helped us to improve the quality of the manuscript. References Douglas Bates, Martin Mächler, Ben Bolker, and Steve Walker. 2015. Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1):1- 48. Ina Bornkessel and Matthias Schlesewsky. 2006. The extended argument dependency model: a neurocog- nitive approach to sentence comprehension across languages. Psychological review, 113(4):787. Ina Bornkessel, Stefan Zysset, Angela D Friederici, D Yves Von Cramon, and Matthias Schlesewsky. 2005. Who did what to whom? the neural basis of argument hierarchies during language comprehen- sion. Neuroimage, 26(1):221-233. Yuen Ren Chao and Yuanren Zhao. 1968. A grammar of spoken Chinese. University of California Press. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555. Angela Cook. 2019. The use of the passive marker bei in spoken mandarin. Australian Journal of Linguistics, 39(1):79-106. D Alan Cruse. 1973. Some thoughts on agentivity. Jour- nal of linguistics, 9(1):11-23. Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi- jin Wang, and Guoping Hu. 2020. Revisiting pre- trained models for chinese natural language process- ing. arXiv preprint arXiv:2004.13922. Monique De Mattia-Viviès. 2009. The passive and the notion of transitivity. Review of European Studies, 1(2):94-109. Xiangjun Deng, Ziyin Mai, and Virginia Yip. 2018. An aspectual account of ba and bei constructions in child mandarin. First Language, 38(3):243-262. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language under- standing. arXiv preprint arXiv:1810.04805. Robert MW Dixon. 1979. Ergativity. Language, 55(1):59-138. Allyson Ettinger. 2020. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:34-48. Kari Fraurud. 1996. Cognitive ontology and NP form. In Thorstain Fertheim and Jeanette K. Gundel, ed- itors, Reference and Referent Accessibility, pages 65-88. John Benjamins Publishing Company. Richard Futrell, Ethan Wilcox, Takashi Morita, Peng Qian, Miguel Ballesteros, and Roger Levy. 2019. Neural language models as psycholinguistic subjects: Representations of syntactic state. arXiv preprint arXiv:1903.03260. Jeffrey S Gruber. 1967. Look and see. Language, 43(4):937-947. Kristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. 2018. Colorless green recurrent networks dream hierarchically. arXiv preprint arXiv:1803.11138. John Hale. 2001. A probabilistic earley parser as a psycholinguistic model. In Second meeting of the north american chapter of the association for com- putational linguistics. Paul J Hopper and Sandra A Thompson. 1980. Transitiv- ity in grammar and discourse. Language, 56(2):251- 299. Yova Kementchedjhieva, Mark Anderson, and Anders Søgaard. 2021. John praised mary because he? im- plicit causality bias and its interaction with explicit cues in Ims. arXiv preprint arXiv:2106.01060. Brenden M Lake and Gregory L Murphy. 2021. Word meaning in minds and machines. Psychological Re- view. Charles N Li and Sandra A Thompson. 1989. Mandarin Chinese: A functional reference grammar, volume 3. Univ of California Press. 262 Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax- sensitive dependencies. Transactions of the Associa- tion for Computational Linguistics, 4:521–535. Feng-Hsi Liu. 1997. An aspectual analysis of ba. Jour- nal of East Asian Linguistics, 6(1):51–99. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. ROBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692. Jian Kang Loar. 2012. Chinese syntactic grammar: Functional and conceptual principles. Peter Lang Inc. Tony McEnery and Richard Xiao. 2007. Quantifying constructions in English and Chinese: A corpus- based contrastive study. In Proceedings of the Cor- pus Linguistics Conference CL2007 University of Birmingham, UK, pages 27–30. Kanishka Misra, Allyson Ettinger, and Julia Taylor Rayz. 2020. Exploring BERT's sensitivity to lex- ical cues using tests from semantic priming. arXiv preprint arXiv:2010.03010. Kanishka Misra, Allyson Ettinger, and Julia Tay- lor Rayz. 2021. Do language models learn typ- icality judgments from text? arXiv preprint arXiv:2105.02987. Markus Philipp, Ina Bornkessel-Schlesewsky, Walter Bisang, and Matthias Schlesewsky. 2008. The role of animacy in the real time comprehension of Mandarin Chinese: Evidence from auditory event-related brain potentials. Brain and Language, 105(2):112–133. John Sinclair and Les Sinclair. 1991. Corpus, concor- dance, collocation. Oxford University Press. Carlota S Smith. 2012. The parameter of aspect, vol- ume 43. Springer Netherlands. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: Enhanced rep- resentation through knowledge integration. arXiv preprint arXiv:1904.09223. Alan Timberlake. 1977. Reanalysis and actualization in syntactic change. In Charles Li, editor, Mechanisms of syntactic change, pages 141–178. University of Texas Press. Linda Tsung and Yang Frank Gong. 2021. A corpus- based study on the pragmatic use of the ba construc- tion in early childhood mandarin chinese. Frontiers in psychology, page 4036. Robert D Van Valin and Randy J LaPolla. 1997. Syn- tax: Structure, meaning, and function. Cambridge University Press. 262 Robert D Van Valin Jr. 1990. Semantic parameters of split intransitivity. Language, 66(2):221–260. Zeno Vendler. 1957. Verbs and times. The Philosophi- cal Review, 66(2):143–160. Susanna Virtanen. 2015. Transitivity in Eastern Mansi: An information structural approach. Ph.D. thesis, University of Helsinki. Fan Wenfang and Kuno Susumu. 2013. Semantic and discourse constraints on Chinese bei-passives. Lin- guistics and the Human Sciences, 8(2):205–240. Ethan Wilcox, Roger Levy, Takashi Morita, and Richard Futrell. 2018. What do RNN language models learn about filler-gap dependencies? arXiv preprint arXiv:1809.00042. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of- the-art natural language processing. arXiv preprint arXiv:1910.03771. Richard Xiao and Tony McEnery. 2004a. Aspect in Mandarin Chinese. Amsterdam: Benjamins. Richard Xiao and Tony McEnery. 2006. Colloca- tion, semantic prosody, and near synonymy: A cross-linguistic perspective. Applied Linguistics, 27(1):103–129. Richard Xiao, Tony McEnery, and Yufang Qian. 2006. Passive constructions in English and Chinese: A corpus-based contrastive study. Languages in Con- trast, 6(1):109–149. Zhonghua Xiao and Anthony McEnery. 2004b. A corpus-based two-level model of situation aspect. Journal of linguistics, 40(2):325–363. Mutsumi Yamamoto. 1999. Animacy and reference. John Benjamins Publishing. Suying Yang. 1995. The aspectual system of Chinese. Ph.D. thesis, University of Victoria Canada. Zheng Ye, Weidong Zhan, and Xiaolin Zhou. 2007. The semantic processing of syntactic structure in sentence comprehension: An ERP study. Brain Research, 1142:135–145. Lang Yu and Allyson Ettinger. 2020. Assessing phrasal representation and composition in transform- ers. arXiv preprint arXiv:2010.03763. Jidong Zhang and Liu Ping. 2006. A corpus-based study of the differences between the three synonyms: happen, occur and 'fasheng'. Foreign Languages Research, (5):19–22. Niina Ning Zhang. 2013. Classifier Structures in Man- darin Chinese. De Gruyter Mouton. Yiyun Zhao, Jian Gang Ngui, Lucy Hall Hartley, and Steven Bethard. 2021. Do pretrained transform- ers infer telicity like humans? In Proceedings of the 25th Conference on Computational Natural Lan- guage Learning, pages 72-81. A Appendix coefficient of variation (Accep) 0.6- 0.5- 0.4 0.3- 1.00- 0.75- 0.50- atelic telic A do happen aim non-volition volition NP2.com NP1.com B coefficient of variation (Surp) Figure 4: Coefficient of variation of human Accep (A) and Surp averaged across six LMs (B) for each condi- tion with BA and BEI. We find that in human Accep, the preferred one between BA and BEI shows a higher coef- ficient than the other one (e.g., the do-condition prefers BA and BA has a higher coefficient than BEI) for all the conditions except for telic-condition. In telic-condition where both BA and BEI are high acceptable in human judgment, their coefficients are also at a relatively high level. LMs show a similar trend. 264 → ba bei → ba bei --- average Surp 9 6- 3 0 9 6 3 0 BERT-base ba - bei ERNIE 1.0 → ba → bei ELECTRA-large ba bei MacBERT-base MacBERT-large ba bei → ba bei atelic telic do happen alon-volitiolition NP2.com NP1.com atelic telic do happen air non-volitiolition NP2com NP1com atelic telic do happen almon-volitiolition p2.com NP1.com Figure 5: Average Surp for BERT-base, ELECTRA-large, ERNIE1.0, MacBERT-large and MacBERT-base. The values from the telic-condition are set as reference lines. Factor Telicity (Sec. atelic bei*** bei*** Condition Humans BERT-base RoBERTa-base ELECTRA-large ERNIE 1.0 MacBERT-base MacBERT-large bei** bei* bei* bei* bei*** 2.1) telic bei** ba*** ba** ba** ba* ba*** aim ba*** ba*** ba*** ba*** ba*** ba*** ba*** do Agency (Sec. ba*** ba*** ba*** ba*** ba*** ba*** ba*** 2.2) happen bei*** ba*** bei*** bei*** bei* non-volition bei*** bei*** bei*** bei*** bei*** bei*** bei*** volition ba*** ba*** ba*** ba*** ba*** ba*** ba*** Individuation NP2.com ba*** bei* bei*** bei*** (Sec. 2.3) NP1.com ba*** ba*** ba*** ba*** ba*** ba*** Table 3: Preference comparison between BA and BEI for humans and LMs for telicy-, agency- and individuation- related conditions (ba: statistically significant BA-preference, bei: statistically significant BEI-preference. Formula: Surp/Accep~ BA/BEI + (1lverb)). * : p < 0.05, ** : p < 0.01, * * * : p < 0.001) Spcially telic context BERT-base ROBERTa-base ELECTRA-large ERNIE 1.0 MacBERT-base MacBERT-large P/P-condition ba*** ba*** ba** ba*** ba*** ba*** Ccl/Ccl-condition ba** ba** bam ba* ba* Ccl/P-condition ba*** ba*** ba*** ba*** ba*** ba*** P/Ccl-condition ba** bei** ba* ba** C/P-condition ba*** ba*** ba* ba*** ba*** ba*** Table 4: Preference comparison between BA and BEI for LMs for individuation-related conditions in Section 4.3 (ba: statistically significant BA-preference, bei: statistically significant BEI-preference. Formula: Surp ~ BA/BEI + (1lverb)). * : p < 0.05, ** : p < 0.01, * * * : p < 0.001,0.05 < m ≤ 0.06) LMs Tasks Chinese Database BERT-base MLM, next sentence prediction 25M sentences (Devlin et al., 2018) ERNIE 1.0 MLM, dialogue, language model task 173M sentences (Sun et al., 2019) ROBERTa-base MLM 5.4B words (Cui et al., 2020) ELECTRA-large replaced token, detection task 5.4B words (Cui et al., 2020) MacBERT-base/large MLM as correction, sentence-order prediction 5.4B words (Cui et al., 2020) Table 5: Comparison between models with respect of tasks in their pre-training process and size of Chinese database (MLM: masked LM task). 265 ---"
  },
  {
    "title": "Construction and Visualization of Key Term Hierarchies",
    "abstract": "This paper presents a prototype system for key term manipulation and visualization in a real-world commercial environment. The system consists of two components. A preprocessor generates a set of key terms from a text dataset which represents a specific topic. The generated key terms are organized in a hierarchical structure and fed into a graphic user interface (GUI). The friendly and interactive GUI toolkit allows the user to visualize the key terms in context and explore the content of the original dataset.",
    "content": "1. INTRODUCTION As the amount of on-line text grows at an exponen- tial rate, developing useful text analysis techniques and tools to access information content from vari- ous electronic sources is becoming increasingly important. In this paper we present an applied research prototype system that intends to accom- plish two major tasks. First, a set of key terms, ranging from single word terms to four word terms, are automatically generated and organized in a hierarchical structure out of a text dataset which represents a specific topic. Second, a graphic user interface (GUI) is established that provides the domain expert or the user with an interactive envi- ronment to visualize the key term hierarchy in the context of the original dataset. 2. SYSTEM DESCRIPTION The ultimate goal of this prototype system is to offer an automated toolkit which allows the domain expert or the user to visualize and examine key terms in a large information collection. Such a tool- kit has proven to be useful in a number of real applications. For example, it has helped us reduce the time and manual effort needed to develop and maintain our on-line document indexing and classi- fication schemes. The system consists of two components: a prepro- cessing component for the automatic construction of key terms and the front-end component for user- guided graphic interface. 2.1 Automatic Generation of Key Terms Automatically identifying meaningful terms from naturally running texts has been an important task for information technologists. It is widely believed that a set of good terms can be used to express the content of the document. By capturing a set of good terms, for example, relevant documents can be searched and retrieved from a large document collection. Though what constitutes a good term still remains to be answered, we know that a good term can be a word stem, a single word, a multiple word term (a phrase), or simply a syntactic unit. Various existing and workable term extraction tools are either statistically driven, or linguistically oriented, or some hybrid of the two. They all target frequently co-occurring words in running text. The earlier work of Choueka (1988) proposed a pure frequency approach in which only quantitative selection criteria were established and applied. Church and Hanks (1990) introduced a statistical measurement called mutual information for extracting strongly associated or collocated words. Tools like Xtract (Smadja 1993) were based on the work of Church and others, but made a step forward by incorporating various statistical measurements like z-score and variance of distribution, as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output. Exemplary linguistic approaches can be found in the work by Strzalkowsky (1993) where a fast and accurate syntactic parser is the prerequisite for the selection of significant phrasal terms. Different applications aim at different types of key terms. For the purpose of generating key terms for our prototype system, we have adopted a \"learn data from data\" approach. The novelty of this approach lies in the automatic comparison of two sample datasets, a topic focused dataset based on a predefined topic and a larger and more general base dataset. The focused dataset is created by the domain expert either through a submission of an on-line search or through a compilation of documents from a specific source. The construction of the corresponding base dataset is performed by pulling documents out of a number of sources, such as news wires, newspapers, magazines and legal databases. The intention is to make the resulted corpora cover a much greater variety of topics or domain subjects than the focused dataset. To identify interesting word patterns in both sam- ples a set of statistical measures are applied. The identification of single word terms is based on the variation of a t-test. Two-word terms are captured through the computation of mutual information (Church et al. 1991), and an extension of mutual information assists in extracting three-word and four-word terms. Once the significant terms of these four types are identified, a comparison algo- rithm is applied to differentiate terms across the two samples. If significant changes in the values of certain statistical variables are detected, associ- ated terms are selected from the focused sample and included in the final generated lists. (For a complete description of the algorithm and prelimi- nary experiments, please refer to Zhou and Dap- kus 1995.) 2.2 Graphic User Interface (GUI) We view our prototype system as a means to achieve information visualization. Analogous to sci- entific visualization that allows scientists to make sense out of intellectually large data collections, information visualization aims at organizing large information spaces so that information technolo- gists can visualize what is out there and how vari- ous parts are related to each other (Robertson et al. 1991). The guiding principle for building the GUI component of our prototype system is to automate the manual process of capturing information con- tent out of large document collections. 2.2.1 General Presentation The design of the GUI component relies on a num- ber of well understood elements which include a suggestive graphic design and a direct manipula- tion metaphor to achieve an easy-to-learn user interface. The layout of the graphic design is intended to facilitate the quick comprehension of the displayed information. The GUI component is divided into two main areas, one for interacting with key terms structures and one for browsing targeted document collections. The following descriptions should be viewed together with the appropriate figures of the GUI component. Figure 1, attached at the end of the paper, represents the overall GUI picture. Figures 2 and 3 capture the area where the interaction with the key term structures occurs. Figures 4 and 5 present the area for document browsing and key terms selection. The topic illustrated in the figures is the legal topic \"Medical Malpractice\". 2.2.2 Term Access Mechanism The left area of the GUI component (see figures 2 and 3) is devoted to selecting, retrieving and oper- ating on the key terms generated by the prepro- cessing component of the prototype system. As can be seen, the key terms, ranging from single word terms to four word terms, are organized in a tree structure. The tree is a two dimensional visual- ization of the term hierarchy. Single word terms are represented as root nodes and multiple word terms can be positioned uniformly below the parent node in the term hierarchy. The goal of the visualization is to present the key term lists in such a way that a high percentage of the hierarchy is visible with min- imal scrolling. medmal3 One Word Index 0-99 200-259 300-399 400-499 500-599 600-699 700-799 800-899 900-999 Two Word Figure 2 The user interaction is structured around term retrieval and navigation as the top level user inter- actions. The retrieval of the key terms is treated as an iterative process in which the user may select single world terms from the term hierarchy and navigate to multiple word terms accordingly. The user begins term navigation by selecting from a list of available topics. In this case, the legal topic \"Medical Malpractice\" (i.e., medmal3) is selected (see figure 2). Often data structures are organized linearly by some metric. Frequency of key term usage is the metric used to organize and partition the term hierarchy in an ascending numerical order. The partitioning is necessary as it is difficult to accommodate the large ratio of the term hierar- chy on the screen. Currently, each partition con- tains 100 root nodes (or folders), representing single word terms. Once a partition has been selected, the corresponding document collection is loaded into the document browser. The browser provides the user with the ability to quickly navigate through the document collection to locate relevant key terms. Rank 127 Frequency 1544 List medical malpractice legal malpractice malpractice action medical malpractice action legal malpractice action malpractice claim malpractice cases alleged malpractice malpractice claims malpractice actions malpractice case Figure 3 The primary interaction with the key term hierarchy is accomplished by direct manipulation of the tree visualization. The user can select individual nodes in the tree structure by pointing and clicking the corresponding folders. When selecting nodes with children, the tree will expand, resulting in the dis- play of multiple word terms of the root key term. For example, when \"malpractice\" is selected as the root key term, a list of multiple word terms will be displayed including multiple key terms such as \"medical malpractice\", \"malpractice cases\", \"medi- cal malpractice action\", \"medical malpractice claims\", \"limitations for medical malpractice\", etc. (see figure 3) Functionality to shrink and collapse subtrees is also in place. When a term is selected from the tree, a corresponding term lookup is conducted on the document collection to locate the selected term within the currently displayed document. Docu- ments representing the four highest frequencies for the selected term will be displayed first. Upon loca- tion the selected term is always highlighted within the document browser. 2.2.3 Document Browsing Mechanism The right area of the GUI component (see figures 4 and 5) is occupied by the document browser. The design of the document browser is intended to pro- vide an easy-to-learn interface for the management and manipulation of the document collection. There are three subwindows: the document identi- fier window, the document window and the naviga- tion window. The document identifier window identifies the document that is currently displayed in the document window. It shows the document id and the total frequency of the selected key term in the document collection. The document window provides a view of the content of the targeted docu- ment (see figure 4). Document 0100016641 Occurrences B ----End Footnotes This court reviews de novo the district court's determination that ERISA preempts a state law claim. Airparts Co. v. Custom Benefit Servs. of Austin, Inc., 28 F.30 1062, 1064 (10th Cir. 1994). ERISA preempts state laws that \"relate to\" employee benefit plans. 29U.S.C. 1144(a). There is no dispute here that the is based on state law and that Pacificare's plan is an employee benefit plan. The Issue is whether the relates to\" the Pacificare plan. \"A law relates to an employee benefit plan, in the normal sense of the phrase, If it has a connection with or reference to such a plan. Shaw v. Dette Air Lines, Inc., 463 U.S. 85, 96-97, 77 L. Ed. 2d 490, 103 S. CL. 2890 (1963). \"There is no simple test for determining when a law \"relates to\" a plan.\" Airparts Co., 28 F.3d at 1064 (quoting National Elevator indus., Inc. v. Celhoo 957 F.2d 1555, 1558 (10th Cir.), cert. denled, 121 L. Ed. 2d 331, 113 S. CI. 406 (1992)). This court has identified the following four categories of laws which relate to an employee benefit plan: \"First, laws that regulate the type of benefits or terms of ERISA plans. Second laws that create reporting, disclosure, funding, or vesting requirements for ERISA plans. Third, laws that provide rules for the calculation of the amount of benefits to be paid under ERISA plans. Fourth, la Document Ranking Figure 4 4 The user can move through the document by mak- ing use of the scroll bar, document buttons in the navigation window, or by dragging the mouse up and down while depressing the middle mouse but- ton. The user can copy relevant key terms to a holding area by selecting \"Edit\" from the menubar. The user is presented with a popup dialog for importing the selected key terms (see figure 5). The navigation window enables the user to navi- gate through the documents to view the selected key terms in context. In addition, the user is pro- vided with information regarding term frequencies and term relevance ranking scores. File Edit malpractice medical malpractice medical malpractice claim medical malpractice suit action for medical malpractice action for medical malpractice negligent wrongful death Insurance company emotional distress deliberate indifference health maintenance organization legal malpractice action Apply Cancel Help Figure 5 2.2.4 Implementation The GUI component described above is imple- mented using the C++ programing language and the OSF Motif graphical user interface toolkit. The user interface consists of a small set of classes that play various roles in the overall architecture. The two major objects of the user interface interac- tion model are the ListTree and the Document Store objects. ListTree is the primary class for implementing the tree visualization. Operations for growing, shrinking and manipulating the tree visualization have been implemented. Document Store provides the interface to docu- ment collections. In particular, a document store provides operations to create, modify and navigate document collections. 3. RESULTS OF USABILITY TESTING The prototype system, despite its prototype mode, has proven to be useful and applicable in the com- mercial business environment. Since the system is in place, we have conducted a series of usability testing within our company. The preliminary results indicate that the system can provide internal spe- cialized library developers, as well as subject indexing domain experts with an ideal automated toolkit to select and examine significant terms from a sample dataset. A number of general topics have been tested for developing specialized libraries for our on-line search system. These include four legal topics \"State Tax\", \"Medical Malpractice\", \"Uniform Com- mercial Code\", and \"Energy\", and three news top- ics \"Campaign\", \"Legislature\", and \"Executives\". Specific subject indexing topics that have been tested are \"Advertising Expenditure\", \"Intranet\", \"Job interview\" and \"Mutual fund\". Two sets of questionnaires were filled out by the domain experts who participated in the usability testing. The overall ranking for the prototype system falls between \"somewhat useful\" to \"very useful\", depending on the topics. They pointed out that the system is particularly helpful when dealing with a completely new or unfamiliar topic. It helps spot significant terms which would normally be missed and objectively examine the significance level of certain fuzzy and ambiguous terms. REFERENCES K. Church and P. Hanks. Word association norms, mutual information and lexicography. Computational Linguistics, 16(1), March 1990. K. Church, et al. Using statistics in lexical analysis. In U. Zernik, editor, Lexical Acquisition: Exploring On-line Resources to Build a Lexicon, Lawrence Erlbaum Association, 1991. Y. Choueka. Looking for needles in a haystack. In Proceedings, RIAO, Conference on User- Oriented Context Based Text and Image Handling. Cambridge, MA. 1988. G. Robertson. Cone trees: Animated 3rd visualizations of hierarchical information. In proceedings SIGCHI '91: Human Factors in Computing Systems, pages 189-194. ACM, 1991. F. Smadja. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1), March 1993. T. Strzalkowski. Document Indexing and Retrieval Using Natural Language Processing. In Proceedings, RIAO, New York, NY. 1994. J. Zhou and P. Dapkus. Automatic Suggestion of Significant Terms for a Predefined Topic. In Proceedings of the 3rd Workshop on Very Large Corpora, Association for Computational Linguistics, MIT, Boston, 1995. File Help Rank 301 Frequency 714 List malpractice medical malpractice legal malpractice malpractice action medical malpractice acti legal malpractice action malpractice claim malpractice cases alleged malpractice malpractice claims malpractice actions malpractice case Document 0100017384 Occurrences 17 Equal protection rights of medical malpractice plaintiffs. The Garcias challenge Section 41-5-13 as a violation of the equal protection guarantee c the New Mexico Constitution. N.M. Const. art. II, 18. Specifically, the Garcias clalm that by requiring plaintifis to file medical malpractice claims within three years of the act of malpractice regardless of the time at which the plaintif discovers his or her injury, Section 41-5-13 infringes Anthony's Important interest in access to the courts. Further, the Garcias claim that the statute of repose conferred upon qualified health care providers does not bea substantial relationship to the legislature's professed goal of alleviating the insurance crisis. See Roberts, 114 N.M. at 252, 257, 837 P.2d at 446, 451 (holding that the limitations period contained within edical Malpractice Act i a \"benefit\" of the Act available only to qualified health care providers and holding that a cause of action for medical malpractice against a nonqualified health care provider is govemed by the discovery rule). We conclude that Section 41-5-13 does not Implicate the equal protection rights of medical malpractice plaintiffs. - Discriminatory classifications. The basic guarantee of the Equal Protection Clause of the New Mexico Constitution is that the legislature may not enact a statute which treats similarly situated persons differently. See Gruschus v. Bureau of Revenue, 74 N.M. 775, 778, 399 P.2d 105, 107 (1965) (stating that satisfy mandates of equal protection, legislative classifications must be \"so framed as to embrace equally all who may be in like circumstances and situations\"). In order to raise a claim that a statute has violated this basic L Document Ranking 2 3 4 Figure 1"
  },
  {
    "title": "Sequential Model Selection for Word Sense Disambiguation",
    "abstract": "Statistical models of word-sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features. Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features. This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word-sense disambiguation.",
    "content": "1 Introduction In this paper word-sense disambiguation is cast as a problem in supervised learning, where a classifier is induced from a corpus of sense-tagged text. Sup- pose there is a training sample where each sense- tagged sentence is represented by the feature vari- ables (F1,..., Fn-1, S). Selected contextual proper- ties of the sentence are represented by (F1,..., Fn-1) and the sense of the ambiguous word is represented by S. Our task is to induce a classifier that will predict the value of S given an untagged sentence represented by the contextual feature variables. We adopt a statistical approach whereby a prob- abilistic model is selected that describes the inter- actions among the feature variables. Such a model can form the basis of a probabilistic classifier since it specifies the probability of observing any and all combinations of the values of the feature variables. Suppose our training sample has N sense-tagged sentences. There are q possible combinations of val- ues for the n feature variables, where each such com- bination is represented by a feature vector. Let *This research was supported by the Office of Naval Research under grant number N00014-95-1-0776. fi and θ; be the frequency and probability of ob- serving the ith feature vector, respectively. Then (f1,..., fq) has a multinomial distribution with pa- rameters (N, θ1,..., θq). The θ parameters, i.e., the joint parameters, define the joint probability distri- bution of the feature variables. These are the pa- rameters of the fully saturated model, the model in which the value of each variable directly affects the values of all the other variables. These parameters can be estimated as maximum likelihood estimates (MLEs), such that the estimate of θ;, θi, is . For these estimates to be reliable, each of the q possible combinations of feature values must occur in the training sample. This is unlikely for NLP data samples, which are often sparse and highly skewed (c.f., e.g. (Pedersen et al., 1996) and (Zipf, 1935)). However, if the data sample can be adequately characterized by a less complex model, i.e., a model in which there are fewer interactions between vari- ables, then more reliable parameter estimates can be obtained: In the case of decomposable models (Dar- roch et al., 1980; see below), the parameters of a less complex model are parameters of marginal distribu- tions, so the MLEs involve frequencies of combina- tions of values of only subsets of the variables in the model. How well a model characterizes the train- ing sample is determined by measuring the fit of the model to the sample, i.e., how well the distribution defined by the model matches the distribution ob- served in the training sample. A good strategy for developing probabilistic clas- sifiers is to perform an explicit model search to se- lect the model to use in classification. This pa- per presents the results of a comparative study of search strategies and evaluation criteria for measur- ing model fit. We restrict the selection process to the class of decomposable models (Darroch et al., 1980), since restricting model search to this class has many computational advantages. We begin with a short description of decompos- able models (in section 2). Search strategies (in sec- tion 3) and model evaluation (in section 4) are de- scribed next, followed by the results of an extensive disambiguation experiment involving 12 ambiguous words (in sections 5 and 6). We discuss related work (in section 7) and close with recommendations for search strategy and evaluation criterion when select- ing models for word-sense disambiguation. 2 Decomposable Models Decomposable models are a subset of the class of graphical models (Whittaker, 1990) which are in turn a subset of the class of log-linear models (Bishop et al., 1975). Familiar examples of decom- posable models are Naive Bayes and n-gram models. They are characterized by the following properties (Bruce and Wiebe, 1994b): 1. In a graphical model, variables are either inter- dependent or conditionally independent of one another. All graphical models have a graphi- cal representation such that each variable in the model is mapped to a node in the graph, and there is an undirected edge between each pair of nodes corresponding to interdependent vari- ables. The sets of completely connected nodes (i.e., cliques) correspond to sets of interdepen- dent variables. Any two nodes that are not di- rectly connected by an edge are conditionally independent given the values of the nodes on the path that connects them. 2. Decomposable models are those graphical mod- els that express the joint distribution as the product of the marginal distributions of the variables in the maximal cliques of the graphical representation, scaled by the marginal distribu- tions of variables common to two or more of these maximal sets. Because their joint distri- butions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; (Berger et al., 1996)). 3. Although there are far fewer decomposable models than log-linear models for a given set of feature variables, it has been shown that they have substantially the same expressive power (Whittaker, 1990). $\\hat{\\theta}_{f_1,f_2,f_3,s} = \\frac{f(F_1=f_1,S=s_i)}{N} \\times \\frac{f(F_2=f_2,F_3=f_3,S=s_i)}{N}$ $\\frac{f(S=s_i)}{N}$ (1) Rather than having to observe the complete fea- ture vector (f1, f2, f3, si) in the training sample to estimate the joint parameter, it is only necessary to observe the marginals (f1, si) and (f2, f3, si). 3 Model Search Strategies The search strategies presented in this paper are backward sequential search (BSS) and forward se- quential search (FSS). Sequential searches evaluate models of increasing (FSS) or decreasing (BSS) lev- els of complexity, where complexity is defined by the number of interactions among the feature variables (i.e., the number of edges in the graphical represen- tation of the model). A backward sequential search (BSS) begins by designating the saturated model as the current model. A saturated model has complexity level $i = n(n-1)$, where n is the number of feature vari- ables. At each stage in BSS we generate the set of decomposable models of complexity level $i - 1$ that can be created by removing an edge from the cur- rent model of complexity level i. Each member of this set is a hypothesized model and is judged by the evaluation criterion to determine which model results in the least degradation in fit from the cur- rent model—that model becomes the current model and the search continues. The search stops when ei- ther (1) every hypothesized model results in an un- acceptably high degradation in fit or (2) the current model has a complexity level of zero. A forward sequential search (FSS) begins by des- ignating the model of independence as the current model. The model of independence has complexity level $i = 0$ since there are no interactions among the feature variables. At each stage in FSS we generate the set of decomposable models of complexity level $i + 1$ that can be created by adding an edge to the current model of complexity level i. Each member of this set is a hypothesized model and is judged by the evaluation criterion to determine which model re- sults in the greatest improvement in fit from the cur- rent model—that model becomes the current model and the search continues. The search stops when either (1) every hypothesized model results in an unacceptably small increase in fit or (2) the current model is saturated. For sparse samples FSS is a natural choice since early in the search the models are of low complexity. 389 The number of model parameters is small and they have more reliable estimated values. On the other hand, BSS begins with a saturated model whose pa- rameter estimates are known to be unreliable. During both BSS and FSS, model selection also performs feature selection. If a model is selected where there is no edge connecting a feature variable to the classification variable then that feature is not relevant to the classification being performed. 4 Model Evaluation Criteria Evaluation criteria fall into two broad classes, signifi- cance tests and information criteria. This paper con- siders two significance tests, the exact conditional test (Kreiner, 1987) and the Log-likelihood ratio statistic G² (Bishop et al., 1975), and two informa- tion criteria, Akaike's Information Criterion (AIC) (Akaike, 1974) and the Bayesian Information Crite- rion (BIC) (Schwarz, 1978). 4.1 Significance tests The Log-likelihood ratio statistic G² is defined as: 9 G² = Σ f; × log i=1 fi (2) where fi and e; are the observed and expected counts of the ith feature vector, respectively. The observed count f; is simply the frequency in the training sam- ple. The expected count e; is calculated from the frequencies in the training data assuming that the hypothesized model, i.e., the model generated in the search, adequately fits the sample. The smaller the value of G² the better the fit of the hypothesized model. The distribution of G2 is asymptotically approx- imated by the x² distribution (G² ~ x²) with ad- justed degrees of freedom (dof) equal to the number of model parameters that have non-zero estimates given the training sample. The significance of a model is equal to the probability of observing its reference G2 in the x2 distribution with appropriate dof. A hypothesized model is accepted if the signif- icance (i.e., probability) of its reference G2 value is greater than, in the case of FSS, or less than, in the case of BSS, some pre-determined cutoff, a. An alternative to using a x² approximation is to define the exact conditional distribution of G². The exact conditional distribution of G2 is the distribu- tion of G2 values that would be observed for com- parable data samples randomly generated from the model being tested. The significance of G² based on the exact conditional distribution does not rely on an asymptotic approximation and is accurate for sparse and skewed data samples (Pedersen et al., 1996). 4.2 Information criteria The family of model evaluation criteria known as information criteria have the following expression: ICK = G2 - к × dof (3) where G² and dof are defined above. Members of this family are distinguished by their different values of K. AIC corresponds to к = 2. BIC corresponds to k = log(N), where N is the sample size. The various information criteria are an alterna- tive to using a pre-defined significance level (a) to judge the acceptability of a model. AIC and BIC re- ward good model fit and penalize models with large numbers of parameters. The parameter penalty is expressed as K × dof, where the size of the penalty is the adjusted degrees of freedom, and the weight of the penalty is controlled by к. During BSS the hypothesized model with the largest negative ICK value is selected as the cur- rent model of complexity level i - 1, while during FSS the hypothesized model with the largest pos- itive ICK value is selected as the current model of complexity level i + 1. The search stops when the ICK values for all hypothesized models are greater than zero in the case of BSS, or less than zero in the case of FSS. 5 Experimental Data The sense-tagged text and feature set used in these experiments are the same as in (Bruce et al., 1996). The text consists of every sentence from the ACL/DCI Wall Street Journal corpus that contains any of the nouns interest, bill, concern, and drug, any of the verbs close, help, agree, and include, or any of the adjectives chief, public, last, and common. The extracted sentences have been hand-tagged with senses defined in the Longman Dictionary of Contemporary English (LDOCE). There are be- tween 800 and 3,000 sense-tagged sentences for each of the 12 words. This data was randomly divided into training and test samples at a 10:1 ratio. A sentence with an ambiguous word is represented by a feature set with three types of contextual fea- ture variables:2 (1) The morphological feature (E) indicates if an ambiguous noun is plural or not. For verbs it indicates the tense of the verb. This feature is not used for adjectives. (2) The POS features have one of 25 possible POS tags, derived from the first letter of the tags in the ACL/DCI WSJ cor- pus. There are four POS feature variables repre- senting the POS of the two words immediately pre- ceding (L1, L2) and following (R1, R2) the ambigu- ous word. (3) The three binary collocation-specific features (C1, C2, C3) indicate if a particular word oc- curs in a sentence with an ambiguous word. 2 An alternative feature set for this data is utilized with an exemplar-based learning algorithm in (Ng and Lee, 1996). The sparse nature of our data can be illustrated by interest. There are 6 possible values for the sense variable. Combined with the other feature variables this results in 37,500,000 possible feature vectors (or joint parameters). However, we have a training sam- ple of only 2,100 instances. 6 Experimental Results In total, eight different decomposable models were selected via a model search for each of the 12 words. Each of the eight models is due to a different com- bination of search strategy and evaluation criterion. Two additional classifiers were evaluated to serve as benchmarks. The default classifier assigns every in- stance of an ambiguous word with its most frequent sense in the training sample. The Naive Bayes clas- sifier uses a model that assumes that each contex- tual feature variable is conditionally independent of all other contextual variables given the value of the sense variable. 6.1 Accuracy comparison The accuracy³ of each of these classifiers for each of the 12 words is shown in Figure 1. The highest accuracy for each word is in bold type while any ac- curacies less than the default classifier are italicized. The complexity of the model selected is shown in parenthesis. For convenience, we refer to model se- lection using, for example, a search strategy of FSS and the evaluation criterion AIC as FSS AIC. Overall AIC selects the most accurate models dur- ing both BSS and FSS. BSS AIC finds the most ac- curate model for 6 of 12 words while FSS AIC finds the most accurate for 4 of 12 words. BSS BIC and the Naive Bayes find the most accurate model for 3 of 12 words. Each of the other combinations finds the most most accurate model for 2 of 12 words ex- cept for FSS exact conditional which never finds the most accurate model. Neither AIC nor BIC ever selects a model that results in accuracy less than the default classifier. However, FSS exact conditional has accuracy less than the default for 6 of 12 words and BSS exact conditional has accuracy less than the default for 3 of 12 words. BSS G² ~ x² and FSS G² ~ x² have less than default accuracy for 2 of 12 and 1 of 12 words, respectively. The accuracy of the significance tests vary greatly depending on the choice of α. Of the various α values that were tested, .01, .05, .001, and .0001, the value of .0001 was found to produce the most accurate models. Other values of α will certainly led to other results. The information criteria do not require the setting of any such cut-off values. A low complexity model that results in high accu- racy disambiguation is the ultimate goal. Figure 1 ³The percentage of ambiguous words in a held out test sample that are disambiguated correctly. shows that BIC and G² ~ x² select lower complexity models than either AIC or the exact conditional test. However, both appear to sacrifice accuracy when compared to AIC. BIC assesses a greater parame- ter penalty (κ = log(N)) than does AIC (κ = 2), causing BSS BIC to remove more interactions than BSS AIC. Likewise, FSS BIC adds fewer interactions than FSS AIC. In both cases BIC selects models whose complexity is too low and adversely affects accuracy when compared to AIC. The Naive Bayes classifier achieves a high level of accuracy using a model of low complexity. In fact, while the Naive Bayes classifier is most accu- rate for only 3 of the 12 words, the average accu- racy of the Naive Bayes classifiers for all 12 words is higher than the average classification accuracy re- sulting from any combination of the search strategies and evaluation criteria. The average complexity of the Naive Bayes models is also lower than the av- erage complexity of the models resulting from any combination of the search strategies and evaluation criteria except BSS BIC and FSS BIC. 6.2 Search strategy and accuracy An evaluation criterion that finds models of simi- lar accuracy using either BSS or FSS is to be pre- ferred over one that does not. Overall the infor- mation criteria are not greatly affected by a change in the search strategy, as illustrated in Figure 3. Each point on this plot represents the accuracy of the models selected for a word by the same evalua- tion criterion using BSS and FSS. If this point falls close to the line BSS = FSS then there is little or no difference between the accuracy of the models selected during FSS and BSS. AIC exhibits only minor deviation from BSS = FSS. This is also illustrated by the fact that the average accuracy between BSS AIC and FSS AIC only differs by .0013. The significance tests, espe- cially the exact conditional, are more affected by the search strategy. It is clear that BSS exact condi- tional is much more accurate than FSS exact condi- tional. FSS G² ~ x² is slightly more accurate than BSS G² ~ x². 6.3 Feature selection: interest Figure 2 shows the models selected by the various combinations of search strategy and evaluation cri- terion for interest. During BSS, AIC removed feature L2 from the model, BIC removed L1, L2, R1 and R2, G² ~ x² removed no features, and the exact conditional test removed C2. During FSS, AIC never added R2, BIC never added C1, C3, L1, L2 and R2, and G² ~ x² and the exact conditional test added all the features. G² ~ x² is the most consistent of the evaluation criteria in feature selection. During both BSS and FSS it found that all the features were relevant to classification. Default Naive Bayes Search G² ~ χ² α = .0001 exact α = .0001 AIC BIC agree .7660 .9362 (8) BSS .8936 (8) .9149 (10) .9220 (15) .9433 (9) FSS .9291 (12) .9007 (15) .9362 (13) .9433 (7) bill .7090 .8657 (8) BSS .6567 (22) .6194 (25) .8507 (26) .8806 (7) FSS .7985 (20) .6866 (28) .8582 (20) .8433 (11) chief .8750 .9643 (7) BSS .9464 (6) .9196 (17) .9643 (14) .9554 (6) FSS .9464 (6) .9196 (18) .9643 (14) .9643 (7) close .6815 .8344 (8) BSS .7580 (12) .7516 (13) .8408 (13) .7580 (3) FSS .7898 (13) .7006 (19) .8408 (10) .7580 (3) common .8696 .9130 (7) BSS .9217 (4) .8696 (10) .8957 (7) .8783 (2) FSS .9217 (4) .7391 (16) .8957 (7) .8783 (2) concern .6510 .8725 (8) BSS .8255 (5) .7651 (15) .8389 (16) .7181 (6) FSS .8255 (17) .7047 (24) .8255 (13) .8389 (9) drug .6721 .8279 (8) BSS .8115 (10) .8443 (7) .8443 (14) .7787 (9) FSS .8115 (10) .5164 (19) .8115 (12) .7787 (9) help .7266 .7698 (8) BSS .7410 (7) .7698 (6) .7914 (6) .7554 (4) FSS .7554 (3) .7770 (9) .7914 (4) .7554 (4) include .9325 .9448 (8) BSS .9571 (6) .9571 (3) .9387 (16) .9387 (8) FSS .9571 (6) .7423 (22) .9448 (9) .9325 (9) interest .5205 .7336 (8) BSS .6885 (24) .4959 (24) .7418 (21) .6311 (6) FSS .7172 (22) .4590 (32) .7336 (15) .6926 (4) last .9387 .9264 (7) BSS .9080 (8) .8865 (9) .9417 (14) .9417 (9) FSS .8804 (15) .8466 (18) .9417 (14) .9387 (2) public .5056 .5843 (7) BSS .5393 (7) .5393 (9) .5169 (8) .5506 (3) FSS .5281 (6) .5506 (11) .5281 (6) .5506 (3) average .7373 .8477 (8) BSS .8039 (10) .7778 (12) .8406 (14) .8108 (6) FSS .8217 (11) .7119 (19) .8393 (11) .8229 (6) Figure 1: Accuracy comparison Criterion Search Model G² ~ χ² BSS (C₁EL₁L₂S)(C₁C₂C₃L₁L₂S)(C₁C₂C₃R₁S) FSS (C₂EL₁L₂S)(C₁R₁R₂S)(C₂C₃L₁L₂S)(C₃R₁R₂S) Exact BSS (C₁EL₁L₂R₁R₂S)(C₃L₁L₂R₁R₂S)(C₂EL₁L₂R₁R₂S) FSS (C₁EL₁L₂R₁R₂S)(C₃L₁L₂R₁R₂S)(C₂EL₁L₂R₁R₂S) AIC BSS (C₁C₂C₃ELS)(C₁C₃R₁S)(C₁C₃R₂S) FSS (EL₁L₂S)(C₂EL₂S)(C₁R₁S)(C₃L₁S)(C₃R₁S) BIC BSS (C₂ES)(C₁C₃S) FSS (C₂ES)(R₁S) Naive Bayes none (C₁S)(C₂S)(C₃S)(ES)(L₁S)(L₂S)(R₁S)(R₂S) Figure 2: Models selected: interest --- 1 0.9 0.8 FSS 0.7 0.6 0.5 0.4 0.4 0.5 0.6 0.7 0.8 0.9 1 BSS Figure 3: Effect of Search Strategy AIC found seven features to be relevant in both BSS and FSS. When using AIC, the only difference in the feature set selected during FSS as compared to that selected during BSS is the part of speech feature that is found to be irrelevant: during BSS L2 is removed and during FSS R2 is never added. All other criteria exhibit more variation between FSS and BSS in feature set selection. 6.4 Model selection: interest Here we consider the results of each stage of the sequential model selection for interest. Figures 4 through 7 show the accuracy and recall for the best fitting model at each level of complexity in the search. The rightmost point on each plot for each evaluation criterion is the measure associated with the model ultimately selected. These plots illustrate that BSS BIC selects mod- els of too low complexity. In Figure 4 BSS BIC has \"gone past\" much more accurate models than the one it selected. We observe the related problem for FSS BIC. In Figure 6 FSS BIC adds too few in- teractions and does not select as accurate a model as FSS AIC. The exact conditional test suffers from the reverse problem of BIC. BSS exact conditional removes only a few interactions while FSS exact con- ditional adds many interactions, and in both cases the resulting models have poor accuracy. The difference between BSS and FSS is clearly il- The percentage of ambiguous words in a held out test sample that are disambiguated, correctly or not. A word is not disambiguated if the model parameters needed to assign a sense tag cannot be estimated from the training sample. lustrated by these plots. AIC and BIC eliminate in- teractions that have high dof's (and thus have large numbers of parameters) much earlier in BSS than the significance tests. This rapid reduction in the number of parameters results in a rapid increases in accuracy (Figure 4) and recall for AIC and BIC (Figure 5) relative to the significance tests as they produce models with smaller numbers of parameters that can be estimated more reliably. However, during the early stages of FSS the num- ber of parameters in the models is very small and the differences between the information criteria and the significance tests are minimized. The major differ- ence among the criteria in Figures 6 and 7 is that the exact conditional test adds many more interactions. 7 Related Work Statistical analysis of NLP data has often been lim- ited to the application of standard models, such as n-gram (Markov chain) models and the Naive Bayes model. While n-grams perform well in part- of-speech tagging and speech processing, they re- quire a fixed interdependency structure that is inap- propriate for the broad class of contextual features used in word-sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)). In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word-sense disambiguation. 5 Alternative probabilistic approaches have involved using a single contextual feature to perform disam- biguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambigua- tion). Maximum Entropy models have been used to express the interactions among multiple feature vari- ables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed. Decision tree induction has been applied to word-sense disambiguation (e.g. (Black, 1988) and (Mooney, 1996)) but, while it is a type of model selection, the models are not parametric. 5 They recommended a model selection procedure us- ing BSS and the exact conditional test in combination with a test for model predictive power. In their proce- dure, the exact conditional test was used to guide the generation of new models and the test of model predic- tive power was used to select the final model from among those generated during the search. --- exact ◇ G² ~ x² + AIC □ BIC × BSS=FSS × + --- 1 0.9 0.8 0.7 % 0.6 0.5 0.4 0.3 35 30 25 20 15 10 5 0 # of interactions in model Figure 4: BSS accuracy: interest 1 0.9 0.8 % 0.7 0.6 0.5 0.4 0.3 0 5 10 15 20 25 30 35 # of interactions in model Figure 6: FSS accuracy: interest 1 0.9 0.8 % 0.7 0.6 0.5 0.4 0.3 35 30 25 20 15 10 5 0 # of interactions in model Figure 5: BSS recall: interest 1 0.9 0.8 0.7 % 0.6 0.5 0.4 0.3 0 5 10 15 20 25 30 35 # of interactions in model Figure 7: FSS recall: interest AIC - BIC Δ. Exact α = .0001 Δ. G² ~ χ² α = .0001 .... AIC - BIC Exact α = .0001 Δ. G² ~ χ² α = .0001 .... AIC - BIC Exact α = .0001 Δ. G² ~ χ² α = .0001 .... AIC - BIC Exact α = .0001 Δ. G² ~ χ² α = .0001 .... 8 Conclusion Sequential model selection is a viable means of choosing a probabilistic model to perform word- sense disambiguation. We recommend AIC as the evaluation criterion during model selection due to the following: 1. It is difficult to set an appropriate cutoff value (a) for a significance test. 2. The information criteria AIC and BIC are more robust to changes in search strategy. 3. BIC removes too many interactions and results in models of too low complexity. The choice of search strategy when using AIC is less critical than when using significance tests. How- ever, we recommend FSS for sparse data (NLP data is typically sparse) since it reduces the impact of very high degrees of freedom and the resultant unreliable parameter estimates on model selection. The Naive Bayes classifier is based on a low com- plexity model that is shown to lead to high accuracy. If feature selection is not in doubt (i.e., it is fairly certain that all of the features are somehow relevant to classification) then this is a reasonable approach. However, if some features are of questionable value the Naive Bayes model will continue to utilize them while sequential model selection will disregard them. All of the search strategies and evaluation crite- ria discussed are implemented in the public domain program CoCo (Badsberg, 1995). References H. Akaike. 1974. A new look at the statistical model identification. IEEE Transactions on Automatic Control, AC-19(6):716-723. J. Badsberg. 1995. An Environment for Graphical Models. Ph.D. thesis, Aalborg University. A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71. Y. Bishop, S. Fienberg, and P. Holland. 1975. Discrete Multivariate Analysis. The MIT Press, Cambridge, ΜΑ. E. Black. 1988. An experiment in computational discrimination of English word senses. IBM Jour- nal of Research and Development, 32(2):185-194. P. Brown, S. Della Pietra, and R. Mercer. 1991. Word sense disambiguation using statistical meth- ods. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 264-304. R. Bruce and J. Wiebe. 1994a. A new approach to word sense disambiguation. In Proceedings of the ARPA Workshop on Human Language Tech- nology, pages 244-249. R. Bruce and J. Wiebe. 1994b. Word-sense dis- ambiguation using decomposable models. In Pro- ceedings of the 32nd Annual Meeting of the Asso- ciation for Computational Linguistics, pages 139- 146. R. Bruce, J. Wiebe, and T. Pedersen. 1996. The measure of a model. In Proceedings of the Confer- ence on Empirical Methods in Natural Language Processing, pages 101-112. I. Dagan, A. Itai, and U. Schwall. 1991. Two lan- guages are more informative than one. In Proceed- ings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 130-137. J. Darroch, S. Lauritzen, and T. Speed. 1980. Markov fields and log-linear interaction models for contingency tables. The Annals of Statistics, 8(3):522-539. W. Gale, K. Church, and D. Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415- 439. S. Kreiner. 1987. Analysis of multidimensional con- tingency tables by exact conditional tests: Tech- niques and strategies. Scandinavian Journal of Statistics, 14:97-112. C. Leacock, G. Towell, and E. Voorhees. 1993. Corpus-based statistical sense resolution. In Pro- ceedings of the ARPA Workshop on Human Lan- guage Technology. R. Mooney. 1996. Comparative experiments on dis- ambiguating word senses: An illustration of the role of bias in machine learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. H.T. Ng and H.B. Lee. 1996. Integrating multi- ple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Society for Com- putational Linguistics, pages 40-47. T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Sig- nificant lexical relationships. In Proceedings of the 13th National Conference on Artificial Intelli- gence, pages 455-460. G. Schwarz. 1978. Estimating the dimension of a model. The Annals of Statistics, 6(2):461-464. J. Whittaker. 1990. Graphical Models in Applied Multivariate Statistics. John Wiley, New York. D. Yarowsky. 1993. One sense per collocation. In Proceedings of the ARPA Workshop on Human Language Technology, pages 266-271. G. Zipf. 1935. The Psycho-Biology of Language. Houghton Mifflin, Boston, ΜΑ."
  },
  {
    "title": "The TICC: Parsing Interesting Text",
    "abstract": "This paper gives an overview of the natural language problems addressed in the Traffic Information Collator/Condenser (TICC) project, and describes in some detail the \"interesting-corner parser\" used in the TICC's Natural Language Summariser. The TICC is designed to take free text input describing local traffic incidents, and automatically output local traffic information broadcasts for motorists in appropriate geographical areas. The \"interesting-corner parser\" uses both syntactic and semantic information, represented as features in a unification-based grammar, to guide its bi-directional search for significant phrasal groups.",
    "content": "1. INTRODUCTION The overall goal of the TICC project is to show the potential benefits of automati- cally broadcasting local traffic information. Our target system, dealing with traffic incidents in the Sussex area, is to be com- pleted by September 1989. The project forms part of the Alvey Mobile Information Sys- tems large-scale Demonstrator. The Natural Language Summariser com- ponent of this system is being developed at Sussex University. Its function is to accept a series of free text messages describing traffic incidents, and to extract from these messages any information that might be relevant for broadcast to other motorists. The Natural Language Summariser is designed to work in a restricted domain, and only needs to solve a subset of the problems of text understanding. The TICC's output messages are short and very simple assem- blies of canned text, posing no significant natural language generation problems. Our main concern is that the messages should be useful to motorists, i.e that they be reliable indications of the state of the roads at the time they are broadcast. Programs such as METEO [Chevalier et al. 1978] have demonstrated that in a res- tricted domain with a restricted sub-language, automatic information broadcasts can be use- ful. Programs such as FRUMP [De Jong 1979, De Jong 1982] have also demonstrated that expectation-driven analysers can often successfully capture the gist of free text. However, the top-down depth-first confirmation of expectations based on sketchy scripts, ignoring most of the input structure, can lead to serious misinterpretations [Ries- beck 82]. Our concern for accuracy of interpretation has led us to a processing stra- tegy in which the Natural Language Sum- mariser analyses the input text at a far greater level of detail than is given in the output messages, so the system \"knows more\" about the traffic incidents it is describing than it says in its broadcasts. Our parser uses both syntactic and semantic information to guide its search for phrases in the input that might be directly or indirectly relevant to motorists, and explores alternative possible interpretations bottom-up using an active chart [Earley 1970, Kay 1973]. This is an ongoing research project, and we do not claim to have solved all the problems involved in developing a successful system yet. The current paper considers the particular natural language problems we are addressing and describes the \"interesting- corner parser\" that has been implemented in the prototype system. 2. THE NATURAL LANGUAGE SUMMARISER'S TASK 2.1 INPUT: Our input data comes from the Sussex Police, who have a computer sys- tem for storing the text of incoming traffic messages from a variety of sources (eg. patrol cars, emergency services, motoring organisations). An example of the style of this text, derived from real input but with names etc. changed, is given in fig.1. The series of messages dealing with a single incident continues over a number of hours, depending on the severity of the incident, and the TICC can afford to spend up to one minute analysing an average length message. All aspects of the police management of the incident are described, and many of the messages are only indirectly relevant to motorists. For example, if one of the vehicles involved in an accident needs a total lift to remove it from the road, the likely delay time given in the broadcast message may be longer, although the need for the total lift will not itself be mentioned in the broadcast. Much of the input is completely uninteresting for the TICC's purposes, such as details of injuries sustained by people involved, or of which police units are dealing with the incident. There is a great variety in prose style, from the \"normal\" to the highly telegraphic, but there is a strong tendency towards the abbreviated. It is a non-trivial task to correctly identify the lexical items in the text. Parts of the input string which are not recognised as entries in the Summariser's lex- icon (or regular derivations from entries) may be of four types: i) Names, numbers etc, which may be recognised as such from the context (e.g pc humphries requests ..., ford cortina reg ABC123). ii) Other English words not in the lexi- con, which cannot reliably be predicted to be proper names (e.g hovis lorry b/dwn o/s bull's head ph). iii) Misspellings of items in the lexicon. iv) Non-standard abbreviations of known words or phrases. Abbreviations are not always of \"canoni- cal\" form, and may be derived from com- plete words in three different ways, as fol- lows: i) Single morpheme roots: These usu- ally have more than one possible abbreviated form and never include punctuation eg. gge, grg or gar for garage. But some words do have canonical abbreviations (eg rd for road and st for street (or saint). ii) Multi-morpheme roots: These often take only the first letter from the first root morpheme, and then either part or all of the second morpheme. They occasionally include slash punctuation eg. cway, c/way for car- riageway, mcycle, m/c for motorcycle, o/s for outside (or offside), and ra for roundabout. iii) Sequences / phrases: Some sequences of words have canonical abbreviations (e.g bbc and not britbrdcrp). Canonical examples seen in Fig. 1. below include rta for road traffic accident and oic for officer in charge. Non-canonical sequences may have a variety of abbreviations for each of the con- stituent words, and may or may not have slash or period punctuation, eg. f/b for fire brigade, eamb or esamb for east (sussex) ambu- lance, hazchem for hazardous chemicals. The problem is compounded for the TICC by the fact that the input we receive is all in upper case, hence the even the con- vention of distinguishing proper name abbre- viations by upper case is not applicable. In order to cope with these different types of input string, we need not only a \"phrasal lexicon\" as advocated by Becker [Becker 1975], but also an \"abbreviation lexicon\". Time: 1634 Location: scaynes hill, haywards heath 1634 rta serious near top scaynes hill persons trapped rqst esamb f/b 1/2 mile 1634 south jw freshfield rd. 1638 fm pc 123 acc inv reqd poss black oic pc 456 1639 fire and amb en route 1642 req total lift for saloon car rota garage 1654 eamb now away from scene 1655 freshfield bodyshop on way 1657 fm pc 456 req rd closed n and s of hill st crnr 1658 req two traff units to assist re closures 1709 can we inform brighton 1234 tell mr fred smith will be late due to this rta 1715 local authority required loose paving stones 1723 fm pc 234 at st george's hosp. dr in charge having examined mr jones now feels this is not likely to be a black. driver of lorry has arrived, will probably be released after treatment for cuts. car 45 will be free from hosp in about 20 min Fig. 1. An extract from an example (fictitious) incident log. Our aim is to have a unified process for identifying idiomatic or fixed phrases and abbreviated sequences as in iii) above, so that for example as soon as poss, asap and a.s.a.p. are all identified as the same \"lexical item\". Work on this is, however, at a preliminary stage, and we have not yet found any gen- eral solution to the problem. 2.2 SUMMARISATION: Deriving a short broadcast for motorists from a long series of messages such as that in fig. 1 requires two main phases. First, the Natural Language Summariser must build up a picture of what is happening at the scene of the incident. Second, a Tactical Inferencer must decide what motorists should be told regarding the incident. The Natural Language Summarising pro- cess also requires two phases. In the first phase a Message Analyser extracts interesting information from a single message. In the second phase an Event Recogniser puts together the information from a series of messages to build up a description of the incident as a whole, or rather those aspects of the incident relevant to other motorists (see fig 2. below). The Message Analyser does not build a complete representation of the syntax and semantics of messages such as those at 1709 and 1723 in fig. 1 above, since they have no bearing on the progress of the traffic incident as far as other motorists are concerned. It just searches for phrases describing \"interest- ing\" events. These fall into two classes: Primary Events: Such as vehicles blocking the road, substances spilling onto the road, all or part of the road being closed, diversions being put into operation, garage coming to remove vehicles from the road, services like fire brigade and county council removing hazards, etc. The input messages rarely describe these events in full, so the Event Recogniser must infer, for example, that if the local council has been called out to remove debris from the road, that at some time earlier debris must have fallen on the road. Secondary Events: These include requests that some of the primary events should happen, and people being informed that primary events have happened are hap- pening or will happen. We will not have any model of the beliefs of the various agents involved in incident handling. As far as the TICC Natural Language Summariser is concerned, the meaning of someone being informed that a primary event has happened is equivalent to the statement that it has happened. But the Tactical Inferencer will use its model of the typical progress of traffic incidents to predict the significance of the primary events for other motorists. For example, if a vehicle is stated to need a front suspended tow, then the Tactical Inferencer will predict that a certain amount of time will elapse before the vehicle is towed away. 2.3 OUTPUT: Not every message input to the system will produce an update to the Event Recogniser's description of the incident, because the Message Analyser may fail to find a description of an interesting event. But even when the Event Recogniser passes a description of a traffic incident to the Tacti- cal Inferencer, this will not necessarily result in a broadcast. For example, the Event Recogniser may recognise a series of messages as describing a traffic light failure incident. The Tactical Inferencer may decide to broad- cast a message about this incident if it has occurred on a busy main road in the rush hour, but not if it has occurred late at night in a small village. Road/Junction Database Free Text Message Analyser Event Recogniser Tactical Inferencer Messages Messages for Incident Description Database Incident Database Broadcaster Fig. 2. Part of the TICC system, showing Message Analyser, Event Recogniser, and Tactical Inferencer. The domain knowledge used in the the Tactical Inferencer is non-linguistic, and con- cerns inferences about the likely time delays for different types of incident, the geographi- cal areas likely to be affected by a given incident, etc. The Transport and Road Research Laboratory, part of the Department of Transport, are assisting us in the develop- ment of rules for this part of the system. There are other components of the TICC system which we do not detail in this paper, such as the graphical interface, via a map of the Sussex area, to a database of of current traffic incidents. Although the TICC is designed to send its messages to a dedicated broadcasting system, the actual broadcasting aspect of the project is the responsibility of RACAL research, one of our other Alvey collaborators. In our current prototype sys- tem, implemented on a Sun-3 workstation, broadcasts to local geographical areas in Sussex are simulated, and the Tactical Inferencer is extremely simple. 3. INTERESTING CORNER PARSING The parser that has been implemented for the Message Analyser searches bidirection- ally for all syntactic parses associated with semantically interesting parts of the input. Before describing the search strategy in more detail, we need to clarify what a syntactic parse looks like in our grammar formalism, and how we specify what is semantically interesting. 3.1 THE GRAMMAR FORMALISM: We use a unification-based grammar formalism, with rules that look similar to context-free phrase-structure rules. Both immediate domi- nance and constituent ordering information are specified by the same rule, rather than by separate statements as in FUG [Kay 1985], LFG [Kaplan & Bresnan 1982] and GPSG [Gazdar et al 1985]. Feature-passing between categories in rules is done explicitly with logical variables, rather than by con- ventions such as the HFC and FFP in GPSG [Gazdar et al 1985]. Thus the rule format is most similar to that used in DCG's [Pereira & Warren 1980]. Categories in rules are feature/value trees, and at each level the value of a feature may itself be another feature/value tree. Feature values may be given logical names, and occurrences of feature values having the same logical name in a rule must unify. The feature trees which constitute categories in our grammar may specify both syntactic and semantic features, so that we can write \"syntactic\" rules which also iden- tify the semantic types of their constituents. For example, if we use the feature sf on categories to specify a tree of semantic features for that category, then the rule: (1) vp=(sf:VSF) -> v=(sf=(patient:P):VSF), np=(sf:P) says that a verb phrase may consist of a verb followed by a noun phrase, and that the semantic features on the noun phrase (labelled P) must unify with the semantic features specified as the value of the patient sub-feature of the verb's semantic features, and additionally that the semantic features on the whole verb phrase (labelled VSF) must unify with the (complete tree of) semantic features on the verb. By adding domain-specific semantic feature information to lexical categories, we gain the power of domain-specific semantic grammars, which have been shown to be suc- cessful for handling ill-formed input in lim- ited domains [Burton 1976]. But because we use unification by extension as the basic cri- terion for node admissability when we test for rules to licence local trees, we can also capture generalisations about syntactic categories that are not domain-specific. So for example if we had a verb-phrase rule such as (2) and a lexical entry as in (3): (2) vp --> v=(tr=trans), np (3) close v=(tr=(trans), sf=(event_type=road_closure, agent=service, patient=roadlocation)) then the verb feature tree specified in (2) would unify with the verb feature tree in (3). Hence close can be treated both as a domain specific verb and as an instance of the general class of transitive verbs. Using a feature-based semantic grammar therefore gives us a compact representation of both domain independent and domain-specific information in a single uniform formalism. Syntactic generalisations are captured by rules such as (2), and domain-specific sub- categorisation information is expressed in feature-trees as in (3), which states that close has the semantic features of a road-closure event, expecting an agent with the semantic features of a service (eg police) and a patient with semantic features indicating (a part of) a road. As with all sub-languages, our lexicon also includes domain-specific meanings for particular lexical items, eg. black mean- ing fatal (cp messages at 1638 and 1723 in fig. 1 above). 3.2 A GRAMMAR FOR THE TICC DOMAIN: Writing a grammar to give ade- quate coverage of the input that our system must handle is a lengthy task, which will continue over the next two years. However, analysis of a corpus of data from police logs of over one hundred incidents in the Sussex area, and trials with experimental grammars, have led us to adopt a style of grammar which we expect will remain constant as the grammar expands. We do not attempt to map telegraphic forms onto \"fully grammatical\" English forms by some variant of constraint relaxation [Kwasny & Sondheimer 1981]. We simply have a grammar with fewer constraints. This is because it is not always easy to decide what is missing from an elliptical sentence, or which constraints should be relaxed. Con- sider for example the message at 1655 from fig. 1, repeated here: (4) freshfield bodyshop on way It is not at all clear what the \"full\" senten- tial form of this message ought to be, since it might also have been phrased as one of: (5.1) freshfield bodyshop is on the way (5.2) freshfield bodyshop is on its way (5.3) freshfield bodyshop are on the way (5.4) freshfield bodyshop are on their way Each of the (5.1)-(5.4) must be allowed to be grammatical (and each might occur in our type of input), since noun phrases nam- ing corporate entities can regularly be regarded as singular or plural (cp. Ford Motors has announced massive profits vs. Ford Motors have announced massive profits). But in each case the semantic representation that the Message Analyser must build only needs to represent the fact that the garage called freshfield bodyshop are going some- where (which the Event Recogniser will expect to be the scene of the incident, in order to remove the damaged vehicle). Since the distinctions between the syntactic forms in these examples is irrelevant for our pur- poses, it would be a waste of the parser's effort to introduce search and inference prob- lems in the attempt to map the syntax of (4) uniquely into the syntax of one or other of the forms in (5). Indeed it is more appropriate for our purposes to regard on way as a domain-specific idiomatic phrase, equivalent to en route, enrte etc (each of which occur in similar contexts). In keeping with this approach to ill- formedness, our grammar contains many categories (ie feature-trees), that would not be recognised as syntactic categories in gram- mars for normal English, eg. we have special rules for phrases containing predicted unk- nowns such as names, car registration numbers, etc. Our parser is looking for phrases describing events rather than sen- tences, and we will not necessarily always assign a structure with a single \"S\" label spanning all the input message. As we noted in 3.1 above, the lexical entries for words that suggest interesting events include trees of semantic features that specify expected fillers for various roles in these events. These feature trees provide selectional restrictions useful for guiding the parse, but do not themselves constitute the \"semantics\" of the lexical entries. The semantics are represented as first-order logical expressions in a separate field of the lexical entry, and representations of the meaning of phrases are built using semantic rules associ- ated with each syntactic rule, as phrases are completed in the bottom-up parse. 3.3 THE SEARCH STRATEGY: Interesting-corner parsing is basically an adaptation of bottom-up chart parsing to allow island-driving through the input string, whilst still parsing each individual rule uni- directionally. This gives a maximally efficient parse for our goal of reliably extracting from the input all and only the information that is relevant to other motor- ists. This form of expectation-driven parsing differs from that used in earlier script-based systems such as MARGIE [Schank 1975], ELI [Riesbeck 1978] and FRUMP in four ways: First, the interesting-corner parser uses an active chart to consider bottom-up all interesting interpretations that might be given to an input message, rather than proceeding left to right and filtering out later (right) candidate interpretations on the basis of ear- lier (left) context. Second, if there are no interesting lexical items in the input string, or if the only interesting items occur at the (right) end of the input, there is no attempt to match all the leftmost items to a series of candidate scripts or frames using top-down expecta- tions. Third, the expectations themselves are expressed declaratively in feature trees that form part of the lexical categories, which control the search via standard unification with declarative rules, where previous sys- tems used procedural \"requests\" in the lexi- con. Fourth, our parser builds an explicit syntactic tree for the input, albeit including semantic features, rather than by building a semantic representation \"directly\". The interesting-corner parser checks the semantic features on every lexical item in the input to see if they are interesting, but this is a far faster operation than testing many times whether a series of lexical items matches the expectations from a top-down script. This does assume that the parser can identify what the lexical items are, which is problematic as we noted in section 2.1 above. But as we shall see, the interesting-corner parser does use predictions about the presence of lexical items with particular features in its search, and hence is in no worse a posi- tion than a strictly top-down parser as regards matching expectations to ill-formed lexical items. 3.3.1 UNIDIRECTIONAL ISLAND- DRIVING: Island-driving is useful for text where one needs to start from clearly identifiable (and in our case, semantically interesting) parts of the input and extend the analysis from there to include other parts. But parsing rules bi-directionally is inherently inefficient. Consider, for example, a chart parse of the input string a b given a single rule: c -> a b. A standard bottom-up left-to-right active chart parse of this input would create three nodes (1 a 2 b 3) two active edges (an empty one at node 1 and one from nodes 1 to 2) and one inactive edge (from node 1 to 3). But a bi-directional parse, allowing the rule to be indexed at any point, would build a total of 7 active edges (one empty one at each node, and 2 pairs with one constituent found, built in different directions, ie 5 dis- tinct edges). It would also build the same inactive edge in two different directions. For a rule with three daughters, a bidirectional parse produces 14 active edges (9 of which are distinct) and again 2 inactive edges. This redundancy in structure-building can be removed by incorporating constituents into rules unidirectionally whilst still parsing the text bidirectionally. We do this by indexing each rule on either left-most or right-most daughter, and parsing in a unique direction away from the indexed daughter. In order to preserve completeness in the search, the chart must contain lists of active and inactive edges for each direction of expansion, although the same structure can be shared in the inactive edge-lists for both directions. The fundamental rule of edge- combination must be augmented so that when an inactive edge is added to the chart, it combines with any appropriate active edges at both of its ends. This process might be called \"indexed-corner parsing\", in that it effectively combines left-corner parsing and right-corner parsing, and the direction of parse at any stage simply depends upon how the individual grammar rules are indexed. The interesting-corner parser implements an indexed-corner chart parser, with the addition of an agenda control mechanism and an indexing principle for grammar rules. 3.3.2 AGENDA CONTROL: The insertion of edges into the agenda is constrained by the value of a \"control-feature\", which specifies where to look in the feature-trees that constitute our categories in order to find the semantically \"interesting\" features. In our examples (1) and (2) above, this control-feature is named sf. When a normal bottom-up chart parse begins, all lexical items are tested to see whether they can spawn higher edges. But in the interesting- corner parse, higher edges are only spawned from lexical items that have a control-feature specification which unifies with a pre-defined initial value of the control feature. Thus by assigning (sf-event_type) to be the initial value of the control feature, we ensure that only those edges are entered into the agenda that have semantic feature trees that are extensions of this tree (eg the semantic feature tree for close in (3) above). This effectively means that parsing must begin from words that suggest some kind of interesting event. Note that the initial active edges may be proposed from any point in the input string, and their direction of expansion from that point is determined by the indexing on the rules. For all active edges proposed from lexi- cal items that were initially recognised to be interesting, the parser checks the list of edges sought for \"interesting\" categories (ie. those with values for the control-feature sf). If there are any, it searches, in the direction of expansion for the current active edge, for any lexical items that have a semantic feature-tree which unifies with the new specification of what is \"interesting\". For example, if the rule given in (1) above is indexed on the left daughter, and an active edge is proposed starting from an inactive edge representing the lexical item close defined as in (3) above, then via the logical name P the features on the noun- phrase being sought become instantiated to (sf-roadlocation). The parser then looks rightwards in the input string for any lexical items having semantic feature trees that are extensions of this new tree. If it finds any, it predicts more active edges from there, and so forth. Fig. 3 below illustrates the numerical order in which the interesting-corner parser incorporates nodes into the parse tree for a very simple \"sentence\" (in our grammar we allow sentences with deleted auxiliaries), but with the details of the feature trees omitted for legibility. Extension unification allows one of the structures to be unified (the target) to be an extension of the other (the pattern), but not vice-versa. This means that it is more res- tricted than graph unification, and hence can be implemented more efficiently. It is less restricted than term unification, and hence less efficient at parse-time, but it does allow the grammar and lexicon to be far more compact than they would be with term- unification in the absence of a grammar pre- processor. However, using extension unification as the basic operation does also mean that that the unification of logical variables in rules is not order-independent, and hence we need an indexing principle to determine the direction in which particular rules should be parsed. 3.3.3 THE INDEXING PRINCIPLE: Our general principle for indexing rules is that we must parse from categories that specify general information (ie. that have small feature-trees) to those that specify particular modifications of that general information (ie. that provide extensions to the smaller trees by unification). This usually means that we parse from syntactic heads to complements, eg indexing sentences on the vp (cf. HPSG [Proudian & Pollard 1985]). In our example rule (1), we index on the verb, because its expectations specify the general semantic type of the object, and the semantic feature tree of the noun-phrase will specify a sub-type of this general type, and therefore will be an extension of the verb's patient semantic feature tree. In the example shown in fig 3, the semantic tree of the np built at node 4 is: (sf-(roadlocation-(name-huntingdon, rtitle-lane))) which unifies by extension with the feature tree (sf-roadlocation)), and this as we saw above became the expected semantic tree for the noun-phrase when rule (1) unified with the verb in (3). Finally, rules for categories that have expected unknowns as daughters are always indexed on the known categories, even if these are not the grammatical head (eg we index on the policeman's title for rules han- dling sgt smith, insp brown etc. and on the known title of a road for cases like hunting- don lane, markworthy avenue etc. 3.3.4 EXTENSIONS TO THE CURRENT SYSTEM: There are many aspects of the TICC's Natural Language Summarisation not dealt with in this paper, such as the seman- tic rules used in the Message Analyser and 10 S 6 vp 5 np 9 np 4 n1 7 poltitle unknown 8 V 1 unknown 3 n 2 pc chisholm closing huntingdon lane Fig. 3. Showing the order in which the interesting-corner parser constructs a parse tree, starting with the most interesting words. the Event Recogniser. There are also many inadequacies in the current implementation of the Message Analyser, eg in its handling of abbreviations/phrases, and in the handling of input that is \"ill-formed\" even with respect to our relatively unconstrained grammar. However, work is currently in progress on these problems, and we believe that the basic mechanisms of interesting-corner parsing are sufficiently powerful to enable us to achieve a practical solution, whilst being sufficiently general to ensure that such a solution will be theoretically interesting. 4. CONCLUSION The automatic production of traffic broadcasts, given the type of free text we have described in this paper, poses many difficult problems. In many ways our overall approach to these problems follows in a long tradition of semantically driven systems, but the processing style of our Message Analyser is much closer to that used in contemporary syntax-driven systems. We make explicit use of rules in a unification-based grammatical formalism that express both semantic and syntactic information declaratively, and our interesting-corner parser provides a search of the input messages that is both thorough and efficient. We believe that complete understanding of free text messages is well beyond the state of the art in computational linguistics, but that we can nevertheless develop the TICC's Natural Language Summariser to have sufficient partial understanding to be practi- cally useful. REFERENCES Becker, J.D. (1975) \"The Phrasal Lexi- con\", in R. C. Schank and B. L. Nash- Webber (eds.), Proceedings of the Workshop on Theoretical Issues in Natural Language Pro- cessing. Cambridge, Mass., Bolt, Beranek and Newman, pp. 70-73. Burton, R. (1976) \"Semantic Grammar: an Engineering Technique for Constructing Natural Language Understanding Systems\", Technical Report 3453, Cambridge, Mass., Bolt, Beranek and Newman. Chevalier, M., Dansereau, J., and Poulin, G. (1978) \"TAUM-METEO: Description Du Systeme.\", Montreal, Groupe TAUM, Univer- site de Montreal. DeJong, G.F. (1979) \"Skimming Stories in Real Time\", Doctoral Thesis, New Haven, Yale University. DeJong, G.F. (1982) \"An Overview of the FRUMP System\", in Wendy G. Lehnert and Martin H. Ringle (eds.), Strategies for Natural Language Processing. Hillsdale, Erlbaum, pp. 149-176. Earley, J. (1970) \"An Efficient Context- free Parsing Algorithm\", Communications of the ACM. vol. 6, no. 8, pp. 451-455. Gazdar, G., Klein, E., Pullum, G., and Sag, I. (1985) Generalised Phrase Structure Grammar. Oxford, Blackwell. Kaplan, R., and Bresnan, J. (1982) \"Lexi- cal Functional Grammar: a Formal System for Grammatical Representation\", in Joan Bresnan (ed.), The Mental Representation of Grammatical Relations. Cambridge MA, MIT Press, pp. 173-281. Kay, M. (1973) \"The MIND System\", in Randall Rustin (ed.), Natural Language Pro- cessing. New York, Algorithmics Press, pp. 155-188. Kay, M. (1985) \"Parsing in Functional Unification Grammar\", in David R. Dowty, Lauri Karttunen and Arnold M. Zwicky (eds.), Natural Language Parsing. Cambridge, Cambridge University Press, pp. 251-278. Kwasny, S.C., and Sondheimer, N.K. (1981) \"Relaxation Theories for Parsing Ill- formed Input\", American Journal of Computa- tional Linguistics. vol. 7, no. 2, pp. 99-108. Pereira, F.C.N., and Warren, D.H.D. (1980) \"Definite Clause Grammars for Language Analysis - a Survey of the Formal- ism and a Comparison with Augmented Transition Networks\", Artificial Intelligence. vol. 13, no. 3, pp. 231-278. Proudian, D., and Pollard, C.J. (1985) \"Parsing Head-driven Phrase Structure Gram- mar\", ACL Proceedings, 23rd Annual Meeting. pp. 167-171. Riesbeck, C.K. (1978) \"An Expectation- driven Production System for Natural Language Understanding\", in Donald A. Waterman and Rick Hayes-Roth (eds.), Pattern-directed Inference Systems. New York, Academic Press, pp. 399-414. Riesbeck, C.K. (1982) \"Realistic Language Comprehension\", in Wendy G. Lehnert and Martin H. Ringle (eds.), Strategies for Natural Language Processing. Hillsdale, Erlbaum, pp. 37-54. Schank, R.C. (1975) Conceptual Informa- tion Processing. Amsterdam, North-Holland."
  },
  {
    "title": "Finite-State Reduplication in One-Level Prosodic Morphology",
    "abstract": "Reduplication, a central instance of prosodic morphology, is particularly challenging for state-of-the-art computational morphology, since it involves copying of some part of a phonological string. In this paper I advocate a finite-state method that combines enriched lexical representations via intersection to implement the copying. The proposal includes a resource-conscious variant of automata and can benefit from the existence of lazy algorithms. Finally, the implementation of a complex case from Koasati is presented.",
    "content": "1 Introduction In the past two decades computational morphology has been quite successful in dealing with the chal- lenges posed by natural language word patterns. Using finite-state methods, it has been possible to describe both word formation and the concomi- tant phonological modifications in many languages, ranging from straightforward concatenative combi- nation (Koskenniemi, 1983) over Semitic-style non- concatenative intercalation (Beesley (1996), Kiraz (1994)) to circumfixional long-distance dependen- cies (Beesley, 1998). However, Sproat (1992) observes that, despite the existence of \"working systems that are сара- ble of doing a great deal of morphological analy- sis\", \"there are still outstanding problems and ar- eas which have not received much serious attention\" (ibid., 123). Problem areas in his view include sub- tractive morphology, infixation, the proper inclu- sion of prosodic structure and, in particular, redu- plication: \"From a computational point of view, one point cannot be overstressed: the copying required in reduplication places reduplication in a class apart from all other morphology.\" (ibid., 60). Productive reduplication is so troublesome for a formal account based on regular languages (or regular relations) because unbounded total instances like Indonesian noun plural (orang-orang 'men') are isomorphic to the copy language ww, which is context-sensitive. In the rest of this paper I will lay out a proposal for handling reduplication with finite-state methods. As a starting point, I adopt Bird & Ellison (1994)'s One-Level Phonology, a monostratal constraint- based framework where phonological representa- tions, morphemes and generalizations are all finite- state automata (FSAs) and constraint combination is accomplished via automata intersection. While it is possible to transfer much of the present pro- posal to the transducer-based setting that is often preferred nowadays, the monostratal approach still offers an attractive alternative due to its easy blend with monostratal grammars such as HPSG and the good prospects for machine learning of its surface- true constraints (Ellison (1992), Belz (1998)). After a brief survey of important kinds of redupli- cation in §2, section §3 explains the necessary ex- tensions of One-Level Phonology to deal with the challenges presented by reduplication, within the larger domain of prosodic morphology in general. A worked-out example from Koasati in §4 illus- trates the interplay of the various components in an implemented analysis, before some conclusions are drawn in section §5. 2 Reduplication A well-known case from the context-sensitivity debate of the eighties is the N-o-N reduplicative construction from Bambara (Northwestern Mande, (Culy, 1985)): (1) a. wulu-o-wulu 'whichever dog' b. wulunyinina-o-wulunyinina 'whichever dog searcher' c. wulunyininafilèla-o-wulunyininafilèla 'whoever watches dog searchers' Beyond total copying, (1) also illustrates the pos- sibility of so-called fixed-melody parts in redupli- cation: a constant /o/ intervenes between base (i.e. original) and reduplicant (i.e. copied part, in bold print).¹ The next case from Semai expressive minor redu- plication (Mon-Khmer, Hendricks (1998)) high- lights the possibility of an interaction between redu- plication and internal truncation: (2) a. cre:t ct-cre:t b. doh dh-doh с. cfa:l cl-cfa:l 'sweet' 'appearance of nod- ding constantly' 'appearance of flick- ering red object' Reduplication copies the initial and final segment of the base, skipping all of its interior segments, which may be of arbitrary length. A final case comes from Koasati punctual-aspect reduplication (Muscogean, (Kimball, 1988)): (3) a. ta.hás.pin t₁ahas-t₁ ó:-pin 'to be light in weight' b. la.pát.kin lapat-l₁ó:-kin 'to be narrow с. ak.lát.lin a₁k-h10-látlin 'to be loose' d. ok.cák.kon o₁k-h1o-cákkon 'to be green or blue' Koasati is particularly interesting, because it shows that copy and original need not always be adjacent here the reduplicant is infixed into its own base and also because it illustrates that the copy may be phonologically modified: the /h/ in the copied part of (3).c,d is best analysed as a voiceless vowel, i.e. the phonetically closest consonantal expression of its source. Moreover, the locus of the infixed redu- plicant is predictable on prosodic grounds, as it is inserted after the first heavy syllable of the base. Heavy syllables in Koasati are long (C)VV or closed (C)VC. Prosodic influence is also responsible for the length alternation of its fixed-melody part /o(o)/, since the heaviness requirement for the penultimate, stressed, syllable of the word causes long [o:] iff the reduplicant constitutes that syllable. 'Culy (1985), who presents a superset of the data under (1) in the context of a formal proof of context-sensitivity, shows that the reduplicative construction in fact can copy the outcome of a recursive agentive construction, thereby becoming truly unbounded. He emphasizes the fact that it is \"very productive, with few, if any restrictions on the choice of the noun\" (p.346). 3 Finite-State Methods The present proposal differs from the state-labelled automata employed in One-Level Phonology by re- turning to conventional arc-labelled ones, but shares the idea that labels denote sets, which is advanta- geous for compact automata. 3.1 Enriched Representations As motivated in §2, an appropriate automaton repre- sentation of morphemes that may undergo redupli- cation should provide generic support for three key operations: (i) copying or repetition of symbols, (ii) truncation or skipping, and (iii) infixation. For copying, the idea is to enrich the FSA rep- resenting a morpheme by encoding stepwise repeti- tion locally. For every content arc ij we add a reverse repeat arc j repeat i. Following repeat arcs, we can now move backwards within a string, as we shall see in more detail below. For truncation, a similar local encoding is avail- able: For every content arc ij, add another skip arc ij. This allows us to move forward while suppressing the spellout of c. skip A generic recipe for infixation ensures that seg- mental material can be inserted anywhere within an existing morpheme FSA. A possible representa- tional enrichment therefore adds a self loopii labelled with the symbol alphabet ∑ to every state i of the FSA.2 Each of the three enrichments presupposes an epsilon-free automaton in order to be wellbehaved. This requirement in particular ensures that techni- cal arcs (skip, repeat) are in 1:1 correspondence with content arcs, which is essential for unambigu- ous positional movement: e.g. add_skips(aeb) would ambiguously require 1 or 2 skips to supress the spellout of b, because it creates a disjunction of the empty string e with skip. It is perhaps worth emphasizing that there is no special interpretation whatsoever for these technical arcs: the standard au- tomaton semantics is unaffected. As a consequence, skip and repeat will be a visible part of the output in word form generation and must be allowed in the input for parsing as well. Taken together, the three enrichments yield an automaton for Bambara wulu, shown in figure 1.a. While skipping is not necessary for this example, 44 is: it will host the fixed-melody /o/. The 2This can be seen as an application of the ignore operator of Kaplan and Kay (1994), where 2* is being ignored. repeat arcs will of course facilitate copying, as we shall see in a moment. skip skip skip skip a. w:1 u:0 1:0 u:1 0 1 2 3 4 repeat repeat repeat repeat Σ Σ Σ Σ Σ b. seg:1 0 1 seg:12 0 3 ③ seg:1 4 seg: 1 seg:0 repeat seg:0 Figure 1: Enriched automata for wulu (a.), Bambara N-o-N reduplication (b.) 3.2 Copying as Intersection Bird & Ellison (1992) came close to discovering a useful device for reduplication when they noted that automaton intersection has at least indexed- grammar power (ibid., p.48). They demonstrated their claim by showing that odd-length strings of indefinite length like the one described by the regular expression (abcdefg)+ can be repeated by intersecting them with an automaton accept- ing only strings of even length: the result is (abcdefgabcdefg)+. Generalizing from their artifical example, let us first make one additional minor enrichment by tag- ging the edges of the reduplicative portion of a base with synchronization bits:1, while using the opposite value :0 for the interior part (see fig- ure 1.a). This gives us a segment-independent handle on those edges and a regular expression seg:1seg:0*seg:1 for the whole synchronized portion (seg abbreviates the set of phonological segments). Assuming repeat-enriched bases, a total redupli- cation morpheme can now be seen as a partial word specification which mentions two synchronized por- tions separated by an arbitrary-length move back- wards: (4) seg:1seg:0*seg:1 repeat* seg:1seg:0*seg:1 Moreover, total reduplicative copying now simply is intersection of the base and (4), or – in the Bam- bara case - a simple variant that adds the /o/ (figure 1.b). Disregarding self loops for the moment, the reader may verify that no expansion of the kleene- starred repeat that traverses less than base seg- ments will satisfy the demand for two synchronized portions. Semai requires another slight variant of (4) which skips the interior of the base in the redu- plicant: (5) seg:1 skip*seg:1 repeat* seg:1seg:0*seg:1 The identification of copying with intersection not only allows for great flexibility in describing the full range of actual reduplicative constructions with reg- ular expressions, it also reuses the central operation for constraint combination that is independently re- quired for one-level morphology and phonology. Any improvement in efficient implementation of intersection therefore has immediate benefits for grammar computation as a whole. In contrast, a hypothetical setup where a dedicated total copy de- vice is sandwiched between finite-state transducers seems much less elegant and may require additional machinery to detect copies during parsing. Note that it is in fact possible to compute reduplication-as-intersection over an entire lexicon of bases (see figure 3 for an example), provided that repeat arcs are added individually to each base. En- riched base FSAs can then be unioned together and undergo further automaton transformations such as determinization or minimization. This restriction is necessary because our finite-state method cannot express token identity as normally required in string repetition. Rather than identifying the same token, it addresses the same string position, using the weaker notion of type identity. Therefore, application of the method is only safe if strings are effectively isolated from one another, which is exactly what per-base enrichment achieves. See $3.4 for a suggestion on how to lift the restriction in practice. 3.3 Resource Consciousness One pays a certain price for allowing general repe- tition and infixation: because of its self loops and technical arcs, the automaton of figure 1.a over- generates wildly. Also, during intersection, self loops can absorb other morphemes in unexpected ways. A possible diagnosis of the underlying de- fect is that we need to distinguish between produc- ers and consumers of information. In analogy to LFG's constraint vs constraining equations, infor- mation may only be consumed if it has been pro- duced at least once. For automata, let us spend a P/C bit per arc, with P/C=1 for producers and P/C=0 for consumer arcs. In open interpretation mode, then, intersection com- bines the P/C bits of compatible arcs via logical OR, making producers dominant. It follows that a re- source may be multiply consumed, which has obvi- ous advantages for our application, the multiple re- alization of string symbols. A final step of closed in- terpretation prunes all consumer-only arcs that sur- vived constraint interaction, in what may be seen as intersection with the universal producer language under logical-AND combination of P/C bits. Using these resource-conscious notions, we can now model both the default absence of material and purely contextual requirements as consumer-type information: unless satisfied by lexical resources that have been explicitly produced, the correspond- ing arcs will not be part of the result. By convention, producers are displayed in bold. Thus, the exact re- sult of figure 1.a∩ 1.b after closed interpretation is: w:1 1:0 1:0 1:0 0 repeat⁴ repeat* w:1 1:0 1:0 4:1 This expression also illustrates that, for parsing, strings like wuluowulu need to be consumer-self- loop-enriched via a small preprocessing step, be- cause intersection with the grammar would other- wise fail due to unmentioned technical arcs such as repeat. Because our proposal is fully declarative, parsing then reduces to intersecting the enriched parse string with the grammar-and-lexicon automa- ton (whose construction will itself involve intersec- tion) in closed interpretation mode, followed by a check for nonemptiness of the result. Whereas the original parse string was underspecified for mor- phological categories, the parse result for a realis- tic morphology system will, in addition to technical arcs, contain fully specified category arcs in some predefined linearization order, which can be effi- ciently retrieved if desired. 3.4 On-demand Algorithms It is clear that the above method is particularly at- tractive if some of its operations can be performed online, since a fullform lexicon of productive redu- plications is clearly undesirable e.g. for Bambara. I therefore consider briefly questions of efficient im- plementation of these operations. Mohri et al. (1998) identify the existence of a local computation rule as the main precondition³ for a lazy implementation of automaton operations, i.e. one where results are only computed when demanded by subsequent operations. Such imple- mentations are very advantageous when large in- termediate automata may be constructed but only a small part of them is visited for any particular in- put. They show that such a rule exists for composi- ³A second condition is that no state is visited that has not been discovered from the start state. It is easy to implement (6) so that this condition is fulfilled as well. tion, hence also for our operation of intersection (A∩B = range(identity(A) o identity(B))). Fortunately, the three enrichment steps all have local computation rules as well: (6) a. q1 ≤ q2 ⇒ q2 repeat q1 skip b. q1 ≤ q2 ⇒ q1 q2 c. q ⇒ q ⇒ q The impact of the existence of lazy implementa- tions for enrichment operations is twofold: we can (a) now maintain minimized base lexicons for stor- age efficiency and add enrichments lazily to the cur- rently pursued string hypothesis only, possibly mod- ulated by exception diacritics that control when en- richment should or should not happen.⁴ And (b), laziness suffices to make the proposed reduplication method reasonably time-efficient, despite the larger number of online operations. Actual benchmarks from a pilot implementation are reported elsewhere (Walther, submitted). 4 A Worked Example In this section I show how to implement the Koasati case from (3) using the FSA Utilities toolbox (van Noord, 1997). FSA Utilities is a Prolog-based finite-state toolkit and extendible regular expression compiler. It is freely available and encourages rapid prototyping. Figure 2 displays the regular expression opera- tors that will be used (italicized operators are mod- ifications or extensions). The grammar will be pre- [] [E1, E2, ..., En] {E1, E2, ..., En} E* E^ E1 & E2 X --> (Y/Z) ~S Head(arg1, ..., argN) := Body empty string concatenation of Ei union of Ei Kleene closure optionality intersection monotonic rule X → Y ⊆ X / ¬Z complement set of S (parametrized) macro definition Figure 2: Regular expression operators sented below in a piecewise fashion, with line num- bers added for easy reference. ⁴See Walther (submitted) for further details. With determin- istic automata, the question of how to recover from a wrong string hypothesis during parsing is not an issue. μ μμ Starting with the definition of stems (line 1), we add the three enrichments to the bare phonological string (2). However, the innermost producer-type string constructed by stringToAutomaton (3) is intersected with phonological constraints (5, 6) that need to see the string only, minus its enrichments. This is akin to lexical rule application. 1 stem(FirstSeg, String) := 2 add_repeats(add_skips(add_self_loops( [FirstSeg, stringToAutomaton(String)) 4 & ignore_technical_symbols_in( 5 moraification&mark_first_heavy_syllable 6 & positional_classification)))). 7 := 8 underspecified_for_voicing(BaseSpec) { producer(BaseSpec & vowel), [producer(h), consumer(skip)) }. 9 10 11 12 tahaspin := stem([], \"tahaspin\"). 13 aklatlin := stem(underspecified_for_ voicing(low), \"klatlin\"). 14 Lines 8-10 capture the V/h alternation that is char- acteristic for vowel-initial stems under reduplica- tion, with the vocalic alternant constituting the de- fault used in isolated pronunciation. In contrast, the /h/ alternant is concatenated with a consumer- type skip that requires a producer from elsewhere. Lines 12-14 define two example stems. μ The following constraint (15-18) enriches a prosodically underspecified string with moras abstract units of syllable weight (Hayes, 1995), a prerequisite to locating (20-24) and synchronization-marking (25-31) the first heavy syllable after which the reduplicative infix will be inserted. 15 moraification := 16 (vowel --> (mora / sigma)) & 17 (consonant --> (mora / consonant)) & 18 (consonant --> ((~mora)/vowel)). 19 20 first_(X) := [not_contains(X), X]. 21 heavy_rime := [consumer(mora), 22 consumer(mora)]. 23 heavy_syllable := [consumer(~mora), 24 heavy_rime]. 25 mark_first_heavy_syllable := 26 [first_(heavy_rime)&synced_constituent, 27 synced_constituent]. 28 right_synced := [consumer(~'1'&seg)*, 29 consumer(':1'&seg)]. 30 synced_constituent := 31 [consumer(':1'&seg), right_synced]. 32 positional_classification := 33 [consumer(initial), consumer(medial)*, 34 consumer(final)]. ||| Note that both the constituent before (t:1 aha 8:1) 11 and after (p:1 in:1) the infixation site need to be marked. Also, it turns out to be useful to classify base string positions for easy reference in the redu- plicative morpheme, which motivates lines 32-34. The main part now is the reduplicative morpheme itself (35), which looks like a mixture of Bambara and Semai: the spellout of the base is followed by it- erated repeats (36) to move back to its synchronized initial position (37), which – recall /h/ – is required to be consonantal. The rest of the base is skipped before insertion of the fixed-melody part /o(0)/ oc- curs (38, 42-44). Proceeding with the interrupted realization of the base, we identify its beginning as a synchronized syllable onset (~mora), followed by a right-synchronized string (39-40). 35 punctual_aspect_reduplication := 36 [synced_constituent, producer(repeat)*, 37 consumer(':1' & initial & consonant), 38 producer(skip)*, fixed_melody, 39 consumer(':1' & seg & ~mora), 40 right_synced). 41 42 fixed_melody := 43 [producer(o & ':1' & medial & mora), 44 producer(o & ':1' & medial & mora)^]. ~ Finally, some obvious word_level_con- straints need to be defined (45-54), before the central intersection of Stem and punctual-aspect reduplication (57) completes our Koasati fragment: 45 word_level_constraints := 46 last_segment_is_moraic & 47 last_two_sylls_are_heavy. 48 49 last_segment_is_moraic := 50 [consumer(sigma)*, consumer(mora)]. 51 52 last_two_sylls_are_heavy := 53 [consumer(sigma)*, 54 heavy_syllable, heavy_syllable]. 55 56 wordform(Stem):=closed_interpretation( 57 word_level_constraints & Stem & 58 punctual_aspect_reduplication). The result of wordform({tahaspin, aklatlin}) is shown in figure 3 ([ and ] are aliases for initial and final position). Space precludes the description of a final automa- ton operation called Bounded Local Optimization (Walther, 1999) that turns out to be useful here to a_mora_:0 repeat repeat 3 h:0 5 7 s_mora_:19 repeat (11) 13 a_mora_:0 p:1 25 32 o_mora_:0 (31 30 @n_mom_imora pilo mora: mora_skip 1 [t:1 (29 0 [a_mora_:1 2 1:0 o_mora_:0 15 repeat (17 repeat skip 20 repeat [t:1 22 skip 28 26 skip 24 23 skip t_mora_:0 (21) (h_mora 110 skip 120 mora_0 16 11 18 mora: 21 morarepeat epeat repea k_mora_:1 4 6 8 skip 14 o_mora_:0 skip 19 Figure 3: Koasati reduplications tahas-too-pin, ak-ho(o)-latlin ban unattested free length variation, as found e.g. in ak-ho(o)-latlin where the length of o is yet to be determined. Suffice to say that a parametriza- tion of Bounded Local Optimization would prune the moraic arc 16 → 19 in figure 3 by considering it costlier than the non-moraic arc 16 → 18, thereby eliminating the last source of indeterminacy. 5 Conclusion This paper has presented a novel finite-state method for reduplication that is applicable for both un- bounded total cases, truncated or otherwise phono- logically modified types and infixing instances. The key ingredients of the proposal are suitably en- riched automaton representations, the identification of reduplicative copying with automaton intersec- tion and a resource-conscious interpretation that differentiates between two types of arc symbols, namely producers and consumers of information. After demonstrating the existence of efficient on- demand algorithms to reduplication's central oper- ations, a case study from Koasati has shown that all of the above ingredients may be necessary in the analysis of a single complex piece of prosodic mor- phology. It is worth mentioning that our method can be transferred into a two-level transducer setting with- out major difficulties (Walther, 1999, appendix B). I conclude that the one-level approach to redu- plicative prosodic morphology presents an attractive way of extending finite-state techniques to difficult phenomena that hitherto resisted elegant computa- tional analyses. Acknowledgements The research in this paper has been funded by the German research agency DFG under grant WI 853/4-1. Particular thanks go to the anonymous re- viewers for very useful comments. References Kenneth R. Beesley. 1996. Arabic finite-state mor- phological analysis and generation. In Proceed- ings of COLING-96, volume I, pages 89-94. Kenneth R. Beesley. 1998. Constraining separated morphotactic dependencies in finite-state gram- mars. In Proceedings of FSMNLP'98, Bilkent University, Turkey. Anja Belz. 1998. Discovering phonotactic finite- state automata by genetic search. In Proceedings of COLING-ACL '98, volume II, pages 1472–74. Steven Bird and T. Mark Ellison. 1992. One-Level Phonology: Autosegmental representations and rules as finite-state automata. Technical report, Centre for Cognitive Science, University of Ed- inburgh. EUCCS/RP-51. Steven Bird and T. Mark Ellison. 1994. One- Level Phonology. Computational Linguistics, 20(1):55-90. Chris Culy. 1985. The complexity of the vocab- ulary of Bambara. Linguistics and Philosophy, 8:345-351. T. Mark Ellison. 1992. Machine Learning of Phonological Representations. Ph.D. thesis, University of Western Australia, Perth. Bruce Hayes. 1995. Metrical stress theory: prin- ciples and case studies. University of Chicago Press. Sean Hendricks. 1998. Reduplication without prosodic templates: A case from Semai. Handout from talk given at LSA annual meeting, January 8. Ron Kaplan and Martin Kay. 1994. Regular mod- els of phonological rule systems. Computational Linguistics, 20(3):331-78. Geoffrey Kimball. 1988. Koasati reduplication. In W. Shipley, editor, In honour of Mary Haas: from the Haas Festival Conference on Native American Linguistics, pages 431-42. Mouton de Gruyter, Berlin. George Anton Kiraz. 1994. Multi-tape two-level morphology: a case study in Semitic nonlinear morphology. In Proceedings of COLING '94, volume 1, pages 180-186. Kimmo Koskenniemi. 1983. Two-Level Morphol- ogy: A General Computational Model for Word- Form Recognition and Production. Ph.D. thesis, University of Helsinki. Mehryar Mohri, Fernando Pereira, and Michael Ri- ley. 1998. A rational design for a weighted finite- state transducer library. In D. Wood and S. Yu, editors, Automata Implementation. Second Inter- national Workshop on Implementing Automata, WIA '97, volume 1436 of Lecture Notes in Com- puter Science, pages 144-58. Springer Verlag. Richard Sproat. 1992. Morphology and Computa- tion. MIT Press, Cambridge, Mass. Gertjan van Noord. 1997. FSA Utilities: A toolbox to manipulate finite-state automata. In Darrell Raymond, Derrick Wood, and Sheng Yu, editors, Automata Implementation, volume 1260 of Lecture Notes in Computer Science, pages 87-108. Springer Verlag. (Software under http://grid.let.rug.nl/~vannoord/Fsa/). Markus Walther. 1999. One-Level Prosodic Mor- phology. Marburger Arbeiten zur Linguistik 1, University of Marburg. 64 pp. (http://www.uni-marburg.de/linguistik/mal). Markus Walther. submitted. On finite-state redupli- cation. In COLING-2000."
  },
  {
    "title": "Exploiting auxiliary distributions in stochastic unification-based grammars",
    "abstract": "This paper describes a method for estimating conditional probability distributions over the parses of \"unification-based\" grammars which can utilize auxiliary distributions that are estimated by other means. We show how this can be used to incorporate information about lexical selectional preferences gathered from other sources into Stochastic \"Unification-based\" Grammars (SUBGs). While we apply this estimator to a Stochastic Lexical-Functional Grammar, the method is general, and should be applicable to stochastic versions of HPSGs, categorial grammars and transformational grammars.",
    "content": "1 Introduction \"Unification-based\" Grammars (UBGs) can capture a wide variety of linguistically impor- tant syntactic and semantic constraints. How- ever, because these constraints can be non-local or context-sensitive, developing stochastic ver- sions of UBGs and associated estimation pro- cedures is not as straight-forward as it is for, e.g., PCFGs. Recent work has shown how to define probability distributions over the parses of UBGs (Abney, 1997) and efficiently estimate and use conditional probabilities for parsing (Johnson et al., 1999). Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus. Unfortunately, large parsed UBG corpora are not yet available. This restricts the kinds of models one can realistically expect to be able to estimate. For example, a model incorporat- ing lexical selectional preferences of the kind * This research was supported by NSF awards 9720368, 9870676 and 9812169. described below might have tens or hundreds of thousands of parameters, which one could not reasonably attempt to estimate from a cor- pus with on the order of a thousand clauses. However, statistical models of lexical selec- tional preferences can be estimated from very large corpora based on simpler syntactic struc- tures, e.g., those produced by a shallow parser. While there is undoubtedly disagreement be- tween these simple syntactic structures and the syntactic structures produced by the UBG, one might hope that they are close enough for lexical information gathered from the simpler syntactic structures to be of use in defining a probability distribution over the UBG's structures. In the estimation procedure described here, we call the probability distribution estimated from the larger, simpler corpus an auxiliary dis- tribution. Our treatment of auxiliary distribu- tions is inspired by the treatment of reference distributions in Jelinek's (1997) presentation of Maximum Entropy estimation, but in our es- timation procedure we simply regard the loga- rithm of each auxiliary distribution as another (real-valued) feature. Despite its simplicity, our approach seems to offer several advantages over the reference distribution approach. First, it is straight-forward to utilize several auxiliary distributions simultaneously: each is treated as a distinct feature. Second, each auxiliary dis- tribution is associated with a parameter which scales its contribution to the final distribution. In applications such as ours where the auxiliary distribution may be of questionable relevance to the distribution we are trying to estimate, it seems reasonable to permit the estimation pro- cedure to discount or even ignore the auxiliary distribution. Finally, note that neither Jelinek's nor our estimation procedures require that an auxiliary or reference distribution Q be a prob- ability distribution; i.e., it is not necessary that Q(Ω) = 1, where Ω is the set of well-formed linguistic structures. The rest of this paper is structured as fol- lows. Section 2 reviews how exponential mod- els can be defined over the parses of UBGs, gives a brief description of Stochastic Lexical- Functional Grammar, and reviews why maxi- mum pseudo-likelihood estimation is both feasi- ble and sufficient of parsing purposes. Section 3 presents our new estimator, and shows how it is related to the minimization of the Kullback- Leibler divergence between the conditional es- timated and auxiliary distributions. Section 4 describes the auxiliary distribution used in our experiments, and section 5 presents the results of those experiments. 2 Stochastic Unification-based Grammars Most of the classes of probabilistic language models used in computational linguistic are ex- ponential families. That is, the probability P(w) of a well-formed syntactic structure ω ∈ Ω is de- fined by a function of the form Ρλ(ω) = Q(ω)ελ⋅f(ω) ZA (1) where f (w) ∈ Rm is a vector of feature values, λ∈R™ is a vector of adjustable feature param- eters, Q is a function of w (which Jelinek (1997) calls a reference distribution when it is not an in- dicator function), and Zλ = ∫Ω Q(ω)eλ⋅f(ω) dw is a normalization factor called the partition func- tion. (Note that a feature here is just a real- valued function of a syntactic structure w; to avoid confusion we use the term \"attribute\" to refer to a feature in a feature structure). If Q(w) = 1 then the class of exponential dis- tributions is precisely the class of distributions with maximum entropy satisfying the constraint that the expected values of the features is a cer- tain specified value (e.g., a value estimated from training data), so exponential models are some- times also called \"Maximum Entropy\" models. For example, the class of distributions ob- tained by varying the parameters of a PCFG is an exponential family. In a PCFG each rule or production is associated with a feature, so m is the number of rules and the jth feature value fj(ω) is the number of times the j rule is used in the derivation of the tree ω ∈ Ω. Simple ma- nipulations show that Pλ(ω) is equivalent to the PCFG distribution if λj = log pj, where pj is the rule emission probability, and Q(ω) = Zλ = 1. If the features satisfy suitable Markovian in- dependence constraints, estimation from fully observed training data is straight-forward. For example, because the rule features of a PCFG meet \"context-free\" Markovian independence conditions, the well-known \"relative frequency\" estimator for PCFGs both maximizes the likeli- hood of the training data (and hence is asymp- totically consistent and efficient) and minimizes the Kullback-Leibler divergence between train- ing and estimated distributions. However, the situation changes dramatically if we enforce non-local or context-sensitive con- straints on linguistic structures of the kind that can be expressed by a UBG. As Abney (1997) showed, under these circumstances the relative frequency estimator is in general inconsistent, even if one restricts attention to rule features. Consequently, maximum likelihood estimation is much more complicated, as discussed in sec- tion 2.2. Moreover, while rule features are natu- ral for PCFGs given their context-free indepen- dence properties, there is no particular reason to use only rule features in Stochastic UBGs (SUBGs). Thus an SUBG is a triple (G, f, λ), where G is a UBG which generates a set of well- formed linguistic structures Ω, and f and λ are vectors of feature functions and feature param- eters as above. The probability of a structure ω ∈ Ω is given by (1) with Q(ω) = 1. Given a base UBG, there are usually infinitely many dif- ferent ways of selecting the features f to make a SUBG, and each of these makes an empirical claim about the class of possible distributions of structures. 2.1 Stochastic Lexical Functional Grammar Stochastic Lexical-Functional Grammar (SLFG) is a stochastic extension of Lexical- Functional Grammar (LFG), a UBG formalism developed by Kaplan and Bresnan (1982). Given a base LFG, an SLFG is constructed by defining features which identify salient constructions in a linguistic structure (in LFG this is a c-structure/f-structure pair and its associated mapping; see Kaplan (1995)). Apart from the auxiliary distributions, we based our features on those used in Johnson et al. (1999), which should be consulted for further details. Most of these feature values range over the natural numbers, counting the number of times that a particular construction appears in a linguistic structure. For example, adjunct and argument features count the number of adjunct and argument attachments, permitting SLFG to capture a general argument attachment pref- erence, while more specialized features count the number of attachments to each grammatical function (e.g., SUBJ, OBJ, COMP, etc.). The flexibility of features in stochastic UBGs permits us to include features for relatively complex constructions, such as date expres- sions (it seems that date interpretations, if possible, are usually preferred), right-branching constituent structures (usually preferred) and non-parallel coordinate structures (usually dispreferred). Johnson et al. remark that they would have liked to have included features for lexical selectional preferences. While such fea- tures are perfectly acceptable in a SLFG, they felt that their corpora were so small that the large number of lexical dependency parameters could not be accurately estimated. The present paper proposes a method to address this by using an auxiliary distribution estimated from a corpus large enough to (hopefully) provide reliable estimates for these parameters. 2.2 Estimating stochastic unification-based grammars Suppose ῶ = w1,..., wn is a corpus of n syn- tactic structures. Letting f;(ῶ) = Σn=1 fj(wi) and assuming each wi ∈ Ω, the likelihood of the corpus Lλ(ῶ) is: n Lλ(ῶ) = Π Pλ(wi) i=1 = eλf(ῶ) Zλ−n (2) ∂ log Lλ(ῶ) = fj(ῶ) – nEx(fj) (3) ∂λj where Ex(fj) is the expected value of f; un- der the distribution Pλ. The maximum likeli- hood estimates are the λ which maximize (2), or equivalently, which make (3) zero, but as John- son et al. (1999) explain, there seems to be no practical way of computing these for realistic SUBGs since evaluating (2) and its derivatives (3) involves integrating over all syntactic struc- tures Ω. However, Johnson et al. observe that parsing applications require only the conditional prob- ability distribution Pλ(w|y), where y is the ter- minal string or yield being parsed, and that this can be estimated by maximizing the pseudo- likelihood of the corpus PLλ(ῶ): PLλ(ῶ) = Π Pλ(wi|yi) i=1 n = eλf(ῶ) Π Zλ−1(yi) (4) i=1 In (4), yi is the yield of wi and Zx(yi) = ∫Ω(yi) eλf(w) dw, (4) where Ω(yi) is the set of all syntactic structures in Ω with yield yi (i.e., all parses of yi gener- ated by the base UBG). It turns out that cal- culating the pseudo-likelihood of a corpus only involves integrations over the sets of parses of its yields Ω(yi), which is feasible for many inter- esting UBGs. Moreover, the maximum pseudo- likelihood estimator is asymptotically consistent for the conditional distribution P(w|y). For the reasons explained in Johnson et al. (1999) we ac- tually estimate λ by maximizing a regularized version of the log pseudo-likelihood (5), where σj is 7 times the maximum value of fj found in the training corpus: log PLλ(ῶ) = Σm=1 λj2 −2σj2 (5) See Johnson et al. (1999) for details of the calcu- lation of this quantity and its derivatives, and the conjugate gradient routine used to calcu- late the λ which maximize the regularized log pseudo-likelihood of the training corpus. 3 Auxiliary distributions We modify the estimation problem presented in section 2.2 by assuming that in addition to the corpus ῶ and the m feature functions f we are given k auxiliary distributions Q1,..., Qk whose support includes Ω that we suspect may be re- lated to the joint distribution P(w) or condi- tional distribution P(w|y) that we wish to esti- mate. We do not require that the Q; be proba- bility distributions, i.e., it is not necessary that ∫Ω Qj(ω)dω = 1, but we do require that they are strictly positive (i.e., Qj(ω) > 0, ∀ω ∈ Ω). We define k new features fm+1,..., fm+k where fm+j(ω) = log Qj(ω), which we call auxiliary features. The m + k parameters associated with the resulting m+k features can be estimated us- ing any method for estimating the parameters of an exponential family with real-valued fea- tures (in our experiments we used the pseudo- likelihood estimation procedure reviewed in sec- tion 2.2). Such a procedure estimates parame- ters λm+1,..., λm+k associated with the auxil- iary features, so the estimated distributions take the form (6) (for simplicity we only discuss joint distributions here, but the treatment of condi- tional distributions is parallel). Pλ(ω) = Πk=1 Qj(ω)λm+j Zx e∑mj=1 λjfj(ω) (6) Note that the auxiliary distributions Qj are treated as fixed distributions for the purposes of this estimation, even though each Qj may it- self be a complex model obtained via a previous estimation process. Comparing (6) with (1) on page 2, we see that the two equations become identical if the reference distribution Q in (1) is replaced by a geometric mixture of the auxiliary distributions Qj, i.e., if: Q(ω) = Πkj=1 Qj(ω)λm+j. The parameter associated with an auxiliary fea- ture represents the weight of that feature in the mixture. If a parameter λm+j = 1 then the corresponding auxiliary feature Qj is equivalent to a reference distribution in Jelinek's sense, while if λm+j = 0 then Qj is effectively ig- nored. Thus our approach can be regarded as a smoothed version Jelinek's reference distribu- tion approach, generalized to permit multiple auxiliary distributions. 4 Lexical selectional preferences The auxiliary distribution we used here is based on the probabilistic model of lexical selectional preferences described in Rooth et al. (1999). An existing broad-coverage parser was used to find shallow parses (compared to the LFG parses) for the 117 million word British National Cor- pus (Carroll and Rooth, 1998). We based our auxiliary distribution on 3.7 million (g,r,a) tu- ples (belonging to 600,000 types) we extracted these parses, where g is a lexical governor (for the shallow parses, g is either a verb or a prepo- sition), a is the head of one of its NP arguments and r is the the grammatical relationship be- tween the governor and argument (in the shal- low parses r is always OBJ for prepositional gov- ernors, and r is either SUBJ or OBJ for verbal governors). In order to avoid sparse data problems we smoothed this distribution over tuples as de- scribed in (Rooth et al., 1999). We assume that governor-relation pairs (g,r) and arguments a are independently generated from 25 hidden classes C, i.e.: P((g,r,a)) = Σc∈C Pe((g,r)|c)Pe(a|c)Pe(c) where the distributions Pe are estimated from the training tuples using the Expectation- Maximization algorithm. While the hidden classes are not given any prior interpretation they often cluster semantically coherent pred- icates and arguments, as shown in Figure 1. The smoothing power of a clustering model such as this can be calculated explicitly as the per- centage of possible tuples which are assigned a non-zero probability. For the 25-class model we get a smoothing power of 99%, compared to only 1.7% using the empirical distribution of the training data. 5 Empirical evaluation Hadar Shemtov and Ron Kaplan at Xerox PARC provided us with two LFG parsed corpora called the Verbmobil corpus and the Homecentre cor- pus. These contain parse forests for each sen- tence (packed according to scheme described in Maxwell and Kaplan (1995)), together with a manual annotation as to which parse is cor- rect. The Verbmobil corpus contains 540 sen- tences relating to appointment planning, while the Homecentre corpus contains 980 sentences from Xerox documentation on their \"homecen- tre\" multifunction devices. Xerox did not pro- vide us with the base LFGs for intellectual prop- erty reasons, but from inspection of the parses Class 16 PROB 0.0340 0.3183 say:s 0.0405 say:o 0.0345 ask:s 0.0276 tell:s 0.0214 be:s. 0.0193 know:s 0.0147 have:s 0.0144 nod:s 0.0137 think:s 0.0130 shake:s 0.0128 take:s 0.0104 reply:s 0.0096 smile:s 0.0094 do:s 0.0094 laugh:s 0.0089 tell:0 0.0084 saw:s 0.0082 add:s. 0.0078 feel:s 0.0071 make:s. 0.0070 give:s. 0.0067 ask:o. 0.0066 shrug:s 0.0061 explain:s 0.0051 like:s 0.0050 look:s 0.0050 sigh:s 0.0049 watch:s 0.0049 hear:s 0.0047 answer:s spokesman we people mother police doctor woman father director night someone reporter officer john girl official ruth voice stephen company god chairman no-one who man edward num nobody everyone peter .............................. .. • Figure 1: A depiction of the highest probability predicates and arguments in Class 16. The class matrix shows at the top the 30 most probable nouns in the Pe(a|16) distribution and their probabil- ities, and at the left the 30 most probable verbs and prepositions listed according to Pre((g,r)|16) and their probabilities. Dots in the matrix indicate that the respective pair was seen in the training data. Predicates with suffix: s indicate the subject slot of an intransitive or transitive verb; the suffix: o specifies the nouns in the corresponding row as objects of verbs or prepositions. it seems that slightly different grammars were used with each corpus, so we did not merge the corpora. We chose the features of our SLFG based solely on the basis of the Verbmobil cor- pus, so the Homecentre corpus can be regarded as a held-out evaluation corpus. We discarded the unambiguous sentences in each corpus for both training and testing (as explained in Johnson et al. (1999), pseudo- likelihood estimation ignores unambiguous sen- tences), leaving us with a corpus of 324 am- biguous sentences in the Verbmobil corpus and 481 sentences in the Homecentre corpus; these sentences had a total of 3,245 and 3,169 parses respectively. The (non-auxiliary) features used in were based on those described by Johnson et al. (1999). Different numbers of features were used with the two corpora because some of the features were generated semi- automatically (e.g., we introduced a feature for every attribute-value pair found in any feature structure), and \"pseudo-constant\" features (i.е., features whose values never differ on the parses of the same sentence) are discarded. We used 172 features in the SLFG for the Verbmobil cor- pus and 186 features in the SLFG for the Home- centre corpus. We used three additional auxiliary features derived from the lexical selectional preference model described in section 4. These were de- fined in the following way. For each governing predicate g, grammatical relation r and argu- ment a, let n(g,r,a) (w) be the number of times that the f-structure: [ PRED = g r= [PRED = a] ] appears as a subgraph of the f-structure of w, i.e., the number of times that a fills the grammatical role r of g. We used the lexical model described in the last section to estimate P(alg, r), and defined our first auxiliary feature as: fi(w) = log P(go) + Σn(g,r,a)(w) log P(alg, r) (g,r,a) where go is the predicate of the root feature structure. The justification for this feature is that if f-structures were in fact a tree, fi(w) would be the (logarithm of) a probability dis- tribution over them. The auxiliary feature fi is defective in many ways. Because LFG f- structures are DAGs with reentrancies rather than trees we double count certain arguments, so fi is certainly not the logarithm of a prob- ability distribution (which is why we stressed that our approach does not require an auxiliary distribution to be a distribution). The number of governor-argument tuples found in different parses of the same sentence can vary markedly. Since the conditional prob- abilities P(alg,r) are usually very small, we found that fi(w) was strongly related to the number of tuples found in w, so the parse with the smaller number of tuples usually obtains the higher fi score. We tried to address this by adding two additional features. We set fc(w) to be the number of tuples in w, i.e.: fc(w) = Σn(g,r,a)(ω). (g,r,a) Then we set fn(w) = fi(w)/fc(w), i.e., fn(w) is the average log probability of a lexical depen- dency tuple under the auxiliary lexical distribu- tion. We performed our experiments with fi as the sole auxiliary distribution, and with fi, fc and fn as three auxiliary distributions. Because our corpora were so small, we trained and tested these models using a 10-fold cross- validation paradigm; the cumulative results are shown in Table 1. On each fold we evaluated each model in two ways. The correct parses measure simply counts the number of test sen- tences for which the estimated model assigns its maximum parse probability to the correct parse, with ties broken randomly. The pseudo- likelihood measure is the pseudo-likelihood of test set parses; i.e., the conditional probability of the test parses given their yields. We actu- ally report the negative log of this measure, so a smaller score corresponds to better performance here. The correct parses measure is most closely related to parser performance, but the pseudo- likelihood measure is more closely related to the quantity we are optimizing and may be more relevant to applications where the parser has to return a certainty factor associated with each parse. Table 1 also provides the number of indistin- guishable sentences under each model. A sen- tence y is indistinguishable with respect to fea- tures f iff f(wc) = f(w'), where wc is the correct parse of y and wc ≠ w' ∈ Ω(y), i.e., the feature values of correct parse of y are identical to the feature values of some other parse of y. If a sentence is indistinguishable it is not possible to assign its correct parse a (conditional) prob- ability higher than the (conditional) probability assigned to other parses, so all else being equal we would expect a SUBG with with fewer indis- tinguishable sentences to perform better than one with more. Adding auxiliary features reduced the already low number of indistinguishable sentences in the Verbmobil corpus by only 11%, while it reduced the number of indistinguishable sentences in the Homecentre corpus by 24%. This probably re- flects the fact that the feature set was designed by inspecting only the Verbmobil corpus. We must admit disappointment with these results. Adding auxiliary lexical features im- proves the correct parses measure only slightly, and degrades rather than improves performance on the pseudo-likelihood measure. Perhaps this is due to the fact that adding auxiliary features increases the dimensionality of the feature vec- tor f, so the pseudo-likelihood scores with dif- ferent numbers of features are not strictly com- parable. The small improvement in the correct parses measure is typical of the improvement we might expect to achieve by adding a \"good\" non- auxiliary feature, but given the importance usu- ally placed on lexical dependencies in statistical models one might have expected more improve- ment. Probably the poor performance is due in part to the fairly large differences between the parses from which the lexical dependencies were estimated and the parses produced by the LFG. LFG parses are very detailed, and many ambiguities depend on the precise grammatical Verbmobil corpus (324 sentences, 172 non-auxiliary features) Auxiliary features used Indistinguishable Correct - log PL (none) 9 180 401.3 fi 8 183 401.6 ft, fc, fn 8 180.5 404.0 Homecentre corpus (481 sentences, 186 non-auxiliary features) Auxiliary features used Indistinguishable Correct - log PL (none) 45 283.25 580.6 fi 34 284 580.6 ft, fc, fn 34 285 582.2 Table 1: The effect of adding auxiliary lexical dependency features to a SLFG. The auxiliary features are described in the text. The column labelled \"indistinguishable\" gives the number of indistinguishable sentences with respect to each feature set, while \"correct\" and \"- log PL\" give the correct parses and pseudo-likelihood measures respectively. relationship holding between a predicate and its argument. It could also be that better perfor- mance could be achieved if the lexical dependen- cies were estimated from a corpus more closely related to the actual test corpus. For example, the verb feed in the Homecentre corpus is used in the sense of \"insert (paper into printer)\", which hardly seems to be a prototypical usage. Note that overall system performance is quite good; taking the unambiguous sentences into account the combined LFG parser and statisti- cal model finds the correct parse for 73% of the Verbmobil test sentences and 80% of the Home- centre test sentences. On just the ambiguous sentences, our system selects the correct parse for 56% of the Verbmobil test sentences and 59% of the Homecentre test sentences. 6 Conclusion This paper has presented a method for incorpo- rating auxiliary distributional information gath- ered by other means possibly from other corpora into a Stochastic \"Unification-based\" Grammar (SUBG). This permits one to incorporate de- pendencies into a SUBG which probably can- not be estimated directly from the small UBG parsed corpora available today. It has the virtue that it can incorporate several auxiliary dis- tributions simultaneously, and because it asso- ciates each auxiliary distribution with its own \"weight\" parameter, it can scale the contribu- tions of each auxiliary distribution toward the final estimated distribution, or even ignore it entirely. We have applied this to incorporate lexical selectional preference information into a Stochastic Lexical-Functional Grammar, but the technique generalizes to stochastic versions of HPSGs, categorial grammars and transfor- mational grammars. An obvious extension of this work, which we hope will be persued in the future, is to apply these techniques in broad- coverage feature-based TAG parsers. References Steven P. Abney. 1997. Stochastic Attribute- Value Grammars. Computational Linguis- tics, 23(4):597-617. Glenn Carroll and Mats Rooth. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of EMNLP-3, Granada. Frederick Jelinek. 1997. Statistical Methods for Speech Recognition. The MIT Press, Cam- bridge, Massachusetts. Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estima- tors for stochastic \"unification-based\" gram- mars. In The Proceedings of the 37th Annual Conference of the Association for Computa- tional Linguistics, pages 535-541, San Fran- cisco. Morgan Kaufmann. Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-Functional Grammar: A formal sys- tem for grammatical representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, chapter 4, pages 173-281. The MIT Press. Ronald M. Kaplan. 1995. The formal architec- ture of LFG. In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors, Formal Issues in Lexical- Functional Grammar, number 47 in CSLI Lecture Notes Series, chapter 1, pages 7-28. CSLI Publications. John T. Maxwell III and Ronald M. Kaplan. 1995. A method for disjunctive constraint satisfaction. In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell III, and Annie Zaenen, editors, Formal Issues in Lexical- Functional Grammar, number 47 in CSLI Lecture Notes Series, chapter 14, pages 381- 481. CSLI Publications. Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll,, and Franz Beil. 1999. Induc- ing a semantically annotated lexicon via EM- based clustering. In Proceedings of the 37th Annual Meeting of the Association for Com- putational Linguistics, San Francisco. Mor- gan Kaufmann."
  },
  {
    "title": "The Effectiveness of Corpus-Induced Dependency Grammars for Post-processing Speech",
    "abstract": "This paper investigates the impact of Constraint Dependency Grammars (CDG) on the accuracy of an integrated speech recognition and CDG parsing system. We compare a conventional CDG with CDGs that are induced from annotated sentences and template-expanded sentences. The grammars are evaluated on parsing speed, precision/coverage, and improvement of word and sentence accuracy of the integrated system. Sentence-derived CDGs significantly improve recognition accuracy over the conventional CDG but are less general. Expanding the sentences with templates provides us with a mechanism for increasing the coverage of the grammar with only minor reductions in recognition accuracy.",
    "content": "1 Background The question of when and how to integrate language models with speech recognition systems is gaining in importance as recognition tasks investigated by the speech community become increasingly more chal- lenging and as speech recognizers are used in hu- man/computer interfaces and dialog systems (Block, 1997; Pieraccini and Levin, 1992; Schmid, 1994; Wright et al., 1994; Zue et al., 1991). Many sys- tems tightly integrate N-gram stochastic language models, with a power limited to a regular grammar, into the recognizer (Jeanrenaud et al., 1995; Ney et al., 1994; Placeway et al., 1993) to build more ac- curate speech recognizers. However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal repre- sentation. Obtaining a syntactic representation for the spoken utterance has a high degree of utility for mapping to a semantic representation. Without a structural analysis of an input, it is difficult to guar- antee the correctness of the mapping from a sentence to its interpretation (e.g., mathematical expressions to internal calculations). We believe that significant additional improvement in accuracy can be gained in specific domains by using a more complex lan- * This research was supported by grants from Intel, Purdue Research Foundation, and National Science Foundation IRI 97-04358. CDA 96-17388, and #9980054-BCS. guage model that combines syntactic, semantic, and domain knowledge. A language processing module that is more pow- erful than a regular grammar can be loosely, mod- erately, or tightly integrated with the spoken lan- guage system, and there are advantages and dis- advantages associated with each choice (Harper et al., 1994). To tightly integrate a language model with the power of a context-free grammar with the acoustic module requires that the power of the two modules be matched, making the integrated system fairly intractable and difficult to train. By separat- ing the language model from the acoustic model, it becomes possible to use a more powerful language model without increasing computational costs or the amount of acoustic training data required by the rec- ognizer. Furthermore, a loosely-integrated language model can be developed independently of the speech recognition component, which is clearly an advan- tage. Decoupling the acoustic and language mod- els also adds flexibility: a wide variety of language models can be tried with a single acoustic model. Systems that utilize a language model that operates as a post-processor to a speech recognizer include (Block, 1997; Seneff, 1992; Zue et al., 1991). The goal of this research is to construct and ex- perimentally evaluate a prototype of a spoken lan- guage system that loosely integrates a speech recog- nition component with an NLP component that uses syntactic, semantic, and domain-specific knowledge to more accurately select the sentence uttered by a speaker. First we describe the system we have built. Then we describe the mechanism used to rapidly de- velop a domain-specific grammar that improves ac- curacy of our speech recognizer. 2 Our System We have developed the prototype spoken language system depicted in Figure 1 that integrates a speech recognition component based on HMMs with a pow- erful grammar model based on Constraint Depen- dency Grammar (CDG). The speech recognizer is implemented as a multiple-mixture triphone HMM with a simple integrated word co-occurrence gram- mar (Ent, 1997; Young et al., 1997). Mel-scale cep- stral coefficients, energy, and each of their their first and second order differences are used as the under- lying feature vector for each speech frame. Model training is done using standard Baum-Welch Max- imum Likelihood parameter re-estimation on diag- onal covariance Gaussian Mixture Model (GMM) feature distributions. The speech recognizer em- ploys a token-passing version of the Viterbi algo- rithm (Young et al., 1989) and pruning settings to produce a pruned recognition lattice. This pruned lattice contains the most likely alternative sentences that account for the sounds present in an utterance as well as their probabilities. Without any loss of in- formation, this lattice is then compressed into a word graph (Harper et al., 1999b; Johnson and Harper, 1999), which acts as the interface between the rec- ognizer and the CDG parser. The word graph algo- rithm begins with the recognition lattice and elim- inates identical subgraphs by iteratively combining word nodes that have exactly the same preceding or following nodes (as well as edge probabilities), pushing excess probability to adjacent nodes when- ever possible. The resulting word graph represents all possible word-level paths without eliminating or adding any paths or modifying their probabilities. Word graphs increase the bandwidth of useful acous- tic information passed from the HMM to the CDG parser compared to most current speech recognition systems. The CDG parser parses the word graph to identify the best sentence consistent with both the acoustics of the utterance and its own additional knowledge. The loose coupling of the parser with the HMM allows us to construct a more powerful combined system without increasing the amount of training data for the HMM or the computational complex- ity of either of the component modules. Our NLP component is implemented using a CDG parser (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b) because of its power and flexibil- ity, in particular: • It supports the use of syntactic, semantic, and domain-specific knowledge in a uniform frame- work. • Our CDG parser supports efficient simultaneous parsing of alternative sentence hypotheses in a word graph (Harper and Helzerman, 1995; Helz- erman and Harper, 1996). • Because CDG is a dependency grammar, it can better model free-order languages. Hence, CDG can be used in processing a wider variety of human languages than other grammar paradigms. • It is capable of representing and using context- dependent information unlike traditional gram- mar approaches, thus providing a finer degree of control over the syntactic analysis of a sentence. • A CDG can be extracted directly from sentences annotated with dependency information (i.e., fea- ture and syntactic relationships). We hypothesize that the accuracy of the combined HMM/CDG system should benefit from the ability to create a grammar that covers the domain as pre- cisely as possible and that does not consider sen- tences that would not make sense given the domain. A corpus-based grammar is likely to have this degree of control. In the next section we describe how we construct a CDG from corpora. Dictionary Syntax Speech Signal HMM Speech Recognizer Word Graph CDG-based NL Processor Semantics Domain Parsed Word Graph Hypothesis Selection Top Sentence Figure 1: Block diagram of the loosely-coupled spo- ken language system. 3 Learning CDG Rules In this section, we introduce CDG and then describe how CDG constraints can be learned from sentences annotated with grammatical information. 3.1 Introduction to CDG Constraint Dependency Grammar (CDG), first introduced by Maruyama (Maruyama, 1990a; Maruyama, 1990b), uses constraints to determine the grammatical dependencies for a sentence. The parsing algorithm is framed as a constraint satis- faction problem: the rules are the constraints and the solutions are the parses. A CDG is defined as a five-tuple, (Σ, R, L, C,T), where Σ = {σ1, ..., σc} is a finite set of lexical categories (e.g., determiner), R = {r1, ..., rp} is a finite set of uniquely named roles or role ids (e.g., governor, need1, need2), L = {l1, ..., lq} is a finite set of labels (e.g., subject), C is a constraint formula, and T is a table that specifies allowable category-role-label combinations. A sentence s = w1w2w3...wn has length n and is an element of Σ*. For each word wi ∈ Σ of a sentence s, there are up to p different roles (with most words needing only one or two (Harper et al., 1999a)), yielding a maximum of n * p roles for the entire sentence. A role is a variable that is assigned a role value, an element of the set L × {1, 2, ..., n}. Role values are denoted as l-m, where l ∈ L and m ∈ {1, 2, ..., n} is called the modifiee. Maruyama originally used a modifiee of NIL to indicate that a role value does not require a modifiee, but it is more parsimonious to indicate that there is no dependent by setting the modifiee to the position of its word. Role values are assigned to roles to record the syn- tactic dependencies between words in the sentence. The governor role is assigned role values such that the modifiee of the word indicates the position of the word's governor or head (e.g., DET-3, when assigned to the governor role of a determiner, indicates its function and the position of its head). Every word in a sentence has a governor role. Need roles are used to ensure the requirements of a word are met. For example, an object is required by a verb that subcategorizes for one, unless it has passive voice. The required object is accounted for by requiring the verb's need role to be assigned a role value with a modifiee that points at the object. Words can have more than one need role, depending on the lex- ical category of the word. The table T indicates the roles that a word with a particular lexical category must support. A sentence s is said to be generated by the gram- mar G if there exists an assignment A that maps a role value to each of the roles for s such that C is satisfied. There may be more than one assignment of role values to the roles of a sentence that satisfies C, in which case there is ambiguity. C is a first- order predicate calculus formula over all roles that requires that an assignment of role values to roles be consistent with the formula; those role values incon- sistent with C can be eliminated. A subformula Pi of C is a predicate involving =, <, or >, or predi- cates joined by the logical connectives and, or, if, or not. A subformula is a unary constraint if it con- tains only a single variable (by convention, we use x1) and a binary constraint if it contains two vari- ables (by convention x1 and x2). An example of a unary and binary constraint appears in Figure 2. A CDG has an arity parameter a, which indicates the maximum number of variables in the subformu- las of C, and a degree parameter d, which is the number of roles in the grammar. An arity of two suffices to represent a grammar at least as power- ful as a context-free grammar (Maruyama, 1990a; Maruyama, 1990b). In (Harper et al., 1999a), we developed a way to write constraints concerning the category and feature values of a modifiee of a role value (or role value pair). These constraints loosely capture binary constraint information in unary con- straints (or beyond binary for binary constraints) and results in more efficient parsing. A unary constraint requiring that a role value assigned to the governor role of a determiner have the label DET and a modifice pointing to a subsequent word. (if (and (= (category ×1) determiner) (= (rid x1) G)) (and (= (label x1) DET) (> (mod x1) (pos x1)))) A binary constraint requiring that a role value with the label S assigned to a needt role of one word point at another word whose governor role is assigned a role value with the label SUBJ and a modiflee that points back at the first word. (if (and (= (label x1) S) (= (rid x1) N1) (= (mod x1) (pos x2)) (= (rid x2) G)) (and (= (label x2) SUBJ) (= (mod x2) (pos x1)))) Figure 2: A Unary and binary constraint for CDG. The white box in Figure 3 depicts a parse for the sentence Clear the screen from the Resource Management corpus (Price et al., 1988) (the ARV and ARVP in the gray box will be discussed later), which is a corpus we will use to evaluate our speech processing system. We have constructed a conven- tional CDG with around 1,500 unary and binary constraints (i.e., its arity is 2) that were designed to parse the sentences in the corpus. This CDG covers a wide variety of grammar constructs (includ- ing conjunctions and wh-movement) and has a fairly rich semantics. It uses 16 lexical categories, 4 roles (so its degree is 4), 24 labels, and 13 lexical fea- ture types (subcat, agr, case, vtype (e.g., progres- sive), mood, gap, inverted, voice, behavior (e.g., mass), type (e.g., interrogative, relative), semtype, takesdet, and conjtype). The parse in Figure 3 is an assignment of role values to roles that is consis- tent with the unary and binary constraints. A role value, when assigned to a role, has access to not only the label and modifiee of its role value, but also the role name of the role to which it is assigned, informa- tion specific to the word (i.e., the word's position in the sentence, its lexical category, and feature values for each feature), and information about the lexical class and feature values of its modifiee. Our unary and binary constraints use this information to elim- inate ungrammatical assignments. Parse for \"Clear the screen\" 1 Clear verb subcat=obj vtype=inf voice=active inverted=no gap=none mood=command semtype=erase agr=none G=root-1 N1=S-1 N2=S-3 N3=S-1 2 the determiner type=definite subcat=count3s 3 screen noun case=common behav=count type=none semtype=display agr=3s G=obj-1 N3=detper-2 ARV for det-3 assigned to G for the: cat1=determiner, type1=definite, subcat1=count3s, rid1=G, label1=det, (< (pos x1) (mod x1)) ARVP for det-3 assigned to G for the and obj-1 assigned to G for screen: cat1=determiner, type1=definite, subcat1=count3s, rid1=G, label1=det, (< (pos x1) (mod x1)), cat2=noun, case2=common, behav2=count, type2=none, semtype2=display, agr2=3s, rid2=G, label2=obj, (< (mod x2) (pos x2)). (< (pos x1) (pos x2)), (< (mod x2) (mod x1)), (< (mod x2) (pos x1)), (= (mod x1) (pos x2)) Figure 3: A CDG parse (see white box) is repre- sented by the assignment of role values to roles as- sociated with a word with a specific lexical category and one feature value per feature. ARVs and ARVPs (see gray box) represent grammatical relations that can be extracted from a sentence's parse. 3.2 Learning CDG Constraints The grammaticality of a sentence in a language de- fined by a CDG was originally determined by apply- ing the constraints of the grammar to the possible role value assignments. If the set of all possible role values assigned to the roles of a sentence of length n is denoted S₁ = 2 × R × POS × L × MOD × F₁ × × Fk, where k is the number of feature types, Fi represents the set of feature values for that type, POS = {1, 2, ..., n} is the set of possible positions, MOD = {1,2,..., n} is the set of possible modi- fiees, and n is sentence length (which can be any arbitrary natural number), then unary constraints partition S₁ into grammatical and ungrammatical role values. Similarly, binary constraints partition the set S2 = S1 × S₁ = Si into compatible and in- compatible pairs. Building upon this concept of role value partitioning, it is possible to construct another way of representing unary and binary constraints because CDG constraints do not need to reference the exact position of a word or a modifiee in the sentence to parse sentences (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b; Menzel, 1994; Menzel, 1995). To represent the relative, rather than the abso- lute, position information for the role values in a grammatical sentence, it is only necessary to repre- sent the positional relations between the modifiees and the positions of the role values. To support an arity of 2, these relations involve either equality or less-than relations over the modifiees and positions of role values assigned to the roles 21 and 22. Since unary constraints operate over role values assigned to a single role, the only relative position relations that can be tested are between the role value's posi- tion (denoted as P21) and its modifiee (denoted as M21); one and only one of the following three re- lations must be true: (Pr₁ < Mx1), (Mx1 < Px1), or (Px1 = M21). Since binary constraints operate over role values assigned to pairs of roles, 21 and 22, the only possible relative position relations that can be tested are between Pr₁ and Mx1, Px2 and Mx2, P21 and Mx2, P22 and Mx1, P21 and Px2, M21 and M22. Note that each of the six has three positional relations (as in the case of unary relations on P21 and M21) such that one and only one of them is simultaneously true. The unary and binary positional relations provide the necessary mechanism to develop an alternative view of the unary and binary constraints. First, we develop the concept of an abstract role value (ARV), which is a finite characterization of all possible role values using relative, rather than absolute, position relations. Formally, an ARV for a particular gram- mar G = (Σ, R, L, C, T, F1, ..., L, C, T, F1, ..., Fr) is an element of the set: A₁ = E×R×L×F₁x...xFxUC, where UC encodes the three possible positional relations be- tween Px₁ and Mx1. The gray box of Figure 3 shows an example of an ARV obtained from the parsed sen- tence. Note that A₁ is a finite set representing the space of all possible ARVs for the grammar¹; hence, the set provides an alternative characterization of the unary constraints for the grammar, which can be partitioned into positive (grammatical) and neg- ative (ungrammatical) ARVs. During parsing, if a role value does not match one of the elements in the positive ARV space, then it should be disallowed. Positive ARVs can be obtained directly from the parses of sentences: for each role value in a parse for a sentence, simply extract its category, feature, role, and label information, and then determine the po- sitional relation that holds between the role value's position and modifiee. Similarly the set of legal abstract role value pairs (ARVPS), A2 = E×R×L× F₁×...× F××R×L× F₁x...x Fx BC, where BC encodes the positional relations among Px1, MX1, Px2, and Mx2, provides an alternative definition for the binary constraints2. The gray box of Figure 3 shows an example of an ARVP obtained from the parsed sentence. Positive ARVPs can be obtained directly from the parses of sentences. For each pair of role values assigned to different roles, simply extract their category, feature, role, and label information, and then determine the positional relations that hold between the positions and modifiees. An enumeration of the positive ARV/ARVPs can be used to represent the CDG constraints, C, and ARV/ARVPs are PAC-learnable from positive ex- amples, as can be shown using the techniques of (Natarajan, 1989; Valiant, 1984). ARV/ARVP con- straints can be enforced by using a fast table lookup method to see if a role value (or role value pair) is allowed (rather than propagating thousands of con- straints), thus speeding up the parser. 4 Evaluation Using the Naval Resource Management Domain An experiment was conducted to determine the plausibility and the benefits of extracting CDG con- straints from a domain-specific corpus of sentences. For our speech application, the ideal CDG should be general enough to cover sentences similar to those that appear in the corpus while being restrictive enough to eliminate sentences that are implausible given the observed sentences. Hence, we investigate whether a grammar extracted from annotated sen- tences in a corpus achieves this precision of cover- age. We also examine whether a learned grammar has the ability to filter out incorrect sentence hy- potheses produced by the HMM component of our system in Figure 1. To investigate these issues, we have performed an experiment using the standard 1A1 can also include information about the possible lexical categories and feature values of the modifiee of 21. 2 A2 can also include information about the possible lexical categories and feature values of the modifiees of 21 and 22. Resource Management (RM) (Price et al., 1988) and Extended Resource Management (RM2) ((DARPA), 1990) corpora. These mid-size speech corpora have a vocabulary of 991 words and contain utterances of sentences derived from sentence templates based on interviews with naval personnel familiar with naval resource management tasks. They were chosen for several reasons: they are two existing speech corpora from the same domain; their manageable sizes make them a good platform for the development of tech- niques that require extensive experimentation; and the sentences have both syntactic variety and rea- sonably rich semantics. RM contains 5,190 separate utterances (3,990 testing, 1,200 training) of 2,845 distinct sentences (2,245 training, 600 testing). We have extracted several types of CDGs from annota- tions of the RM sentences and tested their generality using the 7,396 sentences in RM2 (out of the 8,173) that are in the resource management domain but are distinct from the RM sentences. We compare these CDGs to each other and to the conventional CDG described previously. The corpus-based CDGs were created by extract- ing the allowable grammar relationships from the RM sentences that were annotated by language ex- perts using the SENATOR annotation tool, a CGI (Common Gateway Interace) HTML script written in GNU C++ version 2.8.1 (White, 2000). We tested two major CDG variations: those derived di- rectly from the RM sentences (Sentence CDGs) and those derived from simple template-expanded RM sentences (Template CDGs). For example, \"List MIDPAC's deployments during (date)\" is a sentence containing a date template which allows any date representations. For these experiments, we focused on templates for dates, years, times, numbers, and latitude and longitude coordinates. Each template name identifies a sub-grammar which was produced by annotating the appropriate strings. We then an- notated sentences containing the template names as if they were regular sentences. Approximately 25% of the 2,845 RM sentences were expanded with one or more templates. Although annotating a corpus of sentences can be a labor intensive task, we used an iterative approach that is based on parsing using grammars with vary- ing degrees of restrictiveness. A grammar can be made less restrictive by ignoring: • lexical information associated with a role value's modifiee in the ARVPs, • feature information of two role values in an ARVP not directly related based on their modifiee rela- tions, • syntactic information provided by two role values that are not directly related, • specific feature information (e.g., semantics or subcategorization). Initially, we bootstrapped the grammar by annotat- ing a 200 sentence subset of the RM corpus and ex- tracting a fairly general grammar from the annota- tions. Then using increasingly restrictive grammars at each iteration, we used the current grammar to identify sentences that required annotation and ver- ified the parse information for sentences that suc- ceeded. This iterative technique reduced the time required to build a CDG from about one year for the conventional CDG to around two months (White, 2000). Several methods of extracting an ARV/ARVP grammar from sentences or template-extended sen- tences were investigated. The ARVPs are extracted differently for each method; whereas, the ARVs are extracted in the same manner regardless of the method. Recall that ARVs represent the set of ob- served role value assignments. In our implementa- tion, each ARV includes: the label of the role value, the role to which the role value was assigned, the lexical category and feature values of the word con- taining the role, the relative position of the word and the role value's modifiee, and the modifiee's lexical category and feature values (modifiee constraints). We use modifiee constraints for ARVs regardless of extraction method because their use does not change the coverage of the extracted grammar and not using the information would significantly slow the parser (Harper et al., 1999a). Because the ARVP space is larger than the ARV space, we investigate six varia- tions for extracting the pairs: 1. Full Mod: contains all grammar and feature value information for all pairs of role values from annotated sentences, as well as modifiee con- straints. For a role value pair in a sentence to be considered valid during parsing with this gram- mar, it must match an ARVP extracted from the annotated sentences. 2. Full: like Full Mod except it does not impose modifiee constraints on a pair of role values during parsing. 3. Feature Mod: contains all grammar relations between all pairs of role values, but it consid- ers feature and modifiee constraints only for pairs that are directly related by a modifiee link. Dur- ing parsing, if a role value pair is related by a modifiee link, then a corresponding ARVP with full feature and modifiee information must appear in the grammar for it to be allowed. If the pair is not directly related, then an ARVP must be stored for the grammar relations, ignoring feature and modifiee constraint information. 4. Feature: like Feature Mod except it does not impose modifiee constraints on a pair of role val- ues during parsing. 5. Direct Mod: stores only the grammar, feature, and modifiee information for those pairs of role Table 1: Number of ARVs and ARVPs extracted for each RM grammar. ARVP Sentence Template Percent Variation CDG CDG Increase Full Mod 270,034 408,912 51.43% Full 165,480 200,792 21.34% Feature Mod 49,468 56,758 14.74% Feature 36,558 40,308 10.26% Direct Mod 41,124 47,004 14.30% Direct 28,214 30,554 8.29% ARVs 4,424 4,648 5.06% values that are directly related by a modifiee link. During parsing, if a role value pair is related by such a link, then a corresponding ARVP must ap- pear in the grammar for it to be allowed. Any pair of role values not related by a modifiee link is allowed (an open-world assumption). 6. Direct: like Direct Mod except it does not im- pose modifiee constraints on a pair of role values during parsing. Grammar sizes for these six grammars, extracted either directly from the 2,845 sentences or from the 2,845 sentences expanded with our sub-grammar templates, appear in Table 1. The largest gram- mars were derived using the Full Mod extrac- tion method, with a fairly dramatic growth result- ing from processing template-expanded sentences. The Feature and Direct variations are more man- ageable in size, even those derived from template- expanded sentences. Size is not the only important consideration for a grammar. Other important issues are grammar generality and the impact of the grammar on the accuracy of selecting the correct sentence from the recognition lattice of a spoken utterance. After extracting the CDG grammars from the RM sen- tences and template-expanded sentences, we tested the generality of the extracted grammars by using each grammar to parse the 7,396 RM2 sentences. See the results in Table 2. The grammar with the greatest generality was the conventional CDG for the RM corpus; however, this grammar also has the unfortunate attribute of being quite ambigu- ous. The most generalizable of extracted grammars uses the Direct method on template-expanded sen- tences. In all cases, the template-expanded sen- tence grammars gave better coverage than their cor- responding sentence-only grammars. We have also used the extracted grammars to post-process word graphs created by the word graph compression algorithm of (Johnson and Harper, 1999) for the test utterances in the RM corpus. As was reported in (Johnson and Harper, 1999), the word-error rate of our HMM recognizer with an em- bedded word pair language model on the RM test set of 1200 utterances was 5.0%, the 1-best sentence ac- curacy was 72.1%, and the word graph coverage ac- curacy was 95.1%. Also, the average uncompressed word graph size was 75.15 nodes, and our compres- sion algorithm resulted in a average word graph size of 28.62 word nodes. When parsing the word graph, the probability associated with a word node can ei- ther represent its acoustic score or a combination of its acoustic and stochastic grammar score. We use the acoustic score because (Johnson and Harper, 1999) showed that by using a word node's acoustic score alone when extracting the top sentence candi- date after parsing gave a 4% higher sentence accu- racy. For the parsing experiments, we processed the 1,080 word graphs produced for the RM test set that contained 50 or fewer word nodes after com- pression (out of 1,200 total) in order to efficiently compare the 12 ARV/ARVP CDG grammars and the conventional CDG (the larger word graphs re- quire significant time and space to parse using the conventional CDG). These 1,080 word graphs con- tain 24.95 word nodes on average with a standard deviation (SD) of 10.80, and result in 1-best sen- tence accuracy was 75% before parsing. The num- ber of role values prior to binary constraint propa- gation differ across the grammars with an average (and SD) for the conventional grammar of 504.99 (442.00), for the sentence-only grammars of 133.37 (119.48), and for the template-expanded grammars of 157.87 (145.16). Table 3 shows the word graph parsing speed and the path, node, and role value (RV) ambiguity after parsing; Table 4 shows the sentence accuracy and the accuracy and percent cor- rect for words. Note that percent correct words is calculated using $\\frac{N-D-S}{N}$ and word accuracy using $\\frac{N-D-S-I}{N}$, where N is the number of words, D is the number of deletions, S is the number of substi- tutions, and I is the number of insertions. The most selective RM sentence grammar, Full Mod, achieves the highest sentence accuracy, but at a cost of a greater average parsing time than the other RM sentence grammars. Higher accu- Table 2: Number of successfully parsed sentences in RM2 using the conventional CDG and CDGs derived from sentences only or template-expanded sentences. ARVP # Parsed with Sentence CDG # Parsed with Template CDG Variation Full Mod 3,735 (50.50%) 4,461 (60.32%) Full 4,509 (60.97%) 5,316 (71.88%) Feature Mod 5,365 (72.54%) 5,927 (80.14%) Feature 5,772 (78.04%) 6,208 (83.94%) Direct Mod 5,464 (73.88%) 5,979 (80.84%) Direct 5,931 (80.19%) 6,275 (84.82%) Conventional 7,144 (96.59%) not applicable ARVP Variation Parse Time (sec.) No. Paths No. Nodes No. RVs Full Mod 33.89 (41.12) 2.21 (1.74) 10.59 (3.44) 19.51 (8.32) Template Full Mod 41.85 (51.75) 2.78 (3.75) 10.76 (3.64) 19.93 (8.76) Full 29.73 (36.68) 2.83 (2.92) 10.87 (3.54) 20.32 (8.86) Template Full 36.80 (46.90) 3.40 (5.19) 11.03 (3.74) 20.77 (9.47) Feature Mod 11.46 (14.46) 3.9 (5.97) 11.20 (3.94) 21.43 (10.49) Template Feature Mod 13.80 (18.47) 4.22 (6.93) 11.28 (4.06) 21.81 (11.17) Feature 11.60 (14.97) 5.19 (8.36) 11.72 (4.22) 23.41 (12.72) Template Feature 14.24 (19.63) 6.86 (14.83) 11.94 (4.52) 24.47 (14.41) Direct Mod 13.93 (19.73) 4.25 (6.49) 11.46 (4.27) 22.79 (13.44) Template Direct Mod 17.28 (26.56) 4.62 (8.61) 11.45 (4.28) 22.95 (13.34) Direct 19.95 (36.89) 8.08 (18.52) 12.81 (5.73) 32.85 (34.65) Template Direct 28.02 (69.50) 9.98 (25.52) 12.95 (5.95) 33.36 (35.66) Coventional 83.48 (167.51) 51.33 (132.43) 17.14 (8.02) 77.19 (76.26) Table 3: Average parse times (SD), number of paths (SD), number of nodes (SD), and number of role values (SD) remaining after parsing the 1,080 word graphs of 50 or fewer word nodes produced for the RM test set using the 13 CDGs. Sentence Accuracy % Correct Words ARVP Variation Word Accuracy Full Mod 91.94% 98.55% 98.19% Template Full Mod 91.57% 98.50% 98.14% Full 91.57% 98.49% 98.11% Template Full 91.20% 98.45% 98.05% Feature Mod 90.56% 98.38% 97.95% Template Feature Mod 90.19% 98.34% 97.90% Feature 90.28% 98.35% 97.91% Template Feature 89.91% 98.29% 97.85% Direct Mod 90.46% 98.37% 97.91% Template Direct Mod 90.09% 98.32% 97.86% Direct 89.91% 98.30% 97.82% Template Direct 89.44% 98.25% 97.75% Conventional 81.20% 97.11% 96.10% Table 4: The sentence accuracy, percent correct words, and word accuracy from parsing 1,080 word graphs of 50 or fewer word nodes produced for the RM test set using the 13 CDGs. racy appears to be correlated with the ability of the constraints to eliminate word nodes from the word graph during parsing. The least restrictive sentence grammar, Direct, is less accurate than the other sentence grammars and offers an intermediate speed of parsing, most likely due to the increased ambigu- ity in the parsing space. The fastest grammar was the Feature-Mod grammar, which also offers an intermediate level of accuracy. Its size (even with templates), restrictiveness, and speed make it very attractive. The template versions of each grammar showed a slight increase in average parse times (from processing a larger number of role values) and a slight decrease in parsing accuracy. The conven- tional grammar was the least competitive of the grammars both in speed and in accuracy. 5 Conclusion and Future Directions The ability to extract ARV/ARVP grammars with varying degrees of specificity provides us with the ability to rapidly develop a grammar with the abil- ity to improve sentence accuracy of our speech sys- tem. To achieve balance between precision and cov- erage of our corpus-induced grammars, we have ex- panded the RM sentences with templates for expres- sions like dates and times. The grammars extracted from these expanded sentences gave increased RM2 coverage without sacrificing even 1% of the sentence accuracy. We are currently expanding the number of templates in our grammar in an attempt to obtain full coverage of the RM2 corpus using only template- expanded RM sentences. We have recently added ten semantic templates to the grammar and have improved the coverage by 9.19% without losing any sentence accuracy. We are also developing a stochas- tic version of CDG that uses a statistical ARV, which is similar to a supertag (Srinivas, 1996). References H. U. Block. 1997. Language components in VERB- MOBIL. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 79-82. Defense Advanced Research Projects Agency (DARPA). 1990. Extended resource manage- ment: Continuous speech speaker-dependent corpus (RM2). CD-ROM. NIST Speech Discs 3-1.2 and 3-2.2. Entropic Cambridge Research Laboratory, Ltd., 1997. HTK: Hidden Markov Model Toolkit V2.1. M. P. Harper and R. A. Helzerman. 1995. Exten- sions to constraint dependency parsing for spoken language processing. Computer Speech and Lan- guage, 9:187-234. M. P. Harper, L. H. Jamieson, C. D. Mitchell, G. Ying, S. Potisuk, P. N. Srinivasan, R. Chen, C. B. Zoltowski, L. L. McPheters, B. Pellom, and R. A. Helzerman. 1994. Integrating language models with speech recognition. In Proc. of the AAAI Workshop on the Integration of Natural Language and Speech Processing, pages 139-146. M. P. Harper, S. A. Hockema, and C. M. White. 1999a. Enhanced constraint dependency grammar parsers. In Proc. of the IASTED Int. Conf. on Artificial Intelligence and Soft Computing. M. P. Harper, M. T. Johnson, L. H. Jamieson, and C. M. White. 1999b. Interfacing a CDG parser with an HMM word recognizer using word graphs. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc. R. A. Helzerman and M. P. Harper. 1996. MUSE CSP: An extension to the constraint satisfaction problem. Journal of Artificial Intelligence Re- search, 5:239-288. P. Jeanrenaud, E. Eide, U. Chaudhari, J. Mc- Donough, K. Ng, M. Siu, and H. Gish. 1995. Re- ducing word error rate on conversational speech from the Switchboard corpus. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 53-56. M. T. Johnson and M. P. Harper. 1999. Near min- imal weighted word graphs for post-processing speech. In 1999 Int. Workshop on Automatic Speech Recognition and Understanding. H. Maruyama. 1990a. Constraint Dependency Grammar and its weak generative capacity. Com- puter Software. H. Maruyama. 1990b. Structural disambiguation with constraint propagation. In Proc. of the An- nual Meeting of Association for Computational Linguistics, pages 31-38. W. Menzel. 1994. Parsing of spoken language un- der time constraints. In 11th European Conf. on Artificial Intelligence, pages 560-564. W. Menzel. 1995. Robust processing of natural lan- guage. In Proc. of the 19th Annual German Conf. on Artificial Intelligence. B. Natarajan. 1989. On learning sets and functions. Machine Learning, 4(1). H. Ney, U. Essen, and R. Kneser. 1994. On struc- turing probabilistic dependences in stochastic lan- guage modelling. Computer Speech and Language, 8:1-38. R. Pieraccini and E. Levin. 1992. Stochastic repre- sentation of semantic structure for speech under- standing. Speech Communication, 11:283-288. P. Placeway, R. Schwartz, P. Fung, and L. Nguyen. 1993. The estimation of powerful language mod- els from small and large corpora. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 33-36. P. J. Price, W. Fischer, J. Bernstein, and D. Pallett. 1988. A database for continuous speech recog- nition in a 1000-word domain. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 651-654. L. A. Schmid. 1994. Parsing word graphs using a lin- guistic grammar and a statistical language model. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 41-44. S. Seneff. 1992. TINA: A natural language system for spoken language applications. American Jour- nal of Computational Linguistics, 18:61-86. B. Srinivas. 1996. 'Almost parsing' technique for language modeling. In Proc. of the Int. Conf. on Spoken Language Processing, pages 1173-1176. L. G. Valiant. 1984. A theory of the learnable. Com- munications of the ACM, 27(11):1134-1142. C. M. White. 2000. Rapid Grammar Development and Parsing Using Constraint Dependency Gram- mars with Abstract Role Values. Ph.D. thesis, Purdue University, School of Electrical and Com- puter Engineering, West Lafayette, IN. J. H. Wright, G. J. F. Jones, and H. Lloyd-Thomas. 1994. Robust language model incorporating a substring parser and extended N-grams. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 361-364. S. J. Young, N. H. Russell, and J. H. S. Thornton. 1989. Token passing: a simple conceptual model for connected speech recognition systems. Tech- nical Report TR38, Cambridge University, Cam- bridge, England. S. J. Young, J. Odell, D. Ollason, V. Valtchev, and P. Woodland, 1997. The HTK Book. Entropic Cambridge Research Laboratory Ltd., 2.1 edition. V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips, J. Polifroni, and S. Seneff. 1991. Integration of speech recognition and natural language process- ing in the MIT Voyager system. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 713-716."
  },
  {
    "title": "Natural Language Dialogue Service for Appointment Scheduling Agents",
    "abstract": "Appointment scheduling is a problem faced daily by many individuals and organizations. Cooperating agent systems have been developed to partially automate this task. In order to extend the circle of participants as far as possible we advocate the use of natural language transmitted by e-mail. We describe COSMA, a fully implemented German language server for existing appointment scheduling agent systems. COSMA can cope with multiple dialogues in parallel, and accounts for differences in dialogue behaviour between human and machine agents. NL coverage of the sublanguage is achieved through both corpus-based grammar development and the use of message extraction techniques.",
    "content": "1 Motivation Appointment scheduling is a problem faced daily by many individuals and organizations, and typical- ly solved using communication in natural language (NL) by phone, fax or by mail. In general, cooper- ative interaction between several participants is re- quired. Since appointments are often scheduled only after a sequence of point-to-point connections this will, at times, necessitate repeated rounds of com- munication until all participants agree to some date and place. This is a very time-consuming task that should be automated. Systems available on the market allow for calendar and contact management. As (Busemann and Mer- get, 1995) point out in a market survey, all planning and scheduling activity remains with the user. Co- operative agent systems developed in the field of Dis- tributed AI are designed to account for the schedul- ing tasks. Using distributed rather than centralized *This work has been supported by a grant from the German Federal Ministry of Education, Science, Re- search and Technology (FKZ ITW-9402). calendar systems, they not only guarantee a maxi- mum privacy of calendar information but also offer their services to members or employees in external organizations. Although agent systems allow users to automate their scheduling tasks to a considerable degree, the circle of participants remains restricted to users with compatible systems. To overcome this drawback we have designed and implemented COSMA, a novel kind of NL dialogue systems that serves as a German language front- end system to scheduling agents. Human language makes agent services available to a much broader public. COSMA allows human and machine agents to participate in appointment scheduling dialogues via e-mail. We are concerned with meetings all par- ticipants should attend and the date of which is ne- gotiable. 2 Design guidelines COSMA is organized as a client/server architecture. The server offers NL dialogue service to multiple client agent systems. Up to now, three different types of agent systems have been hooked up to the NL server. Agents developed in-house were used for the early system described in (Busemann et al., 1994). In a subsequent version, the MEKKA agents developed by Siemens AG (Lux et al., 1992) have been adapted. We present in Section 4 a third kind of client system, the PASHA II user agent. Given the use of distributed calendar systems, techniques used by both human and machine agents for cooperatively scheduling appointments must be based on negotiation dialogues. However, human dialogue behaviour differs from interaction between machine agents considerably, as will be discussed in Section 4. A human-machine interface to existing appointment scheduling agent systems should com- ply to the following requirements: • Human utterances must be analyzed to corre- spond closely to agent actions. • Machine utterances must conform to human di- alogue strategies. Artificial communication languages have been de- signed for human discourse, e.g. (Sidner, 1994), as well as for agent-agent interaction, e.g. (Steiner et al., 1995). What would be needed for COSMA is a mapping between strategies implemented in such languages. Since the type of agent system connect- ed to the COSMA server is not restricted by its dia- logue behaviour, preference was given to implement application-dependent mappings instead of develop- ing a generic formalism. As a consequence, COSMA operates with general and reusable processing mod- ules that interpret domain- and task-specific data. The same principle was also adopted for NL anal- ysis. The server must analyze human-generated text and verbalize machine-initiated goals. For a plausi- ble application, the server must be: • complete with respect to a sublanguage: all rel- evant information related to appointments must be analyzed, • sufficiently robust to deal with inconsistent analysis results. Within the HPSG-based approach to grammar de- scription adopted for the early system (Uszkoreit et al., 1994), achieving these goals turned out to be difficult. This \"deep\" approach to NLU describes NL expressions at general linguistic levels (syntax and surface semantics), and attempts to capture the complete meanings of all and only the grammati- cal sentences. However, an NL system in a realis- tic application should not fail on unexpected input. Moreover, the surface semantic representations de- rived by the grammar were too close to NL for an agent system to deal with. With the present version of the NL server these problems are solved by adopting a \"shallow\" anal- ysis approach, which extracts meanings from those portions of a text that are defined as interesting and represents them in an agent-oriented way. Instead of failing on unexpected input, shallow parsing meth- ods always yield results, although they may not cap- ture all of the meaning intended by the user. By just describing the verbalizations of relevant information, shallow parsing grammars are highly domain-specific and task-oriented. In COSMA, shallow analysis is di- vided up into an application of the message extrac- tion component smes (discussed in Section 5) and a semantic analysis component IMAS (Section 6). The former extracts appointment-related informa- tion from users' input texts. It is based on finite- state automata that were defined with help of an annotated corpus of e-mail messages. The task of the latter is to derive a client-oriented semantic rep- resentation, including the communicative intention and the complete specification of time points needed, which is based on context and semantic inferences. The robustness requirement is fulfilled by recog- nizing failures within the server during semantic analysis, and possibly within the client systems, and by clarification dialogues (cf. Section 6.1). After an overview of generation in COSMA (Sec- tion 7) we discuss component interaction in Sec- tion 8. A novel type of object-oriented architecture is needed to treat multiple dialogues in parallel. Vir- tual partial system instances are maintained as long as a dialogue is going on. One such instance is shown in Figure 1. 3 A complete sample dialogue A complete sample dialogue taken from the sys- tem's present performance will serve as a reference throughout the paper. Every utterance is numbered and labeled; the labels indicate speakers. We as- sume a three-party e-mail negotiation between a hu- man (H), who does not use a scheduling agent sys- tem, and two machine agents (A, B) that schedule appointments for their respective owners. In the server, human interactions with multiple machine partners are treated as different NL dialogues (in the present case between H and A, and H and B). In what follows, H is the initiator, but COSMA al- so copes with machine-initiated dialogues (cf. Sec- tion 4).1 H (01) Ich würde Sie gern am Montag, dem 2. 11. 96 wegen der bevorstehenden Projektbegutach- tung treffen. [I would like to meet you on Monday Nov. 2 1996 about the upcoming project review.] (02) A, B COSMA hat die folgende Zeitangabe ver- standen, die nicht konsistent ist: Mon- tag, den 2. 11. 1996. Könnten Sie bitte den Wochentag oder das Datum korrigieren? [COSMA has understood the following time expression, which is not consistent: Monday, Nov. 2 1996. Could you please correct the weekday or the date?] (03) Ich meinte natürlich Montag den 4. 11. [I meant of course Monday, Nov. 4.] H (04) A (05) B (06) H Am 4. 11. 1996 paßt es bei mir zwischen 13 und 18 Uhr. [Nov. 4, 1996 would suit me between 1 and 6 p.m.] Leider kann ich am 4. 11. 1996 nicht kommen. [Unfortunately I can't come on Nov. 4 1996.] Der vorige Vorschlag wurde abgelehnt. Können wir uns stattdessen am Dienstag treffen? [The previous proposal was reject- ed. Can we meet instead on Tuesday?] ¹Agent-agent interaction is based on a formal repre- sentation language rather than on NL. Shallow Analysis Generation Owner SMES Message Extraction Linguistic Knowledge TG/2 IMAS Semantic Interpretation Sem. repr. Text Dialogue Memory Generic Server Interface GSI E-Mail Sem repr Agent-System PASHA II Figure 1: The COSMA architecture: a client connected to a server instance may issue requests to receive a semantic representation for a text, or to generate a text from a semantic representation. The generic server interface invokes the necessary server processes and maintains interaction with the client. (07) Zu folgenden Zeiten geht es bei mir: am 5. A 11. 1996 zwischen 8 und 12 Uhr und zwischen 16 und 18 Uhr. [The following times would suit me: Nov. 5, 1996 between 8 and 12 a.m. and between 4 and 6 p.m.] (08) B Am 5. 11. 1996 paßt es bei mir zwischen 10 und 12 Uhr. [Nov. 4, 1996 would suit me between 10 and 12 a.m.] (09) Wie wäre es dann um 10? [How about 10 H then?] (10) A, B (11) H Ich sage den 5. 11. 1996 um 10 Uhr zu. [I accept Nov. 5, 1996 at 10 a.m.] Das Treffen wird also am 5. 11. 1996 um 10 Uhr stattfinden. [The meeting will take place on Nov. 5, 1996 at 10 a.m.] In (01) H sends by mistake an inconsistent tempo- ral expression to A and B, giving rise to clarification dialogues initiated by each of A and B (02). The repair provided by H (03) is underspecified with re- spect to clock time (see also (06)), hence the agents offer free time slots in accordance to their calendars (04, 07, 08). These time slots are blocked until fur- ther notice.2 Since B rejects the proposed date (05), a new loop is started by H (06). When H notices that Tuesday is promising, she chooses to refine her proposal by suggesting a clock time (09). Dynamic context knowledge allows the server to reconstruct a full time specification that is interpreted by the agents as an alternative proposal. Refinements can 2Cancellations of reserved slots due to a high-priority request are a straight-forward extension of the present coverage. thus be dealt with completely in the server, whereas the agents may or may not have a concept of refine- ment. After all agents accept a proposal, the date is confirmed by the initiator (11). Upon receipt of the confirmation, the agents fix the date in their cal- endars. Server and agents consider the dialogues as completed. 4 Dialoging scheduling agents 4.1 The PASHA II system PASHA II agents (Schmeier and Schupeta, 1996) are designed according to the InterRaP agent ar- chitecture (Fischer et al., 1995), a layer-based agent model that combines deliberative and reactive be- haviour. The \"heart\" of an agent is the cooperative planning layer, in which negotiation strategies are represented as programs and executed by a language interpreter. This supports easy modification and ex- change of plans. The local planning layer consists of a constraint planner which reasons about time slots in the agent's (i.e. its owner's) calendar. In contrast to the planning layers, the behaviour-based layer consists of the agent's basic reactive behaviour and its procedural knowledge. The world interface realizes the agent's sensing and acting capabilities as well as the connection to its owner. PASHA II agents are connected to the Unix CM calendar man- agement tool, but can easily be hooked up to other calendar systems. PASHA II agents are easily adapted to the owner's preferences. For instance, any time slots the owner does not wish the agent to use can be blocked. By virtue of this mechanism, a working day could be defined as an interval from e.g. 8 a.m. until 6 p.m. except for Saturdays, Sundays and holidays. More- over, gaps between appointments may be specified in order to permit sufficient time between meetings. 4.2 Adapting agents to the COSMA server Taking PASHA II as a representative, we describe the requirements for an agent system to connect to the COSMA server. Interface to the server. The four main modules include the basic TCP/IP connection to the server; a parser of semantic representations of the server's analysis results, which yields PASHA II structures; an instantiation mechanism for semantic generation templates; and a control regime that keeps track of the current dialogue. The control regime confirms results of the server, or it activates the server's back- track mechanism if the semantic representation re- ceived does not fit within the current dialogue step, or it issues a request for repair if backtracking should not yield any further results. Receiving and sending e-mail. The PASHA II interaction mechanism includes, besides communica- tion via TCP/IP protocols, e-mail interaction. The agent may poll its owner's mailbox or have one of its own. Either the agent or its owner is referred to as actor in the agent's e-mail messages (see Section 7). Dialogue behaviour. An agent has to generate and understand different dialogue actions represent- ed by corresponding cooperation primitives such as proposing, accepting, rejecting, canceling or fixing a meeting (Steiner et al., 1995). Agent-agent interaction usually relies on an ini- tiating agent being responsible for the success of a negotiation. The initiator's broadcast proposal is triggered by its owner, who determines partners, du- ration and an interval within which the appointment should be scheduled. The agent proposes the first slot in the interval that is available according to its calendar. In case of a rejection of one or more partic- ipants, the initiator would continue to propose new time slots to all partners until everyone agrees to a common date or there is no such slot within the interval. Note that in case of rejection (see (05)) PASHA II agents do not use counter-suggestions. In human-human negotiation, efficiency is a major goal. Humans often follow the least effort principle (Dahlbäck, 1992): the initiator broadcasts a propos- al including a time interval within which the meeting should take place (e.g. (03)) and expects refinements or counter-proposals from the participants. As the example shows this may imply the use of underspec- ified temporal descriptions. This strategy requires less communication because a greater amount of in- formation is exchanged in one dialogue step between the participants. Handling underspecified temporal information by offering free time slots (see (04), (07), and (08)) is among the extensions of PASHA II at the local plan- ning layer. Note that this strategy can be instanti- ated in different ways, as becomes clear from dealing with expression such as next week: Only a selection of free time slots can be provided here, which is ex- plicitly marked using e.g. for instance. Moreover, we consider it indispensable to have agents understand and generate counter-proposals to avoid inefficient plain rejections like (05). 5 Covering the domain language 5.1 Corpus-based annotation In order to determine the coverage of the sub- language relevant for the application and to measure progress during system development, a corpus of 160 e-mails was selected as reference material from sev- eral hundred e-mails collected from the domain of appointment scheduling. The e-mails were manual- ly analyzed and annotated with major syntactic and semantic features as well as speechact information. A combination of two relational database systems was employed to ease the storage, maintenance, ex- tension and retrieval of the NL data: (i) DITO (Nerbonne et al., 1993), a full text database where the e-mails can be accessed, (ii) tsdb (Oepen et al., 1995), an elaborated fact database which permits the extraction of specific linguistic constructions together with the associat- ed linguistic annotations.3 Annotation Example Prepositional Phrases: Wie wäre es [How about] ... PP_temp in dieser Woche? [in this week?] PP_temp-date am 4.11? [on the 4th of Nov.?] PP_temp-day am Montag? [on Monday?] PP_temp-dur von 8 bis 12? [from 8 to 12?] PP_temp-time um 10? [at 10?] Noun Phrases: Ich komme [I come ... NP_temp zwei Stunden später. [two hours later.] NP_temp-date am Montag, den 4. 11. [on Monday, the 4th of Nov.] NP_temp-day Montag, 14 h. [Monday, 2 pm.] NP_temp-time Montag, 14 h. [Monday, 2 pm.] Figure 2: Semantic annotation of PPs and NPs (an- notated linguistic material in italics) The annotation work is based on the TSNLP framework (Lehmann et al., 1996) where detailed category and function lists are defined for the struc- tural and dependency structure annotation of lin- guistic material for NLP test suites. For COSMA, the classification has been extended according to se- mantic information relevant for the appointment do- main. For instance, PPs and NPs were specified fur- ther, introducing a more fine-grained semantic anno- 3DITO and tsdb entries are linked via e-mail identifiers. tation for temporal expressions, as is shown in Fig- ure 2. The results of database queries provided valu- able insights into the range of linguistic phenome- na the parsing system must cope with in the do- main at hand. Grammar development is guided by a frequency-based priority scheme: The most im- portant area temporal expressions of various cate- gories - followed by basic phenomena including dif- ferent verbal subcategorizations, local and thematic PPs, and the verbal complex are successfully cov- ered. 5.2 Message extraction with smes The message extraction system smes (Neumann et al., 1997) is a core engine for shallow processing with a highly modular architecture. Given an ASCII text, smes currently produces predicate argument struc- tures containing shallow semantic analyses of PPs and NPs. The core of the system consists of: • a tokenizer, which scans the input using a set of regular expressions to identify the fragment patterns (e.g. words, date expressions, etc.), • a fast lexical and morphological processing of 1,5 million German word forms, • a shallow parsing module based on a set of finite state transducers, • a result combination and output presentation component. Based on the information delivered by the mor- phological analysis of the identified fragment pat- terns, the system performs a constituent analysis. In order to combine complements and adjuncts into predicate-argument structures, special automata for verbs are then activated over the sequence of con- stituents analyzed so far. Starting from the main verb¹, a bidirectional search is performed whose do- main is restricted by special clause markers. smes output yields information about the utterance rele- vant for the subsequent semantic analysis. 5.3 Semi-automatic grammar development The concrete realization of the automata is based on the linguistic annotations of the e-mail frag- ments in the corpus. The annotations render a semi- automatic description of automata possible. For in- stance, verb classification directly leads to the lexical assignment of a corresponding automaton in smes. By deriving parts of the grammar directly from cor- pus annotations, maintenance and extension of the grammars are eased considerably. On the other hand, corpus extension can be sup- ported by smes analyses. Existing automata can be If no verb is found, a \"dummy\" entry triggers pro- cessing of verbless expressions, which occur frequently in e-mail communication. used to annotate new material with available linguis- tic information. Manual checking of the results re- veals gaps in the coverage and leads to further refine- ment and extension of the automata by the grammar writer. This way, grammar development can be achieved in subsequent feedback cycles between the annotated corpus and smes automata. The implementation of the annotation procedure based on the smes output format is underway. 6 Semantic interpretation Semantic representations produced by smes are mapped into a format suitable for the PASHA-II client by the IMAS component (Information extrac- tion Module for Appointment Scheduling). IMAS is based on a domain-dependent view of semantic interpretation: information-gathering rules explore the input structure in order to collect all and on- ly the relevant information; the resulting pieces of information are combined and enriched in a mono- tonic, non-compositional way, thereby obtaining an IL (Interface Level) expression, which can be inter- preted by the agent systems. In spite of the non- compositionality of this process, the resulting ex- pressions have a clear model-theoretic interpretation and could be used by any system accepting first or- der logic representations as input. IL expressions have been designed with the goal of representing both a domain action that is eas- ily mapped onto an agent system's cooperation primitive, and the associated temporal informa- tion, which should be fully specified due to con- textual knowledge. Temporal information is par- titioned into RANGE, APPOINTMENT and DURATION information. RANGE denotes the interval within which a certain appointment has to take place (e.g. in (03)). APPOINTMENT denotes the interval of the appointment proper (e.g. in (10)). Inter- vals in general are represented by their boundaries. DURATION, on the contrary, encodes the duration of the appointment expressed in minutes. The back- bone of an IL expression is thus the following: [COOP identifier] [RANGE LEFT-BOUND HOUR digit MINUTE digit ] [RIGHT-BOUND HOUR digit MINUTE digit ] [APPT DURATION digit] IMAS relies on three basic data structures. The sentence structure contains all the IL expressions obtained from the analysis of a single sentence. They are ranked according to their informativeness. The text structure contains all the sentence structures obtained from the analysis of a whole mes- sage. Here ranking depends not only on informative- ness but also on \"dialogue expectation\": sentence structures are favoured that contain a domain ac- tion compatible with the IL expression previously stored in the discourse memory. As a result, the NL server will pass to the client the most informative IL expression of the most informative and contextually most relevant sentence of the analyzed text.5 The discourse memory is structured as a se- quence containing all information collected during the dialogue. Thus it contains both IL expressions committed by the client and semantic input struc- tures from generation. The discourse memory is used by IMAS as a stack.- The procedural core of IMAS is represented by the transformation of the input smes representation into a set of IL expressions. This process is organized into three steps: Linguistic extraction. The semantic represen- tation of the input smes structure is explored by a set of rules in such a way that all information rele- vant for the appointment domain is captured. For every type of information (e.g. domain action, hour of appointment, duration, etc.) a different set of rules is used. The rules are coded in a transparent and declarative language that allows for a (possibly underspecified) description of the smes input (rep- resented as a feature structure) with its associated \"information gathering\" action. Anchoring. Most utterances concerning the do- main of appointment scheduling are incomplete at least in two respects. Either they contain expres- sions which need to be delimited in order to be prag- matically plausible (underspecification, e.g. (09)), or they refer to intervals which are not explicitly men- tioned in the sentence (temporal anaphora). The first class includes probably any NL time expres- sion; even a simple expression such as (01) requires some extralinguistic knowledge to be understood in its proper contextual meaning (in (01) the \"working day\" interval of the respective day must be known). The reconstruction of underspecified temporal ex- pressions is performed by a set of template filling functions which make use of parameters specified by the client system at the beginning of the dialogue. Temporal anaphora include expressions such as on Monday, tomorrow, next month, whose inter- pretation depends on the discourse context. Solv- ing anaphoric and deictic relations involves a rather complex machinery which borrows many concepts from Discourse Representation Theory. In particu- lar, we assume a procedure according to which the antecedent of an anaphoric temporal expression is first looked up in the IL expressions of the text al- ready parsed (with a preference for the most recent expressions); if no one is found, the discourse memo- ry is consulted to retrieve from previous parts of the If the client is not satisfied with such an expression, backtracking will pass the next-best structure etc. dialogue a temporal expression satisfying the con- straints under analysis. If the search fails again, the expression is interpreted deictically, and resolved w.r.t. to the time the message was sent. Inferences. IL expressions can be enriched and disambiguated by performing certain inferences in- volving temporal reasoning. Besides trivial cases of temporal constraint resolution, such as guessing the endpoint of an appointment from its startpoint and its duration, our inference engine performs disam- biguation of domain actions by comparing intervals referred to by different dialogue utterances. For in- stance, if an utterance u describing an interval I is ambiguous between a refinement and a modification and the previous utterance refers to an interval Jin- cluding I, then u can be disambiguated safely as de- noting a refinement. Analogous inferences are drawn by just checking the possible combinations of domain actions across the current dialogue (a rejection can hardly be followed by another cancellation, a fixing cannot occur after a rejection, etc.). The constraints guiding this disambiguation procedure are encoded as filters on the output of IMAS and reduce the set of pragmatically adequate IL expressions. 6.1 Handling of analysis failures Sometimes IMAS produces an output which cannot be used by the PASHA-II client. This happens when the human message is either too vague (What about a meeting?), or contains an inconsistent temporal specification (as in (01)). In these cases IMAS stores the available information, and the server generates a request for clarification in order to recover the nec- essary temporal specifications or to fix the already available ones. This request is mailed to the hu- man partner. It includes the list of misspelled words found in the input message, which may give the part- ner a clue for understanding the source of the error. Once a clarification is provided, the server attempts to build an IL expression by merging and/or replac- ing the information already available with the newly extracted one (cf. (03)). If the resulting IL expres- sion satisfies the constraints on well-formedness, it is shipped to the PASHA-II client. Otherwise the clar- ification subdialogue goes on along the same lines. 7 Generation Client systems usually want to express in NL a coop- eration primitive and a date expression. Hence NL generation is based on a semantic template filled by the client. Depending on its content the template is unified with a prefabricated structure specifying linguistic-oriented input to the generator. The same holds for failure messages, such as (02), and for spec- ifications of free time slots, as in (07), where simple rules of aggregation take care not to repeat the full date specification for each clock time mentioned. The production system TG/2 (Busemann, 1996) proved to be sufficiently flexible to accomplish this task by its ability to generate preferred formulations first. For instance, COSMA clients can parameterize TG/2 so as to refer to their owner by a first per- son pronoun or by a full name, or to use formal or informal form of addressing the human hearer, or to prefer deictic time descriptions over anaphorical ones. 8 A novel architecture A NLP server which can both provide a range of nat- ural language services and process multiple dialogues for a variety of applications in parallel requires (1) an architecture that ensures a high degre of reusability of NLP resources, (2) the availability of a robust in- terface that guarantees transparency and flexibility with respect to data representation and task spec- ification, (3) client-driven server parametrization, (4) support for incremental, distributed and asyn- chronous robust data processing, and (5) advanced concepts for synchronization with respect to parallel dialogue processing for multiple clients. Due to the limited functionality of common architectural styles (Garlan and Shaw, 1993) with respect to these re- quirements, a novel object-oriented, manager-based and generic architecture has been designed and im- plemented. It combines techniques from different ar- eas in particular, from object technology (Booch, 1994) and from coordination theory including work- flow management (Malone and Crowston, 1991) and is based on two main concepts: the cooperat- ing managers approach (COCONUTS) and the virtual system architecture model. 8.1 A manager-based approach Managers in the COCONUTS model are control units which coordinate or perform specific activities and cooperate with each other in a client/server form. Their responsabilities, properties, behaviour and in- terface are determined by the classes they belong to. The prominent COCONUTS managers are: the da- ta manager, which provides services related to rep- resentation, printing, conversion and transmission of data; the report manager, which supports spec- ification, generation and printing of processing re- ports; the global interface manager, which provides a generic server interface; the computing components managers (CCMS), which encapsulates the system's components and let them appear as servers; and, fi- nally, the workflow manager, which is the main con- trol unit. 8.2 Coordination and control Coordinating internal system activities with respect to parallel dialogue processing (including backtrack- ing and failure recovery facilities) requires very pow- erful and flexible mechanisms for task scheduling, synchronization and control. In COCONUTS this task is carried out by the workflow manager, which al- so manages interdependencies between these activ- ities while avoiding redundant ones and controlling the flow of work among the involved managers (e.g., passing subtasks from one manager to another in a correct sequence, ensuring that all fulfill their re- quired contributions and taking default actions when necessary). The behaviour and function of the work- flow manager are determined by the following se- quence of operations: identifying and formulating a workflow goal, decomposing it into subgoals, de- termining and allocating resources for achieving the subgoals, elaborating and, eventually, executing an operation plan. It also provides a range of special- ized exception handlers to ensure robustness (see Section 6.1). 8.3 A generic server interface Flexible and reliable client/server communication is made possible by the generic server interface module GSI. It includes a declarative, feature-based repre- sentation and task specification language CCL and an object-oriented communication and data trans- fer module CCI. For CCL a parser, a printer and an inference engine are available. CCI contains various kinds of interface objects containing higher-level pro- tocols and methods for reliable TCP/IP-based com- munication, data encoding/decoding and buffering, as well as priority and reference management. Note that interface objects are accessible through their TCP/IP-based internet addresses and can be asso- ciated to any component (cf. Figure 1). This way, subsystems can, on demand, be used as servers, e.g. smes or the generator. 8.4 Integrating heterogenous components Each COSMA server component is encapsulated by a CCM (computing component manager), which makes its functionality available to other managers. A CCM has, among other things, a working (short- term) memory, a long-term memory and a variety of buffers for storing and managing computed solutions for subsequent use. Using these features a CCM eas- ily simulates incrementality and realizes intelligent backtracking by providing the computed solutions in a selective manner. A component can be released by a CCM it is bound to when the latter does no longer need its services; e.g. if the component has al- ready computed all solutions. This permits efficient resource sharing, as several CCMS can be associat- ed to one component. Thus, associating interface objects with CCMS provides a flexible way of realiz- ing distributed processing performed by components implemented in different languages and running on different machines. 8.5 The virtual system architecture The virtual system architecture allows for efficient parallel dialogue processing. It is based on the con- cept of cooperating object-oriented managers with the ability to define one-to-many relationships be- tween components and CCMS. The key idea consists in adopting a manager-based/object-based view of the architecture shown in Figure 1. This architec- ture represents a virtual system (also called opera- tion context), which is a highly complex object con- sisting of a variety of interacting managers. It may inherit from different classes of operation contexts, whose definitions are determined by the underlying domains of application. Thus, multiple dialogues are processed in parallel just by running each dialogue in a separate virtual system. As soon as a dialogue is completed, the assigned virtual system can be reused to process another one. Conceptually, no constraints are made on the number of active virtual systems in the server software. In order to ensure correct pro- cessing, a manager may operate in only one virtual system at a time. Note that managers can still be shared by virtual systems and they behaviour can vary from one system to another. 9 Conclusion We described COSMA, a NL server system for exist- ing machine agents in the domain of appointment scheduling. The server is implemented in Common Lisp and C. The PASHA II agent is implemented in DFKI-Oz (Smolka, 1995). Robust analysis of human e-mail messages is achieved through message extraction techniques, corpus-based grammar development, and client- oriented semantic processing and representation. The virtual server architecture is a basis for the flex- ible use of heterogeneous NLP systems in real-world applications including, and going beyond, COSMA. Future work includes extensive in-house tests that will provide valuable feedback about the perfor- mance of the system. Further development of Cos- MA into an industrial prototype is envisaged. References Grady Booch. 1994. Object-Oriented Analysis and Design with Applications. Benjamin/Cummings, Menlo Park. Stephan Busemann and Iris Merget. 1995. Eine Untersuchung kommerzieller Terminverwaltungs- Software im Hinblick auf die Kopplung mit natürlichsprachlichen Systemen. Technical Doc- ument D-95-11, DFKI, Saarbrücken. Stephan Busemann et al. 1994. COSMA- multi-participant NL interaction for appointment scheduling. Technical Report RR-94-34, DFKI, Saarbrücken. Stephan Busemann. 1996. Best-first surface realiza- tion. In Donia Scott, editor, Eighth International Natural Language Generation Workshop. Procced- ings, Herstmonceux, Univ. of Brighton. Nils Dahlbäck. 1992. Representations of Discourse. Cognitive and Computational Aspects. Ph.D. the- sis, Department of Computer and Information Sci- ence. Linköping University. Klaus Fischer et al. 1995. Unifying control in a layered agent architecture. Technical Memo TM- 94-05, DFKI, Saarbrücken. David Garlan and Mary Shaw. 1993. An intro- duction to software architecture. SEI-93-TR-033, Software Engineering Institute, Carnegie Mellon University, Pittsburg, Pennsylvania 15213. Sabine Lehmann et al. 1996. TSLNP — Test Suites for Natural Language Processing. In Proceedings of COLING-96, pages 711-716, Copenhagen. Andreas Lux et al. 1992. A Model for Supporting Human Computer Cooperation. In AAAI Work- shop on Cooperation among Heterogeneous Intel- ligent Systems, San Jose, Ca. Thomas W. Malone and Kevin Crowston. 1991. To- ward an interdisciplinary theory of coordination. Technical Report CCS TR 120, Center for Co- ordination Science, Sloan School of Management, MIT, Cambridge, MA. John Nerbonne et al. 1993. A Diagnostic Tool for German Syntax. Machine Translation, 8(1-2). Günter Neumann et al. 1997. An information ex- traction core system for real world German text processing. In this volume. Stephan Oepen et al. 1995. The TSNLP database: From tsct(1) to tsdb(1). Report to LRE 62-089, DFKI. Sven Schmeier and Achim Schupeta. 1996. Pasha II — a personal assistant fo scheduling appointments. In First Conference on Practical Application of Multi Agent Systems, London. Candace L. Sidner. 1994. An artificial discourse language for collaborative negotiation. In Proc. 12th National Conference on Artificial Intelli- gence. Volume 1, pages 814-819, Seattle, WA. Gert Smolka. 1995. The Oz Programming Model. Research Report RR-95-10, DFKI, Saarbrücken. Donald Steiner et al. 1995. The conceptual framework of MAIL. In Cristiano Castelfranchi and Jean-Pierre Müller, editors, From Reaction to Cognition. 5th European Workshop on Mod- elling Autonomous Agents in a Multi-Agent World (MAAMAW 93), pages 217-230. Springer, LNAI, Vol. 957. Hans Uszkoreit et al. 1994. DISCO-An HPSG- based NLP System and its Application for Appointment Scheduling. In Proceedings of COLING-94, Kyoto."
  },
  {
    "title": "FINDING CLAUSES IN UNRESTRICTED TEXT BY FINITARY AND STOCHASTIC METHODS",
    "abstract": "The paper presents and compares two different methods of parsing, a regular expression method and a stochastic method, with respect to their success in identifying basic clauses in unrestricted English text. These methods of parsing were developed in order to be applied to the task of improving the detection of large prosodic units in the Bell Labs text-to-speech system, and were so applied experimentally. The paper also discusses the notion of basic clause that was defined as the parsing target. The result of a comparison of the error rates of the two parsing methods in the recognition of basic clauses showed that there was a 13% error rate for the regular expression method and a 6.5% error rate for the stochastic method.",
    "content": "1. Introduction The present paper describes the procedure that was followed in an extended experiment to reliably find basic surface clauses in unrestricted English text, using various combinations of finitary and stochastic methods. The purpose was to make some improvements in the detection and treatment of large prosodic units above the level of fgroups in the Bell Labs text-to-speech system. This system currently relies exclusively on punctuation (commas and periods) for the detection of such units, i.e. tonal minor and major phrases. Commas are correlated with tonal minor phrases, and sentence final periods with tonal major phrases. The notion of fgroup (one or more function words followed by one or more content words), and its implementation in the Bell Labs text-to-speech system is described in Liberman & Buchsbaum (1985). Correct automatic detection of major syntactic boundaries, in particular clause boundaries, is a prerequisite for automatic insertion of final lengthening, boundary tones and pauses at such boundaries within sentences (cf. Allen, Hunnicutt & Klatt 1987, and Altenberg 1987). These prosodic phenomena make significant of synthetic speech. Unfortunately, the task of parsing unrestricted text correctly, in order to find the relevant sentence internal syntactic boundaries has turned out to be very difficult. This paper is a report of an attempt to provide a better foundation for parsing text by the use of simple finitary and stochastic computational methods. These simple methods have not figured prominently in the theory and practice of natural langauge parsing, with some exceptions (Langendoen 1975, Church 1982, Ejerhed & Church 1983). For an experimental, and more complicated method to derive all prosodic units in the text-to-speech system, i.e. not just tonal minor and major phrases but every type of prosodic unit, from the syntactic structure and length of constituents, see Wright, Bachenko & Fitzpatrick (1986). The first purpose of the experiment was to test the performance of a finite state parser, when the parser was given the rather difficult and substantive tasks of finding basic, non-recursive clauses in continuous text, in which each word had been tagged with a part of speech label. Parts of the tagged Brown corpus were used, representing the genres of both informative and imaginative prose. The clause grammar, consisting of a regular expression for clauses of different kinds, was constructed by the author and it was first applied to text that was guaranteed to have correct parts of speech assigned to the words, so that problems in constructing the grammar could be isolated from problems in assigning correct parts of speech. The finite state parser that used the clause grammar consisted of a program that matched regular expressions for clauses against the longest substrings of tagged words that fit them, and it was constructed and implemented by K. Church. The second purpose was to see whether basic clauses could also be recognized by stochastic programs, after these had been trained on suitable training material. The training material was prepared by hand-correcting the output of expressions for clauses. A stochastic program for assigning unique part of speech tags to words in unrestricted text had been created by K. Church, and trained on the tagged Brown corpus (see Church 1987). The resultant program is 95-99% correct in its performance, depending on the criteria of correctness used, and it can be used as a lexical front end to any kind of parser, i.e. not necessarily stochastic or finite state parsers. However, the question presented itself whether the stochastic procedure that was so successful in recognizing parts of speech could also be applied to more advanced tasks such as recognizing noun phrases and clauses. The present paper concentrates on the parsing of basic clauses. The parsing of noun phrases by the same two methods is compared in Ejerhed (1987), and the stochastic parsing of noun phrases is described in detail in Church (1987). The structure of the paper is as follows. Section 2 defines the target of a basic clause, and reports on the outcome of the search for such units by the two methods. Section 3 discusses the correlations between clause units as defined by this paper, and the prosodic units of tonal minor and major phrases in the Bell Labs text-to- speech system. 2. Finding Clauses 2.1 Why Clauses? Syntactic surface clauses are interesting units of language processing for a variety of reasons. In the surface clause, criteria of form and meaning converge to guarantee both that it can be recognized solely by surface syntactic properties and that it constitutes a meaningful unit (ideally a proposition) in a semantic representation. Clauses have been investigated in psycholinguistic research. Jarvella (1971) found effects of both sentence boundaries and clause boundaries in recall of spoken complex sentences and took them, along with previous results of Jarvella & Pisoni (1970), to support a clause-by- clause view of within-sentence processing. Later research on reading comprehension has found effects on gaze duration not only of word length and word frequency, but also of syntactic local ambiguity (garden paths) and of ends of sentences (Just & Carpenter 1984). However, the study of clause units as distinct from sentence units has not been carried out far, and a lot of basic facts remain to be found out about the role of clause units of different kinds in the processes whereby spoken and written language is comprehended. 2.2 The Definition of A Basic Clause Finding basic noun phrases is important as a stepping stone to finding clauses, on the assumption that an important subset of them have an initial sequence consisting of a noun phrase followed by a tensed verb as a defining characteristic. The result of scoring the respective success of the two methods of parsing basic noun phrases in sample text portions, reported in Ejerhed (1987), was the following. The regular expression output had 6 errors in 185 noun phrases, i.e. a 3.3% error rate. The stochastic output had 3 errors in 218 noun phrases, i.e. a 1.4% error rate. Both results must be considered good in the absolute sense of an automatic analysis of unrestricted text, but the stochastic method has a clear advantage over the regular expression method. Basic noun phrases can be found, which is of important for clause recognition. The definition of basic clause that was used in this study has the following characteristics: a) it concentrates on certain defining characteristics present at the beginnings of clauses; b) it follows from a particular hypothesis about syntactic working memory: that it is limited to processing one clause at the time; and c) it assumes that the recognition of any beginning of a clause automatically leads to the syntactic closure of the previous clause. It should be clear from the above, that the theoretical reasons for pursuing a recursion-free definition of a basic clause have to do with a theory of linguistic performance, rather than with a theory of linguistic competence, in which memory limitations play no part. It is a hypothesis of the author's current clause-by- clause processing theory, that a unit corresponding to the basic clause is a stable and easily recognizable surface unit, and that it is also an important partial result and building block in the construction of a richer linguistic representation that encompasses syntax as well as semantics and discourse structure. 2.3 A Regular Expression for Basic Clauses Several versions of a regular expression for basic clauses were written by the author and preceded applied to 60 files of Brown corpus tagged text of 2000 words each, newspaper texts A01-A20, scientific texts J01-J20 and fiction texts K01- K20. The first half of the definition of *clause* introduces a few auxiliary definitions: comp for a set of complementizers, punct for a set of punctuation marks, and tense for a set of verb forms that are either certainly tensed (\"BED\" \"BEDZ\" \"BEM\" \"BER\" \"BEZ\" \"DOD\" \"DOZ\" \"HVD\" \"HVZ\" \"MD\" \"VBD\" \"VBZ\") or possibly tensed (\"BE\" \"DO\" \"HV\" \"VB\"). The definition of clause also uses the previously defined *brown-np-regex*. The second and larger part of the definition of *clause* consists of a union of six concatenations. The first defines complete main clauses as consisting of a sequence of an optional coordinating conjunction CC followed by an obligatory basic noun phrase followed by optional non-clausal complements and an optional adverb followed by an obligatory tensed verb followed by anything except the punctuations or complementizers indicated in the list after (not ..., followed by optional punctuation. The second defines clauses introduced by an obligatory CC followed by an optional adverb followed by an obligatory element which is either a tensed or participial verb form, followed by the same clause ending as in the first definition. The third concatenation defines a subordinate clause as starting with an optional coordinating conjunction followed by an obligatory complementizer followed by the same clause ending as in the first and second definitions. The remaining three definitions are of clause fragments rather than full clauses. Consider the following sentence: The man [who liked ice cream,] ate too much. In it, the relative clause makes a basic clause unit that breaks up the main clause into two clause fragments. The third concatenation defines noun phrase fragments that begin with a basic noun phrase followed optionally by one or more prepositional phrases, or sequences of CC np or $ np, followed by the same clause ending as in the other definitions. In the example above, [the man] would be a noun phrase fragment. The fifth concatenation defines verb phrase fragments, e.g. [ate too much]. The sixth concatenation defines clause fragments that are adjuncts, i.e. adverbial phrases, prepositional phrases and adjective phrases. The typical case in which such a fragment is recognized is when it precedes another clause: [On a clear day,] [you can see forever]. 2.4 Output of Regular Expression for Clauses The regular expression in Appendix 1 was automatically expanded into a deterministic fsa for clause recognition by Church's program. This rule compilation will not be described here. An excerpt from the result of applying it to the 60 files mentioned in the introduction to this section is presented in Appendix 2, where the location and nature of hand-corrections have been high- lighted. The hand-correction was guided by the following principles. 1) There should be at most one tensed verb per clause. This inserts a clause boundary after a tensed clause and before a tensed verb in the following kind of case, which the current regular expression matcher does not capture: [The announcement] [that the President was late] [was made late in the afternoon]. 2) There should be a clause boundary after a sentence initial prepositional or adverbial phrase and before the sequence np tensed verb, whether or not they are separated by a comma:[At the summit in Iceland] [Gorbachev insisted ...]. 3) There should be a clause boundary before CC followed by a tensed verb. Although the second concatenation in the clause regex aims at capturing such clauses, it is not always successful in doing so because there is no way, given the current implementation of negation in the regular expression program, to state that a clause should end before a concatenation of items, i.e. before (* CC tense). Only single items can be negated at present. Example: [The Purchasing Departments are well operated] [and follow generally accepted practices]. 4) There should be a clause boundary before a preposition (IN) followed by a wh-word, i.e. before (* IN (+ WDT WPO WP$ WRB WQL)). For the same reason given under 3), there is no way currently to state that a clause should end before such a sequence. Example: [The City Executive Committee deserves praise for the Several interesting observation were made in the course of doing these hand-corrections. For one, there were errors in the Brown corpus assignment of tags, in particular several errors confusing VBD and VBN, and there were errors where the sequence TO VB was tagged IN ΝΝ. More seriously, it turned out that the words as and like, which have the property of functioning either like prepostions IN or subordinating conjunctions CS were always tagged CS, thus leading to incorrect recognition of clauses in many cases. Another problem for recognizing clauses on the basis of identifying tensed verbs was that the tag VB is applied to forms that are either infinitival or present tensed (or subjunctive), depending on context. It would have been better if such forms had been considered lexically ambiguous and given distinct tags. However, by and large the tagged Brown corpus is a very good and useful product, both in the choice of tags, and in the consistency with which they have been applied. Doing the hand-correction also forced the realization that the clause recognition program, like the noun phrase recognition program, dependes crucially on accurate assignment of parts of speech to all words, on order to work well. For this task, Church's stochastic parts program is admirably suited, since it gives correct assignments in a very large number of cases, and it holds the potential of further improvement in its performance with further training. 2.5 Stochastic Recognition of Clauses As stated before, the regex *clause* was applied to sixty texts in the Brown corpus, and the output was hand-corrected. The hand-corrected files, containing an estimated total of at least 20,000 basic clauses, including clause fragments, were then used as training material for a stochastic recognition program. The training consisted of observing the location of clause opens and clause closes, and a special training specifically in locating tensed verbs. After training, the stochastic parts program and thereafter the stochastic clause recognizer was applied by K. Church to a large amount of Associated Press newswire text from May 26, 1987 (526 blocks, 2381353(8) bytes). An excerpt of the result is presented in Appendix 3. The result, again, is strikingly good. A comparison of the nature and amount of errors in recognizing basic clauses in a sample of from the stochastic clause program, can be made on the basis of Tables 1 and 2 at the end. It appears that the stochastic program is more successful than the current regular expression method. However, certain improvements in the regex program could change that. What is needed is the facility to process generalized regular expressions, which admit the operations of complement and intersection, in addition to the operations of concatenation, union and Kleene star that characterize regular expressions. In any case there are some interesting differences in the kinds of errors made by the current regex program and the stochastic one for recognizing clauses. The regex program systematically errs by underrecognizing, never by overrecognizing, and in the selected portions that were scored, it only puts a few clause boundaries in the wrong place. It misses lots of clause boundaries, but the ones it gets are mostly correct. The stochastic program, on the other hand, is able to get many clause boundaries correctly that elude the regular expression matcher, e.g. clauses not introduced by complementizers. The stochastic program errs both by overrecognizing and underrecognizing clauses, and sometimes it also places the clause open or clause close in the wrong place. Some cases of incorrect clause recognition are due to incorrect assignments of parts of speech to words. However, the total number of errors with the stochastic method (21) is smaller than the total number of errors with the regex method (40), for approximately the same number of clauses to be recognized, 304 versus 308. This is a very surprising outcome indeed, and if taken literally, without any further weighting of the different types of errors, it means that the error rate for the stochastic method for recognizing clauses is 6.5%, as compared with 13% for the regex method. 3. On the Relation between Clauses and Into. ion Units Finding basic lause units in arbitrary text is necessary in order to locate tonal minor phrases, which, in addition to a phrase accent, also have a boundary tone, and, particularly at slow rates of speech, a pause at the end of the phrase. The current experiment in text analysis has been concerned primarily with informative rather than imaginative prose, and envisages applications of the text-to-speech system to the reading of In the current Bell Labs text-to-speech system, tonal minor phrase boundaries are identified on the basis of commas, and tonal major phrase boundaries are identified on the basis of periods. Finding more tonal minor phrase boundaries by using syntactic structure, in addition to punctuation, is the problem we are trying to address with the methods described in this paper. In order to know where tonal minor phrase boundaries actually occur in the reading of informative texts, which typically have very long sentences (an average of 21 words compared with 14 words in general fiction based on Brown corpus data), it would be necessary to make recordings of several persons reading both authentic and prepared texts in a rhetorically explicit way, to borrow a phrase from Beckman & Pierrehumbert (1986), and then make extensive speech analyses of them, particularly of fundamental frequency movements and pauses. In the absence of such data for American English, the following kinds of boundaries between clauses and clause fragments were hypothesized to constitute intonation breaks with the status of tonal minor phrase boundaries. They are marked with # in the examples below. a) After sentence initial adverbials and before np tense: [At the summit in Iceland] # [Gorbachev insisted ...] b) After a relative clause and before a tensed verb: [A House Committee] [which heard his local option proposal] # [is expected] [to give it a favorable report.] c) After other noun phrases with clausal complements and before a tensed verb: [The announcement] [that the President was late] # [was made by the Press Secretary to the waiting journalists.] d) Before a set of complementizers categorized CS in the Brown corpus, it is frequently the case that there is an intonation break: [that/CS ...], [whether/CS...], [if/CS ...], [since/CS ...], [because/CS...], [as/CS ...]. However, there are some exceptions to this, in particular: (i) Comparatives: [This is not as/QL fast/JJ] [as/CS I would like ... ] or [The theorem is more/RBR general/JJ] [than/CS what we have described] (ii) The words as/CS and like/CS when used as are not subjects of clauses: [Jenkins left the White House in 1984,] [and joined Wedtech] [as/CS its director of marketing two years later.] For testing purposes, short passages of seven consecutive sentences each from the Brown files, and four sentences each from the AP newswire stories were synthesized by the author, using the Bell Labs text-to-speech system. Those boundaries between clauses and clause fragments that are identified above were implemented in the same way that commas are, i.e. with a phrase accent belonging to the tonal minor phrase, final lengthening, a boundary tone, and a short pause of 200 ms. The results have not yet been subjected to perceptual tests. There are some studies of the relation between clause units and intonation units that provide relevant data for future work. Garding (1967) studied prosodic features in spontaneous and read Swedish speech. She found that in the spontaneous speech, pauses were equally divided between syntactic pauses and hesitation pauses, a syntactic pause being defined as one that coincides with a syntactic boundary. In the read speech, all pauses were syntactic pauses: \"They appear between main clause and subordinate clause, before adverbial modifiers and between the different parts of an enumeration. The pause length is shortest in enumerations and before relative clauses (4-10 cs) and longest before adverbial modifiers and between complete sentences.\" (p. 48). In a study of the intonational properties of relative clauses in British English, Taglicht (1977) compared the speech of a news broadcast with impromptu speech, and found that both genres separated nonrestrictive relative clauses prosodically. The news broadcast also separated a large proportion (71%) of the restrictive relative clauses prosodically. A recent and very extensive study of the grammtical properties of intonation units, or tone units (TU) is Altenberg (1987). He studied a monologue of 48 minutes duration from the London-Lund Corpus of spoken English, and his results concerning the correlation of clause boundaries and tone unit boundaries are presented in Table 3 at the end. 4. Conclusion The study reported above shows that basic and surface recognizable units in the definitions they were given here, and that both finitary and stochastic methods can be used to find them in unrestricted text with a high degree of success. The comparison between the error rate of these two methods showed that the stochastic method performed better both in the recognition of basic noun phrases and basic clauses, which is an unexpected result. REFERENCES [1] Allen, Jonathan., Hunnicutt, M.Sharon. & Klatt, Dennis, 1987, From text to speech. The MITalk system, Cambridge, Cambridge University Press. [2] Altenberg, Bengt, 1987, Prosodic patterns in spoken English. Studies in the correlation between prosody and grammar for text-to-speech conversion. Lund Studies in English 76, Lund, Lund University Press. [3] Beckman, Mary & Pierrehumbert, Janet, 1986, Intonational structure in Japanese and English, Phonology Yearbook 3(1986), 255-309. [4] Church, Kenneth W., 1982, On memory limitations in natural language processing, Bloomington, Indiana, Indiana University Linguistics Club. [5] Church, Kenneth W., 1987, A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text, AT&T Bell Labs, (in this volume). [6] Ejerhed, Eva, 1987, Finding Noun Phrases and Clauses in Unrestricted Text: On the Use of Stochastic and Finitary Methods in Text Analysis, (ms), AT&T Bell Labs. [7] Ejerhed, Eva & Church, Kenneth W., 1983, Finite State Parsing, in F. Karlsson (ed.), Papers from the Seventh Scandinavian Conference of Linguistics, Helsinki. University of Helsinki, Department of General Linguistics. [8] Francis, Nelson & Kucera, Henry, 1982, Frequency Analysis of English Usage, Lexicon and Grammar, Boston, Houghton Mifflin Company. [9] Garding, Eva, 1967, Prosodiska drag i (ed.), Svenskt talsprak, Stockholm, Almqvist & Wiksell, 40-85. [10] Jarvella, Robert, 1971, Syntactic Processing of Connected Speech, JVLVB 10, 409-416(1971). [11] Jarvella, Robert & Pisoni, D.B., 1970, The Relation between Syntactic and Perceptual Units in Speech Processing, JASA, 1970, 48, 84 (Α). [12] Just, Marcel & Carpenter, Patricia, 1984, Using Eye Fixations to Study Reading Comprehension, in D. Kieras & M. Just (eds), New Methods in Reading Comprehension Research, Lawrence Earlbaum Associates, Hillsdale, New Jersey, 151-182. [13] Langendoen, D. Terrence, 1975, Finite- State Parsing of Phrase-Structure Languages and the Status of Readjustment Rules in Grammar, Linguistic Inquiry, Vol VI (1975), Number 4. [14] Liberman, Mark & Buchsbaum, Adam, 1985, Structure and Usage of Current Bell Labs Text to Speech Program, (ms), AT&T Bell Labs. [15] Taglicht, J., 1977, Relative clauses as postmodifiers: meaning syntax and intonation, in W.-D. Bald & R. Ilson (eds.), Studies in English usage, Frankfurt/M, Peter Lang, 73-107. [16] Wright, Charles, Bachenko, Joan & Fitzpatrick, Eileen, 1986, The contribution of parsing to prosodic phrasing in an experimental text-to- speech system, Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, Columbia University. APPENDIX 1 Regular expression for basic clauses. (defvar *clause* (let* ((comp '(+ \"CS\" \"TO\" \"WDT\" \"WRB\" \"WPS\" \"WPO\" \"WP$\" \"WQL\")) (punct '(+ \",\" \".\" \"--\" \":\")) (tense '(+ \"BE\" \"BED\" \"BEDZ\" \"BEM\" \"BER\" \"BEZ\" \"DO\" \"DOD\" \"DOZ\" \"HV\" \"HVD\" \"HVZ\" \"MD\" \"VB\" \"VBD\" \"VBZ\"))) ;; main clause: (CC) np tense ... '(+ (* (cl-user::opt \"CC\") ,*brown-np-regex* (cl-user::opt (++ (* (+ \"CC\" \"IN\" \"$\") ,*brown-np-regex*))) (cl-user::opt (+ \"RB\" \"RBR\")) ,tense (cl-user::opt (++ (not \",\" \".\" \"--\" \":\" \"CS\" \"TO\" \"WDT\" \"WRB\" \"WPS\" \"WPO\" \"WP$\" \"WQL\"))) (cl-user::opt,punct)) (* \"CC\" ; main clause: CC tense ... (cl-user::opt (+ \"RB\" \"RBR\")) (+,tense \"VBG\" \"VBN\" \"BEG\" \"HVG\") (cl-user::opt (++ (not \",\" \".\" \"--\" \":\" \"CS\" \"TO\" \"WDT\" \"WRB\" \"WPS\" \"WPO\" \"WP$\" \"WQL\"))) (cl-user::opt,punct)) (* (cl-user::opt \"CC\") ; sub clause (++,comp) (cl-user::opt (++ (not \",\" \".\" \"--\" \":\" \"CS\" \"TO\" \"WDT\" \"WRB\" \"WPS\" \"WPO\" \"WP$\" \"WQL\"))) (cl-user::opt,punct)) (* (cl-user::opt \"CC\") ; np clause fragment ,*brown-np-regex* (cl-user::opt (++ (* (+ \"CC\" \"IN\" \"$\") ,*brown-np-regex*))) (cl-user::opt (++ (not \",\" \".\" \"--\" \":\" \"CS\" \"TO\" \"WDT\" \"WRB\" \"WPS\" \"WPO\" \"WP$\" \"WQL\"))) (cl-user::opt,punct)) ;; vp clause fragment (* (+,tense \"VBG\" \"VBN\" \"BEG\" \"HVG\") (cl-user::opt (++ (not \",\" \".\" \"--\" \":\" \"CS\" \"TO\" \"WDT\" \"WRB\" \"WPS\" \"WPO\" \"WP$\" \"WQL\"))) (cl-user::opt,punct)) ;; adjunt clause fragment (* (cl-user::opt \"CC\") (++ (* (+ \"RB\" \"RBR\" \"RP\" \"QL\" \"*\" \"NR\" \"JJ\" \"JJR\" \"IN\",*brown-np-regex* ))) (cl-user::opt (++ (not \",\" \".\" \"--\" \":\" \"CS\" \"TO\" \"WDT\" \"WRB\" \"WPS\" \"WPO\" \"WP$\" \"WQL\"))) APPENDIX 2 Sample of output of applying the regular expression *clause* as defined in Appendix 1, to Brown newspaper story A01. Hand-corrections are marked by double asterisks for underrecognized, and single asterisks for overrecognized clause boundaries. [the/AT Fulton/NP-TL County/NN-TL Grand/JJ- TL Jury/NN-TL said/VBD Friday/NR ** an/AT investigation/NN of/IN Atlanta/NP 's/$ recent/JJ primary/NN election/NN produced/VBD no/AT evidence/NN] [that/CS any/DTI irregularities/NNS took/VBD place/NN./.] [the/AT jury/NN further/RBR said/VBD in/IN term-end/NN presentments/NNS] [that/CS the/AT City/NN-TL Executive/JJ-TL Committee/NN-TL ,/,] [which/WDT had/HVD over-all/JJ charge/NN of/IN the/AT election/NN ,/,] [deserves/VBZ the/AT praise/NN and/CC thanks/NNS of/IN the/AT City/NN-TL of/IN-TL Atlanta/NP-TL for/IN the/AT manner/NN ** in/IN] [which/WDT the/AT election/NN was/BEDZ conducted/VBN ./.] [the/AT September-October/NP term/NN jury/NN had/HVD been/BEN charged/VBN by/IN Fulton/NN-TL Superior/JJ-TL Court/NN- TL Judge/NN-TL Durwood/NP Pye/NP] [to/TO investigate/VB reports/NNS of/IN possible/JJ irregularities/NNS in/IN the/AT hard-fought/JJ primary/NN] [which/WDT was/BEDZ won/VBN by/IN Mayor-nominate/NN-TL Ivan/NP Allen/NP Jr./NP ./.] [only/RB a/AT relative/JJ handful/NN of/IN such/JJ reports/NNS was/BEDZ received/VBN ,/,] [the/AT jury/NN said/VBD ,/,] [IN considering/IN the/AT widespread/JJ interest/JJ in/IN the/AT election/NN ,/,] [the/AT number/NN of/IN voters/NNS and/CC the/AT size/NN of/IN this/DT city/NN ./.] [the/AT jury/NN said/VBD ** it/PPS did/DOD find/VB] [that/CS many/AP of/IN Georgia/NP 's/$ registration/NN and/CC election/NN laws/NNS are/BER outmoded/JJ or/CC inadequate/JJ and/CC often/RB ambiguous/JJ ./.] [it/PPS recommended/VBD] [that/CS Fulton/NP legislators/NNS act/VB] [to/TO have/HV these/DTS laws/NNS studied/VBN and/CC revised/VBN to/IN the/AT end/NN of/IN modernizing/VBG and/CC improving/VBG them/PPO ./.] [the/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN other/AP topics/NNS,/,] [among/IN them/PPO the/AT Atlanta/NP and/CC Fulton/NP-TL County/NN- TL purchasing/VBG departments/NNS which/WDT it/PPS said/VBD are/BER well/QL operated/VBN ** and/CC follow/VB generally/RB accepted/VBN practices/NNS] [which/WDT inure/VB to/IN the/AT best/JJT interest/NN of/IN both/ABX governments/NNS ./.] APPENDIX 3 Sample of output of stochastic procedure for finding clause boundaries. Tensed verbs should be in bold face. In the recognition of these clauses, the constraint was enforced that there be at most one tensed verb per clause. Hand- corrections marked as in Appendix 2. [former/AP U.S./NP Attorney/NN General/NN Ramsey/NP Clark/NP said/VBD! Monday/NR] [he/PPS believed/VBD!] [he/PPS had/HVD! found/VBN evidence/NN of/IN a/AT growing/VBG CIA/NP role/NN in/IN the/AT Philippines/NPS '/$ war/NN against/IN communist/NN rebels/NNS ./.] [Clark/NP,/,] [who/WPS arrived/VBD! last/AP week/NN] * [as/CS the/AT head/NN of/IN a/AT private/JJ,/,] [human/JJ rights/NNS team/NN,/,] [said/VBD!] [he/PPS hopes/VBZ!] [to/TO! document/VB the/AT evidence/NN] [and/CC present/VB it/PPO to/IN U.S./NP Secretary/NN of/IN State/NN George/NP P./NP Shultz/NP ./.] [our/PP$ concern/NN is/VBZ! the/AT role/NN of/IN the/AT United/VBN States/NNS,/,] [Clark/NP told/VBD! a/AT news/NN conference/NN ./.[ [we/PPSS believe/VB!] [we/PPSS can/MD! see/VB,/,] [and/CC we hope/VB!] [to/TO! be/BE able/JJ] [to/TO! document/VB] * [before/CS we/PPSS are/BER!] * [through/RP in/IN our/PP$ report/NN,/,] [evidence/NN clearly/RB establishing/VBG the/AT implementation/NN of/IN a/AT low-intensity/JJ campaign/NN here/RB,/,] [with/IN violence/NN ,/,] [to/TO! kill/VB off/RP all/ABN opposition/NN,,] [every/AT opposition/NN to/IN authority/NN,/,] [to/IN militarism/NN./.] [Ralph/NP McGehee/NP,/,] [a/AT former/AP Central/JJ Intelligence/NN Agency/NN employee/NN ,/,] [said/VBD!] [he/PPS CIA/NP influence/NN in/IN the/AT Philippine/JJ military/NN 's/$ operations/NNS against/IN the/AT communist/JJ New/JJ People/NNS 's/$ Army/NN ./.] [he/PPS cited/VBD! military/JJ search-and- destroy/JJ missions/NNS,/,] [forced/VBN evacuation/NN of/IN civilians/NNS from/IN rebel-held/JJ areas/NNS and/CC the/AT increase/NN in/IN the/AT strength/NN of/IN civilian/JJ anti-communist/JJ vigilante/JJ groups/NNS./.] [the/AT allegations/NNS of/IN growing/VBG U.S./NP involvement/NN in/IN the/AT support/NN of/IN president/NN Corazon/NP Aquino/NP 's/$ government/NN came/VBD! with/IN claims/NNS by/IN Philippine/JJ leftists/NNS] [that/CS right-wing/JJ death/NN squads/NNS are/BER! operating/VBG freely/RB against/IN suspected/VBN leftists/NNS ./.] TABLES Table 1. Errors in regex recognition of clauses. Regex Output Story Sentences Clauses (before) Clauses (after) Under Wrong-place a01 28 86 104 18 1 j01 28 98 107 9 1 k01 28 87 97 10 1 Total 84 271 308 37 3 Table 2. Errors in stochastic recognition of clauses. Stochastic Output Story Sentences Clauses (before) Clauses (after) Under Over Wrong-place STORY-1 15 64 64 0 1 1 STORY-2 15 52 51 0 1 0 STORY-3 15 45 46 1 0 2 STORY-4 30 141 143 8 4 3 Total 75 302 304 9 6 6 Table 3. The cooccurrence of clause boundaries and tone unit boundaries (from Altenberg 1987:57 Table 4:3). Clause boundaries cooccurring with a TU boundary Clause boundary Total TU % After initial clauses 29 29 100 Around medial clauses 15 15 100 Before finite adverbial clauses 46 46 100 Before adverbial ing-clauses 14 14 100 Before nonrestrictive relative clauses 26 26 100 Before asyndetic clause coordination 15 15 100 Around nonrestrictive appositive clauses 3 3 100 After postmodifying clauses 67 66 99 Before syndetic clause coordination 153 150 98 Before nonfinite postmodifying clauses 25 19 76 Before restrictive relative clauses 26 18 69 After comment clauses 13 9 69 Before adverbial infinitive clauses 12 8 67 Before comment clauses 13 8 62 Before nominal that-clauses 32 19 59 Before direct speech 7 4 57 Before nominal relative/interrogative clauses 16 7 44 Before nonfinite nominal clauses 21 7 33 Before clauses as prepositional complement 21 1 5"
  },
  {
    "title": "Predicting Automatic Speech Recognition Performance Using Prosodic Cues",
    "abstract": "In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodic features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition. We present analytic results indicating that there are significant prosodic differences between correctly and incorrectly recognized turns in the TOOT train information corpus. We then present machine learning results showing how the use of prosodic features to automatically predict correct versus incorrectly recognized turns improves over the use of acoustic confidence scores alone.",
    "content": "1 Introduction One of the central tasks of the dialogue manager in most current spoken dialogue systems (SDSs) is error handling. The automatic speech recognition (ASR) component of such systems is prone to error, especially when the system has to operate in noisy conditions or when the domain of the system is large. Given that it is impossible to fully prevent ASR er- rors, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can take appropriate action, since users have con- siderable difficulty correcting incorrect information that is presented by the system as true (Krahmer et al., 1999). Such action may include verifying the user's input, reprompting for fresh input, or, in cases where many errors have occurred, changing the in- teraction strategy or switching the caller to a human attendant (Smith, 1998; Litman et al., 1999; Langk- ilde et al., 1999). Traditionally, the decision to re- ject a recognition hypothesis is based on acoustic confidence score thresholds, which provide a relia- bility measure on the hypothesis and are set in the application (Zeljkovic, 1996). However, this process often fails, as there is no simple one-to-one mapping between low confidence scores and incorrect recog- nitions, and the setting of a rejection threshold is a matter of trial and error (Bouwman et al., 1999). Also, some incorrect recognitions do not necessarily lead to misunderstandings at a conceptual level (e.g. \"a.m.\" recognized as \"in the morning\"). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Wein- traub et al., 1996), speaker gender and age, na- tive versus non-native speaker status, and, in gen- eral, the deviation of new speech from the training data. Some of this variation is linked to prosody, as prosodic differences have been found to character- ize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Sev- eral other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interac- tions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1997). In this paper we present results of empirical stud- ies testing the hypothesis that prosodic features pro- vide an important clue to ASR performance. We first present results comparing prosodic analyses of correctly and incorrectly recognized speaker turns in TOOT, an experimental SDS for obtaining train information over the phone. We then describe ma- chine learning experiments based on these results that explore the predictive power of prosodic fea- tures alone and in combination with other automat- ically available information, including ASR confi- dence scores and recognized string. Our results in- dicate that there are significant prosodic differences between correctly and incorrectly recognized utter- ances. These differences can in fact be used to pre- dict whether an utterance has been misrecognized, with a high degree of accuracy. 2 The TOOT Corpus Our corpus consists of a set of dialogues between humans and Toot, an SDS for accessing train schedules from the web via telephone, which was collected to study both variations in SDS strat- egy and user-adapted interaction (Litman and Pan, 1999). Toot is implemented on a platform com- bining ASR, text-to-speech, a phone interface, a finite-state dialogue manager, and application func- tions (Kamm et al., 1997). The speech recognizer is a speaker-independent hidden Markov model system with context-dependent phone models for telephone speech and constrained grammars for each dialogue state. Confidence scores for recognition were avail- able only at the turn, not the word, level (Zeljkovic, 1996). An example Toot dialogue is shown in Fig- ure 1. Subjects performed four tasks with one of sev- eral versions of TOOT, that differed in terms of locus of initiative (system, user, or mixed), confirmation strategy (explicit, implicit, or none), and whether these conditions could be changed by the user during the task. Subjects were 39 students, 20 native speak- ers of standard American English and 19 non-native speakers; 16 subjects were female and 23 male. Dia- logues were recorded and system and user behavior logged automatically. The concept accuracy (CA) of each turn was manually labeled by one of the exper- imenters. If the ASR output correctly captured all the task-related information in the turn (e.g. time, departure and arrival cities), the turn was given a CA score of 1 (a semantically correct recognition). Otherwise, the CA score reflected the percentage of correctly recognized task information in the turn. The dialogues were also transcribed by hand and these transcriptions automatically compared to the ASR recognized string to produce a word error rate (WER) for each turn. Note that a concept can be correctly recognized even though all words are not, so the CA metric does not penalize for errors that are unimportant to overall utterance interpretation. For the study described below, we examined 1994 user turns from 152 dialogues in this corpus. The speech recognizer was able to generate a recognized string and an associated acoustic confidence score per turn for 1975 of these turns.¹ 1410 of these 1975 turns had a CA score of 1 (for an overall conceptual accuracy score of 71%) and 961 had a WER of 0 (for an overall transcription accuracy score of 49%, with a mean WER per turn of 47%). ¹For the remaining turns, ASR output \"no speech\" (and TOOT played a timeout message) or \"garbage\" (TOOT played a rejection message). 3 Distinguishing Correct from Incorrect Recognitions We first looked for distinguishing prosodic charac- teristics of misrecognitions, defining misrecognitions in two ways: a) as turns with WER>0; and b) as turns with CA<1. As noted in Section 1, previous studies have speculated that hyperarticulated speech (slower and louder speech which contains wider pitch excursions) may be associated with recognition fail- ure. So, we examined the following features for each user turn:² • maximum and mean fundamental frequency values (FO Max, FO Mean) • maximum and mean energy values (RMS Max, RMS Mean) • total duration • length of pause preceding the turn (Prior Pause) • speaking rate (Tempo) • amount of silence within the turn (% Silence) FO and RMS values, representing measures of pitch excursion and loudness, were calculated from the output of Entropic Research Laboratory's pitch tracker, get_f0, with no post-correction. Timing vari- ation was represented by four features. Duration within and length of pause between turns was com- puted from the temporal labels associated with each turn's beginning and end. Speaking rate was ap- proximated in terms of syllables in the recognized string per second, while % Silence was defined as the percentage of zero frames in the turn, i.e., roughly the percentage of time within the turn that the speaker was silent. These features were chosen based upon previous findings (see Section 1) and observa- tions from our data. To ensure that our results were speaker indepen- dent, we calculated mean values for each speaker's recognized turns and their misrecognized turns for every feature. Then, for each feature, we created vectors of speaker means for recognized and misrec- ognized turns and performed paired t-tests on the vectors. For example, for the feature \"FO max\", we calculated mean maxima for misrecognized turns and for correctly recognized turns for each of our thirty-nine speakers. We then performed a paired t-test on these thirty-nine pairs of means to de- rive speaker-independent results for differences in FO maxima between correct and incorrect recognitions. Tables 1 and 2 show results of these compar- isons when we calculate misrecognition in terms of ²While the features were automatically computed, turn beginnings and endings were hand segmented in dialogue-level speech files, as the turn-level files created by TOOT were not available. Toot: Hi, this is AT&T Amtrak schedule system. This is TOOT. How may I help you? User: I want the trains from New York City to Washington DC on Monday at 9:30 in the evening. Toot: Do you want me to find the trains from New York City to Washington DC on Monday approximately at 9:30 in the evening now? User: Yes. Toot: I am going to get the train schedule for you . . . Figure 1: Example Dialogue Excerpt with TOOT. Table 1: Comparison of Misrecognized (WER>0) vs. Recognized Turns by Prosodic Feature Across Speakers. Table 2: Comparison of Misrecognized (CA<1) vs. Recognized Turns by Prosodic Feature Across Speakers. Feature T-stat Mean Misrec'd P - Rec'd *F0 Max 7.83 30.31 Hz 0 *FO Mean 3.66 4.12 Hz 0 *RMS Max 5.70 235.93 0 RMS Mean -.57 -8.50 .57 *Duration 10.30 2.20 sec 0 *Prior Pause 5.55 .35 sec 0 Tempo -.05 .15 sps .13 *% Silence -5.15 -.06% 0 *significant at a 95% confidence level (p<.05) Feature T-stat Mean Misrec'd P - Rec'd *F0 Max 5.60 29.64 Hz 0 FO Mean 1.70 2.10 Hz .10 *RMS Max 2.86 173.87 .007 RMS Mean -1.85 -27.75 .07 *Duration 9.80 2.15 sec 0 *Prior Pause 4.05 .38 sec 0 *Tempo -4.21 -.58 sps 0 % Silence -1.42 -.02% .16 *significant at a 95% confidence level (p< .05) WER>0 and CA<1, respectively. These results in- dicate that misrecognized turns do differ from cor- rectly recognized ones in terms of prosodic features, although the features on which they differ vary slightly, depending upon the way \"misrecognition\" is defined. Whether defined by WER or CA, mis- recognized turns exhibit significantly higher F0 and RMS maxima, longer durations, and longer preced- ing pauses than correctly recognized speaker turns. For a traditional WER definition of misrecognition, misrecognitions are slightly higher in mean F0 and contain a lower percentage of internal silence. For a CA definition, on the other hand, tempo is a signif- icant factor, with misrecognitions spoken at a faster rate than correct recognitions — contrary to our hy- pothesis about the role of hyperarticulation in recog- nition error. While the comparisons in Tables 1 and 2 were made on the means of raw values for all prosodic fea- tures, little difference is found when values are nor- malized by value of first or preceding turn, or by con- verting to z scores. From this similarity between the performance of raw and normalized values, it would seem to be relative differences in speakers' prosodic values, not deviation from some 'acceptable' range, that distinguishes recognition failures from success- ful recognitions. A given speaker's turns that are higher in pitch or loudness, or that are longer, or that follow longer pauses, are less likely to be recog- nized correctly than that same speaker's turns that are lower in pitch or loudness, shorter, and follow shorter pauses — however correct recognition is de- fined. It is interesting to note that the features we found to be significant indicators of failed recognitions (FO excursion, loudness, long prior pause, and longer du- ration) are all features previously associated with hyperarticulated speech. Since prior research has suggested that speakers may respond to failed recog- nition attempts by hyperarticulating, which itself may lead to more recognition failures, had we in fact simply identified a means of characterizing and iden- tifying hyperarticulated speech prosodically? Since we had independently labeled all speaker turns for evidence of hyperarticulation (two of the authors labeled each turn as \"not hyperarticulated\", \"some hyperarticulation in the turn\", and \"hyperar- ticulated\", following Wade et al. (1992)), we were able to test this possibility. We excluded any turn either labeler had labeled as partially or fully hy- perarticulated, and again performed paired t-tests on mean values of misrecognized versus recognized turns for each speaker. Results show that for both WER-defined and CA-defined misrecognitions, not only are the same features significant differentiators when hyperarticulated turns are excluded from the analysis, but in addition, tempo also is significant for WER-defined misrecognition. So, our findings for the prosodic characteristics of recognized and of misrecognized turns hold even when perceptibly hy- perarticulated turns are excluded from the corpus. 4 Predicting Misrecognitions Using Machine Learning Given the prosodic differences between misrecog- nized and correctly recognized utterances in our corpus, is it possible to predict accurately when a particular utterance will be misrecognized or not? This section describes experiments using the ma- chine learning program RIPPER (Cohen, 1996) to au- tomatically induce prediction models, using prosodic as well as additional features. Like many learning programs, RIPPER takes as input the classes to be learned, a set of feature names and possible values, and training data specifying the class and feature values for each training example. RIPPER outputs a classification model for predicting the class of fu- ture examples. The model is learned using greedy search guided by an information gain metric, and is expressed as an ordered set of if-then rules. Our predicted classes correspond to correct recog- nition (T) or not (F). As in Section 3, we examine both WER-defined and CA-defined notions of cor- rect recognition, and represent each user turn as a set of features. The features used in our learning experiments include the raw prosodic features in Ta- bles 1 and 2 (which we will refer to as the feature set \"Prosody\"), the hyperarticulation score discussed in Section 3, and the following additional potential pre- dictors of misrecognition (described in Section 2): • ASR grammar • ASR confidence • ASR string • system adaptability • dialogue strategy • task number • subject • gender • native speaker The first three features are derived from the ASR process (the context-dependent grammar used to recognize the turn, the turn-level acoustic confidence score output by the recognizer, and the recognized string). We included these features as a baseline against which to test new methods of predicting misrecognitions, although, currently, we know of no ASR system that includes recognized string in its rejection calculations.4 TOOT itself used only the Note that, while the entire recognized string is provided to the learning algorithm, RIPPER rules test for the presence of particular words in the string. first two features to calculate rejections and ask the user to repeat the utterance, whenever the confi- dence score fell below a pre-defined grammar-specific threshold. The other features represent the exper- imental conditions under which the data was col- lected (whether users could adapt TOOT's dialogue strategies, TOOT's initial initiative and confirmation strategies, experimental task, speaker's name and characteristics). We included these features to de- termine the extent to which particulars of task, sub- ject, or interaction influenced ASR success rates or our ability to predict them; previous work showed that these factors impact Toot's performance (Lit- man and Pan, 1999; Hirschberg et al., 1999). Except for the task, subject, gender, native language, and hyperarticulation scores, all of our features are au- tomatically available. Table 3 shows the relative performance of a num- ber of the feature sets we examined; results here are for misrecognition defined in terms of WER.5 A baseline classifier for misrecognition, predicting that ASR is always wrong (the majority class of F), has an error of 48.66%. The best performing feature set includes only the raw prosodic and ASR features and reduces this error to an impressive 6.53% +/- .63%. Note that this performance is not improved by adding manually labeled features or experimen- tal conditions: the feature set corresponding to ALL features yielded the statistically equivalent 6.68% +/- 0.63%. With respect to the performance of prosodic fea- tures, Table 3 shows that using them in conjunction with ASR features (error of 6.53%) significantly out- performs prosodic features alone (error of 12.76%), which, in turn, significantly outperforms any single prosodic feature; duration, with an error of 17.42%, is the best such feature. Although not shown in the table, the unnormalized prosodic features sig- nificantly outperform the normalized versions by 7- 13%. Recall that prosodic features normalized by first task utterance, by previous utterance, or by z scores showed little performance difference in the analyses performed in Section 3. This difference may indicate that there are indeed limits on the ranges in features such as FO and RMS maxima, duration and preceding pause within which recognition per- formance is optimal. It seems reasonable that ex- treme deviation from characteristics of the acoustic training material should in fact impact ASR perfor- mance, and our experiments may have uncovered, if not the critical variants, at least important acoustic correlates of them. However, it is difficult to com- 5 The errors and standard errors (SE) result from 25-fold cross-validation on the 1975 turns where ASR yielded a string and confidence. When two errors plus or minus twice the stan- dard error do not overlap, they are statistically significantly different. Table 3: Estimated Error for Predicting Misrecognized Turns (WER>0). Features Used Error SE Prosody, ASR Confidence, ASR String, ASR Grammar 6.53% .63 ALL 6.68% .63 Prosody, ASR String 7.34% .75 Prosody, ASR Confidence, ASR String, ASR Grammar 9.01% .70 Prosody, ASR Confidence, ASR Grammar 10.63% .88 Prosody, ASR Confidence 10.99% .87 Prosody 12.76% .79 ASR String 15.24% 1.11 Duration 17.42% .88 ASR Confidence, ASR Grammar 17.77% .72 ASR Confidence 22.23% 1.16 ASR Grammar 26.28% .84 Tempo 32.76% 1.03 Hyperarticulation 35.24% 1.46 % Silence 36.46% .79 Prior Pause 36.61% .97 FO Max 38.73% .82 RMS Max 42.23% .96 FO Mean 46.33% 1.10 RMS Mean 48.35% 1.15 Majority Baseline 48.66% 1.15 pare our machine learning results with the statisti- cal analyses, since a) the statistical analyses looked at only a single prosodic variable at a time, and b) data points for that analysis were means calculated per speaker, while the learning algorithm operated on all utterances, allowing for unequal contributions by speaker. We now address the issue of what prosodic fea- tures are contributing to misrecognition identifica- tion, relative to the more traditional ASR tech- niques. Do our prosodic features simply correlate with information already in use by ASR systems (e.g., confidence score, grammar), or at least avail- able to them (e.g., recognized string)? First, the error using ASR confidence score alone (22.23%) is significantly worse than the error when prosodic features are combined with ASR confidence scores (10.99%) — and is also significantly worse than the use of prosodic features alone (12.76%). Similarly, the error using ASR confidence scores and the ASR grammar (17.77%) is significantly worse than prosodic features alone (12.76%). Thus, prosodic features, either alone or in conjunction with tradi- tional ASR features, significantly outperform these traditional features alone for predicting WER-based misrecognitions. Another interesting finding from our experiments is the predictive power of information available to current ASR systems but not made use of in calcu- lating rejection likelihoods, the identity of the recog- nized string. This feature is in fact the best perform- ing single feature in predicting our data (15.24%). And, at a 95% confidence level, the error using ASR confidence scores, the recognized string, and grammar (9.01%) matches the performance of our best performing feature set (6.53%). It seems that, at least in our task and for our ASR system, the appearance of particular words in the recognized strings is an extremely useful cue to recognition accuracy. So, even by making use of information currently available from the traditional ASR process, ASR systems could improve their performance on identifying rejections by a considerable margin. A caveat here is that this feature, like grammar state, is unlikely to generalize from task to task or recognizer to recognizer, but these findings suggest that both should be considered as a means of improving rejection performance in stable systems. The classification model learned from the best performing feature set in Table 3 is shown in Figure 2.6 The first rule RIPPER finds with this feature set is that if the user turn is less than 9 seconds and the recognized string contains the word \"yes\" (and possibly other words as well), with an acoustic confidence score ≥ -2.6, then predict that the turn will be correctly recognized. Note that all of the prosodic features 6 Rules are presented in order of importance in classifying data. When multiple rules are applicable, RIPPER uses the first rule. 7The confidence scores observed in our data ranged from a high of -0.087662 to a low of -9.884418. if (duration ≤ 0.897073) ∧ (confidence ≥ -2.62744) ∧ (string contains 'yes') then T if (duration ≤ 1.03872) ∧ (confidence ≥ -2.69775) ∧ (string contains 'no') then T if (duration ≤ 0.982051) ∧ (confidence ≥ -1.99705) ∧ (tempo ≥ 3.1147) then T if (duration ≤ 0.813633) ∧ (duration ≥ 0.642652) ∧ (confidence ≥ -3.33945) ∧ (FO Mean ≥ 176.794) then T if (duration ≤ 1.30312) ∧ (confidence ≥ -3.37301) ∧ (% silences ≥ 0.647059) then T if (duration ≤ 0.610734) ∧ (confidence ≥ -3.37301) ∧ (% silences ≥ 0.521739) then T if (duration ≤ 0.982051) ∧ (string contains 'Baltimore') then T if (duration ≤ 0.982051) ∧ (string contains 'no') then T if (duration ≤ 1.1803) ∧ (confidence ≥ -2.93085) ∧ (grammar = date) then T if (duration ≤ 1.09537) ∧ (confidence ≥ -2.30717) ∧ (% silences ≥ 0.356436) ∧ (FO Max ≥ 249.225) then T if (duration ≤ 0.868743) ∧ (confidence ≥ -4.14926) ∧ (% silences ≥ 0.51923) ∧ (FO Max ≥ 205.296) then T if (duration ≤ 1.18036) ∧ (string contains 'Philadelphia') then T else F Figure 2: Ruleset for Predicting Correctly Recognized Turns (WER = 0) from Prosodic and ASR Features. tures except for RMS mean, max, and prior pause appear in at least one rule, and that the features shown to be significant in our statistical analyses (Section 3) are not the same features as in the rules. But, as noted above, our data points in these two experiments differ. It is useful to note though, that while this ruleset contains all three ASR features, none of the experimental parameters was found to be a useful predictor, suggesting that our results are not specific to the particular conditions of and participants in the corpus collection, although they are specific to the lexicon and grammars. Results of our learning experiments with mis- recognition defined in terms of CA rather than WER show the overall role of the features which predict WER-defined misrecognition to be less successful in predicting CA-defined error. Table 4 shows the relative performance of the same feature sets discussed above, with misrecognition now defined in terms of CA<1. As with the WER experiments, the best performing feature set makes use of prosodic and ASR-derived features. However, the predictive power of prosodic over ASR features decreases when misrecognition is defined in terms of CA — which is particularly interesting since ASR confidence scores are intended to predict WER rather than CA; the error rate using ASR confidence scores alone (13.52%) is now significantly lower than the error obtained using prosody (18.18%). However, prosodic features still improve the predictive power of ASR confidence scores, to 11.34%, although this difference is not significant at a 95% confidence level. And the error rate of the three ASR features combined (11.70%) is reduced to the lowest error rate in our table when prosodic features are added (10.43%); this error rate is (just) significantly different from the use of ASR confidence scores alone. Thus, for CA-defined mis- recognitions, our experiments have uncovered only minor improvements over traditional ASR rejection calculation procedures. 5 Discussion A statistical comparison of recognized versus mis- recognized utterances indicates that FO excursion, loudness, longer prior pause, and longer duration are significant prosodic characteristics of both WER and CA-defined failed recognition attempts. Results from a set of machine learning experiments show that prosodic differences can in fact be used to improve the prediction of misrecognitions with a high degree of accuracy (12.76% error) for WER-based misrecognitions — and an even higher degree (6.53% error) when combined with information currently available from ASR systems. The use of ASR confidence scores alone had a predicted WER of 22.23%, so the improvement over traditional methods is quite considerable. For CA-defined misrecognitions, the improvement provided by prosodic features is considerably less. One of our future research directions will be to understand this difference. Another future direction will be to address the issue of just why prosodic features provide such useful indicators of recognition failure. Do the features themselves make recognition difficult, or are they instead indirect correlates of other phenomena not captured in our study? While the negative influence of speaking rate variation on ASR has been reported before (e.g. (Ostendorf et al., 1996), it is traditionally assumed that ASR is impervious to differences in FO and RMS; yet, it is known that F0 and RMS variations co-vary to some extent with spectral characteristics (e.g. (Swerts and Veldhuis, 1997; Fant et al., 1995)), so that it is not unlikely that utterances with extreme values for these may differ critically from the training data. Other prosodic features may be more indirect indicators of errors. Longer utterances may simply provide more chance for error than shorter ones, while speakers who pause longer before utterances and take more time making them may also produce more disfluencies than others. We are currently replicating our experiment on a new domain with a new speech recognizer. We are examining the W99 corpus, which was collected in a Table 4: Estimated Error for Predicting Misrecognized Turns (CA<1). Features Used Error SE Prosody, ASR Confidence, ASR String, ASR Grammar 10.43% .63 ALL 10.68% .71 Prosody, ASR Confidence, ASR Grammar 11.24% .68 Prosody, ASR Confidence 11.34% .64 ASR Confidence, ASR String, ASR Grammar 11.70% .68 ASR Confidence 13.52% .82 ASR Confidence, ASR Grammar 13.52% .84 ASR String 13.62% .83 Prosody, ASR String 15.04% .84 Prosody 18.18% .85 Duration 18.38% .90 ASR Grammar 22.73% .96 Tempo 24.61% 1.28 Hyperarticulation 25.27% 1.05 FO Mean 28.61% 1.19 FO Max 28.76% .90 RMS Mean 28.86% 1.17 % Silence 28.91% 1.23 RMS Max 29.01% 1.16 Prior Pause 29.22% 1.26 Majority Baseline 28.61% spoken dialogue system that supported registration, checking paper status, and information access for the IEEE Automatic Speech Recognition and Under- standing Workshop (ASRU99) (Rahim et al., 1999). This system employed the AT&T WATSON speech recognition technology (Sharp et al., 1997). Prelim- inary results indicate that our TOOT results do in fact hold up across recognizers. We also are extend- ing our TOOT corpus analysis to include prosodic analyses of turns in which users become aware of misrecognitions and correct them. In addition, we are exploring whether prosodic differences can help explain the \"goat\" phenomenon—the fact that some voices are recognized much more poorly than others (Doddington et al., 1998; Hirschberg et al., 1999). Our ultimate goal is to provide prosodically- based mechanisms for identifying and reacting to ASR failures in SDS systems. Acknowledgements We would like to thank Jennifer Chu-Carroll, Candy Kamm, participants in the AT&T \"SLUG\" seminar series, and participants in the 1999 JHU Summer Language Engineering Workshop, for providing us with useful comments on this research and on earlier versions of this paper. References Linda Bell and Joakim Gustafson. 1999. Repe- tition and its phonetic realizations: Investigat- ing a Swedish database of spontaneous computer- directed speech. In Proceedings of ICPhS-99, San Francisco. International Congress of Phonetic Sci- ences. E. Blaauw. 1992. Phonetic differences between read and spontaneous speech. In Proceedings of IC- SLP92, volume 1, pages 751-758, Banff. A. G. Bouwman, J. Sturm, and L. Boves. 1999. Incorporating confidence measures in the dutch train timetable information system developed in the ARISE project. In Proc. International Con- ference on Acoustics, Speech and Signal Process- ing, volume 1, pages 493-496, Phoenix. William Cohen. 1996. Learning trees and rules with set-valued features. In 14th Conference of the American Association of Artificial Intelligence, AAAI. George Doddington, Walter Liggett, Alvin Martin, Mark Przybocki, and Douglas Reynolds. 1998. Sheep, goats, lambs and wolves: A statistical anal- ysis of speaker performance in the NIST 1998 speaker recognition evaluation. In Proceedings of ICSLP-98. G. Fant, J. Liljencrants, I. Karlsson, and M. Båvegård. 1995. Time and frequency do- main aspects of voice source modelling. BR Speechmaps 6975, ESPRIT. Deliverable 27 WP 1.3. Keikichi Hirose. 1997. Disambiguating recogni- tion results by prosodic features. In Computing Prosody: Computational Models for Processing Spontaneous Speech, pages 327-342. Springer. Julia Hirschberg, Diane Litman, and Marc Swerts. 1999. Prosodic cues to recognition errors. In Pro- ceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU'99). Julia Hirschberg. 1991. Using text analysis to pre- dict intonational boundaries. In Proceedings of the Second European Conference on Speech Commu- nication and Technology, Genova. ESCA. C. Kamm, S. Narayanan, D. Dutton, and R. Rite- nour. 1997. Evaluating spoken dialog systems for telecommunication services. In 5th European Conference on Speech Technology and Communi- cation, EUROSPEECH 97. Hans Kraayeveld. 1997. Idiosyncrasy in prosody. Speaker and speaker group identification in Dutch using melodic and temporal information. Ph.D. thesis, Nijmegen University. E. Krahmer, M. Swerts, M. Theune, and M. Weegels. 1999. Error spotting in human- machine interactions. In Proceedings of EUROSPEECH-99. Irene Langkilde, Marilyn Walker, Jerry Wright, Al Gorin, and Diane Litman. 1999. Automatic prediction of problematic human-computer dia- logues in 'how may i help you?'. In Proceedings of the Automatic Speech Recognition and Under- standing Workshop (ASRU'99). Gina-Anne Levow. 1998. Characterizing and recog- nizing spoken corrections in human-computer dia- logue. In Proceedings of the 36th Annual Meeting of the Association of Computational Linguistics, COLING/ACL 98, pages 736-742. Diane J. Litman and Shimei Pan. 1999. Empirically evaluating an adaptable spoken dialogue system. In Proceedings of the 7th International Conference on User Modeling (UM). Diane J. Litman, Marilyn A. Walker, and Michael J. Kearns. 1999. Automatic detection of poor speech recognition at the dialogue level. In Pro- ceedings of the 37th Annual Meeting of the As- sociation of Computational Linguistics, ACL99, pages 309-316. M. Ostendorf, B. Byrne, M. Bacchiani, M. Finke, A. Gunawardana, K. Ross, S. Roweis, E. Shriberg, D. Talkin, A. Waibel, B. Wheatley, and T. Zep- penfeld. 1996. Modeling systematic variations in pronunciation via a language-dependent hid- den speaking mode. Report on 1996 CLSP/JHU Workshop on Innovative Techniques for Large Vo- cabulary Continuous Speech Recognition. S. L. Oviatt, G. Levow, M. MacEarchern, and K. Kuhn. 1996. Modeling hyperarticulate speech during human-computer error resolution. In Pro- ceedings of ICSLP-96, pages 801-804, Philadel- phia. M. Rahim, R. Pieracini, W. Eckert, E. Levin, G. Di Fabbrizio, G. Riccardi, C. Lin, and C. Kamm. 1999. W99 a spoken dialogue system for the asru'99 workshop. In Proc. ASRU'99. R.D. Sharp, E. Bocchieri, C. Castillo, S. Parthasarathy, C. Rath, M. Riley, and J Rowland. 1997. The watson speech recognition engine. In Proc. ICASSP97, pages 4065-4068. Ronnie W. Smith. 1998. An evaluation of strate- gies for selectively verifying utterance meanings in spoken natural language dialog. International Journal of Human-Computer Studies, 48:627-647. Hagen Soltau and Alex Waibel. 1998. On the in- fluence of hyperarticulated speech on recognition performance. In Proceedings of ICSLP-98, Syd- ney. International Conference on Spoken Lan- guage Processing. M. Swerts and M. Ostendorf. 1997. Prosodic and lexical indications of discourse structure in human-machine interactions. Speech Communica- tion, 22:25-41. Marc Swerts and Raymond Veldhuis. 1997. Interac- tions between intonation and glottal-pulse char- acteristics. In A. Botinis, G. Kouroupetroglou, and G. Carayiannis, editors, Intonation: Theory, Models and Applications, pages 297-300, Athens. ESCA. Nanette Veilleux. 1994. Computational Models of the Prosody/Syntax Mapping for Spoken Language Systems. Ph.D. thesis, Boston University. E. Wade, E. E. Shriberg, and P. J. Price. 1992. User behaviors affecting speech recognition. In Pro- ceedings of ICSLP-92, volume 2, pages 995-998, Banff. M. Weintraub, K. Taussig, K. Hunicke-Smith, and A. Snodgrass. 1996. Effect of speaking style on LVCSR performance. In Proceedings of ICSLP- 96, Philadelphia. International Conference on Spoken Language Processing. Ilija Zeljkovic. 1996. Decoding optimal state se- quences with smooth state likelihoods. In Interna- tional Conference on Acoustics, Speech, and Sig- nal Processing, ICASSP 96, pages 129-132."
  },
  {
    "title": "Removing Left Recursion from Context-Free Grammars",
    "abstract": "A long-standing issue regarding algorithms that manipulate context-free grammars (CFGs) in a \"top-down\" left-to-right fashion is that left recursion can lead to nontermination. An algorithm is known that transforms any CFG into an equivalent non-left-recursive CFG, but the resulting grammars are often too large for practical use. We present a new method for removing left recursion from CFGs that is both theoretically superior to the standard algorithm, and produces very compact non-left-recursive CFGs in practice.",
    "content": "Introduction A long-standing issue regarding algorithms that ma- nipulate context-free grammars (CFGs) in a \"top- down\" left-to-right fashion is that left recursion can lead to nontermination. This is most familiar in the case of top-down recursive-descent parsing (Aho et al., 1986, pp. 181-182). A more recent motivation is that off-the-shelf speech recognition systems are now available (e.g., from Nuance Communications and Microsoft) that accept CFGs as language models for constraining recognition; but as these recogniz- ers process CFGs top-down, they also require that the CFGs used be non-left-recursive. The source of the problem can be seen by consid- ering a directly left-recursive grammar production such as A → Aa. Suppose we are trying to parse, or recognize using a speech recognizer, an A at a given position in the input. If we apply this pro- duction top-down and left-to-right, our first subgoal will be to parse or recognize an A at the same input position. This immediately puts us into an infinite recursion. The same thing will happen with an indi- rectly left-recursive grammar, via a chain of subgoals that will lead us from the goal of parsing or recogniz- ing an A at a given position to a descendant subgoal of parsing or recognizing an A at that position. In theory, the restriction to non-left-recursive CFGs puts no additional constraints on the lan- guages that can be described, because any CFG can in principle be transformed into an equivalent non-left-recursive CFG. However, the standard algo- rithm for carrying out this transformation (Aho et al., 1986, pp. 176-178) (Hopcroft and Ullman, 1979, p. 96)—attributed to M. C. Paull by Hopcroft and Ullman (1979, p. 106)—can produce transformed grammars that are orders of magnitude larger than the original grammars. In this paper we develop a number of improvements to Paull's algorithm, which help somewhat but do not completely solve the prob- lem. We then go on to develop an alternative ap- proach based on the left-corner grammar transform, which makes it possible to remove left recursion with no significant increase in size for several grammars for which Paull's original algorithm is impractical. 2 Notation and Terminology Grammar nonterminals will be designated by \"low order\" upper-case letters (A, B, etc.); and termi- nals will be designated by lower-case letters. We will use \"high order\" upper-case letters (X, Y, Z) to denote single symbols that could be either ter- minals or nonterminals, and Greek letters to denote (possibly empty) sequences of terminals and/or non- terminals. Any production of the form A → a will be said to be an A-production, and a will be said to be an expansion of A. We will say that a symbol X is a direct left corner of a nonterminal A, if there is an A-production with X as the left-most symbol on the right-hand side. We define the left-corner relation to be the reflexive transitive closure of the direct-left-corner relation, and we define the proper-left-corner relation to be the transitive closure of the direct-left-corner rela- tion. A nonterminal is left recursive if it is a proper left corner of itself; a nonterminal is directly left re- cursive if it is a direct left corner of itself; and a nonterminal is indirectly left recursive if it is left re- cursive, but not directly left recursive. 3 Test Grammars We will test the algorithms considered here on three large, independently-motivated, natural-language grammars. The CT grammar¹ was compiled into a CFG from a task-specific unification grammar ¹Courtesy of John Dowding, SRI International Toy CT ATIS PT Grammar Grammar | Grammar Grammar Grammar size Terminals Nonterminals Productions LR nonterminals Productions for LR nonterminals 88 55,830 16,872 67,904 40 1,032 357 47 16 3,946 192 38 55 24,456 4,592 15,039 4 535 9 33 27 2,211 1,109 14,993 Table 1: Grammars used for evaluation. written for CommandTalk (Moore et al., 1997), a spoken-language interface to a military simulation system. The ATIS grammar was extracted from an internally generated treebank of the DARPA ATIS3 training sentences (Dahl et al., 1994). The PT gram- mar2 was extracted from the Penn Treebank (Mar- cus et al., 1993). To these grammars we add a small \"toy\" grammar, simply because some of the algo- rithms cannot be run to completion on any of the \"real\" grammars within reasonable time and space bounds. Some statistics on the test grammars are con- tained in Table 1. The criterion we use to judge effectiveness of the algorithms under test is the size of the resulting grammar, measured in terms of the total number of terminal and nonterminal symbols needed to express the productions of the grammar. We use a slightly nonstandard metric, counting the symbols as if, for each nonterminal, there were a single production of the form A → 01 | | an This reflects the size of files and data structures typ- ically used to store grammars for top-down process- ing more accurately than counting a separate occur- rence of the left-hand side for each distinct right- hand side. It should be noted that the CT grammar has a very special property: none of the 535 left recursive nonterminals is indirectly left recursive. The gram- mar was designed to have this property specifically because Paull's algorithm does not handle indirect left recursion well. It should also be noted that none of these gram- mars contains empty productions or cycles, which can cause problems for algorithms for removing left recursion. It is relatively easy to trasform an arbi- trary CFG into an equivalent grammar which does not contain any of the probelmatical cases. In its initial form the PT grammar contained cycles, but these were removed at a cost of increasing the size of the grammar by 78 productions and 89 total sym- bols. No empty productions or cycles existed any- where else in the original grammars. 20 4 Paull's Algorithm Paull's algorithm for eliminating left recursion from CFGs attacks the problem by an iterative procedure for transforming indirect left recursion into direct left recursion, with a subprocedure for eliminating direct left recursion. This algorithm is perhaps more familiar to some as the first phase of the textbook algorithm for transfomrming CFGs to Greibach nor- mal form (Greibach, 1965).3 The subprocedure to eliminate direct left recursion performs the following transformation (Hopcroft and Ullman, 1979, p. 96): Let Α → Ααι | | Αατ be the set of all directly left recursive A- productions, and let Αβ₁ | βε be the remaining A-productions. Replace all these productions with and Α - βι | βιΑ' \\ βεβεΑ', Α' - αι | αι Α' | | ας ας Α', where A' is a new nonterminal not used elsewhere in the grammar. This transformation is embedded in the full algo- rithm (Aho et al., 1986, p. 177), displayed in Fig- ure 1. The idea of the algorithm is to eliminate left re- cursion by transforming the grammar so that all the direct left corners of each nonterminal strictly follow that nonterminal in a fixed total ordering, in which case, no nonterminal can be left recursive. This is accomplished by iteratively replacing direct left cor- ners that precede a given nonterminal with all their expansions in terms of other nonterminals that are greater in the ordering, until the nonterminal has only itself and greater nonterminals as direct left 3 This has led some readers to attribute the algorithm to Greibach, but Greibach's original method was quite different and much more complicated [ERROR: Failed to process this page - Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.] --- original grammar PA LF LF+PA LF+NLRG+PA CT Grammar ATIS Grammar | PT Grammar 55,830 16,872 67,904 62,499 > 5,000,000 > 5,000,000 54,991 11,582 37,811 59,797 2,004,473 > 5,000,000 57,924 72,035 > 5,000,000 Table 3: Grammar size comparisons with Paull's algorithm variants ordering respecting the constraint; we are unaware of any method for finding a unique best ordering, other than trying all the orderings respecting the constraint. As a neutral comparison, we also ran the algo- rithm with the nonterminals ordered lexicographi- cally. Finally, to test how bad the algorithm could be with a really poor choice of nonterminal ordering, we defined a \"worst\" ordering to be one with increas- ing numbers of distinct left corners. It should be noted that with either the lexicographical or worst ordering, on all of our three large grammars Paull's algorithm exceeded a cut-off of 5,000,000 grammar symbols, which we chose as being well beyond what might be considered a tolerable increase in the size of the grammar. Let PA refer to Paull's algorithm with the non- terminals ordered according to decreasing number of distinct left corners. The second line of Table 3 shows the results of running PA on our three large grammars. The CT grammar increases only mod- estly in size, because as previously noted, it has no indirect left recursion. Thus the combinatorial phase of Paull's algorithm is never invoked, and the in- crease is solely due to the transformation applied to directly left-recursive productions. With the ATIS grammar and PT grammar, which do not have this special property, Paull's algorithm exceeded our cut- off, even with our best ordering of nonterminals. Some additional optimizations of Paull's aglo- rithm are possible. One way to reduce the num- ber of substitutions made by the inner loop of the algorithm is to \"left factor\" the grammar (Aho et al., 1986, pp. 178-179). The left-factoring transfor- mation (LF) applies the following grammar rewrite schema repeatedly, until it is no longer applicable: LF: For each nonterminal A, let α be the longest nonempty sequence such that there is more than one grammar production of the form A → αβ. Replace the set of all productions A → αβ1, ..., A → αβn with the productions A → αA', A' → β1, ..., A' → βn, With left factoring, for each nonterminal A there will be only one A-production for each direct left corner of A, which will in general reduce the number of substitutions performed by the algorithm. The effect of left factoring by itself is shown in the third line of Table 3. Left factoring actually re- duces the size of all three grammars, which may be unintuitive, since left factoring necessarily increases the number of productions in the grammar. How- ever, the transformed productions are shorter, and the grammar size as measured by total number of symbols can be smaller because common left factors are represented only once. The result of applying PA to the left-factored grammars is shown in the fourth line of Table 3 (LF+PA). This produces a modest decrease in the size of the non-left-recursive form of the CT gram- mar, and brings the non-left-recursive form of the ATIS grammar under the cut-off size, but the non- left-recursive form of the PT grammar still exceeds the cut-off. The final optimization we have developed for Paull's algorithm is to transform the grammar to combine all the non-left-recursive possibilities for each left-recursive nonterminal under a new nonter- minal symbol. This transformation, which we might call \"non-left-recursion grouping\" (NLRG), can be defined as follows: NLRG: For each left-recursive nontermi- nal A, let α1, ..., αn be all the expansions of A that do not have a left recursive non- terminal as the left most symbol. If n > 1, replace the set of productions A → α1, ..., A → αn with the productions A → A', A' → α1, ..., A' → αn, where A' is a new nonterminal symbol. Since all the new nonterminals introduced by this transformation will be non-left-recursive, Paull's al- gorithm with our best ordering will never substitute the expansions of any of these new nonterminals into the productions for any other nonterminal, which in general reduces the number of substitutions the original grammar LF LF+NLRG+PA CT Grammar ATIS Grammar PT Grammar 16,872 11,582 67,904 37,811 72,035 > 5,000,000 EM LC LCLR LF+LCLR LF+NLRG+LCLR 55,830 54,991 57,924 762,576 60,556 58,893 57,380 287,649 40,660 13,641 12,243 1,567,162 1,498,112 67,167 50,277 Table 4: Grammar size comparisons for LC transform variants the effect on grammar size of applying the NLRG transformation by itself, but it is easy to see that it increases the grammar size by exactly two sym- bols for each left-recursive nonterminal to which it is applied. Thus an addition of twice the number of left-recursive nonterminals will be an upper bound on the increase in the size of the grammar, but since not every left-recursive nonterminal necessarily has more than one non-left-recursive expansion, the in- crease may be less than this. The fifth line of Table 3 (LF+NLRG+PA) shows the result of applying LF, followed by NLRG, fol- lowed by PA. This produces another modest de- crease in the size of the non-left-recursive form of the CT grammar and reduces the size of the non- left-recursive form of the ATIS grammar by a factor of 27.8, compared to LF+PA. The non-left-recursive form of the PT grammar remains larger than the cut-off size of 5,000,000 symbols, however. 5 Left-Recursion Elimination Based on the Left-Corner Transform An alternate approach to eliminating left-recursion is based on the left-corner (LC) grammar transform of Rosenkrantz and Lewis (1970) as presented and modified by Johnson (1998). Johnson's second form of the LC transform can be expressed as follows, with expressions of the form A-a, A-X, and A-B being new nonterminals in the transformed grammar: 1. If a terminal symbol a is a proper left corner of A in the original grammar, add A → aA-a to the transformed grammar. 2. If B is a proper left corner of A and B → Χβ is a production of the original grammar, add A-X → BA-B to the transformed grammar. 3. If X is a proper left corner of A and A → Χβ is a production of the original grammar, add A-X → ẞ to the transformed grammar. In Rosenkrantz and Lewis's original LC transform, schema 2 applied whenever B is a left corner of A, including all cases where B = A. In Johnson's ver- sion schema 2 applies when B = A only if A is a proper left corner of itself Johnson then introduces schema 3 handle the residual cases, without intro- ducing instances of nonterminals of the form A-A that need to be allowed to derive the empty string. The original purpose of the LC transform is to allow simulation of left-corner parsing by top-down parsing, but it also eliminates left recursion from any noncyclic CFG.5 Furthermore, in the worst case, the total number of symbols in the transformed gram- mar cannot exceed a fixed multiple of the square of the number of symbols in the original grammar, in contrast to Paull's algorithm, which exponentiates the size of the grammar in the worst case. Thus, we can use Johnson's version of the LC transform directly to eliminate left-recursion. Be- fore applying this idea, however, we have one gen- eral improvement to make in the transform. Johnson notes that in his version of the LC transform, a new nonterminal of the form A-X is useless unless X is a proper left corner of A. We further note that a new nonterminal of the form A-X, as well as the orginal nonterminal A, is useless in the transformed grammar, unless A is either the top nonterminal of the grammar or appears on the right-hand side of an original grammar production in other than the left-most position. This can be shown by induction on the length of top-down derivations using the pro- ductions of the transformed grammar. Therefore, we will call the original nonterminals meeting this condition \"retained nonterminals\" and restrict the LC transform so that productions involving nonter- minals of the form A-X are created only if A is a retained nonterminal. Let LC refer to Johnson's version of the LC trans- form restricted to retained nonterminals. In Table 4 the first three lines repeat the previously shown sizes for our three original grammars, their left-factored form, and their non-left-recursive form using our best variant of Paull's algorithm (LF+NLRG+PA). The fourth line shows the results of applying LC to the three original grammars. Note that this pro- duces a non-left-recursive form of the PT gram- mar smaller than the cut-off size, but the non-left- recursive forms of the CT and ATIS grammars are In the case of a cyclic CFG, the schema 2 fails to guar- anto a non loft tourgiue trongformed an considerably larger than the most compact versions created with Paull's algorithm. We can improve on this result by noting that, since we are interested in the LC transform only as a means of eliminating left-recursion, we can greatly reduce the size of the transformed grammars by ap- plying the transform only to left-recursive nonter- minals. More precisely, we can retain in the trans- formed grammar all the productions expanding non- left-recursive nonterminals of the original grammar, and for the purposes of the LC transform, we can treat non-left-recursive nonterminals as if they were terminals: 1. If a terminal symbol or non-left-recursive non- terminal X is a proper left corner of a re- tained left-recursive nonterminal A in the orig- inal grammar, add A → XA-X to the trans- formed grammar. 2. If B is a left-recursive proper left corner of a retained left-recursive nonterminal A and B → Xẞ is a production of the original grammar, add A-X → BA-B to the transformed grammar. 3. If X is a proper left corner of a retained left- recursive nonterminal A and A → Xẞ is a pro- duction of the original grammar, add A-X →β to the transformed grammar. 4. If A is a non-left-recursive nonterminal and A → ẞ is a production of the original grammar, add A→B to the transformed grammar. Let LCLR refer to the LC transform restricted by these modifications so as to apply only to left- recursive nonterminals. The fifth line of Table 4 shows the results of applying LCLR to the three orig- inal grammars. LCLR greatly reduces the size of the non-left-recursive forms of the CT and ATIS gram- mars, but the size of the non-left-recursive form of the PT grammar is only slightly reduced. This is not surprising if we note from Table 1 that almost all the productions of the PT grammar are produc- tions for left-recursive nonterminals. However, we can apply the additional transformations that we used with Paull's algorithm, to reduce the num- ber of productions for left-recursive nonterminals before applying our modified LC transform. The effects of left factoring the grammar before apply- ing LCLR (LF+LCLR), and additionally combining non-left-recursive productions for left-recursive non- terminals between left factoring and applying LCLR (LF+NLRG+LCLR), are shown in the sixth and seventh lines of Table 4. With all optimizations applied, the non-left- recursive forms of the ATIS and PT grammars are smaller than the originals (although not smaller than the left-factored forms of these grammars), and the non-left-recursive form of the CT gram- mar is only slightly larger than the original In all cases, LF+NLRG+LCLR produces more compact grammars than LF+NLRG+PA, the best variant of Paull's algorithm-slightly more compact in the case of the CT grammar, more compact by a factor of 5.9 in the case of the ATIS grammar, and more compact by at least two orders of magnitude in the case of the PT grammar. 6 Conclusions We have shown that, in its textbook form, the standard algorithm for eliminating left recur- sion from CFGs is impractical for three diverse, independently-motivated, natural-language gram- mars. We apply a number of optimizations to the algorithm-most notably a novel strategy for order- ing the nonterminals of the grammar-but one of the three grammars remains essentially intractable. We then explore an alternative approach based on the LC grammar transform. With several optimiza- tions of this approach, we are able to obtain quite compact non-left-recursive forms of all three gram- mars. Given the diverse nature of these grammars, we conclude that our techniques based on the LC transform are likely to be applicable to a wide range of CFGs used for natural-language processing. References A. V. Aho, R. Sethi, and J. D. Ullman. 1986. Compilers: Principles, Techniques, and Tools. Addison-Wesley Publishing Company, Reading, Massachusetts. D. A. Dahl et al. 1994. Expanding the scope of the ATIS task: the ATIS-3 corpus. In Proceedings of the Spoken Language Technology Workshop, pages 3-8, Plainsboro, New Jersey. Advanced Research Projects Agency. S. A. Greibach. 1965. A new normal-form theorem for context-free phrase structure grammars. Jour- nal of the Association for Computing Machinery, 12(1):42-52, January. J. E. Hopcroft and J. D. Ullman. 1979. Introduc- tion to Automata Theory, Languages, and Com- putation. Addison-Wesley Publishing Company, Reading, Massachusetts. M. Johnson. 1998. Finite-state approximation of constraint-based grammars using left-corner grammar transforms. In Proceedings, COLING- ACL '98, pages 619-623, Montreal, Quebec, Canada. Association for Computational Linguis- tics. M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large anno- tated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330, June. R. Moore, J. Dowding, H. Bratt, J. M. Gawron, Y Gorfu and A Chever 1997 Commandtalk. A spoken-language interface for battlefield simu- lations. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 1-7, Washington, DC. Association for Computational Linguistics. S. J. Rosenkrantz and P. M. Lewis. 1970. Deter- ministic left corner parser. In IEEE Conference Record of the 11th Annual Symposium on Switch- ing and Automata Theory, pages 139-152."
  },
  {
    "title": "APPLICATION-SPECIFIC ISSUES IN NATURAL LANGUAGE INTERFACER DEVELOPMENT FOR A DIAGNOSTIC EXPERT SYSTEM",
    "abstract": "This paper describes a natural language interface developed for an expert system, Page-X. The interface accepts English descriptions of observed symptoms and maps those descriptions to hypotheses used as initial input to the Page-X diagnosis system. The interface describes an application-independent linguistic interface and an application-specific hypothesis identification component.",
    "content": "PROBLEM DESCRIPTION The natural language interface (NLI) we describe here makes up one part of the user interface to Page-X [STRA85], an expert system for the diagnosis of problems in non- impact page printers. To better understand some of our design decisions, it is useful to see the context in which this system was developed and deployed. Page-X has been in develop- ment since 1984 and was first deployed in early 1986. The knowledge base continues to be expanded to handle new printers. Two copies of the system reside in a central location. Field technicians who have the responsibility of serving customer sites are trained to diagnose problems in these printers; however, they often must rely on more expert knowledge. They can access Page-X via modem over a standard ASCII terminal. Page-X performs diagnosis by reasoning from an initial set of symptom hypotheses to a probable cause, typically asking the technician to perform various tests along the way. The task of the user interface is to allow the technician to efficiently state what symptoms he/she has observed, both initially and as a result of the requested tests. Menus were used as the original mode of specifying symptoms, and these form a part of the current interface. However, as the knowledge base grew to thousands of hypotheses, it became unwieldy to input the hypotheses. A keyword interface was added to allow technicians to describe the situation more directly. This interface functioned by matching a predetermined set of keywords in the user's typed free-form English input to the hypotheses' names, which were actually short descriptions. Page-X would then start reasoning from this initial set, but dynamically compose menus to receive input about the results of requested tests. This mode of interaction proved to be very easy to implement. However, the limitations of keyword matching soon became restrictive, e.g.,: (1) it was not possible to be precise enough in the match process; (2) it was difficult to treat synonymous words properly, and (3) in some subdomains (e.g., print quality), the amount of synonymy and near synonymy made exact keyword matching nearly useless. These factors motivated the developers to replace the keyword part of the interface with a restricted NLI. Subsequent use of dynamically created menus is proceeding as before. This project is currently in its infancy, having started in March of 1986. At present it deals with only a subset of the domain covered by Page-X, namely print quality, and is not as yet complete with respect to this subdomain. However, the results are extremely promising based on a comparison of the possible performance of the keyword interface and the performance of the implemented natural language interface. SYSTEM OVERVIEW The Page-X interface can be divided into two major modules: The linguistic interface component (LIC) and the symptom identifica- tion component (SIC). The LIC is based almost entirely on an interface developed under the ATOZ project [LARS85). The SIC had to be specifically developed for the Page-X application and domain. LINGUISTIC INTERFACE COMPONENT The LIC consists of several subcomponents that apply various grammars and lexicons to yield a domain- and application-specific interpretation of the input. As in most such NLIs, the components are: (1) a parser that makes use of a grammar and lexicon to produce a constituent structure representation, (2) a logical interpretation component, which uses a set of Montague grammar style rules to produce a \"logical form\" representation, (3) a lexical translation component, which uses a domain model and rules to translate English terms to domain-model terms, and (4) a logical form translation component, which uses a set of logical form transformation rules to produce a representation adhering to the specific formal language syntax required to interface to the application. A box diagram of the LIC is presented in Figure 1. Adapting the ATOZ LIC to Page-X required writing a new domain model and new lexicons as well as extending each of the three grammars to handle linguistic phenomena not encountered in the original query applications. No soft- ware modifications were necessary. Input Parse Translation Lexical Translation to LF LF Transformation Symptom ID Page-X ES G7363-3457-1 Figure 1. Diagram of the LIC The output of the LIC cannot serve as direct input to the application program. There will typically be a need for translation from domain predicates to application predicates and/or a matching of input propositions to application propositions. Page-X symptom hypotheses are stated in terms of domain model predicates, so no predicate translation is necessary. However, for reasons that will be explained in detail below, it is necessary to have an explicit component to match the representation of the input to the hypothesis. This is the function of the SIC. DETAILED INTERFACE TRACE In this section, we will discuss a detailed trace of one example, showing the overall operation of the system. Figure 2 shows the initial input, the output of each stage of semantic interpreta- tion, and the final set of symptoms identified as closest to the original input. For space considerations, we have suppressed the parser output which is an unremarkable feature- value graph of the constituent structure. > (px-atoz (white line going down page) t) Parse completed. Logical Form: ((((WHITE Y1159)) ((^GO E1173 Y1159) (VERB-SUBJ E1173 Y1159))) ((PAGE Y1169) (DOWN E1173 Y1169))) Lexical and Logical Transformation: ((AFFECTED-OBJECT Y1159) (COLOR-OF Y1159 !1176) (LIGHT-COLOR !1176)(WHITE !1176)(INTENSITY-OF Y1159 !1177) (LIGHT !1177)(BAND-LINE Y1159) (ORIENTATION-OF Y1159 !1178)(VERTICAL !1178) (NIL)(NIL)(ENTIRE-PAGE Y1169)(AFFECTED-OBJECT Y1159) (ORIENTATION-OF Y1159 !1179)(VERTICAL !1179) LOCATION-OF Y1159 Y1169) (ENTIRE-PAGE Y1169)) Matcher: (HYPO-1503.P3-PRINT-HAS-LIGHT- VARYING-WIDTH-BANDS HYPO-1151.P3-PRINT-HAS-LIGHT- VERTICAL-BANDS HYPO-1367.P3-PRINT-HAS- SYMMETRICAL-BANDS HYPO-251.P3-PRINT-HAS-LIGHT- BANDS-CONSTANT-WIDTH HYPO-1374.P3-TONER-STARVATION HYPO-1371.P3-PRINT-HAS-UNEVEN- PRINT DENSITY-SIDE-TO-SIDE HYPO-165.P-3-THE-PRINTED-PAGE- IS-WASHED-OUT HYPO-111.P3-PRINT-HAS-VERTICAL- LINES HYPO-1370.P3-PRINT-HAS-SLASHES Figure 2. Trace Output The keyword interface operated on matching the content words found in the hypothesis names. As can be seen from this example, that method would fail to identify a significant number of possible causes. LINGUISTIC COVERAGE SYNTACTIC ANALYSIS Syntactic analysis is performed by means of a unification-based chart parser applied to a combinatory categorial grammar. The parser was developed under the LUCY project at MCC [WITT86]. The grammar employed is the ATOZ grammar, with extensions to handle some aspects of \"telegraphic\" speech. The scope of this project did not permit a thorough determination of the sublanguage used in this domain and application. However, we were able to study approximately 20 user transcripts and complete one field engineer interview to help us arrive at a working grammar and lexicon. Because the application requires that the user give symptom descriptions, input is almost always in the form of simple declarative clauses and phrases. Some examples are given here: (1) Fat characters, (2) No format print, (3) Overprinting garbage, (4) Characters smeared down left side of page, (5) Character too dark, (6) Characters repeating down page. As can be seen, structures omitting determiners, copula, whole predicates and whole subjects are often used. Since this appears to be the norm for this application, these are treated as fully grammatical. Logical form rules ensure that they receive the same interpretation as their fully specified counterparts. Because this grammar is an outgrowth of a grammar previously developed for database query, there is also coverage of standard interrogative and imperative structures, though these forms are not present in our user input. SEMANTIC TRANSLATION The translation to initial logical form is driven by a Montague-style grammar that pairs constituent structure graphs to expressions of lambda calculus. The translation component itself simply applies these rules recursively and performs formula reduction. An example rule is given here. This would be for translating a noun modified by an adjective: [X: [cat:N lf: [rule: (x (1(x) 2(x))) arg1: <1> arg2: <2>]] Y: [cs: [head: 2[cat: N] mod: 1[cat: Adj]]]] Graph unification is used to match a graph with the appropriate rule and to supply the arguments to the lambda calculus expression. Since the purpose of this component is to yield a logical formula for every input phrase, the coverage is identical to that of the syntactic grammar. At this point, semantic translation primarily serves to reveal the predicate argu- ment structure of the constituent structure graph. Negation and quantifiers are also handled, although the current versions of the domain model and the SIC do not have mechanisms to deal with these. Because of the limitations of the SIC< certain other semantic distinctions that appear to be important within this domain, such as iterative aspect, tense, and degree, are ignored at this point. ISSUES UNGRAMMATICAL INPUT AND PARTIAL INTERPRETATIONS In describing the grammatical coverage above, we mentioned that certain kinds of nonstandard constructions were treated as fully grammatical. However, it is not possible to take this approach everywhere the input deviates from the standard. For the system to be of use during development, and to take care of idiolectic and random deviance, some treatment of true ungrammaticality is necessary. One approach would be to revert at this point to the simple keyword match. However, even in unparsed sentences, there is frequently enough information available to better identify the intended symptom. The approach we take is to heuristically assemble a partial syntactic analysis made up of three fragments, translate the fragments, and then again apply heuristics to get the best domain- dependent connection between the logical form fragments. Syntactic phrase assembly is accomplished by using path finding techniques on the chart to discover the best set of chart edges that exhaustively cover the input with no overlaps. An A* technique is employed with path score based on path length and edge length. Short paths are favored over long ones. This ensures that the partial analysis is made up of a few large constituent structure graphs. These general heuristics could be replaced or augmented by heuristics based on more grammar-specific characteristics such as major category or presence of a required subcategory. Translation of logical form and insertion of domain predicates proceed as normal, yielding a set of partially connected formulas with default translations where contexts could not be met. The SIC, by itself, would not work well at all if given a formula where the variables are not properly shared. It would simply ignore all of the information that was not tied, through variable bindings, to the affected-object predicate. This could be most of the formula. To prevent this, whenever a complete parse is not found, variable bindings are forced in the hypothesis matcher input so that all predicates are tied through shared variables to the affected-object predicate. These forced bindings may, of course, be incorrect, but initial tests indicate that the rather simple- minded approach to the ungrammaticality we employ here still brings noticeable improvement over the straight keyword match. LEXICAL TRANSLATION ISSUES Lexical translation is the process of substituting one or more predicates of a domain model for one or more English lexical items. The lexical translation step in the Page-X system is a subpart of the step of translation from English to an intermediate domain model. Lexical translation as used here is not exactly the same as the problem of mapping from English lexical items to standard database constructs. The problem of translating English lexical items to standard database constructs can be broken into at least two steps: (1) English to intermediate domain model, and (2) intermediate domain model to database model. Some work specifically focusing on mapping problems from domain model expressions to database target models has been done by [STAL84], [STAL86] and [SCHA82]. The system-specific semantics (i.e., the problem of matching inputs to the appropriate set of relevant hypotheses) has an effect on the structure of the domain model and the problem of lexical translation. The system semantics determine the degree to which synonymous or only nearly synonymous terms should be distinguished. Distinctiveness of terms is determined by the relevance of their semantics to distinguishing a hypothesis' relevance to a given user input. Synonymous lexical items are of course translated to the same set of predicates. In this system, for example, 'dot' and 'spots' are both translated to a predicate DOTS. The task of hypothesis matching requires additional semantic distinctions to capture the notion of near synonymy. For example, 'streak' and 'slash' are not treated as synonyms at the lowest level in the domain model because for some hypotheses, the distinction between streak and slash is significant. However, user input referring to 'streaks' and 'slashes' may be relevant to hypotheses that describe conditions such as 'dark lines down page.' The near synonymy of 'streak' and 'slash' are defined as subconcepts. Lexical ambiguity in this system is dealt with by defining more than one lexical translation rule for each lexical item and supplying contexts that must be satisfied before the rule can be applied. In general all input can be sufficiently disambiguated by appealing to the context supplied by the rest of the user input. In occasional cases this is not sufficient. To handle such cases, all predicates have one context-free rule associated with them. This is sometimes necessary for elliptical and ungrammatical input (i.e., cases where the appropriate contest is frequently not present). The context-free rules guarantee that some translation will always be produced for any user input; however, there are aspects of the translation that are not adequately handled by the current mechanisms used to specify contextually dependent and default rules. The formal mechanisms of rule contexts and default rules are the primary means of accounting for lexical ambiguity in this system. Other systems propose techniques of constraint satisfaction ([RICH87]) and marker passing ([HIRS84]) to deal with the same types of problems. We have investi- gated the possibility of assuming a lexical translation rule context (by entering it into a context work area where the partial transla- tion is developed) so that the associated lexical translation rule could apply. If, under the assumed context, a translation for the entire input can be completed, the translation is considered to be correct. If a translation cannot be completed, all translated clauses that depend on the original assumption would be removed and a new assumption made (if necessary). This is similar to the constraint satisfaction approach. We make a distinction not clearly made in either of the cited approaches in the type of contextual information that may be specified in the rules. Lexical translation rules can contain both linguistic contextual information and domain model contextual information. This is useful since some contextual information is more easily specified using one set of terms rather than the other. Currently we have no facility for specifying negative contexts. This facility would occasionally be convenient for the specification of rules. HYPOTHESIS MATCHING ISSUES The problem of matching hypotheses to the output of the semantic interpretation component introduces its own set of problems. It is rarely, if ever, the case that the user input will match exactly to any hypothesis represen- tation. It is necessary to specify what kind and how much of a match is needed between the input and the stored hypotheses to justify identifying the hypothesis as a start state for the expert system. READINESS EXTENSION TO OTHER SUBDOMAIN PLANS To fully replace the keyword matching component of the current Page-X interface, we must extend our work to include the other subdomains of the general Page-X problem domain. These include areas of mechanical problems, electrical problems, printing problems, power problems and exceptional cases. This will require additional linguistic extensions and a generalization of the hypothesis identification routine. The overall strategy of the heuristic matching process would remain the same. LINGUISTIC ROBUSTNESS Linguistic robustness can be enhanced by conducting experiments to determine sublanguage and by analyzing and making use of the results. Further improvement in the treatment of ungrammatical input is necessary. Currently, there is no technique for handling words that are not in the lexicon. Also, the heuristics employed in assembling a partial interpretation can be made more dependent on linguistic and domain facts. It is possible that the heuristics that are effective for grammatical input will not be as effective for ungrammatical input. This is because an ungrammatical parse will have an effect on the shared variables between predicates in the translated user input. The alfalpha rules and the lexical translation rules can produce new predicates with variables shared across predicates. If default lexical translation rules or nonstandard alfalpha rules must be applied because the parse has not completely succeeded, then there will be cases where two (or more) predicates in the translated user output will not share variables that would have been shared in an equivalent grammatical input translation. GENERALIZED SYMPTOM DESCRIPTION INTERFACES It is an open question as to whether or not this interface could be used as the basis for a generalized symptom description interface. The generalization would have to include both the strictly linguistic aspects of the interface and the application-specific aspects. The strictly linguistic portions of the interface (the parser, the three-stage semantic interpreta- tion routines) are applicable to any natural language symptom description interface. The modifications to support partial interpretation of ungrammatical input are also useful in any domain. The application- specific aspects of the interface may be generalizable under restricted conditions. If the new domain is one where 'important concepts' can be identified, then there is a good chance that some version of the matching heuristics could be applied. BIBLIOGRAPHY [HIRS84] Hirst, Semantic Interpretation Against Ambiguity, Ph.D. dissertation, Brown University, 1984. [LARS85] J.A. Larson, W.F. Kaemmerer, K.L. Ryan, J. Slagle, W.T. Wood, \"ATOZ - A Prototype Intelligent Interface to Multiple Systems,\" Foundations for Human Computer Communication, K. Hopper and I.A. Newman (eds), Elsevier Science Publishers B. V. (North Holland) New York, 1986. [RICH87] Rich, Barnett, Wittenberg, Wroblewski, \"Ambiguity Procrastination,\" Proceedings of AAAI87, July 13-17, 1987, Seattle, Washington, pp 571-576. [SCHA82] J.H. Remko Scha, \"English Words and Databases: How to Bridge the Gap,\" Proceedings of 20th Annual ACL, June 16-18, 1982, Toronto, Ontario, pp 57-59. [STAL84] Stallard, \"Data Modelling for Natural Language Access,\" The First Conference on Artificial Intelligence Applications, Denver, Colorado, December 5- 7, 1984, pp 19-24. [STAL86] Stallard, \"A Terminological Simplification Transformation for Natural Language Question-Answering Systems,\" Proceedings of 24th Annual ACL, New York, New York, 1986, pp 241-246. [STRA85] Strandberg, Abromovich, Mitchell, Prill, \"Page-1: A Troubleshooting Aid for Non-Impact Page Printing Systems,\" Proceedings of the Second Conference on Artificial Intelligence Applications, Miami Beach, Florida, December 11-13, 1985, pp 68-74. [WITT86] Kent Wittenberg, \"A Parser for Portable NL Interfaces Using Graph- Unification-Based Grammars,\" Proceedings of AAAI86, August 11-15. 1986, Philadelphia, Pennsylvania, pp 1053-1058."
  },
  {
    "title": "Noun Phrase Recognition by System Combination",
    "abstract": "The performance of machine learning algorithms can be improved by combining the output of different systems. In this paper we apply this idea to the recognition of noun phrases. We generate different classifiers by using different representations of the data. By combining the results with voting techniques described in (Van Halteren et al., 1998) we manage to improve the best reported performances on standard data sets for base noun phrases and arbitrary noun phrases.",
    "content": "1 Introduction (Van Halteren et al., 1998) and (Brill and Wu, 1998) describe a series of successful experiments for im- proving the performance of part-of-speech taggers. Their results have been obtained by combining the output of different taggers with system combination techniques such as majority voting. This approach cancels errors that are made by the minority of the taggers. With the best voting technique, the com- bined results decrease the lowest error rate of the component taggers by as much as 19% (Van Hal- teren et al., 1998). The fact that combination of classifiers leads to improved performance has been reported in a large body of machine learning work. We would like to know what improvement combi- nation techniques would cause in noun phrase recog- nition. For this purpose, we apply a single memory- based learning technique to data that has been rep- resented in different ways. We compare various com- bination techniques on a part of the Penn Treebank and use the best method on standard data sets for base noun phrase recognition and arbitrary noun phrase recognition. 2 Methods and experiments In this section we start with a description of our task: recognizing noun phrases. After this we introduce the different data representations we use and our machine learning algorithms. We conclude with an outline of techniques for combining classifier results. 2.1 Task description Noun phrase recognition can be divided in two tasks: recognizing base noun phrases and recognizing arbi- trary noun phrases. Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase. For example, the sentence In [early trading] in [Hong Kong] [Monday], [gold] was quoted at [$ 366.50] [an ounce ]. contains six baseNPs (marked as phrases between square brackets). The phrase $ 366.50 an ounce is a noun phrase as well. However, it is not a baseNP since it contains two other noun phrases. Two baseNP data sets have been put forward by (Ramshaw and Marcus, 1995). The main data set consist of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material and one section (20) as test material¹. The baseNPs in this data are slightly different from the ones that can be derived from the Treebank, most notably in the attachment of genitive markers. The recognition task involving arbitrary noun phrases attempts to find both baseNPs and noun phrases that contain other noun phrases. A stan- dard data set for this task was put forward at the CONLL-99 workshop. It consist on the same parts of the Penn Treebank as the main baseNP data set: WSJ sections 15-18 as training data and section 20 as test data2. The noun phrases in this data set are the same as in the Treebank and therefore the baseNPs in this data set are slightly different from the ones in the (Ramshaw and Marcus, 1995) data sets. In both tasks, performance is measured with three scores. First, with the percentage of detected noun phrases that are correct (precision). Second, with the percentage of noun phrases in the data that were found by the classifier (recall). And third, 1This (Ramshaw and Marcus, 1995) baseNP data set is available via ftp://ftp.cis.upenn.edu/pub/chunker/ 2Software for generating the data is available from http://lcg-www.uia.ac.be/conll99/npb/ with the Fβ=1 rate which is equal to (2*preci- sion*recall)/(precision+recall). The latter rate has been used as the target for optimization. 2.2 Data representation In our example sentence in section 2.1, noun phrases are represented by bracket structures. Both (Muñoz et al., 1999) and (Tjong Kim Sang and Veenstra, 1999) have shown how classifiers can process bracket structures. One classifier can be trained to recog- nize open brackets (O) while another will process close brackets (C). Their results can be converted to baseNPs by making pairs of open and close brackets with large probability scores (Muñoz et al., 1999) or by regarding only the shortest phrases between open and close brackets as baseNPs (Tjong Kim Sang and Veenstra, 1999). We have used the bracket repre- sentation (O+C) in combination with the second baseNP construction method. An alternative representation for baseNPs has been put forward by (Ramshaw and Marcus, 1995). They have defined baseNP recognition as a tagging task: words can be inside a baseNP (I) or outside of baseNPs (O). In the case that one baseNP immedi- ately follows another baseNP, the first word in the second baseNP receives tag B. Example: Ino early, trading, ino Hong Kongr Monday, gold, waso quotedo ato $1 366.50, an ouncer o This set of three tags is sufficient for encoding baseNP structures since these structures are non- recursive and nonoverlapping. (Tjong Kim Sang and Veenstra, 1999) have pre- sented three variants of this tagging representation. First, the B tag can be used for the first word of every noun phrase (IOB2 representation). Second, instead of the B tag an E tag can be used to mark the last word of a baseNP immediately before another baseNP (IOE1). And third, the E tag can be used for every noun phrase final word (IOE2). They have used the (Ramshaw and Marcus, 1995) representa- tion as well (IOB1). We will use these four tagging representations as well as the O+C representation. 2.3 Machine learning algorithms We have used the memory-based learning algorithm IB1-IG which is part of TiMBL package (Daelemans et al., 1999b). In memory-based learning the train- ing data is stored and a new item is classified by the most frequent classification among training items which are closest to this new item. Data items are represented as sets of feature-value pairs. In IB1-IG each feature receives a weight which is based on the amount of information which it provides for com- puting the classification of the items in the training data. These feature weights are used for computing the distance between a pair of data items (Daele- mans et al., 1999b). IB1-IG has been used success- fully on a large variety of natural language process- ing tasks. Beside IB1-IG, we have used IGTREE in the combi- nation experiments. IGTREE is a decision tree vari- ant of IB1-IG (Daelemans et al., 1999b). It uses the same feature weight method as IB1-IG. Data items are stored in a tree with the most important features close to the root node. A new item is classified by traveling down from the root node until a leaf node is reached or no branch is available for the current feature value. The most frequent classification of the current node will be chosen. 2.4 Combination techniques Our experiments will result in different classifica- tions of the data and we need to find out how to combine these. For this purpose we have evaluated different voting mechanisms, effectively the voting methods as described in (Van Halteren et al., 1998). All combination methods assign some weight to the results of the individual classifier. For each input to- ken, they pick the classification score with the high- est total score. For example, if five classifiers have weights 0.9, 0.4, 0.8, 0.6 and 0.6 respectively and they classify some token as npstart, null, npstart, null and null, then the combination method will pick npstart since it has a higher total score (1.7) than null (1.6). The values of the weights are usually es- timated by processing a part of the training data, the tuning data, which has been kept separate as training data for the combination process. In the first voting method, each of the five classi- fiers receives the same weight (majority). The sec- ond method regards as the weight of each individual classification algorithm its accuracy on the tuning data (TotPrecision). The third voting method com- putes the precision of each assigned tag per classifier and uses this value as a weight for the classifier in those cases that it chooses the tag (TagPrecision). The fourth method uses the tag precision weights as well but it subtracts from them the recall val- ues of the competing classifier results. Finally, the fifth method uses not only a weight for the current classification but it also computes weights for other possible classifications. The other classifications are determined by examining the tuning data and reg- istering the correct values for every pair of classifier results (pair-wise voting). Apart from these five voting methods we have also processed the output streams with two classifiers: IB1-IG (memory-based) and IGTREE (decision tree). This approach is called classifier stacking. Like (Van Halteren et al., 1998), we have used different input versions: one containing only the classifier output and another containing both classifier output and a compressed representation of the classifier input. train O C All correct 96.21% 96.66% Majority correct 1.98% 1.64% Minority correct 0.88% 0.75% All wrong 0.93% 0.95% Table 1: Token classification agreement between the five classifiers applied to the baseNP training data after conversion to the open bracket (O) and the close bracket representation (C). For the latter purpose we have used the part-of- speech tag of the current word. 3 Results Our first goal was to find out whether system combi- nation could improve performance of baseNP recog- nition and, if this was the fact, to select the best combination technique. For this purpose we per- formed a 10-fold cross validation experiment on the baseNP training data, sections 15-18 of the WSJ part of the Penn Treebank (211727 tokens). Like the data used by (Ramshaw and Marcus, 1995), this data was retagged by the Brill tagger in or- der to obtain realistic part-of-speech (POS) tags³. The data was segmented into baseNP parts and non- baseNP parts in a similar fashion as the data used by (Ramshaw and Marcus, 1995). The data was converted to the five data represen- tations (IOB1, IOB2, IOE1, IOE2 and O+C) and IB1-IG was used to classify it by using 10-fold cross validation. This means that the data was divided in ten consecutive parts of about the same size af- ter which each part was used as test data with the other nine parts as training data. The standard pa- rameters of IB1-IG have been used except for k, the number of examined nearest neighbors, which was set to three. Each word in the data was represented by itself and its POS tag and additionally a left and right context of four word-POS tag pairs. For the first four representations, we have used a second pro- cessing stage as well. In this stage, a word was repre- sented by itself, its POS tag, a left and right context of three word-POS tag pairs and a left and right context of two classification results of the first pro- cessing stage (see figure 1). The second processing stage improved the Fβ=1 scores with almost 0.7 on average. The classifications of the IOB1, IOB2, IOE1 and IOE2 representations were converted to the open bracket (O) and close bracket (C) representations. ³No perfect Penn Treebank POS tags will be available for novel texts. If we would have used the Treebank POS tags for NP recognition, our performance rates would have been unrealistically high. train Representation IOB1 IOB2 IOE1 IOE2 O+C Simple Voting Majority TotPrecision TagPrecision Precision-Recall Pairwise Voting TagPair Memory-Based Tags Tags + POS Decision Trees Tags Tags + POS O C 98.01% 98.14% 97.80% 98.08% 97.97% 98.04% 97.89% 98.08% 97.92% 98.13% 98.19% 98.30% 98.19% 98.30% 98.19% 98.30% 98.19% 98.30% 98.19% 98.30% 98.19% 98.34% 98.19% 98.35% 98.17% 98.34% 98.17% 98.34% Table 2: Open and close bracket accuracies for the baseNP training data (211727 tokens). Each com- bination performs significantly better than any of the five individual classifiers listed under Represen- tation. The performance differences between the combination methods are not significant. After this conversion step we had five O results and five C results. In the bracket representations, to- kens can be classified as either being the first token of an NP (or the last in the C representation) or not. The results obtained with these representations have been measured with accuracy rates: the percentage of tokens that were classified correctly. Only about one in four tokens are at a baseNP boundary so guessing that a text does not contains baseNPs will already give us an accuracy of 75%. Therefore the accuracy rates obtained with these representations are high and the room for improvement is small (see table 1). However, because of the different treatment of neighboring chunks, the five classifiers disagree in about 2.5% of the classifications. It seems useful to use combination methods for finding the best classi- fication for those ambiguous cases. The five O results and the five C results were pro- cessed by the combination techniques described in section 2.4. The accuracies per input token for the combinations can be found in table 2. For both data representations, all combinations perform sig- nificantly better than the best individual classifier (p<0.001 according to a x² test)4. Unlike in (Van ⁴We have performed significance computations on the bracket accuracy rates because we have been unable to find a satisfactory method for computing significance scores for --- trading/NN in/IN Hong/NNP Kong/NNP Monday/NNP,/, gold/NN was/VBD quoted/VBN in/IN Hong/NNP/I Kong/NNP/I Monday/NNP,/,/O gold/NN/I was/VBD Figure 1: Example of the classifier input features used for classifying Monday in the example sentence. The first processing stage (top) contains a word and POS context of four left and four right while the second processing stage (bottom) contains a word and POS context of three and a chunk tag context of two. section 20 Majority voting (Muñoz et al., 1999) (Tjong Kim Sang and Veenstra, 1999) (Ramshaw and Marcus, 1995) (Argamon et al., 1998) section 00 Majority voting (Tjong Kim Sang and Veenstra, 1999) (Ramshaw and Marcus, 1995) accuracy O:98.10% C:98.29% O:98.1% C:98.2% 97.58% 97.37% precision recall Fβ=1 93.63% 92.89% 93.26 93.1% 92.4% 92.8 92.50% 92.25% 92.37 91.80% 92.27% 92.03 91.6% 91.6% 91.6 accuracy O:98.59% C:98.65% 98.04% 97.8% precision 95.04% 93.71% 93.1% recall Fβ=1 94.75% 94.90 93.90% 93.81 93.5% 93.3 Table 3: The results of majority voting of different data representations applied to the two standard data sets put forward by (Ramshaw and Marcus, 1995) compared with earlier work. The accuracy scores indicate how often a word was classified correctly with the representation used (O, C or IOB1). The training data for WSJ section 20 contained 211727 tokens while section 00 was processed with 950028 tokens of training data. Majority voting outperforms all earlier reported results for the two data sets. Halteren et al., 1998), the best voting technique did not outperform the best stacked classifier. Further- more the performance differences between the com- bination methods are not significant (p>0.05). To our surprise the five voting techniques performed the same. We assume that this has happened because the accuracies of the individual classifiers do not dif- fer much and because the classification involves a binary choice. Since there is no significant difference between the combination methods, we can use any of them in the remaining experiments. We have chosen to use ma- jority voting because it does not require tuning data. We have applied it to the two data sets mentioned in (Ramshaw and Marcus, 1995). The first data set uses WSJ sections 15-18 as training data (211727 tokens) and section 20 as test data (47377 tokens). The second one uses sections 02-21 of the same cor- pus as training data (950028 tokens) and section 00 as test data (46451 tokens). All data sets were pro- cessed in the same way as described earlier. The results of these experiments can be found in table 3. With section 20 as test set, we managed to reduce the error of the best result known to us with 6% with the error rate dropping from 7.2% to 6.74%, and for section 00 this difference was almost 18% with the Fβ=1 rates. error rate dropping from 6.19% to 5.10% (see table 3). We have also applied majority voting to the NP data set put forward on the CoNLL-99 workshop. In this task the goal is to recognize all NPs. We have approached this as repeated baseNP recogni- tion. A first stage detects the baseNPs. The recog- nized NPs are replaced by their presumed head word with a special POS tag and the result is send to a second stage which recognizes NPs with one level of embedding. The output of this stage is sent to a third stage and this stage finds NPs with two levels of embedding and so on. In the first processing stage we have used the five data representations with majority voting. This ap- proach did not work as well for other stages. The O+C representation outperformed the other four representations by a large margin for the valida- tion data. This caused the combined output of all five representations being worse than the O+C result. Therefore we have only used the O+C repre- sentation for recognizing non-baseNPs. The overall system reached an Fβ=1 score of 83.79 and this is slightly better than the best rate reported at the 5 The validation data is the test set we have used for esti- mating the best parameters for the CoNLL experiment: WSJ section 21. --- CONLL-99 workshop (82.98 (CONLL-99, 1999), an error reduction of 5%). 4 Related work (Abney, 1991) has proposed to approach parsing by starting with finding correlated chunks of words. The chunks can be combined to trees by a sec- ond processing stage, the attacher. (Ramshaw and Marcus, 1995) have build a chunker by apply- ing transformation-based learning to sections of the Penn Treebank. Rather than working with bracket structures, they have represented the chunking task as a tagging problem. POS-like tags were used to account for the fact that words were inside or out- side chunks. They have applied their method to two segments of the Penn Treebank and these are still being used as benchmark data sets. Several groups have continued working with the Ramshaw and Marcus data sets for base noun phrases. (Argamon et al., 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks. This method records POS tag se- quences which contain chunk boundaries and uses these sequences to classify the test data. Its per- formance is somewhat worse than that of Ramshaw and Marcus (Fβ=1=91.6 vs. 92.0) but it is the best result obtained without using lexical information. (Cardie and Pierce, 1998) store POS tag sequences that make up complete chunks and use these se- quences as rules for classifying unseen data. This approach performs worse than the method of Arga- mon et al. (Fβ=1=90.9). Three papers mention having used the memory- based learning method IB1-IG. (Veenstra, 1998) in- troduced cascaded chunking, a two-stage process in which the first stage classifications are used to im- prove the performance in a second processing stage. This approach reaches the same performance level as Argamon et al. but it requires lexical informa- tion. (Daelemans et al., 1999a) report a good per- formance for baseNP recognition but they use a dif- ferent data set and do not mention precision and recall rates. (Tjong Kim Sang and Veenstra, 1999) compare different data representations for this task. Their baseNP results are slightly better than those of Ramshaw and Marcus (Fβ=1=92.37). (XTAG, 1998) describes a baseNP chunker built from training data by a technique called supertag- ging. The performance of the chunker was an improvement of the Ramshaw and Marcus results (Fβ=1=92.4). (Muñoz et al., 1999) use SNOW, a net- work of linear units, for recognizing baseNP phrases 6 We have applied majority voting of five data represen- tations to the Ramshaw and Marcus data set without using lexical information and the results were: accuracy O: 97.60%, accuracy C: 98.10%, precision: 92.19%, recall: 91.53% and Fβ=1: 91.86. and SV phrases. They compare two data representa- tions and report that a representation with bracket structures outperforms the IOB tagging representa- tion introduced by (Ramshaw and Marcus, 1995). SNoW reaches the best performance on this task (Fβ=1=92.8). There has been less work on identifying general noun phrases than on recognizing baseNPs. (Os- borne, 1999) extended a definite clause grammar with rules induced by a learner that was based upon the maximum description length principle. He pro- cessed other parts of the Penn Treebank than we with an Fẞ=1 rate of about 60. Our earlier effort to process the CoNLL data set was performed in the same way as described in this paper but with- out using the combination method for baseNPs. We obtained an Fp=1 rate of 82.98 (CONLL-99, 1999). 5 Concluding remarks We have put forward a method for recognizing noun phrases by combining the results of a memory-based classifier applied to different representations of the data. We have examined different combination tech- niques and each of them performed significantly bet- ter than the best individual classifier. We have cho- sen to work with majority voting because it does not require tuning data and thus enables the indi- vidual classifiers to use all the training data. This approach was applied to three standard data sets for base noun phrase recognition and arbitrary noun phrase recognition. For all data sets majority voting improved the best result for that data set known to us. Varying data representations is not the only way for generating different classifiers for combination purposes. We have also tried dividing the training data in partitions (bagging) and working with artifi- cial training data generated by a crossover-like oper- ator borrowed from genetic algorithm theory. With our memory-based classifier applied to this data, we have been unable to generate a combination which improved the performance of its best member. An- other approach would be to use different classifica- tion algorithms and combine the results. We are working on this but we are still to overcome the prac- tical problems which prevent us from obtaining ac- ceptable results with the other learning algorithms. Acknowledgements We would like to thank the members of the CNTS group in Antwerp, Belgium, the members of the ILK group in Tilburg, The Netherlands and three anony- mous reviewers for valuable discussions and com- ments. This research was funded by the European TMR network Learning Computational Grammars7. http://lcg-www.uia.ac.be/ References Steven Abney. 1991. Parsing by chunks. In Principle- Based Parsing. Kluwer Academic Publishers. Shlomo Argamon, Ido Dagan, and Yuval Krymolowski. 1998. A memory-based approach to learning shal- low natural language patterns. In Proceedings of COLING-ACL '98. Association for Computational Linguistics. Eric Brill and Jun Wu. 1998. Classifier combination for improved lexical disambiguation. In Proceedings of COLING-ACL '98. Association for Computational Linguistics. Claire Cardie and David Pierce. 1998. Error-driven pruning of treebank grammars for base noun phrase identification. In Proceedings of COLING-ACL '98. Association for Computational Linguistics. CONLL-99. 1999. Conll-99 home page. http://lcg- www.uia.ac.be/conll99/. Walter Daelemans, Antal van den Bosch, and Jakub Za- vrel. 1999a. Forgetting exceptions is harmful in lan- guage learning. Machine Learning, 34. Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 1999b. TiMBL: Tilburg Mem- ory Based Learner, version 2.0, Reference Guide. ILK Technical Report 99-01. http://ilk.kub.nl/~ilk/ papers/ilk9901.ps.gz. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated cor- pus of English: the Penn Treebank. Computational Linguistics, 19. Marcia Muñoz, Vasin Punyakanok, Dan Roth, and Dav Zimak. 1999. A learning approach to shallow parsing. In Proceedings of EMNLP-WVLC'99. Association for Computational Linguistics. Miles Osborne. 1999. MDL-based DCG induction for NP identification. In Miles Osborne and Erik Tjong Kim Sang, editors, CoNLL-99 Computational Natural Language Learning. Association for Compu- tational Linguistics. Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third ACL Workshop on Very Large Corpora. Association for Computational Lin- guistics. Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep- resenting text chunks. In Proceedings of EACL '99. As- sociation for Computational Linguistics. Hans van Halteren, Jakub Zavrel, and Walter Daele- mans. 1998. Improving data driven wordclass tagging by system combination. In Proceedings of COLING- ACL'98. Association for Computational Linguistics. Jorn Veenstra. 1998. Fast NP chunking using memory- based learning techniques. In BENELEARN-98: Pro- ceedings of the Eighth Belgian-Dutch Conference on Machine Learning. ATO-DLO, Wageningen, report 352. The XTAG Research Group. 1998. A Lexicalized Tree Adjoining Grammar for English. IRCS Tech Report 98-18, University of Pennsylvania. (also cs.CL/9809024)."
  },
  {
    "title": "Understanding Location Descriptions in the LEI System",
    "abstract": "Biological specimens have historically been labeled with English descriptions of the location of collection. To perform spatial, statistical, or historic studies, these descriptions must be converted into geodetic coordinates. A study of the sublanguage used in the descriptions shows much less frequent than typical usage of observer-relative relations such as \"left of,\" but shows problems with name ambiguity, finding the referents of generic terms like \"the stream,\" ordinal numbering of river forks and valley branches, object-oriented prepositions (\"behind\"), fuzzy boundaries (how close is \"at,\" how far is still \"north of\"), etc. The LEI system implements a semi-automated understanding of such location descriptions. Components of LEI include a language analyzer, a geographical reasoner, an object-oriented geographic knowledge base derived from US Geological Survey digital maps with user input, and a graphical user interface. LEI parses prepositional phrases into spatial relations, converts these into areas, then computes polygon overlays to find the intersection, and returns the minimum bounding rectangle. The user is consulted on unknown words/phrases and ambiguous descriptions.",
    "content": "1 Introduction Many biological specimens collected in the past¹ are la- beled with only an English description of their location of collection.2 To perform any statistical or spatial anal- ysis of this historical data, these descriptions must be converted into geodetic coordinates (latitude-longitude or UTM), a time-consuming process that requires eye- straining poring over maps to search for each location. ¹Current collectors can use hand-held satellite-based geo- positioning systems to record collection coordinates. ²There are an estimated several hundred millions of such labeled specimens. To automate this process requires understanding the nat- ural language descriptions, reasoning about the spatial re- lations described by the natural language, and mapping these into a geographical object base to derive the collec- tion coordinates. 2 Previous Research Talmy [1983], Herskovits [1986], and André et al. [1986] among others have documented the many problems in in- terpreting and using spatial prepositions. For example, in and on have similar but different meanings: \"in the car\" means within the car, while \"on the car,\" means on top of the car. However, \"on the bus/plane,\" means within the bus or plane. Also, each preposition typically has several different meanings or usages. For example, one says \"at home,\" but \"at the bank,\" and the meaning of \"the plane is at Honolulu airport,\" is within the area of Honolulu air- port, but the meaning of \"the dog is at the telephone pole\" is not within the telephone pole, but near it. These context dependent usages make interpretation and application of spatial prepositions problematic. Kuipers [1985], Davis [1986], Peuquet and Ci-Xiang [1987], and Frank [1991] have investigated qualitative and/or quantitative reasoning techniques for dealing with spatial relations. Freeman [1975] and Mark and Frank [1991] have identified commonly used spatial relations. 3 Characteristics of the Domain Although general purpose natural language processing (NLP) is beyond current state-of-the-art, limited domains have frequently been amenable to NLP using specific techniques because the domains use a \"sublanguage,\" a fairly restricted subset of a general natural language, which may have its own syntax and peculiarities. In this case, an analysis of one thousand three hundred and forty sample location descriptions from the Bishop Museum's Herbarium Pacificum collection (accumulated by about two hundred different collectors over a period of 160 years) shows a highly restricted use of language that is amenable to understanding using specialized techniques. Because these descriptions are meant to be read later by a reader who is not at the site, they contain very few observer relative descriptions (e.g., behind). Also, there are limits to the scale of the descriptions. For example, out of a thousand descriptions that were located manually, about half of the descriptions were judged to be accurate to within 1/3 of a mile and 73% accurate to within 1 mile. At the other end, there were no descriptions with accu- racy in the meter range and the best descriptions were only accurate to within several tens of meters. A typical location description is: \"Punaluu Valley; Castle Trail from Punaluu to Kaluanui Valley + stream, on E. side of Northern Koolaus.\" Associated acces- sion information typically includes the date, collector name(s), genus, elevation, the museum's collection num- ber, and the collector's accession number. This sublan- guage is made up mostly of named objects (e.g., \"Punaluu Valley\" and \"Castle Trail\") and prepositional phrases (e.g., from Punaluu to Kaluanui Valley + stream, on E. side of Northern Koolaus). The relation of the collection location to the unmodified named objects is almost al- ways \"within,\" that is, the collection location is within the area designated by the named geographic object. The in- terpretation of the prepositions is somewhat simpler than the general case because the sublanguage deals only with a two dimensional cartographic space supplemented by elevation markings. This sublanguage is relatively sim- ple syntactically, but there are still many problems for automatic interpretation of the sublanguage. One of the most common problems in interpreting this sublanguage is the inconsistent use of names. For exam- ple, Waikane-Schofield Trail also appears as Schofield- Waikane Trail, Schofield-Waikane Ditch Trail, Schofield Trail, W-S Ditch Trail, Schofield Waikane Trail, and Waikane Ditch Trail. A mountain like Kaala may be re- ferred to as Mt Kaala, Mt. Kaala, Kaala Mts., Kaala Mountain(s), Mount Kaala, Kaala Puu (puu is the Hawai- ian word for mountain), Kaala Summit, or Kaala Range. Another problem is that names are often not unique. For example, Manoa is the name for Manoa Falls, Manoa Valley, Manoa Valley Park, Manoa Triangle Park, Manoa Stream, Manoa Elementary School, Manoa Japanese Language School, Manoa Tunnel, and Manoa Falls. When Manoa appears by itself, which Manoa is meant is usually clear from the context. In many cases, the same name is even used for the same type of object (e.g., many cities have Elm Streets and Main Streets). Luckily, similar objects with shared names tend to be geograph- ically separated, otherwise confusion would result. An- other very frequent problem is missing names. Collec- tors often use generic terms like stream and gulch to re- fer to landmarks that do have names. As before, the con- text is usually enough to find the correct object reference even without knowing the name. This heavy reliance on context for disambiguation is also a frequent problem for general purpose NLP systems. A difficult problem is the interpretation of ordinal numberings, which are used to differentiate forks of streams and branches of valleys. For example, \"3rd branch S of S fork of Kahanaiki Stream,\" refers to the third branch after the main South fork of Kahanaiki Stream counting from the head of the stream. Unfortu- nately, this description could also refer to the 3rd branch following the path of the collector going up the stream. Similarly, \"Honolulu Valley, 4th small gulch,\" refers to 4th small gulch counting from the open head of the valley, although this could easily be interpreted as the 4th small gulch along some trail that might enter the valley from some pass over the mountains at the tail end of the val- ley. Another problem is the occasional use of land cover- age types such as \"middle Metrosideros forest,\" \"wooded gulch,\" and \"Fern Forest.\" Not only do most geograph- ical databases lack land coverage information, but such information changes frequently over time. Also, descrip- tions sometimes refer to rainfall frequency, sun exposure or other ephemeral attributes of the area: \"in dryish for- est,\" \"wet valley,\" \"deep shade in wet gulch,\" and \"shady hillside.\" Even after converting the location descriptions into the appropriate spatial relations, there are still many prob- lems in the correct interpretation of the relations. For example, \"along a stream\" does not mean that the col- lection site was in the stream, but within some distance of the stream. The problem is what exactly is the value of that distance. Even cardinal directions like \"north of\" are fuzzy concepts. The region north of a point can be bounded by two vectors pointing NE and NW (the trian- gular model), but this model fails when computing north of an object that is elongated in the E-W direction. Some spatial relations like \"in front of,\" \"behind,\" and \"be- yond\" are relative to the observer's direction. Although these are not very frequent (only 25 cases in the 1340 sample descriptions), they still appear. Other spatial re- lations like \"above\" are dependent on understanding the slope of elevation around the object. To solve some of these problems, we have developed and implemented the LEI system to partially automate interpretation of this sublanguage. LEI is described in the following section. 4 The LEI System 4.1 Organization The LEI³ (Location and Elevation Interpreter) system is an implementation of our algorithms for interpreting the sublanguage of location description labels for biological specimens. LEI is composed of four main components: the language analyzer PPI, the geographical reasoner GR, the user interface LEIview, and the geographic knowl- edge base GKB. The geographic knowledge base con- tains an object-oriented description of geographical ob- jects such as valleys, streams, and waterfalls with their associated locations and names. The user interface dis- plays maps and allows users to add or modify object lo- cations. The language analyzer parses the English loca- tion description and produces a collection of spatial rela- tions that relate the actual collection point to geograph- ical objects. It uses knowledge of geographical objects ³Lei is also the Hawaiian word for \"garland,\" typically made out of flowers, leaves, or feathers. and their associated names from the geographic knowl- edge base. The geographical reasoner translates spatial relations from the language analyzer into polygons and performs polygon intersection calculations to obtain the area specified by the spatial relations. Each component is described in detail below. 4.2 GKB, the Geographic Knowledge Base LEI uses three U.S. Geological Survey (USGS) digital cartographic databases as the starting point for GKB, the Geographic Knowledge Base. These include the DLG (Digital Line Graph), GNIS (Geographic Name Informa- tion System), and DEM (Digital Elevation Model). Un- fortunately, these databases are not object-oriented, that is, they do not link the names in GNIS to the object loca- tions in DLG. The GNIS database contains only names, USGS quadrangles, a feature class, and the coordinates of the name as it appears on a USGS map. The DLG database contains a hierarchical organization of points, line segments (composed of points), and areas (composed of line segments) along with a two-level type hierarchy composed of major and minor codes. Unfortunately, the 60 plus GNIS feature classes do not correspond to the over 200 DLG major and minor codes. The DEM database consists of a raster style set of elevation values. To convert the three USGS databases into a useful object-oriented format requires human intervention to as- sociate the names with the line and area objects (point objects are already correctly located by the GNIS coordi- nates). This process uses the LElview component to dis- play the appropriate section of the map to the user, then displays names one by one along with the geographically closest object of the same type as the name. The user can confirm the match, ask for alternatives, or modify the set of line segments or areas to give the actual location of the named object. In many cases, there are no corresponding objects, so the user must draw the outline of the area from scratch. This is required for all valleys and mountains since these are missing from the DLG specifications. Determining the closest object of the same type re- quires matching the GNIS feature class to appropriate DLG major and minor codes. This is done using LEI's own type hierarchy that includes type classes correspond- ing to each GNIS feature class and to each DLG major and minor code along with many bridging type classes and higher level types. Given a GNIS feature class, LEI first indexes into LEI's type hierarchy to find the cor- responding LEI type. If this type has a corresponding DLG code, then that is the most likely match. Less likely matches consist of any subtypes that might have associ- ated DLG codes. If there are no DLG codes at this type level nor at subtype levels, then LEI searches up the hi- erarchy for supertypes that have associated DLG codes. Using this algorithm, the matching process manages to find the correct match most of the time, so the user's time is freed to worry about the many missing entries and er- rors in USGS databases (e.g., rivers that extend into what should be coastlines, disconnected lines, etc.). 4.3 LEIview, the User Interface The LEIview component provides a graphical interface that allows users to view maps; zoom; scroll; rearrange, add, and delete layers of the map (including DLG lines, GNIS names, and DEM elevations); search for named ob- jects; enter points, line segments, or areas for new ob- jects; modify existing objects; and view the results of interpreting location descriptions (both the English de- scription and the area resulting from processing are dis- played). LEIview is written in C under X windows with Motif widgets. LEIview is used to associate names with object loca- tions in building the GKB geographic knowledge base. It is also used to display the results of interpreting location descriptions. When there are sections of the description that are not comprehensible to the PPI language analyzer, LEI sends the description to LEIview, which displays the description with the incomprehensible parts highlighted and displays the regions corresponding to the understood portions of the description. The user can tell LEI to ig- nore the unknown parts of the description, delay process- ing this description until later, send the description back for reprocessing, or add new geographic objects by en- tering new points, line segments, or areas and selecting the corresponding words in the description. Any new ob- jects are stored in GKB and the correspondence between the words and the new object are stored in the PPI lan- guage knowledge base. 4.4 PPI, the Language Analyzer The PPI (Prepositional Phrase Interpreter) component is responsible for parsing the natural language location de- scriptions and converting them into spatial relations. PPI uses the PAU4 parser and understander [Chin, 1992] to interpret the English descriptions and convert them into spatial relations represented in the MERA (Meta En- tity Relation Attribute) semantic-network-style knowl- edge representation language [Takeda et al., 1992]. PAU is an all-paths, chart-based, unification parser that com- pletely integrates syntactic and semantic processing. Figure 1 shows the MERA graph for the grammar rule, PP ← Prep NP (i.e., a Prepositional-Phrase is a Preposi- tion followed by Noun-Phrase), along with its semantic interpretation. The node PP-pat represents the left-hand- side of the rule, and the relations Pca (pattern component A) and Pcb (pattern component B) point to the compo- nents on the right-hand-side of the rule. The Ref relation denotes the meaning of the rule: a Geographic-object that has a Spatial-relation to some other Geographic-object. The Unify relation between the Prep and the Spatial- relation indicates that the meaning of the Prep should be unified with the relation, Spatial-relation. Likewise, the Unify relation between the NP and the lower Geographic- object indicates that the meaning of the NP should be uni- fied with the lower Geographic-object. Figure 2 speci- fies the meaning of the Prep, \"along\" as an instance of 4 Pau is also the Hawaiian word for \"finished.\" Ref PP-pat Geographic-object Spatial-relation Prep NP Unify Unify Geographic-object PP-pat Ref Geographic-object Near Prep word=along NP Unify Linear-object Figure 1: Rule for PP ← Prep NP. Figure 3: The PP-pat rule after parsing \"along\" and be- fore parsing \"Ainapo.\" the Spatial-relation Near relating a Geographic-object to a Linear-object (a subtype of Geographic-object). Geographic-object Ref Prep word=along Near Linear-object Figure 2: The meaning of the preposition \"along.\" The interpretation of the PP \"along Ainapo\" demon- strates how the integration of syntactic and semantic pro- cessing in PAU allows the early use of semantic con- straints to reject semantically anomalous parses. \"Along Ainapo\" is ambiguous because Ainapo is both a trail and an area. However, since \"along\" only applies to linear objects such as trails, the Ainapo area interpretation is rejected by PAU. This happens when PAU is applying the grammar rule of Figure 1. When unifying the mean- ing of the Prep \"along\" (shown in Figure 2) with the Spatial-relation, the result is a Near relation. However, the sources and sinks of both relations must also be uni- fied. This changes the lower Geographic-object into a Linear-object as seen in Figure 3, which shows the state of the \"PP - Prep NP\" rule just before parsing \"Ainapo.\" In PAU, both meanings of Ainapo are tried in parallel. The area meaning of Ainapo is rejected because an Area- object cannot unify with a Linear-object. This leaves only the Ainapo trail meaning to parse successfully. Table 1 shows the spatial relations in PPI along with the corresponding prepositions. 4.5 GR, the Geographical Reasoner The GR (Geographical Reasoner) component takes the output from the PPI component, which is a set of spatial relations, converts these into polygons, performs poly- gon overlay operations to find the common intersection of all the polygons, computes the center of the mini- mum bounding rectangle (mbr) of the polygon intersec- tion, then returns the coordinates and centroid of the mbr. GR like PPI is written in Common LISP and converses with LEIview through UNIX sockets. The first step is the most difficult since there are no generally accepted algorithms for converting spatial rela- tions into areas. For the spatial relations based on cardi- nal directions such as East-of, Peuquet and Zhan (1987) give a complex algorithm for determining if one poly- gon is in a particular directional relationship with another. Their algorithm takes into account the shapes of the poly- gons (e.g., east of an elongated N-S polygon covers a dif- ferent area than east of a small point polygon) and con- siders polygons that partially enclose or intertwine one another. Their algorithm is a refinement of the basic trian- gular model (in which North is the open-ended triangular region between two vectors pointing NE and NW), but it still does not give any limits concerning the distance between the polygons. Unfortunately there is no abso- lute distance that forms the edge of the region North-of some polygon. In a sense, the edge is given by the limit of human sight in that direction. The algorithm currently used in GR for interpreting cardinal-direction relations around an geographic object starts by computing the minimum bounding rectangle (mbr) for the object. The area next to the mbr with the same size as the mbr is taken as the meaning of the spa- tial relation. Since the resultant area is the same size as the original object, this makes the meaning of cardinal di- rections relative to the size of the reference object, taking into account the fact that larger objects are visible from farther away. Cardinal directions relative to point objects are interpreted as a square, 500 meters on a side, lying in the appropriate direction. The observer or object oriented relations such as Adjacent-to (\"beside waterfall,\" \"on Kona-Hilo Hwy\"), Beyond (\"1 1/2 mile beyond end of 20 Mile Road,\" \"at back of Waihoi Valley\"), Front-of (no examples in the sample data), Right-of (\"right hand side of Kupu Kai Gap\"), and Left-of (\"Kulani Prison Road, toward Ku- lani Prison from intersection w/ Volcano Road, left road- Spatial-relation Prepositions Adjacent-to adjacent to, beside, next to, on At-elevation above, at, below, down, up Between between Beyond [in/at] back of, behind, beyond East-of east of From from Front-of before, [in] front of Left-of [to [the]] left [hand side] of Near adjacent to, along, around, at, by, near, outside [of] North-of north of Right-of [to [the]] right [hand side] of South-of south of Toward to, toward West-of west of Within among, at, in, inside, into, on, on top of Table 1: Spatial Relations and Prepositions in PPI. side\") require understanding the orientation of the object or observer. Currently in GR, only object-oriented rela- tions are processed. Given an object with a front, back, and sides (left and right), the corresponding relations are Front-of, Beyond, and Adjacent-to (Left-of and Right- of). These areas are calculated from the object's mbr in a similar fashion to the cardinal direction relations. The At-elevation relation with respect to a given al- titude requires computing the subregion of the common intersection area that is within 40 meters of the given elevation. The At-elevation relation relative to an object (e.g., \"above Schofield\") requires computing the preva- lent slope of the terrain around the object. GR takes a 200 meter square on the up/down side of the object. Between, From, and Toward are handled by taking the mbr of the two objects, then computing the two corner points on each mbr that is furthest on either side from the line connecting the centers of the mbrs. These four points are then connected to form the area between the two objects. The Near relation is converted into a buffer zone around the area. Currently GR uses a fixed distance of 200 meters for simplicity, however further study is needed to determine if this corresponds to most people's interpretation. There may be individual, cultural, or re- gional differences in interpretation. Also, the size of the buffer zone may depend on the size of the geographic ob- ject. Currently GR does not handle references to terrain type, wetness, or typical sun exposure because this type of data is not available in the USGS databases. Refer- ences to ordinal forks and branches are assumed to start from the head of the rivers or valleys. Generic terms are handled after processing all other spatial relations by ex- haustively searching for any instances of the same type ⁵USGS DEM data have a vertical resolution of one meter and a horizontal resolution of thirty meters. (or subtypes) that intersect with the intersection of the other known areas. In cases of multiple matches, the user is asked to help disambiguate through LEIview. 5 Future Directions Because collectors often collect specimens on trips (ei- ther day hikes or multi-day camping expeditions), an analysis of the path of the collectors should yield valu- able information about the location of collection. Speci- mens are typically labeled with the collection date and the collector's accession number, which provides the relative time of collection for specimens on that day. This infor- mation can be used to disambiguate location descriptions and to pinpoint vague locations. For example, in Hawaii, there are not only two Waihee Streams, but also a Wai- hee River. In a location description that mentions, \"along Waihee Stream,\" there is ambiguity as to which of these three waterways is actually meant. In the current version of LEI, disambiguation is possible only if the description contains more information that specifies an area that in- tersects with only one of the three streams. By adding reasoning about time using accession dates and numbers and combining this with reasoning about paths, LEI could determine that it is unlikely that the collector stopped col- lecting specimens along one Waihee Stream, flew to an- other island to collect a specimen along another Waihee Stream, then flew back to continue collecting along the first Waihee Stream. This type of reasoning can also help to pinpoint which part of Waihee Stream is meant by \"along Wai- hee Stream.\" If LEI knows that the previous specimen was collected at point A and the following specimen was collected at point B, then LEI can make the reasonable assumption that this specimen was collected somewhere near the intersection of Waihee Stream and a region be- tween points A and B. Using this type of reasoning, LEI can even make a reasonable guess about the collection location of specimens that have no location labels (pro- vided only that they have an accession number and acces- sion date given by the collector and the adjacent specimen numbers can be located). Adding such reasoning about time and paths would improve the accuracy of LEI's pro- cessing. 6 Conclusions The LEI system demonstrates the feasibility of under- standing the sublanguage used in location descriptions for biological specimens. Although this is an important and valuable task in and of itself, there is a much greater potential for application of the NLP and geographical reasoning techniques demonstrated in LEI to other areas such as natural language interfaces to general GISs (Geo- graphic Information Systems). There is a need for valida- tion of these techniques and a study is currently planned to compare the results of LEI with results obtained man- ually. Finally, the problems encountered in building LEI point to several new directions. First, the GKB com- ponent shows how object-oriented geographic databases should be organized in the future. Second, many new studies are required to determine the limits of fuzzy spa- tial relations like North-of, Front-of, and Near. Such studies should investigate task dependencies, context de- pendencies, individual variances, and cultural/regional variances. Such studies would lead to advances in under- standing human cognition of spatial relations that would be directly applicable in GISs like LEI. 7 Acknowledgements This research was sponsored in part by the National Sci- ence Foundation BSR&SES Grant No. BSR-9019041. References André, E., G. Bosch, G. Herzog, and T. Rist (1986). Coping with the Intrinsic and Deictic Uses of Spatial Prepositions. In Ph. Jorrand and V. Sgureg (Eds.), Artificial Intelligence II, Proceedings of AIMSA-86, pp. 375-382. Chin, D. N. (1992). \"PAU: Parsing and Understanding with Uniform Syntactic, Semantic, and Idiomatic Representations.\" In Computational Intelligence, 8(3), pp. 456-476. Davis, E. (1986). Representing and Acquiring Geo- graphic Knowledge. Morgan Kaufman, Los Altos, CA. Frank, A. (1991). Qualitative Reasoning about Cardinal Directions. In D. Mark and D. White (Eds), Pro- ceedings of Autocarto 10, pp. 148-167. Freeman, J. (1975). The Modeling of Spatial Relations. In Computer Graphics and Image Processing, 4, pp. 156-171. Herskovits, A. (1986). Language and Spatial Cogni- tion. Cambridge University Press, Cambridge. Kuipers, B. J. (1985). Modeling Human Knowledge of Routes: Partial Knowledge and Individual Varia- tion. In the Proceedings of the Third National Con- ference on Artificial Intelligence, pp. 216-219. Mark, D.M. and A.U. Frank (1991). (Eds.), Cogni- tive and Linguistic Aspects of Geographic Space, Klewer Academic Publishers, Boston. Peuquet, D. and Z. Ci-Xiang (1987). An Algorithm to Determine the Directional Relationship between Arbitrarily-Shaped Polygons in the Plane. In Pat- tern Recognition 20(1), pp. 65-74. Takeda K., D. N. Chin, and I. Miyamoto (1992). MERA: Meta Language for Software Engineering. In the Proceedings of the 4th International Confer- ence on Software Engineering and Knowledge En- gineering, Capri, Italy, June, pp. 495-502. Talmy, L. (1983). How Language Structures Space. In H. Pick and L. Acredolo, Eds., Spatial Orientation: Theory, Research, and Application Plenum Press, New York, pp. 225-282."
  },
  {
    "title": "Modelling Grounding and Discourse Obligations Using Update Rules",
    "abstract": "This paper describes an implementation of some key aspects of a theory of dialogue processing whose main concerns are to provide models of GROUNDING and of the role of DISCOURSE OBLIGATIONS in an agent's deliberation processes. Our system uses the TrindiKit dialogue move engine toolkit, which assumes a model of dialogue in which a participant's knowledge is characterised in terms of INFORMATION STATES which are subject to various kinds of updating mechanisms.",
    "content": "Introduction In this paper we describe a preliminary implemen- tation of a 'middle-level' dialogue management sys- tem. The key tasks of a dialogue manager are to update the representation of dialogue on the basis of processed input (generally, but not exclusively, lan- guage utterances), and to decide what (if anything) the system should do next. There is a wide range of opinions concerning how these tasks should be per- formed, and in particular, how the ongoing dialogue state should be represented: e.g., as something very specific to a particular domain, or according to some more general theory of (human or human inspired) dialogue processing. At one extreme, some systems represent only the (typically very rigid) transitions possible in a perceived dialogue for the given task, often using finite states in a transition network to represent the dialogue: examples of this are sys- tems built using Nuance's DialogueBuilder or the CSLU's Rapid Application Prototyper. The other extreme is to build the dialogue processing theory on top of a full model of rational agency (e.g., (Bretier and Sadek, 1996)). The approach we take here lies in between these two extremes: we use rich repre- sentations of information states, but simpler, more dialogue-specific deliberation methods, rather than a deductive reasoner working on the basis of an ax- iomatic theory of rational agency. We show in this paper that the theory of information states we pro- pose can, nevertheless, be used to give a character- isation of dialogue acts such as those proposed by the Discourse Resource Initiative precise enough to formalise the deliberation process of a dialogue man- ager in a completely declarative fashion. Our implementation is based on the approach to dialogue developed in (Traum, 1994; Poesio and Traum, 1997; Poesio and Traum, 1998; Traum et al., 1999). This theory, like other action-based theories of dialogue, views dialogue participation in terms of agents performing dialogue acts, the effects of which are to update the information state of the partici- pants in a dialogue. However, our view of dialogue act effects is closer in some respects to that of (All- wood, 1976; Allwood, 1994) and (Singh, 1998) than to the belief and intention model of (Sadek, 1991; Grosz and Sidner, 1990; Cohen and Levesque, 1990). Particular emphasis is placed on the social commit- ments of the dialogue participants (obligations to act and commitments to propositions) without mak- ing explicit claims about the actual beliefs and in- tentions of the participants. Also, heavy empha- sis is placed on how dialogue participants socially GROUND (Clark and Wilkes-Gibbs, 1986) the infor- mation expressed in dialogue: the information state assumed in this theory specifies which information is assumed to be already part of the common ground at a given point, and which part has been introduced, but not yet been established. The rest of this paper is structured as follows. The theory of dialogue underlying the implementation is described in more detail in Section 2. Section 3 de- scribes the implementation itself. Section 4 shows how the system updates its information state while participating in a fairly simple dialogue. 2 Theoretical Background One basic assumption underlying this work is that it is useful to analyse dialogues by describing the relevant 'information' that is available to each par- ticipant. The notion of INFORMATION STATE (IS) is therefore employed in deciding what the next action should be, and the effects of utterances are described in terms of the changes they bring about in ISs. A particular instantiation of a dialogue manager, from this point of view, consists of a definition of the con- tents of ISs plus a description of the update processes which map from IS to IS. Updates are typically trig- gered by 'full' dialogue acts such as assertions or directives, of course, but the theory allows parts of utterances, including individual words and even sub- parts of words, to be the trigger. The update rules for dialogue acts that we assume here are a simpli- fied version of the formalisations proposed in (Poesio and Traum, 1998; Traum et al., 1999) (henceforth, PTT). The main aspects of PTT which have been im- plemented concern the way discourse obligations are handled and the manner in which dialogue partic- ipants interact to add information to the common ground. Obligations are essentially social in nature, and directly characterise spoken dialogue; a typical example of a discourse obligation concerns the rela- tionship between questions and answers. Poesio and Traum follow (Traum and Allen, 1994) in suggesting that the utterance of a question imposes an obliga- tion on the hearer to address the question (e.g., by providing an answer), irrespective of intentions. As for the process by which common ground is es- tablished, or GROUNDING (Clark and Schaefer, 1989; Traum, 1994), the assumption in PTT is that classi- cal speech act theory is inherently too simplistic in that it ignores the fact that co-operative interaction is essential in discourse; thus, for instance, simply as- serting something does not make it become mutually 'known' (part of the common ground). It is actually necessary for the hearer to provide some kind of ac- knowledgement that the assertion has been received, understood or not understood, accepted or rejected, and so on. Poesio and Traum view the public in- formation state as including both material that has already been grounded, indicated by GND here, and material that hasn't been grounded yet. These com- ponents of the information state are updated when GROUNDING ACTS such as acknowledgement are per- formed. Each new contribution results in a new DIS- COURSE UNIT (DU) being added to the information state (Traum, 1994) and recorded in a list of 'un- grounded discourse units' (UDUS); these DUs can then be subsequently grounded as the result, e.g., of (implicit or explicit) acknowledgements. 3 Implementing PTT In this section, we describe the details of the im- plementation. First, in Section 3.1, we describe the TrindiKit tool for building dialogue managers that we used to build our system. In Section 3.2, we de- scribe the information states used in the implemen- tation, an extension and simplification of the ideas from PTT discussed in the previous section. Then, in Section 3.3, we discuss how the information state is updated when dialogue acts are observed. Finally, $^1$ We assume here the DRI classification of dialogue acts (Discourse Resource Initiative, 1997). Input Control Dialogue Move Engine (DME) Interpreter Update module Selection module Generator Output Information State (IS) [ ... ] Figure 1: TrindiKit Architecture in Section 3.4, we describe the rules used by the sys- tem to adopt intentions and perform its own actions. An extended example of how these mechanisms are used to track and participate in a dialogue is pre- sented in Section 4. 3.1 TrindiKit The basis for our implementation is the TrindiKit dialogue move engine toolkit implemented as part of the TRINDI project (Larsson et al., 1999). The toolkit provides support for developing dialogue sys- tems, focusing on the central dialogue management components. The system architecture assumed by the TrindiKit is shown in Figure 1. A prominent feature of this ar- chitecture is the information state, which serves as a central 'blackboard' that processing modules can ex- amine (by means of defined CONDITIONS) or change (by means of defined OPERATIONS). The structure of the IS for a particular dialogue system is defined by the developer who uses the TrindiKit to build that system, on the basis of his/her own theory of dialogue processing; no predefined notion of infor- mation state is provided.$^2$. The toolkit provides a number of abstract data-types such as lists, stacks, and records, along with associated conditions and operations, that can be used to implement the user's theory of information states; other abstract types can also be defined. In addition to this customis- able notion of information state, TrindiKit provides a few system variables that can also used for inter- module communication. These include input for the raw observed (language) input, latest_moves which $^2$In TRINDI we are experimenting with multiple instanti- ations of three different theories of information state (Traum et al., 1999). contains the dialogue moves observed in the most recent turn, latest_speaker, and next moves, con- taining the dialogue moves to be performed by the system in the next turn. A complete system is assumed to consist of sev- eral modules interacting via the IS. (See Figure 1 again.) The central component is called the DIA- LOGUE MOVE ENGINE (DME). The DME performs the processing needed to integrate the observed di- alogue moves with the IS, and to select new moves for the system to perform. These two functions are encapsulated in the UPDATE and SELECTION sub- modules of the DME. The update and select mod- ules are specified by means of typed rules, as well as sequencing procedures to determine when to apply the rules. We are here mainly concerned with UP- DATE RULES (urules), which consist of four parts: a name, a type, a list of conditions to check in the in- formation state, and a list of operations to perform on the information state. urules are described in more detail below, in Section 3.3. There are also two modules outside the DME proper, but still cru- cial to a complete system: INTERPRETATION, which consumes the input and produces a list of dialogue acts in the latest_moves variable (potentially mak- ing reference to the current information state), and GENERATION, which produces NL output from the dialogue acts in the next_moves variable. Finally, there is a CONTROL module, that governs the se- quencing (or parallel invocation) of the other mod- ules. In this paper we focus on the IS and the DME; our current implementation only uses very simple interpretation and generation components. 3.2 Information States in PTT In this section we discuss the information state used in the current implementation. The main difference between the implemented IS and the theoretical pro- posal in (Poesio and Traum, 1998) is that in the im- plementation the information state is partitioned in fields, each containing information of different types, whereas in the theoretical version the information state is a single repository of facts (a DISCOURSE REPRESENTATION STRUCTURE). Other differences are discussed below. An example IS with some fields filled is shown in Figure 2; this is the IS which results from the second utterance in the example dialogue discussed in Section 4, A route please.<sup>3</sup> The IS in Figure 2 is a record with two main parts, W and C. The first of these represents the system's (Wizard) view of his own mental state and of the (semi-)public information discussed in the di- alogue; the second, his view of the user's (Caller) information state. This second part is needed to <sup>3</sup>All diagrams in this paper are automatically generated from TrindiKit system internal representations and displayed using the Thistle dialogue editor (Calder, 1998). Some have been subsequently edited for brevity and clarity. OBL: GND: DH: SCP: COND: UDUS: <understandingAct(W,DU3)> <address(C,CA2)> <CA3: C2, acknowledge (C,DU2)> <CA2: C2, info_request(W,?helpform)> < > < > <DU3> OBL: DH: TOGND: SCP: COND: PDU: W: ID: DU2 OBL: DH: TOGND: CDU: SCP: <address(C,CA2)> <CA2: C2, info_request(W,?helpform)> < > < > < > <address(W,CA6)> <CA6: C2, direct(C,giveroute(W))> <CA5: C2, answer (C,CA2,CA4)> <CA4: C2, assert(C,want(C,route))> <scp(C,want(C,route))> <accept(W,CA6) -> obl(W,giveroute(W))> DU3 info_request(W,?start) giveroute(W) accept(W,CA6) acknowledge(W,DU3) [C: [INT: <getroute(C)>] COND: INT: ID: Figure 2: Structure of Information States model misunderstandings arising from the dialogue participants having differing views on what has been grounded; as we are not concerned with this problem here, we will ignore C in what follows. w contains information on the grounded mate- rial (GND), on the ungrounded information (UDUS, PDU and CDU), and on W's intentions (INT). GND contains the information that has already been grounded; the other fields contain information about the contributions still to be grounded. As noticed above, in PTT it is assumed that for each new ut- terance, a new DU is created and added to the IS. The current implementation differs from the full the- ory in that only two DUs are retained at each point; the current DU (CDU) and the previous DU (PDU). The CDU contains the information in the latest con- tribution, while the PDU contains information from the penultimate contribution. Information is moved from PDU to GND as a result of an ack (acknowl- edgement) dialogue act (see below.) The DUs and the GND field contain four fields, representing obligations (OBL), the dialogue history (DH), propositions to which agents are socially com- mitted (SCP), and conditional updates (COND). The value of OBL is a list of action types: actions that agents are obliged to perform. An action type is specified by a PREDICATE, a DIALOGUE PARTICI- PANT, and a list of ARGUMENTS. The value of SCP is a list of a particular type of mental states, so- cial commitments of agents to propositions.<sup>4</sup> These are specified by a DIALOGUE PARTICIPANT, and a PROPOSITION. Finally, the elements in DH are dia- <sup>4</sup>SCPs play much the same role in PTT as do beliefs in many BDI accounts of speech acts. logue actions, which are instances of dialogue action types. A dialogue action is specified by an action type, a dialogue act id, and a confidence level CONF (the confidence that an agent has that that dialogue act has been observed). The situation in Figure 2 is the result of updates to the IS caused by utterance [2] in the dialogue in (6), which is assumed to generate a direct act as well as an assert act and an answer act.5 That utterance is also assumed to contain an implicit acknowledge- ment of the original question; this understanding act has resulted in the contents of DU2 being grounded (and subsequently merged with GND), as discussed below. GND.OBL in Figure 2 includes two obligations. The first is an obligation on W to perform an under- standing act (the predicate is understandingAct, the participant is W, and there is just one argument, DU3, which identifies the DU in CDU by referring to its ID). The second obligation is an obligation on C to address conversational act CA2; this ID points to the appropriate info_request in the DH list by means of the ID number. Obligations are specified in CDU and PDU, as well. Those in PDU are simply a subset of those in GND, since at point in the up- date process shown in Figure 2 this field contains information that has already been grounded (note that DU2 is not in UDUS anymore); but CDU con- tains obligations that have not been grounded yet— in particular, the obligation on W to address CA6. GND.DH in this IS contains a list of dialogue ac- tions whose occurrence has already been grounded: the info_request performed by utterance 1, with ar- gument a question, and the implicit acknowledge performed by utterance 2.7 The DH field in CDU con- tains dialogue acts performed by utterance 2 that do need to be grounded: a directive by C to W to per- form an action of type giveroute, and an assert by C of the proposition want(C, route), by which C provides an answer to the previous info_request CA2. The COND field in CDU contains a conditional up- date resulting from the directive performed by that utterance. The idea is that directives do not imme- diately lead to obligations to perform the mentioned action: instead (in addition to an obligation to ad- dress the action with some sort of response), their ef- fect is to add to the common ground the information that if the directive is accepted by the addressee, 5 The fact that the utterance of a route please constitutes an answer is explicitly assumed; however, it should be possible to derive this information automatically (perhaps along the lines suggested by Kreutel (Kreutel, 1998)). 6 We use the notation ?p to indicate a question of the form ?([x],p(x)). 7 We assume here, as in (Traum, 1994) and (Poesio and Traum, 1998), that understanding acts do not have to be grounded themselves, which would result in a infinite regress. then he or she has the obligation to perform the ac- tion type requested. (In this case, to give a route to C.) 3.3 Update Rules in PTT We are now in a position to examine the update mechanisms which are performed when new dia- logue acts are recognised. When a dialogue par- ticipant takes a turn and produces an utterance, the interpretation module sets the system variable latest_moves to contain a representation of the di- alogue acts performed with the utterance. The up- dating procedure then uses update rules to modify the IS on the basis of the contents of latest_moves and of the previous IS. The basic procedure is de- scribed in (1) below.8 (1) 1. Create a new DU and push it on top of UDUs. 2. Perform updates on the basis of backwards grounding acts. 3. If any other type of act is observed, record it in the dialogue history in CDU and apply the update rules for this kind of act 4. Apply update rules to all parts of the IS which contain newly added acts. The first step involves moving the contents of CDU to PDU (losing direct access to the former PDU con- tents) and putting in CDU a new empty DU with a new identifier. The second and third steps deal explicitly with the contents of latest_moves, ap- plying one urule (of possibly a larger set) for each act in latest_moves. The relevant effects for each act are summarised in (2), where the variables have the following types: IDx DUx DP Q PROP Act o(DP) P(ID) Q(ID) Dialogue Act Identification Number DU Identification Number Dialogue Participant (i.e., the speaker) A Question A Proposition An Action The other dialogue participant The content of the ID, a proposition The content of the ID, a question 8 See (Poesio et al., 1999; Traum et al., 1999) for different versions of this update procedure used for slightly different versions of the theory. (2) act ID:2, accept(DP,ID2) effect accomplished via rule resolution act ID:2, ack(DP,DU1) effect peRec(w.Gnd,w.pdu.tognd) effect remove(DU1,UDUS) act ID:2, agree(DP,ID2) effect push(SCP,scp(DP,ID2)) act ID:2, answer(DP,ID2,ID3) effect push(SCP,ans(DP,Q(ID2),P(ID2))) act ID:2, assert(DP,PROP) effect push(SCP,scp(DP,PROP)) act ID:1, assert(DP,PROP) effect push(COND, accept(o(DP),ID)→ scp(o(DP),PROP)) act ID:2, check(DP,PROP) effect push(COND, accept(o(DP),ID)→ scp(DP,PROP)) act ID:2, direct(DP,Act) effect push(OBL,address(o(DP),ID)) effect push(COND, accept(o(DP),ID)→ obl(o(DP),Act)) act ID:2, info_request(DP,Q) effect push(OBL,address(o(DP),ID)) The ack act is the only backward grounding act implemented at the moment. The main effect of an ack is to merge the information in the acknowledged DU (assumed to be PDU) into GND, also removing this DU from UDUS. Unlike the other acts described below, ack acts are recorded directly into GND.DH, rather than into CDU.TOGND.DH. All of the other updates are performed in the third step of the procedure in (1). The only effect of ac- cept acts is to enable the conditional rules which are part of the effect of assert and direct, leading to social commitments and obligations, respectively. agree acts also trigger conditional rules introduced. by check; in addition, they result in the agent be- ing socially committed to the proposition introduced by the act with which the agent agrees. Perform- ing an answer to question ID2 by asserting propo- sition P(ID3) commits the dialogue participant to the proposition that P(ID3) is indeed an answer to Q(ID2). The two rules for assert are where the confidence levels are actually used, to implement a simple ver- ification strategy. The idea is that the system only assumes that the user is committed to the asserted proposition when a confidence level of 2 is observed, while some asserts are assumed not to have been sufficiently well understood, and are only assigned a confidence level 1. This leads the system to perform a check, as we will see shortly. The next three update rules, for check, direct, and info_req, all impose an obligation on the other dialogue participant to address the dialogue act. In addition, the direct rule introduces a conditional act: acceptance of the directive will impose an obli- gation on the hearer to act on its contents. In addition, all FORWARD ACTS in the DRI scheme (Discourse Resource Initiative, 1997) impose an obligation to perform an understanding act (e.g., an acknowledgement): (3) act ID:c, forward-looking-act(DP) effect push(OBL,u-act(o(DP),CDU.id)) The internal urules implementing the updates in (2) have the format shown in (4), which is the urule for info_request. (4) urule(doInfoReq, ruletype3, [ hearer(DP), latest_moves: in(Move), Move: valRec(pred, inforeq)], [ incr_set(update_cycles,), incr_set(next_dh_id, HID), next_du_name(ID), pushRec(w^cdu^tognd^dh, record([atype=Move, clevel=2,id=HID])), pushRec(w^cdu^tognd^obl, record([pred=address, dp=DP, args=stackset( [record([item=HID])))]))), pushRec(w^gnd^obl, record([pred=uact, dp=P, args=stackset( [record([item=ID])))))]). As noted above, these rules have four parts; a name, a type, a list of conditions, and a list of ef- fects. The conditions in (4) state that there must be a move in latest_moves whose predicate is inforeq. The effects10 state that the move should be recorded in the dialogue history in CDU, that an obligation to address the request should be pushed into OBL in CDU, and that the requirement for an understand- ing act by W should be pushed directly into the list in W.GND. The fourth and final step of the algorithm cycles through the updating process in case recently added facts have further implications. For instance, when an action has been performed that matches the an- tecedent of a rule in COND, the consequent is es- tablished. Likewise, when an action is performed it releases any obligations to perform that action. Thus, accept, answer, and agree are all ways of releasing an obligation to address, since these are all appropriate backward looking actions. Similarly, an agent will drop intentions to perform actions it has already (successfully) performed. 3.4 Deliberation We assume, in common with BDI-approaches to agency (e.g., (Bratman et al., 1988)) that intentions $^Forward acts include assert, check, direct, and info_request. $^{10}$The ID and HID values simply contain numbers identifying the discourse units and conversational acts. are the primary mental attitude leading to an agen- t's actions. The main issues to explain then become how such intentions are adopted given the rest of the information state, and how an agent gets from intentions to actual performance. For the latter question, we take a fairly simplistic approach here: all the intentions to perform dia- logue acts are simply transferred to the next moves system variable, with the assumption that the gen- eration module can realise all of them as a single ut- terance. A more sophisticated approach would be to weight the importance of (immediate) realisation of sets of intentions and compare this to the likelihood that particular utterances will achieve these effects at minimal cost, and choose accordingly. We leave this for future work (see (Traum and Dillenbourg, 1998) for some preliminary ideas along these lines), concentrating here on the first issue how the sys- tem adopts intentions to perform dialogue acts from other aspects of the mental state. The current system takes the following factors into account: • obligations (to perform understanding acts, to address previous dialogue acts, to perform other actions) • potential obligations (that would result if an- other act were performed, as represented in the COND field) • insufficiently understood dialogue acts (with a 1 confidence level in CDU.DH) • intentions to perform complex acts The current deliberation process assumes maxi- mal cooperativity, in that the system always chooses to meet its obligations whenever possible, and also chooses to provide a maximally helpful response when possible. Thus, when obliged to address a previous dialogue act such as a question or direc- tive, it will choose to actually return the answer or perform the action, if possible, rather than reject or negotiate such a performance, which would also be acting in accordance with the obligations (see (Kreu- tel, 1998) on how acts might be rejected). In the current implementation, the following rules are used to adopt new intentions (i.e., to update the INT field): (5) 1. add an intention to acknowledge edge(W,CDU), given an obligation to perform a u-act, if everything in CDU is sufficiently understood (i.e., to level 2); 2. add an intention to accept a directive or an- swer a question as the result of an obligation to address a dialogue act; 3. add an intention to perform an action if COND contains a conditional that will estab- lish an obligation to perform the action, and the antecedent of this conditional is another action that is already intended. (This an- ticipatory planning allows the obligation to be discharged at the same time it is invoked, e.g., without giving an intermediate acceр- tance of an directive.) 4. add an intention to perform a (dialogue) ac- tion motivated by the intention to perform the current task. In the case of the Au- toroute domain, we have two cases: the sys- tem may decide (a) to check any dialogue acts in CDU at confidence level 1, which contain infor- mation needed to discharge the intention to give a route; or (b) to perform a question asking about a new piece of information that has not been es- tablished (this is decided by inspecting GND.SCP and CDU.SCP). For example, it may decide to ask about the starting point, the time of departure, etc. 4 Extended Example In this section, we discuss more examples of how the information state changes as a result of processing and performing dialogue acts. It is useful to do this by looking briefly at a typical Autoroute dialogue, shown in (6).11 Our implementation can process this sort of dialogue using very simple interpretation and generation routines that provide the dialogue acts in latest moves from the text strings, and produce W's output text from the dialogue acts which the system places in next moves. (6) W [1]: How can I help? C [2]: A route please W [3]: Where would you like to start? C [4]: Malvern W [5]: Great Malvern? C [6]: Yes W [7]: Where do you want to go? C [8]: Edwinstowe W [9]: Edwinstowe in Nottingham? C [10]: Yes W [11]: When do you want to leave? C [12]: Six pm W [13]: Leaving at 6 p.m.? C [14]: Yes W [15]: Do you want the quickest or the shortest route? C [16]: Quickest W [17]: Please wait while your route is cal- culated. We assume that before the dialogue starts, W has the intention to ask C what kind of help is required, 11 The interchanges have been cleaned up to some extent here, mainly by removing pauses and hesitations. understandingAct(C,DU6) giveroute(W) CA13: C2, acknowledge( , CA12: C2, answer(C,CA8) CAII: C1, assert(C.start(malvern))/ giveroute(W) OBL: OBL: understanding Act(W,DU5) address(C,CA8) CAJO: C2, acknowledge (C,DU4) GND: DH: GND: DH: CA9: C2, accept(W.CA6) CA8: C2, info_request(W,?start), SCP: < > SCP: < > COND: < > COND: < > UDUS: <DU6> UDUS: <DUS> OBL: <address(C,CA8)> CA9: C2, accept(W.CA6) DH: TOGND: CA8: C2, info_request(W,?start) OBL: W: DH: TOGND: PDU: SCP: W: PDU: SCP: < > ID: COND: < > DU4 OBL: < > DH: TOGND: CDU: ID: INT: CA12: C2, answer (C.CA8,CA11) CA11: C1, assert(C.start(malvern) < > SCP: COND: < > DUS check(W.start(malvern)) acknowledge(W,DUS) giveroute(W) [C: [INT: <getroute(C)>] < > CA12: C2, answer (C.CA8.CA11) CA11: CI, assert(C.start(malvern))/ < > COND: < > OBL: <address (C,CA14) > ID: DU5 DH: <CA14: C2, check (W.start(malvern))> TOGND: CDU: SCP: < > COND: <agree (C.CA14) -> scp(W.start(malvern))> ID: INT: DU6 <giveroute(W)> C: [INT: <getroute(C)>] Figure 3: Information State Prompting Check in [5] and that C has the intention to find a route. We also assume that W has the turn, and that the presence of the how can I help intention triggers an utterance directly. Figure 2, presented above, shows the in- formation state after utterance [2]. The intentions in that figure lead directly to the system producing utterance [3]. Looking a little further ahead in the dialogue, Fig- ure 3 shows the information state after utterance [4].12 Here we can see in CDU.TOGND.DH (along with the ack act CA10, in GND.DH) the dialogue moves that this utterance has generated. Note that the as- sert, CA11, is only at confidence level 1, indicating lack of sufficient certainty in this interpretation as the town 'Great Malvern'. This lack of certainty and the resulting lack of a relevant SCP in CDU.TOGND lead the deliberation routines to produce an inten- tion to check this proposition rather than to move directly on to another information request. This intention leads to utterance [5], which, after inter- pretation and updating on the new dialogue acts, leads to the information state in Figure 4. The in- teresting thing here is the condition which appears in CDU.TOGND.COND as a result of the check; the interpretation of this is that, if C agrees with the check, then W will be committed to the proposition that the starting place is Malvern (C would also be committed to this by way of the direct effects of an agree act). 12 The actual information state contains all the previously established dialogue acts, SCPs and Obligations in GND, from Figure 2 and intermediate utterances. Here we have deleted these aspects from the figures for brevity and clarity. Figure 4: Information State Following Check in [5] OBL: DH: GND: SCP: /understandingAct(C,DU8) giveroute(W) /CA17: C2, acknowledge (W,DU7) CA16: C2, agree(C,CA14) /scp(C.start(malvern))) scp(W.start(malvern))/ COND: < > UDUS: <DU8> W: TOGND: PDU: ID: CDU: INT: TOGND: ID: OBL: < > DH: <CA16: C2, agree (C,CA14)> SCP: <scp(C.start(malvern))> COND: < > DU7 OBL: <address (C,CA18)> DH: SCP: <CA18: C2, info_request(W.?dest) > < > COND: < > DU8 <giveroute(W)> C: [INT: <getroute(C)>) Figure 5: Information state following [7] After C's agreement in [6], the deliberation rou- tine is able to move past discussion of the start- ing point, and add an intention to ask about the next piece of information, the destination. This leads to producing utterance [7], which also implic- itly acknowledges [6], after which C's agreement is grounded, leading to the IS shown in Figure 5. Note that the list in W.GND.SCP in Figure 5 indicates that both C and W are committed to the proposition that the starting place is Malvern. 5 Conclusions It has only been possible here to introduce the basic concerns of the PTT account of dialogue modelling and to pick out one or two illustrative examples to highlight the implementational approach which has been assumed. Current and future work is directed towards measuring the theory against more challeng- ing data to test its validity; cases where ground- ing is less automatic are an obvious source of such tests, and we have identified a few relevant problem cases in the Autoroute dialogues. We do claim, how- ever, that the implementation as it stands validates a number of key aspects of the theory and provides a good basis for future work in dialogue modelling. Acknowledgments The TRINDI (Task Oriented Instructional Dia- logue) project is supported by the Telematics Appli- cations Programme, Language Engineering Project LE4-8314. Massimo Poesio is supported by an EP- SRC Advanced Research Fellowship. References J. Allwood. 1976. Linguistic Communication as Action and Cooperation. Ph.D. thesis, Göteborg University, Department of Linguistics. J. Allwood. 1994. Obligations and options in dia- logue. Think Quarterly, 3:9-18. M. E. Bratman, D. J. Israel and M. E. Pollack. 1988. Plans and Resource-Bounded Practical Reason- ing. Computational Intelligence, 4(4). P. Bretier and M. D. Sadek. 1996. A rational agent as the kernel of a cooperative spoken dialogue system: Implementing a logical theory of inter- action. In J. P. Müller, M. J. Wooldridge, and N. R. Jennings, editors, Intelligent Agents III — Proceedings of the Third International Workshop on Agent Theories, Architectures, and Languages (ATAL-96), Lecture Notes in Artificial Intelli- gence. Springer-Verlag, Heidelberg. J. Calder. 1998. Thistle: diagram display en- gines and editors. Technical Report HCRC/TR- 97, HCRC, University of Edinburgh, Edinburgh. H. H. Clark and E. F. Schaefer. 1989. Contributing to discourse. Cognitive Science, 13:259-294. H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition, 22:1-39. Also appears as Chapter 4 in (Clark, 1992). H. H. Clark. 1992. Arenas of Language Use. Uni- versity of Chicago Press. P. R. Cohen and H. J. Levesque. 1990. Rational in- teraction as the basis for communication. In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, In- tentions in Communication. MIT Press. Discourse Resource Initiative. 1997. Standards for dialogue coding in natural language processing. Report no. 167, Dagstuhl-Seminar. B. J. Grosz and C. L. Sidner. 1990. Plans for dis- course. In P. R. Cohen, J. Morgan, and M. E. Pol- lack, editors, Intentions in Communication. MIT Press. J. Kreutel. 1998. An obligation-driven computa- tional model for questions and assertions in dia- logue. Master's thesis, Department of Linguistics, University of Edinburgh, Edinburgh. S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999. Trindikit manual. Technical Report Deliverable D2.2 — Manual, Trindi. M. Poesio, R. Cooper, S. Larsson, D. Traum, and C. Matheson. 1999. Annotating conversations for information state update. In Proceedings of Am- stelogue 99, 3rd Workshop on the Semantics and Pragmatics of Dialogues. M. Poesio and D. R. Traum. 1997. Conversational actions and discourse situations. Computational Intelligence, 13(3). M. Poesio and D. R. Traum. 1998. Towards an ax- iomatization of dialogue acts. In Proceedings of Twendial'98, 13th Twente Workshop on Language Technology, pages 207-222. M. D. Sadek. 1991. Dialogue acts are rational plans. In Proceedings of the ESCA/ETR workshop on multi-modal dialogue. M. P. Singh. 1998. Agent communication lan- guages: Rethinking the principles. IEEE Com- puter, 31(12):40-47. D. R. Traum and J. F. Allen. 1992. A speech acts approach to grounding in conversation. In Pro- ceedings 2nd International Conference on Spoken Language Processing (ICSLP-92), pages 137-40, October. D. R. Traum and J. F. Allen. 1994. Discourse obli- gations in dialogue processing. In Proceedings of the 32nd Annual meeting of the Association for Computational Linguistics, pages 1-8, June. D. R. Traum, J. Bos, R. Cooper, S. Larsson, I. Lewin, C. Matheson, and M. Poesio. 1999. A model of dialogue moves and information state re- vision. Technical Report Deliverable D2.1, Trindi. D. R. Traum and P. Dillenbourg. 1998. Towards a Normative Model of Grounding in Collaboration. In Proceedings of the ESSLLI98 workshop on Mu- tual Knowledge, Common Ground and Public In- formation. D. R. Traum. 1994. A computational theory of grounding in natural language conversation. Ph.D. thesis, Computer Science, University of Rochester, New York, December."
  },
  {
    "title": "CogentHelp: NLG meets SE in a tool for authoring dynamically generated on-line help",
    "abstract": "CogentHelp is a prototype tool for authoring dynamically generated on-line help for applications with graphical user interfaces, embodying the \"evolution-friendly\" properties of tools in the literate programming tradition. In this paper, we describe CogentHelp, highlighting the usefulness of certain natural language generation techniques in supporting software-engineering goals for help authoring tools—principally, quality and evolvability of help texts.",
    "content": "1 Introduction CogentHelp is a prototype tool for authoring dynam- ically generated on-line help for applications with graphical user interfaces (GUIs). In this paper, we describe CogentHelp, highlighting the usefulness of certain natural language generation (NLG) tech- niques in supporting software-engineering (SE) goals for help authoring tools—principally, quality and evolvability of help texts. To our knowledge, CogentHelp is unique in that it is the first operational prototype to embody the \"evolution-friendly\" properties of tools in the lit- erate programming tradition (Knuth, 1992)—e.g., the by now well-known javadoc utility for generat- ing API documentation from comments embedded in Java source code (Friendly, 1995; cf. also John- son and Erdem, 1995; Priestly et al., 1996; Korgen, 1996)—in a tool for generating end user-level doc- umentation. CogentHelp is also unusual in that it is (to date) one of the few tools to bring NLG tech- niques to bear on the problem of authoring dy- namically generated documents (cf. Paris and Van- der Linden, 1996; Knott et al., 1996; Hirst and Di- Marco, 1995); traditionally, most applied NLG sys- tems have focused on niches where texts can be gen- erated fully automatically, such as routine reports of various types (e.g. Goldberg et al., 1994; Kukich et al., 1994) or explanations of expert system reasoning (cf. Moore, 1995 and references therein). While striving to design highly sophisticated, fully automatic systems has undoubtedly led to a deeper understanding of the text generation process, it has had the unfortunate effect (to date) of limiting the use of techniques pioneered in the NLG community to just a few niches where high knowledge acqui- sition costs stand a chance of being balanced by substantial volume of needed texts (cf. Reiter and Mellish, 1993). By joining the emerging authoring support crowd and endeavoring to create new op- portunities in automated documentation, we hope to contribute to the broader acceptance and visi- bility of NLG technology in the overall computing community. The rest of the paper is organized as follows. In Section 2 we discuss the software engineering goals for CogentHelp. In Section 3 we provide back- ground on automated documentation and identify where CogentHelp fits in this picture. In Section 4 we give a brief overview of the CogentHelp system. In Section 5, we highlight the NLG techniques used in support of the software engineering goals identi- fied in Section 2. In Section 6 we describe Cogent- Help's authoring interface. Finally, in Section 7 we conclude by discussing the outlook for CogentHelp's use and further development. 2 Software Engineering Goals From a software engineering perspective, we set out to achieve three main goals in designing Cogent Help, each of which has various aspects. The first of these goals is end user-oriented, whereas the latter two are developer-oriented. The first goal is to promote quality in the resulting help systems, which includes promoting • Consistency—the grouping of material into help pages, the use of formatting devices such as headings, bullets, and graphics, and the general writing style should be consistent throughout the help system; • Navigability — the use of grouping and for- matting should make it easier to find informa- tion about a particular GUI component in the help system; • Completeness — all GUI components should be documented; • Relevance — information should be limited to that which is likely to be of current relevance, given the current GUI state; • Conciseness — redundancy should be avoided; • Coherence — information about GUI compo- nents should be presented in a logical and con- textually appropriate fashion. The second goal is to facilitate evolution, which includes facilitating • Fidelity — the help author should be assisted in producing complete and up-to-date descrip- tions of GUI components; • Reuse — wherever possible, the help author should not have to write the same text twice. The final goal is to lower barriers to adopting the technology, which has principally meant providing an authoring interface which makes the benefits of the system available at a reasonable cost in terms of the understanding and effort required of the help author. 3 Automated Documentation The main idea of CogentHelp is to have developers or technical writers author the reference-oriented part of an application's help system¹ in small pieces, in- dexed to the GUI components themselves, instead of in separate documents (or in one monolithic doc- ument). CogentHelp then dynamically assembles these pieces into a set of well-structured help pages for the end user to browse. The principal advantage of this approach is that it makes it possible to keep the reference-oriented part ¹By the reference-oriented part, we mean the part of a help system which describes the functions of individ- ual windows, widgets, etc., as opposed to more general or more task-oriented information about the application; other than providing an easy way of linking to and from CogentHelp-generated pages, CogentHelp leaves task- oriented help entirely to the author. of an on-line help system up-to-date automatically, since both the application GUI and the documen- tation can be evolved in sync. This benefit of gen- erating both code and documentation from a single source² has long been recognized, both in the NLG community (cf. Reiter and Mellish, 1993; Moore, 1995; and references therein) and in the SE commu- nity, where it is recognized under the banner of lit- erate programming tradition (Knuth, 1992). Other important benefits stem from supporting the sepa- ration of the content of the document to be gener- ated (descriptions of individual components) from the structure of the document (how the content is distributed and formatted): this allows the author to focus on writing accurate component descriptions, and avoid much of the drudgery involved in creating a complex hypertext document manually. To better understand where CogentHelp fits in, it is instructive to compare it with the closest ref- erence points in the NLG and SE communities. On the NLG side, there is the Drafter system (Paris and Vander Linden, 1996), which generates drafts of in- structions in both English and French from a single formal representation of the user's task. Drafter is considerably more ambitious in aiming to automate the production of multilingual, task-oriented help; at the same time, however, Drafter is more limited in that it is not evolution-oriented, aiming only to gen- erate satisfactory initial drafts (whence its name). This is in large part due to the fact that Drafter's input is centered around task representations which are not typically used for GUI-level tasks in software engineering practice; in contrast, nearly every GUI builder provides some form of GUI resource database which could be used as input to CogentHelp. On the SE side, the nearest reference points are a bit more distant, as CogentHelp has more in common with developer-oriented tools such as the javadoc API documentation generator dis- tributed with Sun Microsystems' Java Developers Kit (Friendly, 1995) than with currently available help authoring tools. While several current GUI- development environments include tools which gen- erate an initial, \"skeleton\" help system for an appli- cation (with topics that correspond to the widgets in the GUI), to our knowledge CogentHelp is the first operational prototype to implement the \"evolution- friendly\" properties of a tool like javadoc in a sys- tem for generating end user-level documentation. Unlike the \"skeleton generators\" mentioned above, ²Note that this \"single source\" need only be virtual — physically single-source code and documentation can have its drawbacks, which are not however inherent to the approach (Priestley et al., 1996). which require the help author to start from scratch each time the skeleton is regenerated in response to GUI modifications, CogentHelp supports help au- thoring throughout the software life cycle. 4 System Overview CogentHelp takes as input various human-written text fragments (or \"help snippets\") indexed to GUI resource databases, which provide some useful help- related information in the form of types, labels, lo- cations and part-whole relations for GUI widgets. CogentHelp generates HTML help files, which can be displayed on any platform for which a Web browser is available. It is designed to support ef- ficient navigation through the help system, through the use of intelligent, functionally structured layout as well as through an expandable/collapsible table of contents and \"thumbnail sketch\" applet. These points will be elaborated upon below. CogentHelp operates in two modes: a \"static\" mode, which does not make use of run-time informa- tion, and a \"dynamic\" mode, which uses widget run- time status information to produce more specialized, contextually appropriate messages. The static mode is useful in the authoring process, since it displays all available help information and has simpler archi- tectural requirements. In evolving the design of CogentHelp, we have employed a rapid-prototyping approach in work- ing with our TRP/ROAD³ consortium partners at Raytheon, who are serving as a trial user group. The full CogentHelp component architecture and dependencies reflects the particular requirements of this group, and are as follows. Cogent Help itself is a hypertext server written in Java, making it highly cross-platform. CogentHelp currently works with applications built using the Neuron Data cross- platform GUI builder; while it is not dependent on this product in any conceptually substantial way, us- ing other GUI builders would require a porting ef- fort. To retrieve run-time information, CogentHelp uses Expersoft's PowerBroker ORB for inter-process communication; in comparison to the Neuron Data connection, other IPC methods could be more eas- ily substituted. Finally, CogentHelp displays hyper- text in Netscape Navigator, using HTTP to medi- ate access to the dynamically generated texts; since Netscape Navigator remains the most widely used cross-platform browser, we have yet to investigate using other browsers. ³A DARPA-sponsored Technology Reinvestment Pro- gram for Rapid Object Application Development, led by Andersen Consulting. 5 NLG Techniques Although CogentHelp is by no means a typical NLG system insofar as it is incapable of generating useful texts in the absence of human-authored help snippets it does employ certain natural language generation techniques in order to support the soft- ware engineering goals described above. These tech- niques fall into two categories, those pertaining to knowledge representation and those pertaining to text planning. 5.1 Knowledge Representation In developing CogentHelp, we have taken a mini- malist approach to knowledge representation follow- ing a methodology for building text generators de- veloped over several years at CoGenTex. As will be explained below, this approach has led us to (i) make use of what amounts to a large-grained \"phrasal\" lexicon and (ii) devise and implement a widget-clustering algorithm for recovering functional groupings, as part of an Intermediate Knowledge Representation System (IKRS). 5.1.1 Phrasal Lexicon As Milosavljevic et al. (1996) argue, to optimize coverage and cost it makes sense to choose an un- derlying representation which • makes precisely those distinctions that are rele- vant for the intended range of generated texts; and • is no more abstract than is required for the in- ference processes which need to be performed over the representation. They go on to argue that besides eliminating a great deal of unnecessary 'generation from first principles,' this approach complements their use of a phrasal lexicon (Kukich, 1983; Hovy, 1988) at the linguistic level. Applying essentially the same approach to the de- sign of CogentHelp, we first determined that for the intended range of generated texts it suffices to as- sociate with each widget to be documented a small number of atomic propositions and properties, iden- tifiable by type. Next we determined that since no inference is required beyond checking for equality, these propositions and properties can be conflated with their linguistic realizations - i.e., the indexed, human-authored help snippets CogentHelp takes as input. While we did not originally think of Cogent- Help's collection of input help snippets as a phrasal lexicon a la Milosavljevic et al., in retrospect it be- comes evident that this collection can be viewed as Operators without Work SR KNOTT LJ COONEY RA FIGUEREDO PA HETL HP RYAN PA LAMBERT CL DAGATA Hot Parts G416506 G416506 (5186) 6427803 G427804 G596122 Assinn 7 1 Ramove 8 Operators on Jobs TL PFEIFFER KJ SILK JL CHARLAND GD HOLLAND EF TIHE 2 Operator View Foreman View Ski Sets 5 6 ssignmen <-Factor Notes 3 4 9 Parts G415961 G415961-SAP1 G415961-SAP1-HP G415961-1-SAP1-H10. G416249-1-HP Assignments Made in Session 13 View Operator Facing Assted 11 12 14 15 Save Done Figure 1: Sample application window tantamount to one; of course, since these snippets vary in size from phrases to paragraphs, the term \"phrasal\" is not entirely accurate. The types of snippets in current use include a one- sentence short description of the relevant GUI com- ponent; a paragraph-sized elaboration on the short description; various phrase-sized messages concern- ing the conditions under which it is visible or en- abled, if appropriate; and a list of references to other topics. In the case of the phrase-sized messages, each of these fields is accompanied by a syntactic frame which prompts the author to provide consistent syn- tax — for example, entries in the WHEN ENABLED field should fit the frame This element is enabled when To facilitate equality checking and promote text reuse, a mechanism is provided enabling the author to alias the message for one widget to that of an- other. 5.1.2 IKRS To enhance the modularity and robustness of a practical text generator, Korelsky et al. (1993) ar- gue for the use of an Intermediate Knowledge Rep- resentation System (IKRS, pronounced \"Icarus\") to bridge the gap between what the text planner would like to access and what is actually found in the infor- mation base of an application program. A remark- ably similar idea has been independently developed by Lester and Porter (1996), under the heading of KB Accessors. One purpose of an IKRS, Korelsky et al. sug- gest, is to provide a component in which to locate domain-level inferencing not provided by the appli- cation program. Note that while this type of infer- encing is motivated by text planning needs, it is still about the domain rather than about natural lan- guage communication, and thus does not belong in the text planner itself. In developing CogentHelp, we encountered a need for just this sort of inferencing in order to support sensible layout. The problem we faced was how to logically arrange descriptions of widgets within a help page (or set of help pages) describing a window, which is the basic unit of organization in Cogent- Help. As will be explained below, grouping widgets by type was considered inadequate, because doing so would obscure functional relationships between widgets of different types. A naive spatial sorting was likewise considered inadequate, as this would inevitably separate elements of a functional group appearing in a certain area of the window. Unfor- tunately, since these functional groups are often not explicitly represented in GUI resource databases, we appeared to be at an impasse. To illustrate the problem, consider the sample ap- plication window shown in Figure 1, from a proto- type of an application under development by our trial user group at Raytheon. This window, whose purpose is to allow a manufacturing shop floor fore- man to assign operators to parts, is organized as follows: on the left there are two list boxes for op- erators (1, 2), with buttons beneath them to access information about these operators (3, 4, 5, 6); on the right there are two list boxes for parts (9, 10), with buttons beneath them to access information about these parts (11, 12); in the middle there are two buttons for making and removing assignments (7, 8); towards the bottom there is a list box showing the assignments made so far (of which there are none here — 13); and at the bottom there are standard buttons such as Save and Done (14, 15 — the Help button would go here). Given this organization, con- sider first arranging descriptions by type, and alpha- betizing: besides cutting across the implicit func- tional groupings, arranging descriptions in this way would end up putting the two View K-Factors but- tons (4, 11) in sequence, without any indication of which was which! Now consider a simple top-down, left-to-right spatial sort: again, this would inevitably yield a rather incoherent ordering, such as the Op- erators without Work list box (1), the Hot Parts list box (9), the Assign button (7), the Operators on Jobs list box (2), the Parts list box (10), the Remove Assignment button (8), etc. The solution to this problem was to develop, as part of our IKRS, a method of recovering these func- tional groups using spatial cues as heuristics; the rea- son this approach might be expected to work is that in a well-designed GUI, functionally related widgets are usually clustered together spatially in order to make the end user's life a bit easier. We began with the fairly standard hierarchical agglomerative algo- rithm found in (Stolcke, 1996). Stolcke's algorithm is an order n² one that iteratively merges smaller clusters into bigger ones until only one cluster re- mains; new clusters are formed out of the two nearest clusters in the current set, ensuring that the results are independent of the order in which clusters are examined. After some experimentation, we modified this algorithm to better suit our needs, resulting in the following three differences: (i) to create clusters with more than two elements, we continue adding elements to the newly created cluster until a certain distance threshold is exceeded; (ii) we represent the new cluster using its bounding box, rather than us- ing an average of its elements; and (iii) we restrict the clustering to not operate across explicit groups, such as those formed using panels. With any clustering approach, there is always the tricky matter of determining a suitable distance measure. After trying out a variety of features, what we found to work surprisingly well was a simple weighted combination of proximity, alignment and type identity. In particular, in a test suite of around 15 windows provided to us by our trial user group, we obtained reasonable results (no egregiously bad groupings) on all of them without undue sensitiv- ity to the exact weights. In the case of the window shown in Figure 1, the clustering procedure performs exactly as desired, yielding precisely the groupings used in the description of this window given above — i.e.: (((1, 2), (3, 4, 5, 6) ), ( (9, 10), (11, 12) ), (7, 8), 13, (14, 15) ). Once the IKRS has heuristically recovered clus- ters of widgets likely to form a functional group, these clusters as well as any explicitly represented groups, e.g. widgets contained within a panel of a window — can be used as a basis for help layout, as discussed below. 5.2 Text Planning At the core of CogentHelp is the text planner. The text planner builds up HTML trees starting from an initial goal, using information provided by the IKRS, following the directives coded in CogentHelp's text planning rules. These HTML trees are then lin- earized into an ascii stream by a separate formatter, so that they can be displayed in a web browser (cf. Section 4). The text planner is constructed using Exem- Netscape - [CogentHelp] Back Previous Next Assignments: Work Con Operators without We Operators on Jobs Operator Assignameri View Factor: Foremari Notes View Skull Sets Hot Parts Reguler Parts View K-Factors Operators Assigned Assign Renove Assignment These commands provide further information on the selected operator These commands are currently disabled. To enable them, select exactly one operator in either the Free Operators list box or the Working Operators list box. View K-Factors - Help Displays the K-Factors for the selected operator. Foreman Notes Displays the Foreman Notes for the selected operator. For help on a window component, click on its image in the diagram Figure 2: A sample help page plars for Java, a lightweight framework for build- ing object-oriented text planners in Java which has been developed in parallel with CogentHelp (White, 1997). In this framework, text planning rules — the exemplars, so-called because they are meant to cap- ture an exemplary way of achieving a communicative goal in a given communicative context — are objects which cooperate to efficiently produce the desired texts. While space precludes a detailed description, it is worth noting that Exemplars for Java supports abstraction, specialization and content-based revi- sions, all of which have proved useful in the present effort. In developing the exemplars for CogentHelp, we have made use of three NLG techniques: structuring texts by traversing domain relations, automatically grouping related information, and using revisions to simplify the handling of constraint interactions. The first two of these make life simpler for the end user, while the third makes life simpler for the developer. 5.2.1 Capitalizing on Domain Structure When text structure follows domain structure, one can generate text by selectively following appropri- ate links in the input (Paris, 1988; Sibun, 1992). In the case at hand, we have chosen to use the group and cluster structure combined with a top-down, left-to-right spatial sort: while such a spatial sort alone is insufficient, as we saw above, a spatial sort which respects functional groups turns out to work well. Returning to the example of Section 5.1.2 (re- garding the window shown in Figure 1), travers- ing the clusters in this way yields a listing which (naturally!) mirrors the order and groupings of the one-sentence description of the window's organiza- tion we have provided — that is, following a general description of the window, there are descriptions of the two operators list boxes (1, 2), followed by de- scriptions of the four buttons providing additional information on operators (3, 4, 5, 6), followed next by the part list boxes (9, 10) and the buttons asso- ciated with them (11, 12), and so on. This is (par- tially) illustrated in Figure 2, which shows a sample CogentHelp-generated help topic. Note that the list of widgets in the dynamic TOC on the left side of the page is arranged according to this traversal; con- sequently, stepping through the contents (using the TOC or the Next button) for this window will lead from widget to widget and cluster to cluster in a sen- sible fashion. In the particular topic shown, the user has reached the second button (View K-Factors) of the group of four buttons beneath the Operators list boxes, as can be seen from the highlighting in the thumbnail sketch applet (cf. Section 6). The use of domain structure-driven text plan- ning is central to supporting the software engi- neering goals identified in Section 2. Rather ob- viously, generating-by-rule helps to achieve consis- tency, completeness and fidelity, eliminating much mind-numbing drudgery along the way. A bit less obvious, perhaps, is the fact that this technique should help to achieve navigability and coherence: by presenting descriptions of widgets in a natural order — i.e., in the order in which the user is apt to encounter them in scanning the GUI — we hope to make it easier for the user to find desired informa- tion; and, by keeping together descriptions of wid- gets which are heuristically determined to be func- tionally related, we hope to make it easier for the user to quickly grasp the organization of both the interface and the on-line help. 5.2.2 Grouping Grouping related information and presenting shared parts just once is a well-known NLG tech- nique for achieving conciseness and coherence (Re- iter and Mellish, 1993). In a reference-oriented docu- ment such as an on-line help system, similar or iden- tical descriptions will often be appropriate for ele- ments which have similar or identical functions. To indicate these similarities, as well as to save space, it makes sense to group these descriptions when possi- ble. As mentioned in Section 5.1.1, we allow develop- ers to alias messages to promote text reuse, as well as to facilitate equality checking. When the text planner detects (via the IKRS) that a phrase-sized message (such as TO ENABLE) is the same for a group of widgets, it generates a description that applies to the whole group, rather than repeating the same de- scription several times in close proximity. Note that this group description is made possible by the use of a phrasal lexicon, which has been designed to allow the author's messages to make sense in a variety of contexts. To illustrate, let us have another look at Figure 2. In the upper right frame of the page, note that there is the following description of how to enable all of the four buttons below the operators list boxes, rather than a repetition of the same message four times in close proximity: These commands are currently disabled. To enable them, select exactly one opera- tor in either the Free Operators list box or the Working Operators list box. This group-level description appears here because (i) the author, realizing that these buttons are enabled under the same conditions, entered the TO ENABLE message \"select exactly one operator in either the Free Operators list box or the Working Operators list box\" for one button, and aliased this message for the other; and (ii) the text planner, detecting (via the IKRS) that the TO ENABLE messages for the entire group were the same, and that these buttons were currently disabled (a run-time fact), prepended \"These commands are currently disabled. To enable them,\" to the shared message to yield what appears in Figure 2. 5.2.3 Revisions While the central use of domain structure-driven text planning makes it possible to generate text in a relatively straightforward, top-down fashion, as vari- ations are added a one-pass top-down approach can become cumbersome. The reason why is this: In evolving the text planning rule base, it makes sense to localize decisions as much as possible; however, to handle rule interactions in a single pass, one is forced to centralize these decisions (which can become cum- bersome). To simplify matters, it is often appropri- ate to generate an initial version naively, then carry out revisions on it in a subsequent pass (cf. Robin, 1994; Wanner and Hovy, 1996). In CogentHelp, interactions that are cumbersome to anticipate arise in dealing with the various op- tional phrase-sized messages whose inclusion condi- tions differ between the static and dynamic mode. To elaborate, let us consider once more the help page shown in Figure 2. Had the four buttons described in the lower right frame been enabled rather than dis- abled (at run-time), the group-level TO ENABLE mes- sage would have simply been left out, in order to en- hance relevance and conciseness; on the other hand, had this page been generated in static mode (where such run-time conditions are not known), the text planner would have again included the description, though this time with the less specific \"To enable these commands,\" prepended instead. Now, since the various messages associated with a widget have slightly different inclusion conditions, it makes sense to localize these inclusion conditions to a text plan- ning rule for each message (the common parts of these conditions are shared via inheritance). At the same time, however, there is a need to know whether any of these messages will in fact appear, in order to decide whether to include the second paragraph in the upper right frame, as well as the italics element. In a one-pass top-down approach, this need would force the various inclusion conditions to be cumber- somely centralized; with revisions, in contrast, one can simply add the paragraph and italics element during the first pass, then check during a second pass whether any of the optional messages for this paragraph did in fact appear, removing the super- fluous HTML elements if not. 6 Authoring In informal usability tests, we have gathered much useful information about areas in which CogentHelp could be improved — the most important of these being ease of authoring. A previous version of the authoring interface, which relied on the resource ed- itor of the Neuron Data GUI builder, proved unsat- isfactory, as it (i) required excessive clicking for the author to navigate from snippet to snippet, and (ii) failed to provide sufficient context, making it un- necessarily difficult for the author to adhere to the CogentHelp authoring guidelines. The current authoring interface, shown in Fig- ure 3, uses CogentHelp's existing architecture (to- gether with the HTTP forms protocol) to allow the user to edit the text snippets for each widget in sub- stantially the same context they would inhabit in generated help topics. This design provides maxi- mal realism for the author, especially since one can switch between editing and browsing mode at the click of a button to preview the generated help. Another feature illustrated in Figure 3, as well as Figure 2, owes its inspiration to our trial user group at Raytheon. Our users were concerned about the ease of visual navigation of help pages, and had experimented with using manually coded (and 4With a perfectly intuitive GUI, the user would never need to know this information, as commands would al- ways be enabled when the user expects them to be. thus difficult to evolve) image maps superimposed over bitmaps of each application window. This con- cern prompted us to develop an automatically gen- erated \"thumbnail sketch\" of the current GUI win- dow, which appears in the upper left corner of the help window (in a Java applet along with the ta- ble of contents) and contains hyperlinks for each of the widgets on the window (these hyperlinks dis- play the corresponding help topics on the right-hand side). The automatically generated thumbnail im- ages require no intervention on the part of the help author, and thus are guaranteed to be up-to-date; furthermore, their abstract nature gives them cer- tain advantages over actual bitmaps: they do not present information which is redundant (since the actual window in question will usually be visible) or inconsistent (static bitmaps fail to capture widgets which are enabled/disabled or change their labels in certain situations). 7 Outlook To date we have gathered substantial feedback on CogentHelp functionality from our trial user group at Raytheon, especially on the need for authoring support and visual navigation aids. We are opti- mistic that this group will find CogentHelp suitable for actual use in developing a production-quality help system by the end of our Rome Laboratory- sponsored software documentation SBIR project, in mid-1997. Also by project end, we hope to port CogentHelp to a more affordable, Java-based GUI builder, in order to make it useful to a much broader community of developers. Acknowledgements We gratefully acknowledge the helpful comments and ad- vice of Ehud Reiter, Philip Resnik, Keith Vander Lin- den, Terri SooHoo, Marsha Nolan, Doug White, Colin Scott, Owen Rambow, Tanya Korelsky, Benoit Lavoie and Daryl McCullough. This work has been supported by SBIR award F30602-94-C-0124 from Rome Labora- tory (USAF) and by the TRP/ROAD cooperative agree- ment F30602-95-2-0005 with the sponsorship of DARPA and Rome Laboratory. References Lisa Friendly. 1995. The design of distributed hyper- linked programming documentation. In International Workshop on Hypermedia Design. Eli Goldberg, Norbert Driedger, and Richard Kittredge. 1994. Using natural-language processing to produce weather forecasts. IEEE Expert, pages 45-53. Graeme Hirst and Chrysanne DiMarco. 1995. Health- Doc: Customizing patient information and health ed- ucation by medical condition and personal characteris- tics. In Al in Patient Education Workshop, Glasgow, Scotland, August. Eduard H. Hovy. 1988. Generating language with a phrasal lexicon. In D. D. McDonald and L. Bolc, editors, Natural Language Generation Systems, pages 353-384. Springer-Verlag, New York. W. Lewis Johnson and Ali Erdem. 1995. Interactive ex- planation of software systems. In Proceedings of the Tenth Knowledge-Based Software Engineering Confer- ence (KBSE-95), pages 155-164, Boston, Mass. Alistair Knott, Chris Mellish, Jon Oberlander, and Michael O'Donnell. 1996. Sources of flexibility in dynamic hypertext generation. In Proceedings of the Eighth International Natural Language Generation Workshop (INLG-96), pages 151-160, Herstmonceux Castle, Sussex, UK. D. E. Knuth, editor. 1992. Literate Programming. CSLI. Tanya Korelsky, Daryl McCullough, and Owen Rambow. 1993. Knowledge requirements for the automatic gen- eration of project management reports. In Proceedings of the Eighth Knowledge-Based Software Engineering Conference (KBSE-93), pages 2-9, Chicago, Illinois. Susan Korgen. 1996. Object-oriented, single-source, on- line documents that update themselves. In Proceed- ings of The 14th Annual International Conference on Computer Documentation (SIGDOC-96), pages 229- 238. K. Kukich, K. McKeown, J. Shaw, J. Robin, N. Mor- gan, and J. Phillips. 1994. User-needs analysis and design methodology for an automated document gen- erator. In A. Zampolli, N. Calzolari, and M. Palmer, editors, Current Issues in Computational Linguistics: In Honour of Don Walker. Kluwer Academic Press, Boston. Karen Kukich. 1983. Design of a knowledge-based re- port generator. In Proceedings of the 21st Annual Meeting of the Association for Computational Linguis- tics, Cambridge, Mass. James C. Lester and Bruce W. Porter. 1996. Scaling up explanation generation: Large-scale knowledge bases and empirical studies. In Proceedings of the National Conference on Artificial Intelligence (AAAI-96), Port- land, Oregon. Maria Milosavljevic, Adrian Tulloch, and Robert Dale. 1996. Text generation in a dynamic hypertext en- vironment. In Proceedings of the 19th Australasian Computer Science Conference, pages 229-238, Mel- bourne, Australia. Johanna Moore. 1995. Participating in Explanatory Di- alogues. MIT Press. Cecile Paris and Keith Vander Linden. 1996. Drafter: An interactive support tool for writing. IEEE Com- puter, Special Issue on Interactive Natural Language Processing, July. Cecile Paris. 1988. Tailoring object descriptions to the user's level of expertise. Computational Linguistics, 11(3):64-78. Michael Priestley, Luc Chamberland, and Julian Jones. 1996. Rethinking the reference manual: Using database technology on the www to provide com- plete, high-volume reference information without overwhelming your readers. In Proceedings of The 14th Annual International Conference on Computer Documentation (SIGDOC-96), pages 23-28. Owen Rambow and Tanya Korelsky. 1992. Applied text generation. In Third Conference on Applied Natural Language Processing, pages 40-47, Trento, Italy. Ehud Reiter and Chris Mellish. 1993. Optimizing the costs and benefits of natural language generation. In Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI-93), volume 2, pages 1164-1169. Jacques Robin. 1994. Revision-Based Generation of Natural Language Summaries Providing Historical Background. Ph.D. thesis, Columbia University. Penelope Sibun. 1992. Generating text without trees. Computational Intelligence, 8(1):102-22. Andreas Stolcke. 1996. Cluster 2.9. URL go- pher://gopher.icsi.berkeley.edu/1/usr/local/ftp/ai/ stolcke/software/cluster-2_9_tar.Z. Leo Wanner and Eduard Hovy. 1996. The HealthDoc sentence planner. In Proceedings of the Eighth In- ternational Natural Language Generation Workshop (INLG-96), pages 1-10, Herstmonceux Castle, Sussex, UK. Michael White. 1997. Exemplars for Java: a lightweight framework for building object-oriented text planners in Java. Technical report, CoGenTex, Inc."
  },
  {
    "title": "Encoding information on adjectives in a lexical-semantic net for computational applications",
    "abstract": "The goal of this paper is to describe how the EuroWordNet framework for representing lexical meaning is being modified within an Italian National Project in order to include information on adjectives. The focus is on the 'new' semantic relations being encoded and on the revisions we have made to the EuroWordNet Top Ontology structure. We also briefly discuss the utility of the information which is being encoded for computational applications.",
    "content": "Introduction The Princeton WordNet (henceforth WN) is a lexical semantic network in which the meanings of words are represented in terms of their conceptual and lexical relations to other words. The basic notion around which it is developed is that of a synset (synonyms set), i.e. a set of words with the same Part-of-Speech (PoS) that can be interchanged in a certain context. Various conceptual and lexical relations are then encoded between synsets of the same PoS: e.g., hyponymy, antonymy, meronymy, etc. (Miller et Within the EuroWordNet (henceforth EWN) project¹ a similar (multilingual) lexical resource was developed, retaining the basic underlying design of WN, but enriching the set of lexical- semantic relations to be encoded for nouns and verbs in various ways², in order to obtain a maximally re-usable resource for computational applications. Thus, a) cross-POS (XPOS) relations were added so that different surface realizations of similar concepts within and across languages could be matched (e.g., the noun research and the verb to research could be linked as 1 EWN was a project in the EC Language Engineering (LE-4003 and LE-8328) programme. In a first phase, the partners involved were the University of Amsterdam (coordinator); the Istituto di Linguistica Computazionale, CNR, Pisa; the Fundacíon Universidad Empresa (a cooperation of UNED, Madrid, Politecnica de Catalunya, Barcelona, and the University of Barcelona); the University of Sheffield; and Novell Linguistic Development (Antwerp), changed to Lernout & Hauspie during the project. In a further phase, the database was extended with German, French, Estonian and Czech. Complete information on EWN can be found at its web site: http://www.hum.uva.nl/~ewn/. 2 Adjectives and adverbs were encoded in EWN only 'XPOS_NEAR_SYNONYMS' in EWN); b) some relations were identified which provide detailed information on semantic components lexicalized within word roots (e.g., to hammer could be linked to the noun hammer by means of an 'INVOLVED_INSTRUMENT' relation); c) some labels were distinguished which could be added to relations to make their semantic entailments more explicit and precise (cf. Alonge et al. 1998). The links among the wordnets of different languages were realized by means of an Interlingual-Index (ILI), constituted by an unstructured list of the Princeton WN (version 1.5) synsets. In addition, a hierarchy of language-independent concepts, reflecting fundamental semantic distinctions (e.g., Object and Substance, Dynamic and Static, Cause, Manner, etc.), was built: the Top Ontology (TO). The TO consists of language-independent features which may (or may not) be lexicalized in various ways, or according to different patterns, in different languages (Rodriguez et al. 1998). Via the ILI, all the concepts in the monolingual wordnets are directly or indirectly linked to the TO. The following picture shows the EWN data structure (see also Vossen 1999): Italian WN Spanish WN GEWING TOP ONTOLOGY ANIMAL HUMAN dog ILI Dutch WN English WN (henceforth IWN)³, by extending the network built for Italian in EWN. Thus, we are both increasing the coverage for nouns and verbs and adding adjectives and some adverbs4. To be able to encode information on adjectives we have enriched the set of the EWN lexical-semantic relations, aiming at encoding data which can be useful for computational applications. Moreover, we have revised the TO in order to account for the semantics of the new lexical categories being encoded. In this paper we describe the main changes made to the EWN framework in order to encode information on adjectives in IWN. Firstly, we provide a brief overview of the WN treatment of adjectives. Then, we discuss the set of relations being encoded for this category in IWN. Finally, we show the integration made to the EWN TO. We then conclude the paper by adding some remarks on the utility of the data being encoded for computational applications. 1 Adjectives in WN In WN adjectives are divided into two major classes: descriptive adjectives and relational adjectives. A descriptive adjective is \"one that ascribes a value of an attribute to a noun\" (Fellbaum et al. 1993:27). Descriptive adjectives combine with nouns to express some qualities of the thing, person or concept they designate. Typically, in this group we find adjectives that designate the physical dimension of an object, its weight, abstract values etc. Besides these referent-modifying adjectives, we also find reference-modifying adjectives (cf. Bolinger 1967; Chierchia & McConnel-Ginet 1990 name them intensional adjectives). Typical examples of the latter are former, future, present. French WN German WN Estonian WN Czech WN While in WN all PoSs are represented, in EWN detailed information was encoded only for nouns and verbs and no analysis was carried out with respect to lexical-semantic relations which could be used to describe the semantics of adjectives and adverbs. In an Italian National Project we are building a large wordnet, ItalWordNet 3 ItalWordNet will be the reference lexical resource among various integrated language resources and software tools for the automatic treatment of the Italian written and spoken language which are being developed within the SI-TAL ('Integrated System for the Automatic Treatment of Language') National Project. 4 Actually, we shall only encode adverbs derived from adjectives by adding the suffix -mente, for which a derivation relation with an adjective will be encoded Relational adjectives, on the other hand, mean something like \"relating/pertaining to, associated with\", and usually have a morphologically strong link with a noun. Typical examples are musical, atomic, chemicals. Synonymy is the basic relation encoded for all the PoSs (since it is used to build synsets). While in WN the noun and verb networks are then mainly developed around the superordinate (hyper/hyponymy) relationship, the organization of descriptive adjectives can \"be visualized in terms of barbell-like structures, with a direct antonym in the centre of each disk surrounded by its semantically similar adjectives (which constitute the indirect antonyms of the adjectives in the opposed disk)\" (Fellbaum 1998a: 212). The main relation encoded for these adjective synsets is antonymy, claimed to be the most prominent relation, both from a psycholinguistic point of view and from a more strictly lexical- semantic one, in the definition of the semantics of descriptive adjectives. Hyponymy is substituted by a 'similarity' relation. Relational adjectives, on the contrary, are not organized in this way, because their semantics cannot be described by using these relations. Indeed, they only point to the noun to which they pertain (e.g. atomic is linked to atom). Finally, information on the selectional preferences of both descriptive and relational adjectives is sometimes encoded (e.g., between high and degree), by using an 'is_attribute_of relation. 2 The IWN relations for adjectives As for the other lexical categories, also in IWN the basic relation encoded for adjectives is synonymy, on the basis of which synsets are built. Following EWN, we also encode a NEAR_SYNONYMY relation when two synsets are very close in meaning but their members cannot be put in the same synset (and no other relation results appropriate to link them; see Alonge et 5 Note that some adjectives have both a descriptive sense and a relational one. For example, musicale (musical) can modify the noun voce (voice) when we want to say that a voice is sweet-sounding and melodious, but can also be combined with the noun strumento (instrument) when we want to indicate that an instrument can be used to produce music. al. 1998 for a discussion of this relation, and Alonge et al., in prep., for a complete and detailed discussion of the linguistic design of IWN). Then, we encode a number of additional relations, which have been identified by taking into consideration i) theoretical works; ii) the EAGLES recommendations on semantic encoding (cf. Sanfilippo et al. 1999); iii) the data available in our sources; iv) possible use of data encoded in computational applications. 2.1 Hyponymy Together with synonymy, the hyponymy relation constitutes the 'bone structure' of both WN and EWN. However, as we have seen, in WN the possibility of encoding hyponymy for adjectives is denied and the basic relation encoded for adjectives is antonymy, while EWN did not really deal with adjectives and a complete network for them was not built. Within IWN we have reconsidered the possibility of encoding hyponymy for adjectives. By analysing data coming from machine- readable dictionaries we find subsets of adjectives which have a genus + differentia definition, like nouns or verbs. That is, these adjectives seem to be organised into classes sharing a superordinate. This is the case, e.g., of adjectives indicating a 'containing' property (acquoso - watery; alcalino - alkaline), or a 'suitable-for' property (difensivo - defensive; educativo - educational), etc. In IWN we have decided, therefore, to encode hyponymy also for these sets of adjectives. The taxonomies which can be built on the basis of this relation are different from those built for nouns or verbs, since they are generally very flat, consisting almost always of two levels only (an exception is the color adjectives taxonomy). However, by encoding a hyponymy relation for these adjectives, we obtain classes for which it will be possible to make various inferences. For instance, it will be possible to infer semantic preferences of certain classes: e.g., all the adjectives occurring in the taxonomy of contenente (containing) will occur as attributes of concrete nouns; adjectives found in the taxonomy of affetto (affected by an illness) will never be predicated of nouns referring to objects, etc. Furthermore, it will also be possible to infer information on syntactic characteristics of adjectives found in the same taxonomy: e.g., the hyponyms of atto (suitable for) are always found in predicative position (and do not accept any complements); the hyponyms of privo (lacking) may occur both in attributive and in predicative position (and may take certain prepositional complements), etc. As it was done for all the relations identified in EWN, we have built substition tests or diagnostic frames based on normality judgements (cf. Cruse 1986). Inserting two words in the test sentences built evokes a 'normality'/ 'abnormality' judgement on the basis of which each relation can be determined. These tests are used by encoders both to verify the existence of relations between synsets and to encode them in a consistent way (for a complete lists of the tests built see Alonge et al., in prep.) 2.2 Antonymy As in WN, also in IWN the antonymy relation remains an important relation to describe the semantics of various adjectives. Following theoretical work (Lyons 1977; Cruse 1986), we have further distinguished between COMPLEMENTARY_ANTONYMY and GRADABLE ANTONYMY6. The former relation links adjectives referring to opposing properties/ concepts: when one holds the other is excluded (alive/dead). The latter relation is used for those antonym pairs which refer to gradable properties (long/short). In case it is not clear if two opposing adjectives refer to complementary or gradable properties, we can still use an underspecified ANTONYMY relation. Also this information can be useful for computational applications since word pairs presenting one of the two kinds of opposition may occur in different contexts (cf. Cruse 1986). 2.3 Other relations In WN a relation between adjectives and nouns is encoded for relational adjectives which point to a noun to which they 'pertain': atomic/atom, 6 A similar distinction is also made within the SIMPLE EC project (LE-8346), whose goal is adding semantic information to the set of harmonized lexicons built within the PAROLE project for twelve European languages. Of course, the sub-classification of antonymy can also be used for nouns and verbs. industrial/industry, etc. This relation will be encoded also in IWN, by using the label PERTAINS_TO. Another relation 'inherited' from WN can be useful to distinguish both adjective senses and their semantic preferences7: alto, (tall) IS_A_VALUE_OF statura (stature) alto2 (high) IS_A_VALUE_OF altezza (height). Other relations are then being encoded which are not in WN, but are encoded for nouns and verbs in EWN. In WN each PoS forms a separate system of language-internal relations and conceptually close concepts are totally separated only because they differ in PoS. In EWN, instead of using as main classificatory criterion the traditional distinction among PoSs, drawn upon heterogeneous criteria, a purely semantic distinction was adopted (following Lyons 1977). Thus, a distinction was drawn among 1st order entities (loes - referred to by concrete nouns), 2nd order entities (2oes - referred to by verbs, adjectives or nouns indicating properties, states, processes or events), and 3rd order entities (3oes referred to by abstract nouns indicating propositions existing independently of time and space). By drawing this distinction it was possible to relate lexical items that, either within a language or across different languages, refer to close concepts, although they belong to different PoSs. Thus, as said above, the possibility to encode 'near-synonymy' between synsets of the same order (but different PoSs) was provided. Furthermore, other cross-PoS relations were identified which allow to obtain a better description of word meanings. In IWN we maintain the same distinction among semantic orders and encode for adjectives some relations which can be encoded for the other 2oes. In particular, we encode the 'INVOLVED' and 'CAUSE' relations. The INVOLVED relation links a 2oes with a loe or 3oe referring to a concept incorporated within 7 Furthermore, these relations being encoded between an adjectival synset and a nominal or verbal one are also useful to distinguish adjective classes as described by Dixon (1991), and reported in Sanfilippo et al. (1999). Indeed, such classes are often indicated by the nouns linked to adjectives. the meaning of the 2oes. Examples for adjectives are given in the following: filoso (= pieno di fili) (thready, filamentous) HAS_HYPERONYM pieno (full) INVOLVED filo (thread) imberbe (= privo di barba) (beardless) HAS_HYPERONYM privo (lacking) INVOLVED barba (beard). Another relation which can be encoded is the CAUSE relation: depuratorio (= atto a depurare) (depurative, HAS_HYPERONYM purifying) atto, adatto (suitable for) CAUSES non-factive depurare (to purify) compensativo (= che serve a compensare) (compensatory) HAS_HYPERONYM atto, adatto CAUSES non-factive (suitable for) compensare (to compensate) A new relation, not present either in WN or in EWN, will be encoded for a class of adjectives indicating the possibility of some events occurring: giudicabile (= che può essere giudicato) (triable) LIABLE_TO giudicare (to judge) inaccostabile, inavvicinabile (= che non può essere avvicinato) (which cannot be approached) LIABLE_TO avvicinare, accostare (to approach) negative. 8 E.g., to lapidate has as INVOLVED_INSTRUMENT stone; to work has as INVOLVED_AGENT worker (see Alonge et al. 1998). 9 As said above, in EWN various features were encoded to make implications of relations explicit: conjunction and disjunction (for multiple relations of the same kind encoded for a synset); non-factivity (to indicate that a causal relation does not necessarily apply); intention (added to a cause relation to indicate intention to cause a certain result); negation (to explicitly encode the impossibility of a relation occurring). These features are also used in IWN. The table below gives an overview of the main relations being encoded for adjectives in IWN (for the other relations being encoded for adjectives see Alonge et al., in prep.): Relation POS Example linked ΑΝΤΟΝΥMY adj/adj happy/unhappy GRAD_ANTONYMY adj/adj beautiful/ugly COMP_ANTONYMY adj/adj alive/dead HYPONYMY adj/adj watery/containing PERTAINS TO adj/noun chemical/chemistry IS A VALUE OF adj/noun tall/stature INVOLVED adj/noun dental/tooth CAUSE adj/verb; depurative/ adj/noun to depurate LIABLE_TO adj/verb; triable/to judge adj/noun 3 The IWN Top Ontology In the EWN TO all the entities belonging to the 2nd order have been organized into two different classification schemes, which represent the first division below 2nd Order Entity: • Situation Type: the event-structure or Aktionsart (or lexical aspect) of a situation; • Situation Components: the most salient semantic components that characterize situations. The Situation Types provide a classification of 2oes in terms of the event-structure (or Aktionsart) of the situation they refer to: a basic distinction was drawn between Static and Dynamic. The Situation Components represent a more conceptual and intuitive classification of word meanings because they can be viewed like the most salient semantic components of a concept. Examples of Situation Components are: Manner, Existence, Communication, Cause. Situation Type represents disjoint features that cannot be combined, whereas it is possible to assign any combination of Situation Components to a word meaning. Here below the Top Concepts identified for 2oes are shown: 2<sup>ND</sup> ORDER ENTITY SITUATION COMPONENT SITUATION TYPE Cause Communication Condition Existence Experience Location Manner Mental Modal Physical Possession Purpose Quantity Social Time Dynamic Static BoundedEvent UnboundedEvent Property Relation In order to be able to draw generalizations on adjective meanings by using the TO, we partially modified this scheme. First of all, we moved the PROPERTY and RELATION nodes under the SITUATION COMPONENT node. This was done for two interconnected reasons: first of all, because this distinction is not directly linked to Aktionsart (lexical aspect), while the distinctions under SITUATION TYPE are Aktionsart distinctions, i.e. they are connected with the \"the procedural characteristics (i.e. the 'phasal structure', 'time extension' and 'manner of development') ascribed to any given situation referred to by a verb phrase\" (Bache 1982: 70)<sup>10</sup>. Secondly, adjectives may refer to PROPERTIES or RELATIONS, but they may be either stative or not (cfr. e.g. Lakoff 1966; Quirk et al. 1985; Peters et al. 1999). Thus, in our system it has to be possible to specify that an adjective expresses a PROPERTY while being DYNAMIC. In any case, since many adjectives may have both a DYNAMIC sense and a STATIC one, we have also the possibility to under-specify this information <sup>10</sup> Of course, in EWN all 2oes (and therefore also nouns or adjectives) can be classified according to their Aktionsart. by linking adjectives directly to the SITUATION TYPE node. Adjectives may indicate many different types of properties: temporal (passeggiata mattutina - morning walk), psychological (canzone triste - sad song), social (uomo ricco - rich man), physical (superficie legnosa - wooden surface), physiological (bambino magro - thin child), perceptive (minestra calda - hot soup), quantitative (magra ricompensa - poor reward) and intensity properties (vino forte - strong wine). In the EWN TO there are already nodes which may be used to represent these distinctions (TIME, MENTAL, SOCIAL, PHYSICAL, QUANTITY) but we needed to better specify or also add some features. For example, we have added, under the already present node PHYSICAL, the node MATERIAL, to represent, among others, some Italian adjectives ending in -oso (for example legnoso - wooden, acquoso - watery) which indicate the property of containing a certain material. Moreover, we added the node PHYSIOLOGICAL (to classify adjectives corresponding to tired, hungry, sick, etc.) under PHYSICAL. For adjectives denoting an intensity, we then added the node INTENSITY directly under the SITUATION COMPONENT node. One of the main problem we had was that no Top Concept in the EWN TO could be used to classify the reference-modifying adjectives (cf. above). These are a very particular kind of adjectives, because they do not indicate a property of the referent of the noun they modify. So, aiming at showing the distinction between referent-modifiers and reference-modifiers, we created two new Top Concepts under the node PROPERTY: ATTRIBUTE and FUNCTIONAL, where the latter can be used for reference-modifying adjectives (according to the definition provided by Chierchia & McConnel-Ginet 1990 for the category referred to by these adjectives: \"a function from properties to properties\"). Like all descriptive adjectives, also the reference-modifiers classified under the node FUNCTIONAL can be linked to other SITUATION COMPONENTS. Functional adjectives for which the temporal aspect prevails (ex - former, presente - present) can be classified under the node TIME; adjectives referring to some 'epistemological' property (potenziale - potential, necessario - necessary) can be linked to MODAL<sup>11</sup>; etc. A particular case of functional adjectives are the 'argumental' ones. They introduce a comparison between different entities (e.g., simile - similar, diverso - different, etc.). A comparison presupposes a relation between different entities so these adjectives can be linked to both PROPERTY and RELATION. Since in the EWN TO these two Top Concepts were two different kinds of SITUATION TYPE, they were mutually exclusive; now, in the IWN revised TO they can be conjoined. Here below the IWN Top Concepts for 2<sup>nd</sup> Order Entities are shown: 2<sup>ND</sup> ORDER ENTITY SITUATION COMPONENT Cause Communication Condition Existence Experience Location Manner Mental Modal Physical Material Physiological Possession Purpose Quantity Social Time Intensity Property Attribute Functional Relation SITUATION TYPE Dynamic BoundedEvent UnboundedEvent Static Concluding remarks Although the ANTONYMY, PERTAINS_TO, and IS_ATTRIBUTE_OF relations, already encoded in the Princeton WN, are fundamental relations to describe the adjective semantics, and they can be <sup>11</sup> Since this node is used for situations involving the possibility or likelihood of other situations. useful for computational applications which exploit our resource, we believe that the 'new' relations being encoded may provide equally relevant information, especially because many adjectives cannot be defined by means of the WN relations. Let's take into consideration just a few examples. The adjective depresso is ambiguous in that it has (at least) three readings: 1) which has been lowered, flattened (said of a land); 2) being in bad physical or moral conditions (said of an area, a country); 3) affected by depression (said of a person). For these three senses of the adjective we would encode different relations, extractable from our sources: depresso<sub>1</sub> IS_CAUSED_BY deprimere (to lower) IS_A_VALUE_OF terreno (land) depresso<sub>2</sub> HAS_HYPERONYM colpito (affected) IS_CAUSED_BY depressione<sub>1</sub> (depression - economic sense) depresso<sub>3</sub> HAS_HYPERONYM affetto (affected, suffering from sthg.) IS_CAUSED_BY depressione<sub>2</sub> (depression - medical sense) The relations encoded could, e.g., help disambiguate the occurrences of the adjective in contexts such as: Gianni era depresso (Gianni was depressed) or Quella regione è depressa (That area is depressed). Indeed, by checking the semantic information encoded for the two senses of depressione linked to depresso, it's possible to provide the right interpretation for the sentences under analysis; on the other hand, the first sense of the adjective would be excluded because of the IS_A_VALUE_OF relation encoded. In this case, also the TO links could be helpful: actually, depresso<sub>2</sub> would be linked to SOCIAL and CONDITION, while depresso<sub>3</sub> would be linked to MENTAL and EXPERIENCE. Many adjectives found in our sources do not seem to have (lexicalized) antonyms, nor cannot be defined by using the PERTAINS_TO or IS_A_VALUE_OF relations. These are often linked to nouns or verbs, by various relations: piumato = coperto di piume (plumed = covered with plumage) HAS_HYPERONYM INVOLVED coperto piuma distillabile = che può essere distillato (distillable = which can be distilled) LIABLE_TO distillare. For these and many other adjectives the 'new' relations identified in IWN are necessary, given that we often cannot encode other relations for them. The relations encoded provide fundamental semantic information on them, which can, for instance, be used to infer semantic preferences (e.g., only certain loes can be modified by piumato, etc.). The inclusion of this information in a large database which is mainly intended for computational applications can be very useful, mainly because it may help in resolving ambiguities and may be used to draw inferences of different nature. References Alonge A., Calzolari N., Vossen P., Bloksma L., Castellon I., Marti T., Peters W. (1998) The Linguistic Design of the EuroWordNet Database. In \"Special Issue on EuroWordNet. Computers and the Humanities\" N. Ide, D. Greenstein, P. Vossen (eds.), 32, 2-3, pp. 91-115. Alonge A., Bertagna F., Calzolari N., Magnini B., Roventini A. (in prep.) ItalWordNet: a Large Semantic Database for Italian. Bache C. (1982) Aspect and Aktionsart: towards a Semantic Distinction. Journal of Linguistics, 18, 1. Bolinger D. (1967) Adjectives in English: Attribution and Predication. Lingua, 18, pp. 1-34. Chierchia G. and McConnel-Ginet S. (1990) An Introduction to Semantics. The MIT Press, Cambridge. Cruse D. A. (1986) Lexical Semantics. Cambridge University Press, Cambridge. Dixon R. M. W. (1991) A new Approach to English Grammar on Semantic Principles. Clarendon Press, Oxford. Fellbaum C. (1998a) A Semantic Network of English: the Mother of all WordNets. In \"Special Issue on EuroWordNet. Computers and the Humanities\" N. Ide, D. Greenstein, P. Vossen (eds.), 32, 2-3, pp. 209-220. Fellbaum C. (1998b) WordNet, An Electronic Lexical Database, The MIT Press, Cambridge. Fellbaum C., Gross D., Miller K.J. (1993) Adjectives in WordNet, Five Papers on WordNet. Lakoff G. (1966) Stative Adjectives and Verbs in English. Computation Laboratory, Harvard University Report No. NSF-17. Lyons J. (1977) Semantics. Cambridge University Press, London. Miller G., Beckwith R., Fellbaum C., Gross D., Miller K.J (1990) Introduction to WordNet: An On- line Lexical Database. International Journal of Lexicography, Vol 3, No.4, pp. 235-244. Quirk R., Greenbaum S., Leech G. Svartvik J. (1985) A Comprehensive Grammar of the English Language. Longman, London. Peters I., Peter W. and Gaizauskas R. (1999) The Representation of Adjectives in SIMPLE. Ms. Richardson S. D., Dolan W. B., Vanderwende L. (1998) MindNet: Acquiring and Structuring Semantic Information from Text. In \"COLING- ACL Proceedings\", Montreal, pp. 1098-1102. Rodriguez H., Climent S., Vossen P., Bloksma L., Roventini A., Bertagna F., Alonge A., Peters W. (1998) The Top-Down Strategy for Building EuroWordNet: Vocabulary Coverage, Base Concepts and Top Ontology. In \"Special Issue on EuroWordNet. Computers and the Humanities\", N. Ide, D. Greenstein, P. Vossen (eds.), 32, 2-3, pp. 117-152. Vossen P. (ed.) (1999) EuroWordNet General Document. In \"The EWN CD-Rom\" (see also: http://www.hum.uva.nl/~ewn). Sanfilippo A., Calzolari N., Ananiadou S., Gaizauskas R., Saint-Dizier P., Vossen P. (eds.) (1999) Preliminary Recommendations on Lexical Semantic Encoding. EAGLES LE3-4244 Final Report."
  },
  {
    "title": "Layout & Language: Preliminary experiments in assigning logical structure to table cells",
    "abstract": "We describe a prototype system for assigning table cells to their proper place in the table's logical (relational) structure, based on a simple model of table structure combined with a number of measures of cohesion between cell contents. Preliminary results suggest that very simple string-based cohesion measures are not sufficient for the extraction of relational information, and that future work should pursue the aim of more knowledge/data-intensive approximations to a notional subtype/supertype definition of the relationships between value and label cells.",
    "content": "1 Introduction Real technical documents are full of text in tabu- lar and other complex layout formats. Most repre- sentations of tabular data are layout or geometry- based: in SGML, in particular, Marcy Thompson notes \"table markup contains a great deal of infor- mation about what a table looks like... but very lit- tle about how the table relates the entries. [This] prevents me from doing automated context-based data retrieval or extraction.\"1 1.1 Views of tables In (Douglas, Hurst, and Quinn, 1995) an analysis of table layout and linguistic characteristics was of- fered which emphasised the potential importance of linguistic information about the contents of cells to the task of assigning a layout-oriented table repre- sentation to the logical relational structure it em- bodies. Two views of tables were distinguished: a denotational and a functional view. 1(Thompson 1996) 151 The denotation is the table viewed as a set of n- tuples, forming a relation between values drawn from n value-sets or domains. Domains typically consist of a set of values with a common supertype in some actual or notional Knowledge Representation scheme. The actual table may also include label cells which typically can be interpreted as a lexical- isation of the common supertype. We hypothesize that the contents of value cells and corresponding label cells for a given domain are significantly re- lated in respect of some measures of cohesion that we can identify. The functional view is a description of how the information presentation aspects of tables embody a decision structure (Wright, 1982) or reading path, which determines the order in which domains are accessed in building or looking up a tuple. To express a given table denotation according to a given functional view, there is a repertoire of lay- out patterns that express how domains can be grouped and ordered for reading in two dimensions. These layout patterns constitute a syntax of table structure, defining the basic geometric configura- tions that domain values and labels can appear in. 1.2 An information extraction task Our application task is shallow information extrac- tion in construction industry specification docu- ments, containing many tables, which come to us via the miracles of OCR as formatted ASCII, e.g., in Figure 1. The predominant argument type of this genre of specification documents can be thought of as a form of 'assignment', similar to that in programming lan- guages. Our aim is to fit each assignment into a frame that contains various elements represented in terms of the sublanguage world model, a simple part-of/type-of knowledge representation. The elements we are looking for are entities, at- tributes which the KB accents as appropriate for Mix Prestressed concrete Maximum total chloride ion content (% by weight of cement, including GGBS) 0.1 0.2 Concrete made with sulphate resisting Portland cement or supersulphated cement Concrete made with Portland cement, Portland blastfurnace cement or combinations of GGBS or PFA with 0.4 ordinary Portland cement and containing embedded metal Figure 1: Example from the application domain those entities, a unit or type for each attribute, a value which the assignment gives to each attribute, and a relationship expressing the semantic content of the assignment. To extract these components, we would like to have a basic representation of the tuple structure of the table, plus information about any la- bels and how they relate to the values, in order to specify fully the relationship and its arguments. 1.3 Aims of the current work Without some way of identifying domains we cannot extract the table relation we require. Our aim is to investigate the usefulness of a range of cohesion mea- sures, from knowledge-independent to knowledge- intensive, in allowing us to select, from among those areas of table cells which are syntactically capable of being domains, those which in fact form the domains of the table. This paper reports the very beginning of the process. 2 The current prototype system The system operates in two phases. In the first, a set of areas that might constitute domains is identified, using the constraints of table structure (geometric configuration) and cell cohesion. In the second, this candidate set is filtered to produce a consistent tiling over the table. 2.1 A simplified table structure model The potential geometric configurations that we allow for a set of domain values (plus optional label) are called templates. A notation for specifying simple domain templates is defined as follows. A template is delimited by a pair of brackets [...]. Within the brackets is a list of sub-templates, currently recursive only to depth 1 and taken to be stacked vertically in the physical table. If a template has no sub-templates, it consists of a triple (ww, dd, t). w and dare either integers or the wild card ?, and specify respectively the x-extent and y-extent of an area of cells that can match the template; the wild card matches any width, or depth, as appropri- ate. t specifies whether the (sub) template is to be counted as a value (tv) or a label area (tl). A selection from a set of four possible restric- tions on a template can be defined: RESTRICTION -top -left +right +bottom AREA MUST not contain top row not contain leftmost column contain rightmost column contain bottom row The following templates are used currently: lc: [[w1 di tl] [w1 d? tv]] A label above a sin- gle column of values, of any height. lr: [[w1 di tl] [w? d1 tv]] A label above a single row of values, of any width. v: [w? d? tv] {-top -left +right +bottom} A rectangular area consisting of only values, re- stricted to domains at the bottom right margin, typically accessed using both x and y keys. c: [w1 d? tv] A single column of values. 2.2 A simplified cohesion model The 'goodness' of a rectangular area of the table, viewed as a possible instantiation of a given tem- plate, is given by its score on the various cohesion attributes. Values assigned for each of the chosen at- tributes are combined in a weighted sum to yield two overall cohesion scores for each MatchedArea, the value-value cohesion (v-v) and the label-value cohesion (l-v) as follows. v-v We have a set of n v-v cohesion functions {f-f-...f-V} which each take two cells and return a value between 0 and 1 which reflects how similar the two cells are on that function, and a set of n weights {w-ν, ων-ν...ω-V} which deter- mine the relative importance of each function's re- sult. Then for any area A composed of a set of cells we can calculate a measure of the area's cohesion as a set of domain values: VS = ∑ v-vScore(ci, Cj) (ci,c) EA (where (ci, ci) is an ordered pair of cells) v-vScore = Σων-ν-ν/Σων-ν V-V V-V i=0 i=0 v-v 1 We have a set of m 1-v cohesion functions {f0-v, f1-v ... f} which each take two cells and return a value between 0 and 1 which reflects how likely one of the cells is to be a label for the other, and a set of m weights {w0-v, w1-v ... wm-v} which determine the relative importance of each function's result. Then for an area A composed of a set of cells and a label cell cą we calculate a measure of the area's cohesion as a label plus set of domain values: LS = Σ 1-vScore (C1,Cu):CuEA m l-v el-v wf m Σω-Σω i=0 i=0 1-v 3 Experiments To test our system, we created a corpus of tables marked up in SGML with basic cell boundaries, al- lowing the template mechanism to determine the x and y position of cells. This representation is similar in relevant information content to many SGML table DTDS, and is also a plausible output from completely domain-independent techniques for table recognition in ASCII text or images, e.g., (Green and Krish- namoorthy, 1995). To this basic representation we added human-judgment information about the do- mains in each table (using an interface written in XEmacs lisp), specifying cell areas of values and la- bels for each domain. The tables were taken from a corpus of format- ted ASCII documents in the domain of construction A final score for the area is calculated as follows, industry specifications. 29 tables consisting of 91 depending on the type of template: If the area's template contains values and a label: finalScore = ων-νVS + w1_LS ων-ν + ωι-ν where wv-v and why are weights reflecting the rela- tive importance given to the VS and LS respectively. If the area's template contains only values: finalScore = VS The cohesion attributes reported here have values between 0 and 1, where 0 corresponds to high and 1 to low similiarity: ALPHA-NUMERIC RATIO: Given by Aa-Naab – |Νο| (( No)/2) +0.5 |aa|+|Na ab+No where aa is the number of alphabetic characters in string a and Na | is the number of numeric characters in string a. STRING LENGTH RATIO: A nondirectional com- parison of string length. 2.3 Selecting a set of Matched Areas Given a set of templates, we find a set of MatchedAr- eas, rectangular areas of cells which satisfy a tem- plate definition and which reach a given cohesion threshold. The set of Matched Areas contains no ar- eas that are wholly contained in other matched areas for the same template. From the set of Matched Areas we select the ar- eas we believe to be the domains of the table using a greedy algorithm which selects a set of cells that form a complete, non-overlapping tiling over the ta- ble. domains were open to examination during the ex- perimental development; 4 tables consisting of 13 domains were held back as a test set. The tests we ran had different combinations of the cohesion measures alphanum and string-length with a factor ignorelabel, which corresponds to re- ducing the weighting wi-v for the goodness of the label match to 0. The unseen condition is the last (best-performing) combination, run on the held back data. 4 Results and future work The recall results are given in Table 1. The experi- ment column specifies the trial in terms of the fac- tors defined above. The templates columns specify which templates are included in the trial. The re- call score for each trial is the number of matched areas that perfectly agree with the boundary and type of a domain as marked by the human judge, as a percentage of the number of domains identified by the human judge. (Since the selection algorithm produces only a single tiling for each table, precision was not explicitly measured.) 4.1 Effect of templates Increasing the number of templates available at one time reduces the recall performance because of con- fusion during the selection process; if we used only the Ic template, for instance, we would get better performance overall per domain (in this application area). The true performance of the system has to be judged with respect to the complete set (the right- most column in the results table), however, since all these templates are needed to match even quite sim- ple tables. --- Templates available Experiment {c} {r} {v}{c} {lc, {lc, {lc, {lr, ir, v, {lc, {lc, {lc, {lr, {lc, ir} v} c} } c} c} Ir, Ir, υ, υ, lr, }c} c} c} υ, c} alphanum 84 3 3 3 82 32 60 5 2 7 33 59 24 9 26 stringlength 41 1 0 0 42 30 35 1 1 0 31 36 26 1 27 alphanum, ignorelabel 84 3 3 3 84 34 84 5 2 7 36 84 34 9 36 stringlength, ignorelabel 41 1 0 0 42 34 41 1 1 0 35 42 34 1 35 alphanum, stringlength 75 2 3 3 75 45 68 4 1 7 45 67 42 8 42 alphanum, stringlength, ignorelabel 75 2 3 3 76 47 75 5 2 7 48 76 47 8 48 unseen 77 8 0 0 77 62 62 8 8 0 62 62 54 0 54 Table 1: Recall results for all experimental conditions: % of actual domains correctly identified The simple templates used here are also not ade- quate for more complex tables with patterns of reca- pitulation and multiply layered spanning labels. We intend to take a more sophisticated view of possi- ble geometric configurations, perhaps similar to the treatment in (Wang, 1996), and use the idea of read- ing paths to extract the tuples by relating the ap- propriate values from different domains. 4.2 Effect of cohesion measures The alphanum and stringlength measures in combi- nation do perform rather better than alone. How- ever, ignoring 1-v cohesion always improves recall; these cohesion measures do not help in distinguish- ing between labels and values, or in linking labels with value-sets. This will be more of a problem when we deal with more complex tables with complex multi-cell labels. In future, we intend to investigate the ef- fect of more sophisticated cohesion measures, includ- ing the use of thesaural information from domain- independent sources and corpus-based Knowlege Ac- quisition, e.g., (Mikheev and Finch, 1995), which should form better approximations to the super- type/subtype distinction. Combining a number of measures, in the kind of framework we have presented here, should allow graceful performance over a wide range of domains using as much information as is available, from what- ever source, as well as convenient evaluation of the relative contribution of different sources. Acknowledgements We acknowledge the support of BICC plc who sup- plied data and funded the first author during most of this work, and of the Engineering and Phys- ical Sciences Research Council of the UK, who funded the second author under the CISAU project (IED4/1/5818). References Douglas, Shona, Matthew Hurst, and David Quinn. 1995. Using natural language processing for iden- tifying and interpreting tables in plain text. In Fourth Annual Symposium on Document Analy- sis and Information Retrieval, pages 535-545. Green, E. and M. Krishnamoorthy. 1995. Recogni- tion of tables using tables grammars. In Proceed- ings of the Fourth Annual Symposium on Docu- ment Analysis and Information Retrieval, pages 261-278, University of Nevada, Las Vegas, April. Mikheev, A. and S. Finch. 1995. A workbench for acquisition of ontological knowledge from natural text. In Proceedings of the 7th conference of the European Chapter for Computational Linguistics, pages 194-201, Dublin, Ireland. Thompson, Marcy. 1996. A tables manifesto. In Proceedings of SGML Europe, pages 151 153, Munich, Germany. Wang, Xinxin. 1996. Tabular Abstraction, Editing, and Formatting. Phd, University of Waterloo, On- tario, Canada. Wright, Patricia. 1982. A user-oriented approach to the design of tables and flowcharts. In David H. Jonassen, editor, The Technology of Text. Educa- tional Technology Publications, pages 317-340. ---"
  },
  {
    "title": "Morphological Processing in the Nabu System",
    "abstract": "The Nabu morphological processor is designed to perform a number of different functions, of which five have so far been identified: analysis, guessing (about unknown words), synthesis, defaulting (proposing the most likely inflectional paradigm for a new base form), and coding (producing all possible inflectional paradigm variants for a new base form). Complete or very substantial analyzers have been produced for a number of diverse languages; other functions have been implemented as well. This paper discusses our design philosophy, as well as our technique and its implementation.",
    "content": "INTRODUCTION Nabu is a multilingual Natural Language Processing system under development in the Human Interface Laboratory at MCC, for shareholder companies. Its morphological com- ponent is designed to perform a number of dif- ferent functions. This has been used to produce a complete analyzer for Arabic; very substantial analyzers for English, French, German, and Spanish; and small collections of rules for Rus- sian and Japanese. In addition, other functions have been implemented for several of these lan- guages. In this paper we discuss our philosophy, which constrained our design decisions; elaborate some specific functions a morphological com- ponent should support; survey some competing approaches; describe our technique, which provides the necessary functionality while meet- ing the other design constraints; and support our approach by characterizing our success in developing/testing processors for various com- binations of language and function. DESIGN PHILOSOPHY Before we set about designing our mor- phological processor, we elaborated our philosophical commitments regarding an NLP system. These include: (1) multilingual applica- tion, (2) fault-tolerant, fail-soft behavior, (3) rule reversibility, (4) disparate functionality, (5) inherent parallelism, (6) grammatical clarity, and (7) rigorous testing. MULTILINGUAL APPLICATION The algorithms and their software instantia- tion must admit application to any human lan- guage likely to confront our system; these in- clude the languages of major demographic and/or economic significance (and their relatives). Our selected representatives are English, French, German, Spanish, Russian, Arabic, Chinese, and Japanese. FAULT-TOLERANT, FAIL-SOFT BEHAVIOR Real-world NLP applications, whether for text or interactive dialog, will be confronted with numerous errors of various types. As far as users are concerned, guaranteed failure in the presence of any error is intolerable: a system must overcome simple mistakes without discern- able problems. For example, insignificant typing and/or spelling mistakes should be ignored, as should minor morphological blunders. Users do not like to be asked for corrections of (seemingly) simple mistakes, and of course printed texts cannot be queried in any case. In the presence of more serious problems, perfor- mance should degrade only gradually. This is nothing more than a commitment to understanding the utterance, rather than punishing the user for errors in it. We contend that human-like fault-tolerant, fail-soft behavior must be incorporated in the fundamental design of a system: it cannot be tacked-on after system development. Creating an applied system for a \"perfect\" natural language that is hypothesized, but never observed, is misguided, aside from be- ing wasteful. RULE REVERSIBILITY To the extent feasible and reasonable, the linguistic rules in an NLP system should be re- versible-- useful for both analysis and synthesis. But it is not enough to have one, completely re- versible grammar performing two functions. Indeed, reversibility may not be always desirable: in keeping with the commitment to fault-tolerant, fail-soft behavior, an analyzer should over-generate (accepting some incorrect forms as input), while a synthesizer should never produce them as output. Since these two func- tions are, therefore, rather different processes, one must search for a means to distinguish the rules (linguistic descriptions) from the strategies (linguistic processes, called grammars) controll- ing their application: the former can be revers- ible, while the latter might not be. DISPARATE FUNCTIONALITY Analysis and synthesis constitute two obvious linguistic processes (grammars imposed upon rule sets). There are, however, even more processes than these of interest in a practical set- ting: that is, there may be a number of gram- mars built from a single set of linguistic rules, as we demonstrate below. Thus, a processor must admit the simultaneous instantiation of a num- ber of grammars in order to be called general. INHERENT PARALLELISM Acceptable runtime performance in any sig- nificant real-world NLP setting is now under- stood to require implementation on parallel machines. Thus, grammars must be inherently suited to parallel execution, and such oppor- tunities must somehow be representable in the formalism in which the grammars are expressed. GRAMMATICAL CLARITY In any real-world application, the number of linguistic rules and the complexities of grammars imposed upon them is considerable. Successful implementation and maintenance thus requires that the grammars be clearly and concisely stated, however powerful they may be. Not only must the rule formalism be relatively clean and simple, but also a grammar must be viewable at various levels of detail. Variable granularity in the presentation enhances the opportunity for comprehensibility. RIGOROUS TESTING In order to become practical, a system must admit -- and have undergone -- rigorous testing. It is not enough to develop a micro- implementation, then claim that the system can be scaled up to become a real application (presumably to be tested on end-users). More than just a philosophical commitment to the idea of testing, this requires that the system ac- tually be fast enough during the development phase that thorough testing (of grammars, etc.) can take place at that time. If its speed is gla- cial during the development phase, a complex system cannot be completed, and its practicality will never be shown. MORPHOLOGICAL PROCESSES We have, so far, identified five interesting morphological processes: analysis, guessing, syn- thesis, defaulting, and coding. The first two concern comprehension; the other three, produc- tion. ANALYSIS Morphological analysis is a relatively well- understood notion -- which is not to say that there is agreement concerning what the result should be, or how it should be produced. But, generally speaking, analysis is agreed to involve the decomposition of a surface-form string (usually in English called a word) into its con- stituent base-form morphemes and their func- tional attachments; this may be finer-grained, as when each morpheme is associated with lexical or other linguistic information, and indeed the process is usually understood to imply access to a stored lexicon of morphemes, in order to cor- rectly identify those contained in the string. Analysis is assumed to perform this decomposi- tion according to a set of language-specific strategies (i.e., a grammar) limiting the possible decompositions. GUESSING In keeping with a commitment to fault- tolerant, fail-soft behavior, a system must, e.g., deal with unknown words in an utterance by making plausible guesses regarding their mor- phological, lexical, syntactic, and even semantic properties. A morphological guessing grammar, presumably operating after the analysis grammar has failed, must embody heuristic strategies, and these may well differ from those of the analyzer, even though the rule stock upon which they draw might be identical. For example, while the analyzer must, sooner or later, entertain all possible decomposition hypotheses, the guesser might best be con- strained to produce only the \"most likely/plausible\" hypotheses. SYNTHESIS Synthesis, like analysis, is a relatively well- understood notion, albeit characterized by debate concerning the details. Generally speak- ing, synthesis is the composition of a surface- form string encoding the information contained in one or more base-form morphemes having known functional attachments. Synthesis, like analysis, is assumed to perform this composition as dictated by a grammar. Note again that, in practice, this grammar cannot be the simple in- verse of the one controlling analysis: a syn- thesizer should be prohibited from making mis- takes that are tolerated (if, indeed, even noticed) by an analyzer. DEFAULTING Guessing is relevant to end-users, dealing as it does with unknown words in an input ut- terance. Developers, on the other hand, faced with teaching the system (how to synthesize only) the correct surface forms of words being defined, can make use of additional functions, such as a defaulting grammar. Given a root or stem form and part of speech, a lexical defaulter can propose the most likely inflectional paradigm for a word. This is better than requir- ing the lexicographer to type in the variants, or manually encode the paradigm in some other fashion: greater human reliability is experienced when validating good guesses and correcting a few wrong ones than when entering everything from scratch. CODING When the lexical defaulter guesses incor- rectly, a coding grammar could render all potential inflectional paradigms (as permitted by the language), from which the lexicographer could select the correct one(s) for the newly- defined word. This is desirable, because greater human reliability is experienced in selecting from among possible inflections than in producing all and only the correct variants. SURVEY One of the more popular frameworks for morphological processing is the two-level model developed by Koskenniemi (1983), modified and extended by Karttunen [1983]. Two-level models are fully reversible, performing both analysis and synthesis, and correspond to finite-state machines, hence they appear to enjoy some com- putational advantages. However, there are both theoretical and practical problems. It appears that the model is not sufficiently powerful to ac- count for some human languages (e.g., Icelandic, which exhibits recursive vowel harmony). The model can be decidedly inelegant in some respects (e.g., in handling alternations such as the English nominate/nominee by positing a \"lexical entry\" nomin). There is a very sub- stantial time penalty involved in compiling two- level grammars (it may be measured in hours), which impedes rapid debugging and testing. Finally, the \"advantages\" of reversibility are ar- guable, for reasons discussed above and below. Affix-stripping models are commonly employed, especially for English [Slocum, 1981). The IBM system [Byrd, 1983) also uses affix rules in a strictly word-based model of morphol- ogy, but the rules are unordered and thus only marginally may be said to constitute a gram- mar; certainly, morphosyntactic behavior is not highlighted. These two systems were developed for English analysis only; they are not reversible in any sense, and have not been tested on other languages. A serendipitous situation exists, in that they have a certain degree of fault-tolerance built in, though this was not a goal of the design/implementation process. In order to elucidate morphosyntax, some ap- proaches use a simple (e.g., two-level, or similar) model to account for orthographic behavior, and a \"grammar of words\" to analyze morpheme se- quences syntactically (e.g., (Bear, 1986], [Russell et al., 1986]). It does not seem that these ap- proaches, as described, lend themselves to any sort of reversal, or other form of rule-sharing; furthermore, only analysis is mentioned as an application. Even simpler models have employed allomor- phic segmentation; morphosyntax may be ap- proximated by a longest-match heuristic (Pounder and Kommenda, 1986) or defined by a finite-state grammar (Bennett and Slocum, 1985). The required entry of every allomorphic variant of every word is both a theoretical and a practical disadvantage, but runtime processing is speeded up as a positive consequence due to total neglect of spelling changes. Fault-tolerant be- havior is not built-in, but can be almost trivially added (whereas, in many other models, it would be difficult to incorporate). The systems cited here are used for analysis only. No previous models of morphology seem to have been used for anything but analysis, and occasionally synthesis; the other three functions mentioned above, and others that might exist, are neglected. Although the author has dis- cussed other functionality in earlier work [Slocum and Bennett, 1982), even there the mor- phological processors used for analysis, synthesis, and defaulting/coding were distinct, being im- plemented by entirely different software modules, and shared only the dictionary entries. THE NABU TECHNIQUE In Nabu, rules are separate from the strategies imposed upon them. Rules may be thought of as declarative in nature; they are or- ganized in an inheritance hierarchy. The \"grammars of words\" imposed upon them, however, may be thought of as procedures -- ac- tually, dataflow networks. RULE HIERARCHY The structure of the rule hierarchy is deter- mined by linguists, purely for their own con- venience, and implies no runtime behavior of any kind. That is, the rule hierarchy is purely static and declarative in nature. The one con- straint is that, at the top level, collections of rules are distinguished by the language they belong to -- i.e., the first division is by language. Typically, though not necessarily, the second- level division imposed by the lingusts is that of category: the part-of-speech of the surface-forms for which the rule subset is relevant. For lan- guages that distinguish inflection from deriva- tion, our linguists have generally found it con- venient to divide rules at the third level in terms of these two classes. Other than the top-level language division, however, the structure of the hierarchy is entirely at the discretion of the responsible linguists. Consequently, we have ob- served that different linguists each responsible for the morphological rules (and grammars) of entire languages -- prefer and employ different organizational principles. Rules may also be thought of as declarative in nature -- though, in fact, for the purposes of maintenance they embody certain procedural be- haviors such as a self-test facility. A mor- phological rule is an equation between one letter- string+feature-set, which we call a glosseme, and another. One side of the equation describes what might be called the \"surface\" side of a glosseme, as it represents an encoding of infor- mation nearer to (but not necessarily at!) the surface-string level: all this really means is that relatively more information is expressed in the string part than in the feature part. The other side, in turn, describes what might be called the \"base\" glosseme, as it represents an encoding of information closer to (but not necessarily at!) the base-form level, with relatively less infor- mation expressed in the string part than in the feature part. It is important to note that the in- formation content of the two sides is the same -- only the form of the information changes. This is why we classify a rule as an equation, and this also admits rule reversibility in the im- plementation. As a trivial example of such an equation, consider the English inflectional rule [\"+s\" ()] = [\"+\" (NOUN PL)]. The \"surface\" side, on the left, describes a glosseme string whose last character is the letter s [by means of the pattern \"+s\"] and places no constraints on the glosseme's feature set [by means of the empty list ()). The \"base\" side, on the right, describes an equivalent glosseme whose string lacks the final letter s [by means of the pattern \"+\"] and constrains the feature set [by means of the two features (NOUN PL)]. Ex- ecuted in one direction, this rule removes an s and adds the two features (NOUN PL); reversed, the rule removes the two features (NOUN PL) and adds the morpheme s. Obviously, this English rule conveys the notion that a NOUN may be inflected for PLural by means of the [bound] morpheme s at its right end. The plus sign (+) in a pattern is used to in- dicate whether prefixation or suffixation of the glosseme string is being described, depending on whether the pattern precedes or follows it; or a pattern may describe an entire glosseme string via omission of the sign. In a pattern, al- phabetic case is important: a lower-case letter signifies a constant, and must be matched by the same letter (in any case) in the glosseme; an upper-case letter signifies a restricted variable, and must be matched by some letter from the set over which it is defined. Thus, for example, in English rules we use the letter V to stand for Vowel; C, for Consonant; and G, for Gemminat- ing consonant. (Of course, the variable restric- tions are entirely arbitrary as far as Nabu is concerned. Each linguist defines sets of vari- ables and their match restrictions according to taste and the language at hand.) If the same variable appears more than once in a pattern, it is required to match the same letter in the glosseme: the equation [\"+GGing\" ()] = [\"+G\" (VERB PRPL)] thus describes doubling of the last consonant in an English verb, before suffixation of the present participial morpheme. Another facility is required for convenience in describing alter- nation. In German, for example, certain mor- phological operations involve the umlauting of vowels; thus, an unmarked vowel on the \"base\" side may require replacement by an umlauted vowel on the \"surface\" side. If only one vowel behaved this way, this would be no problem: the corresponding letters would simply appear in their places in the patterns. But there are three vowels that behave like this in German (a, o, and u) in the identical context. In order to eliminate the need for tripling the size of the rule base otherwise required to describe this phenomenon, we provide the linguists with a means for pairing-up variables, so that a charac- ter matching one may be replaced by the cor- responding character in the other's set. Many other languages exhibit this kind of alternation, making this a useful technique. A character in a pattern string matches one and only one character in a glosseme string. Generally speaking, the characters appearing in a pattern string are those of the language being described, as one would expect. Some languages, however, lack a case distinction -- Arabic, for ex- ample -- rendering the variable notation prob- lematic. In this situation, upper-case letters from the Roman alphabet are used to represent variables in rules. Given this framework, creating a bidirec- tional rule interpreter is relatively straightfor- ward. Rule execution is a matter of matching according to one side of the equation and (if that is successful) transforming according to the other side. So long as every rule is truly an equation (and only a human linguist can decide this, ex- cept in trivial cases) then every rule is reversible -- that is, can be used for comprehension as well as production, because the interpreter can trans- form a rule's output back into the original in- put. In Nabu, as noted earler, there are cur- rently two comprehension processes (analysis and guessing) and three production processes (synthesis, defaulting, and coding). But neither collections of rules nor their hierarchical struc- ture describes such functionality. CONTROL GRAPHS A morphological grammar is an execution strategy imposed upon a body of morphological rules. Bearing in mind that morphological rules can apply to the outputs of other rules (consider the derivational process in English, as for ex- ample when the word derivational is constructed from derive + ation + al), and that such com- pounding is not free, but linguistically con- strained, it is obvious that the compounding constraints -- as well as the individual rules themselves -- must be accounted for. In Nabu, an execution strategy is represented as a dataflow network, which we loosely term a control graph. A control graph is a collection of nodes con- nected by directed arcs. Each node is composed of a bundle of morphological rules, which may be applied when input reaches that node, and whose output may be passed across arcs to other nodes. There will be one or more designated start nodes, where input appears and process- ing begins (conceptually, in parallel, if there are multiple start nodes). From each start node, there will be a path to one or more designated terminal nodes, where processing ends and the graph's output is emitted. The path may be of arbitrary length; start nodes may themselves be terminal nodes. In analysis, encountering a ter- minal node entails dictionary look-up; in syn- thesis, output of a surface-form. There are two types of arcs, Success and Failure; the former are arcs across which successfully-applied rules will pass their output, and the latter are arcs across which the original input to a node is passed, should no rule inside it be successful. A successful rule is one whose input side (\"surface\" or \"base,\" depending on whether the graph is engaged in comprehension or production) matches the input glosseme, and whose output side represents the same infor- mation as was present in the input, only refor- mulated. Conceptually, the rules inside a node may be executed in series or in parallel. The linguist controls this by setting a flag in each node; thus, some nodes may fire rules in parallel, while others fire their rules serially. (In serial nodes, rule execution terminates as soon as one rule succeeds; there is no backup.) In either case, all success arcs are traversed in parallel, or else all failure arcs are traversed in parallel, depending on whether any rule(s) succeeded -- meaning all possible morphological analyses are produced for later consideration. To take a simple example, consider the word derivations, and assume that neither it nor the singular form derivation is in the dictionary. An English analyzer graph might have multiple start nodes, one of which is intended (by the linguist) for inflected nouns. The input glosseme [\"derivations\" ()] is thus passed to the node PLURAL-NOUN, which contains, among others, the rule [\"+s\" ()] = [“+” (NOUN PL)]. The suffix pattern \"+s\" matches the glos- seme string, and no features are required to be present in the glosseme, thus this rule succeeds and produces the output glosseme [\"derivation\" (NOUN PL)]. If PLURAL-NOUN has been marked as a ter- minal node (in addition to being a start node), then dictionary look-up will take place. If so, by our assumption above it fails. Our hypothetical graph contains a Success arc from PLURAL- NOUN to DEVERBAL-NOUN, which contains, among others, the rule [\"+ation\" (NOUN)] = [\"+e\" (VERB (DERIVE NOUN +ION))]. When (and if) this rule fires, it would match the suffix (ation) in the glosseme string derivation, and the feature (NOUN), and there- fore transform that glosseme into [\"derive\" (VERB (DERIVE NOUN +ION) PL)]. Note the e restoration: rules can in principle remove and add any letter sequence, hence affix alternation is handled in a straightforward man- ner. If DEVERBAL-NOUN has been marked as a terminal node, then dictionary look-up will take place: the VERB entry derive is retrieved. The glosseme feature list, in addition to indicat- ing the main-entry (\"derive\") and lexical catego- ry (VERB) to look for in the dictionary, contains sufficient information to allow transformation of the stored entry for derive into a representation of its surface-form realization (derivations), in terms of syntactic category (NOUN), sub- categorization features (PL), and semantics (the meaning of derive, transformed by +ION). There remains only one problem to account for: that of compositional vs. non-compositional readings. Words with strictly non-compositional readings (e.g., fruitful, which is not computable from fruit and ful) simply must be stored in the dictionary; this is not a contentious claim. Words with strictly compositional readings (e.g., derivation, which is computable from derive and ation) may or may not be stored in the diction- ary: this is an efficiency consideration, based on the usual time vs. space trade-off, and is also not subject to significant debate -- at least, not with respect to theoretical implications. The problem arises for cases where both com- positional and non-compositional readings exist for the same word (e.g., largely). In such situa- tions, the non-compositional reading must of course be stored, but it would be nice if this did not require storage of the compositional reading as well. In Nabu, we solve this by means of a DECOMPOSABLE flag that must appear within the non-compositional definition of a word, in case that word also has a compositional reading which is to be computed. During the course of morphological analysis, performing successful dictionary look-up (at a \"terminal\" node) will affect subsequent processing: if the DECOM- POSABLE flag is noted, then the glosseme just submitted to dictionary look-up will also be passed across any success arcs leaving that node, for further analysis. In the absence of a DECOMPOSABLE flag, successful dictionary look-up marks a truly terminal case, and the results are returned (possibly along with read- ings from other paths in the graph) forthwith. The operations of other types of control graphs are analogous to those of the analyzer. The principles are the same, with small excep- tions (some noted above), and so are not ex- emplified here. PROCESSORS IMPLEMENTED In addition to the rule and graph interpreters per se, delivered to MCC shareholders in mid-1985, Nabu includes a variety of tools sup- porting the development, testing, maintenance, and multilingual documentation of morphologi- cal rule hierarchies and grammars. These tools (not described in this paper, for reasons of space) have been used to create a great many rules and several morphological processors. Non-terminals included, the English morph-rule hierarchy num- bers 626 nodes; French, 434; German, 493; Spanish, 1395; and Arabic, 882. In addition to these mature rule hierarchies, some preliminary work has been done on the Russian and Japanese rule hierarchies. ANALYZERS The English analyzer is complete with respect to inflection; it has been successfully tested on, among other things, the entire collec- tion of inflectional variants presented in Webster's Seventh New Collegiate Dictionary (ca. 42,500 nouns, 8,750 verbs, and 13,250 adjectives). It also accounts for the great bulk of English derivation, as determined by various word frequency lists, and is undergoing gradual, evolutionary extension to the missing (low- frequency) affixes and their combinations. A first version of this grammar was delivered to MCC shareholders in mid-1985, followed by upgrades in 1986 and 1987. The current analyzer numbers 20 nodes and 60 arcs. As mentioned earlier, a complete Arabic mor- phological analyzer exists; so far as we are aware, it accounts for all morphological phenomena in the language -- no mean feat, for a language in which a single root form could in theory be realized as over 200,000 surface forms, and in which morphemes are frequently discon- tinuous (i.e., cannot be described by simple af- fixation models) [Aristar, 1987]. This 371-node, 1133-arc analyzer was delivered to MCC shareholders in mid-1986, and may represent the first complete analyzer ever produced for Arabic. The French and German analyzers are com- plete with respect to inflection (highly irregular forms, like sein in German, naturally excepted). The former numbers 71 nodes and 121 arcs; the latter, 54 nodes and 79 arcs. The 19-node, 17- arc Spanish analyzer is nearly complete with respect to inflection; adjectives remain a tem- porary exception. With respect to verbs, for ex- ample, it has been tested on an extensive list of conjugated verbs [Noble and Lacasa, 1980], com- prising over 6,000 surface forms, and in the first such test it was 97% accurate. GUESSERS A 76-node, 116-arc German guessing graph has been implemented and tested. It is still ex- perimental, and incomplete, but it does go beyond inflection to account for some deriva- tional processes. Our Arabic guesser is actually the Arabic analyzer graph: such strategy sharing is not always appropriate, as discussed above, but it would seem to be so for languages that, like Arabic, are morphologically very rich, ad- mitting as a consequence very strong predictions. DEFAULTER A 21-node, 23-arc English defaulting graph exists. It seems to be complete (insofar as such a processor might be), in that it constitutes a seemingly adequate component of a dictionary entry coding tool. CONCLUSIONS Morphological grammars in Nabu are able to account for all compositional readings of arbitrarily-complex surface-forms in a wide range of languages. Furthermore, the formalism and development environment are reasonably comfortable. These claims are supported by our implementation and large-scale testing of several diverse grammars. For philosophical reasons, we are opposed to the idea that grammars (as opposed to in- dividual rules) must be reversible: even if it were not for the need of five-fold rather than merely dual functionality, the need for fault-tolerance in a practical system, without consequent fault- exhibition, argues for separate analysis and syn- thesis grammars. We also point out that, in our implementations, the [non-reversible] control graphs tend to be much smaller in size than the hierarchies of [reversible] rules, hence the storage penalty for \"redundancy\" is inconsequential. REFERENCES Aristar, A., \"Unification and the Computa- tional Analysis of Arabic,\" Computers and Translation 2, 2, (April-June) 1987, pp. 67-75. Bear, J., \"A Morphological Recognizer with Syntactic and Phonological Rules,\" Proceedings of COLING86, Bonn, 1986, pp. 272-276. Bennett, W.S., and J. Slocum, \"The LRC Machine Translation System,\" Computational Linguistics 11 (2-3), 1985, pp. 111-121. Byrd, R.J., \"Word Formation in Natural Language Processing Systems,\" Proceedings of the 8th IJCAI, Karlsruhe, 1983, pp. 704-706. Karttunen, L., \"Kimmo - A General Mor- phological Processor,\" Texas Linguistic Forum 22, 1983, pp. 165-186. Koskenniemi, K., \"Two-level Model for Mor- phological Analysis,\" Proceedings of the 8th IJ- CAI, Karlsruhe, 1983, pp. 683-685. Noble, J., and J. Lacasa. Handbook of Spanish Verbs. Iowa State University Press, Ames, Iowa, 1980. Pounder, A., and M. Kommenda, \"Morphological Analysis for a German Text-to- Speech System,\" Proceedings of COLING86, Bonn, 1986, pp. 263-268. Russell, G.J., S.G. Pulman, G.D. Ritchie, and A.W. Black, \"A Dictionary and Morphologi- cal Analyser for English,\" Proceedings of COLING86, Bonn, 1986, pp. 277-279. Slocum, J., \"An English Affix Analyzer with Intermediate Dictionary Look-up,\" Working Paper LRC-81-1, Linguistics Research Center, University of Texas, February 1981. Slocum, J., and W.S. Bennett, \"The LRC Machine Translation System: An Application of State-of-the-Art Text and Natural Language Processing Techniques to the Translation of Technical Manuals,\" Working Paper LRC-82-1, Linguistics Research Center, University of Texas, July 1982."
  },
  {
    "title": "Advances in domain independent linear text segmentation",
    "abstract": "This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering.",
    "content": "1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language mod- elling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent meth- ods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinc- tion of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We pro- pose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They pro- posed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repe- tition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaar- i, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic simi- larity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include s- liding window (Hearst, 1994), lexical chains (Mor- ris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomer- ative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typi- cally used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Rey- nar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phras- es, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilis- tic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely mo- tivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broad- cast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disam- biguation algorithm (Palmer and Hearst, 1994; Rey- nar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text docu- ment into the acceptable input format. 3.1 Similarity measure Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern matcher and a stopword list. A stemming algorithm (Porter, 1980) is then applied to the re- maining tokens to obtain the word stems. A dic- tionary of word stem frequencies is constructed for each sentence. This is represented as a vector of frequency counts. Let fij denote the frequency of word j in sentence i. The similarity between a pair of sentences x, y is computed using the cosine measure as shown in equation 1. This is applied to all sentence pairs to generate a similarity matrix. sim(x,y) = Σj faj × fyj Σ V Jx,j X Σj fyj 2 (1) Figure 1 shows an example of a similarity matrix¹. High similarity values are represented by bright pix- els. The bottom-left and top-right pixel show the self-similarity for the first and last sentence, respec- tively. Notice the matrix is symmetric and contains bright square regions along the diagonal. These re- gions represent cohesive text segments. Each value in the similarity matrix is replaced by its rank in the local region. The rank is the num- ber of neighbouring elements with a lower similarity value. Figure 2 shows an example of image ranking using a 3 x 3 rank mask with output range {0,8}. For segmentation, we used a 11 × 11 rank mask. The output is expressed as a ratio r (equation 2) to cir- cumvent normalisation problems (consider the cases when the rank mask is not contained in the image). Similarity matrix Rank matrix 2 3 2 8 5 4 7 1 7 8 6 3 4 9 1 6 7 Similarity matrix 2 3 2 8 5 4 7 1 7 8 6 3 4 9 1 6 Rank matrix 3 7 4 Step 1 Step 3 2 3 2 8 5 4 7 1 7 8 6 3 4 9 1 6 2 3 2 8 5 4 7 1 7 8 6 3 4 9 1 6 2 6 7 4 Step 2 Step 4 Figure 1: An example similarity matrix. 3.2 Ranking For short text segments, the absolute value of sim(x, y) is unreliable. An additional occurrence of a common word (reflected in the numerator) causes a disproportionate increase in sim(x,y) unless the denominator (related to segment length) is large. Thus, in the context of text segmentation where a segment has typically < 100 informative tokens, one can only use the metric to estimate the order of sim- ilarity between sentences, e.g. a is more similar to b than c. Furthermore, language usage varies throughout a document. For instance, the introduction section of a document is less cohesive than a section which is about a particular topic. Consequently, it is inap- propriate to directly compare the similarity values from different regions of the similarity matrix. In non-parametric statistical analysis, one com- pares the rank of data sets when the qualitative be- haviour is similar but the absolute quantities are un- reliable. We present a ranking scheme which is an adaptation of that described in (O'Neil and Denos, 1992). ¹The contrast of the image has been adjusted to highlight the image features. Figure 2: A working example of image ranking. r= # of elements with a lower value # of elements examined (2) To demonstrate the effect of image ranking, the process was applied to the matrix shown in figure 1 to produce figure 3². Notice the contrast has been improved significantly. Figure 4 illustrates the more subtle effects of our ranking scheme. r(x) is the rank (1 × 11 mask) of f(x) which is a sine wave with decaying mean, amplitude and frequency (equation 3). Figure 3: The matrix in figure 1 after ranking² ²The process was applied to the original matrix, prior to contrast enhancement. The output image has not been en- hanced. 1 0.9 08 0.7 0.6 0.5 0.4 03 0.2 0.1 f(x) g(z) = = g(xx) 200 (e-/2+e-/2(1 + sin(1020.7))) f(x) r(x) (3) 0 0 50 100 X 150 200 Rank matrix Step 2 Step 1 Step 3 Cut Figure 4: An illustration of the more subtle effects of our ranking scheme. 3.3 Clustering The final process determines the location of the topic boundaries. The method is based on Reynar's max- imisation algorithm (Reynar, 1998; Helfman, 1996; Church, 1993; Church and Helfman, 1993). A text segment is defined by two sentences i, j (inclusive). This is represented as a square region along the di- agonal of the rank matrix. Let si,j denote the sum of the rank values in a segment and ai,j = (j − i + 1)² be the inside area. B = {b1,..., bm} is a list of m coherent text segments. sk and ar refers to the sum of rank and area of segment k in B. Dis the inside density of B (see equation 4). D = 772 k=1 Sk 2k 1 ak m Figure 5: A working example of the divisive cluster- ing algorithm. figure 7). Let uand v be the mean and variance of δD(η), η ∈ {2, ..., b + 1}. m is obtained by applying the threshold, μ + c × √v, to dD (c = 1.2 works well in practice). Inside density 1 0.9 0.8 0.7 0,6 0.5 0.4 (4) 0.3 To initialise the process, the entire document is placed in B as one coherent text segment. Each step of the process splits one of the segments in B. The split point is a potential boundary which maximises D. Figure 5 shows a working example. The number of segments to generate, m, is deter- mined automatically. D(n) is the inside density of n segments and SD(n) = D(n) _ D(n-1) is the gradient. For a document with b potential boundaries, b step- s of divisive clustering generates {D(1), ..., D(b+1)} and {8D(2), ..., 8D(6+1)} (see figure 6 and 7). An unusually large reduction in D suggests the opti- inal clustering has been obtained³ (see n = 10 in In practice, convolution (mask {1,2,4,8,4,2,1}) is first applied to SD to smooth out sharp local changes 0.2 0.1 0 10 20 30 40 Number of segments 50 60 70 80 Figure 6: The inside density of all levels of segmen- tation. 3.4 Speed optimisation The running time of each step is dominated by the computation of sk. Given sij is constant, our algo- rithm pre-computes all the values to improve speed performance. The procedure computes the values a- long diagonals, starting from the main diagonal and [ERROR: Failed to process this page - Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.] boundaries as real boundaries. B(r,b) randomly se- lects b boundaries as real boundaries. The accuracy of the last two algorithms are com- puted analytically. We consider the status of m po- tential boundaries as a bit string (1→ topic bound- ary). The terms p(miss) and p(fa) in equation 6 cor- responds to p(samek) and p(diff(k) = 1-p(samek). Equation 7, 8 and 9 gives the general form of p(samek), B(r,?) and B(r,b), respectively7. Table 2 presents the experimental results. The values in row two and three, four and five are not actually the same. However, their differences are insignificant according to the Kolmogorov-Smirnov, or KS-test (Press et al., 1992). p(samelk) = # valid segmentations # possible segmentations (7) p(samelk, B(r,?)) = 2m-k 2m = 2-k (8) p(samelk, m, B(r,b)) = (m-k)Cb mCb (9) Cy x! y!(x-y)! 3-11 3-5 6-8 9-11 Be Br = = 45% 38% 39% 36% 47% 47% 47% 46% Β(1,6) 47% 47% 47% 46% Ba 53% 53% 53% 54% B(r,?) 53% 53% 53% 54% Table 2: The error rate of the baseline algorithms. 4.3 Experiment 2 - Text Tiling We compare three versions of the TextTiling algo- rithm (Hearst, 1994). H94(c,d) is Hearst's Cim- plementation with default parameters. H94(c,r) us- es the recommended parameters k = 6, w = 20. H94(j,r) is my implementation of the algorithm. Experimental result (table 3) shows H94(c,d) and H94(c,r) are more accurate than H94(j,r). We sus- pect this is due to the use of a different stopword list and stemming algorithm. 4.4 Experiment 3 - DotPlot Five versions of Reynar's optimisation algorithm (Reynar, 1998) were evaluated. R98 and R98(min) are exact implementations of his maximisation and minimisation algorithm. R98 (s,cos) is my version of the maximisation algorithm which uses the cosine coefficient instead of dot density for measuring sim- ilarity. It incorporates the optimisations described The full derivation of our method is available from the author. 3-11 3-5 6-8 9-11 H94(c,d) 46% 44% 43% 48% H94(c,r) 46% 44% 44% 49% H94(j,r) 54% 45% 52% 53% H94(c,d) 0.67s 0.52s 0.66s 0.88s H94(c,r) 0.68s 0.52s 0.67s 0.92s H94(j,r) 3.77s 2.21s 3.69s 5.07s Table 3: The error rate and speed performance of Text Tiling. in section 3.4. R98(m,dot) is the modularised version of R98 for experimenting with different similarity measures. R98(m,sa) uses a variant of Kozima's semantic sim- ilarity measure (Kozima, 1993) to compute block similarity. Word similarity is a function of word co- occurrence statistics in the given document. Word- s that belong to the same sentence are considered to be related. Given the co-occurrence frequen- cies f(wi, wj), the transition probability matrix t is computed by equation 10. Equation 11 defines our spread activation scheme. s denotes the word sim- ilarity matrix, x is the number of activation steps and norm(y) converts a matrix y into a transition matrix. x = 5 was used in the experiment. ti,j = p(wj|wi) = s = norm f(wi, wj) Σj f(wi, wj) Σ i=1 ti (10) (11) Experimental result (table 4) shows the cosine co- efficient and our spread activation method improved segmentation accuracy. The speed optimisations sig- nificantly reduced the execution time. 3-11 3-5 6-8 9-11 R98(m,sa) 18% 20% 15% 12% R98(s,cos) 21% 18% 19% 18% R98(m,dot) 22% 21% 18% 16% R98 22% 21% 18% 16% R98(min) n/a 34% 37% 37% R98(s,cos) 4.54s 2.24s 4.36s 6.99s R98 29.58s 9.29s 28.09s 55.03s R98(m,sa) 41.02s 7.34s 40.05s 113.5s R98(m,dot) 46.58s 9.24s 42.72s 115.4s R98(min) n/a 19.62s 58.77s 122.6s Table 4: The error rate and speed performance of Reynar's optimisation algorithm. 4.5 Experiment 4 - Segmenter We compare three versions of Segmenter (Kan et al., 1998). K98(p) is the original Perl implementation of the algorithm (version 1.6). K98(j) is my imple- mentation of the algorithm. K98(j,a) is a version of K98(j) which uses a document specific chain break- ing strategy. The distribution of link distances are used to identify unusually long links. The threshold is a function μ + c × √ of the mean u and variance v. We found c = 1 works well in practice. Table 5 summarises the experimental results. K98(p) performed significantly better than K98(j,*). This is due to the use of a different part-of-speech tagger and shallow parser. The difference in speed is largely due to the programming languages and term clustering strategies. Our chain breaking strategy improved accuracy (compare K98(j) with K98(j,a)). 3-11 3-5 6-8 9-11 K98(p) 36% 23% 33% 43% K98(j,a) n/a 41% 46% 50% K98(j) n/a 44% 48% 51% K98(p) 4.24s 2.57s 4.21s 6.00s K98(j) n/a 21.43s 65.54s 129.3s K98(j,a) n/a 21.44s 65.49s 129.7s Table 5: The error rate and speed performance of Segmenter. 4.6 Experiment 5 - Our algorithm, C99 Two versions of our algorithm were developed, C99 and C99(b). The former is an exact implementation of the algorithm described in this paper. The latter is given the expected number of topic segments for fair comparison with R98. Both algorithms used a 11 x 11 ranking mask. The first experiment focuses on the impact of our automatic termination strategy on C99(b) (table 6). C99(a) is marginally more accurate than C99. This indicates our automatic termination strategy is effec- tive but not optimal. The minor reduction in speed performance is acceptable. 3-11 3-5 6-8 9-11 C99(b) 12% 12% 9% 9% C99 13% 18% 10% 10% C99(b) 4.00s 1.91s 3.73s 5.99s C99 4.04s 2.12s 4.04s 6.31s Table 6: The error rate and speed performance of our algorithm, C99. The second experiment investigates the effect of different ranking mask size on the performance of C99 (table 7). Execution time increases with mask size. A 1×1 ranking mask reduces all the elements in the rank matrix to zero. Interestingly, the increase in ranking mask size beyond 3 × 3 has insignificant effect on segmentation accuracy. This suggests the use of extrema for clustering has a greater impact on accuracy than linearising the similarity scores (figure 4). 3-11 3-5 6-8 9-11 1 × 1 48% 48% 50% 49% 3 × 3 12% 11% 10% 8% 5 × 5 12% 11% 10% 8% 7 × 7 12% 11% 10% 8% 9 × 9 12% 11% 10% 9% 11 × 11 12% 11% 10% 9% 13 × 13 12% 11% 10% 9% 15 × 15 12% 11% 10% 9% 17 × 17 12% 10% 10% 8% 1 × 1 3.92s 2.06s 3.84s 5.91s 3 × 3 3.83s 2.03s 3.79s 5.85s 5 × 5 3.86s 2.04s 3.84s 5.92s 7 × 7 3.90s 2.06s 3.88s 6.00s 9 × 9 3.96s 2.07s 3.92s 6.12s 11 × 11 4.02s 2.09s 3.98s 6.26s 13 × 13 4.11s 2.11s 4.07s 6.41s 15 × 15 4.20s 2.14s 4.14s 6.60s 17 × 17 4.29s 2.17s 4.25s 6.79s Table 7: The impact of mask size on the performance of C99. 4.7 Summary Experimental result (table 8) shows our algorith- m C99 is more accurate than existing algorithms. A two-fold increase in accuracy and seven-fold in- crease in speed was achieved (compare C99(b) with R98). If one disregards segmentation accuracy, H94 has the best algorithmic performance (linear). C99, K98 and R98 are all polynomial time algorithms. The significance of our results has been confirmed by both t-test and KS-test. 3-11 3-5 6-8 9-11 C99(b) 12% 12% 9% 9% C99 13% 18% 10% 10% R98 22% 21% 18% 16% K98(p) 36% 23% 33% 43% H94(c,d) 46% 44% 43% 48% H94(c,r) 3.77s 2.21s 3.69s 5.07s C99(b) 4.00s 1.91s 3.73s 5.99s C99 4.04s 2.12s 4.04s 6.31s R98 29.58s 9.29s 28.09s 55.03s K98(j) n/a 21.43s 65.54s 129.3s Table 8: A summary of our experimental results. 5 Conclusions and future work A segmentation algorithm has two key elements, a clustering strategy and a similarity measure. Our results show divisive clustering (R98) is more precise than sliding window (H94) and lexical chains (K98) for locating topic boundaries. Four similarity measures were examined. The co- sine coefficient (R98(s,cos)) and dot density measure (R98(m,dot)) yield similar results. Our spread activa- tion based semantic measure (R98(m,sa)) improved accuracy. This confirms that although Kozima's ap- proach (Kozima, 1993) is computationally expen- sive, it does produce more precise segmentation. The most significant improvement was due to our ranking scheme which linearises the cosine coefficien- t. Our experiments demonstrate that given insuffi- cient data, the qualitative behaviour of the cosine measure is indeed more reliable than the actual val- ues. Although our evaluation scheme is sufficient for this comparative study, further research requires a large scale, task independent benchmark. It would be interesting to compare C99 with the multi-source method described in (Beeferman et al., 1999) using the TDT corpus. We would also like to develop a linear time and multi-source version of the algorith- m. Acknowledgements This paper has benefitted from the comments of Mary McGee Wood and the anonymous reviewer- s. Thanks are due to my parents and department for making this work possible; Jeffrey Reynar for discussions and guidance on the segmentation prob- lem; Hideki Kozima for help on the spread activation measure; Min-Yen Kan and Marti Hearst for their segmentation algorithms; Daniel Oram for references to image processing techniques; Magnus Rattray and Stephen Marsland for help on statistics and mathe- matics. References James Allan, Jaime Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic detection and tracking pilot study final report. In Proceedings of the DARPA Broadcast News Tran- scription and Understanding Workshop. Doug Beeferman, Adam Berger, and John Lafferty. 1997a. A model of lexical attraction and repul- sion. In Proceedings of the 35th Annual Meeting of the ACL. Doug Beeferman, Adam Berger, and John Lafferty. 1997b. Text segmentation using exponential mod- els. In Proceedings of EMNLP-2. Doug Beeferman, Adam Berger, and John Laffer- ty. 1999. Statistical models for text segmentation. Machine learning, special issue on Natural Lan- guage Processing, 34(1-3):177-210. C. Cardie and R. Mooney (editors). Freddy Y. Y. Choi. 2000. A speech interface for rapid reading. In Proceedings of IEE colloquium: Speech and Language Processing for Disabled and Elderly People, London, England, April. IEE. Kenneth W. Church and Jonathan I. Helfman. 1993. Dotplot: A program for exploring self-similarity in millions of lines of text and code. The Journal of Computational and Graphical Statistics. Kenneth W. Church. 1993. Char_align: A program for aligning parallel texts at the character level. In Proceedings of the 31st Annual Meeting of the ACL. David Eichmann, Miguel Ruiz, and Padmini S- rinivasan. 1999. A cluster-based approach to tracking, detection and segmentation of broadcast news. In Proceedings of the 1999 DARPA Broad- cast News Workshop (TDT-2). Gregory Grefenstette and Pasi Tapanainen. 1994. What is a word, what is a sentence? problems of tokenization. In Proceedings of the 3rd Conference on Computational Lexicography and Text Research (COMPLEX'94), Budapest, July. Mochizuki Hajime, Honda Takeo, and Okumura Manabu. 1998. Text segmentation with mul- tiple surface linguistic cues. In Proceedings of COLING-ACL'98, pages 881-885. Michael Halliday and Ruqaiya Hasan. 1976. Cohe- sion in English. Longman Group, New York. Marti Hearst and Christian Plaunt. 1993. Subtopic structuring for full-length document access. In Proceedings of the 16th Annual International ACM/SIGIR Conference, Pittsburgh, PA. Marti A. Hearst. 1994. Multi-paragraph segmenta- tion of expository text. In Proceedings of the A- CL'94. Las Crces, NM. Oskari Heinonen. 1998. Optimal multi-paragraph text segmentation by dynamic programming. In Proceedings of COLING-ACL '98. Jonathan I. Helfman. 1996. Dotplot patterns: A lit- eral look at pattern languages. Theory and Prac- tice of Object Systems, 2(1):31-41. Min-Yen Kan, Judith L. Klavans, and Kathleen R. McKeown. 1998. Linear segmentation and seg- ment significance. In Proceedings of the 6th International Workshop of Very Large Corpora (WVLC-6), pages 197-205, Montreal, Quebec, Canada, August. Stefan Kaufmann. 1999. Cohesion and collocation: Using context vectors in text segmentation. In Proceedings of the 37th Annual Meeting of the As- sociation of for Computational Linguistics (Stu- dent Session), pages 591-595, College Park, USA, June. ACL. Hideki Kozima. 1993. Text segmentation based on similarity between words. In Proceedings of A- CL '93, pages 286-288, Ohio. Sadao Kurohashi and Makoto Nagao. 1994. Auto- matic detection of discourse structure by checking surface information in sentences. In Processings of COLING '94, volume 2, pages 1123-1127. Diane J. Litman and Rebecca J. Passonneau. 1995. Combining multiple knowledge sources for dis- course segmentation. In Proceedings of the 33rd Annual Meeting of the ACL. S. Miike, E. Itoh, K. Ono, and K. Sumita. 1994. A full text retrieval system. In Proceedings of SI- GIR '94, Dublin, Ireland. J. Morris and G. Hirst. 1991. Lexical cohesion com- puted by thesaural relations as an indicator of the structure of text. Computational Linguistic- s, (17):21-48. Jane Morris. 1988. Lexical cohesion, the thesaurus, and the structure of text. Technical Report CSRI 219, Computer Systems Research Institute, Uni- versity of Toronto. M. A. O'Neil and M. I. Denos. 1992. Practical ap- proach to the stereo-matching of urban imagery. Image and Vision Computing, 10(2):89-98. David D. Palmer and Marti A. Hearst. 1994. Adap- tive sentence boundary disambiguation. In Pro- ceedings of the 4th Conference on Applied Natural Language Processing, Stuttgart, Germany, Octo- ber. ACL. Jay M. Ponte and Bruce W. Croft. 1997. Text seg- mentation by topic. In Proceedings of the first Eu- ropean Conference on research and advanced tech- nology for digital libraries. U.Mass. Computer Sci- ence Technical Report TR97-18. M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130-137, July. William H. Press, Saul A. Teukolsky, William T. Vettering, and Brian P. Flannery, 1992. Numeri- cal recipes in C: The Art of Scientific Computing, chapter 14, pages 623-628. Cambridge University Press, second edition. Jeffrey Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sen- tence boundaries. In Proceedings of the fifth con- ference on Applied NLP, Washington D.C. Jeffrey Reynar, Breck Baldwin, Christine Doran, Michael Niv, B. Srinivas, and Mark Wasson. 1997. Eagle: An extensible architecture for general lin- guistic engineering. In Proceedings of RIAO '97, Montreal, June. Jeffrey C. Reynar. 1994. An automatic method of finding topic boundaries. In Proceedings of A- CL '94 (Student session). Jeffrey C. Reynar. 1998. Topic segmentation: Algo- rithms and applications. Ph.D. thesis, Computer and Information Science, University of Pennsylva- nia. Jeffrey C. Reynar. 1999. Statistical models for topic segmentation. In Proceedings of the 37th Annual Meeting of the ACL, pages 357-364. 20-26th June, Maryland, USA. C. J. van Rijsbergen. 1979. Information Retrieval. Buttersworth. Yaakov Yaari. 1997. Segmentation of expository texts by hierarchical agglomerative clustering. In Proceedings of RANLP '97. Bulgaria. Gilbert Youmans. 1991. A new tool for discourse analysis: The vocabulary-management profile. Language, pages 763-789."
  },
  {
    "title": "A Prototype of a Grammar Checker for Czech¹",
    "abstract": "This paper describes the implementation of a prototype of a grammar based grammar checker for Czech and the basic ideas behind this implementation. The demo is implemented as an independent program cooperating with Microsoft Word. The grammar checker uses specialized grammar formalism which generally enables to check errors in languages with a very high degree of word order freedom.",
    "content": "Introduction Automatic grammar checking is one of the fields of natural language processing where simple means do not provide satisfactory results. This statement is even more true with respect to grammar checking of the so-called free word order languages. With the growing degree of word order freedom the usability of simple pattern matching techniques decreases. In languages with such a high degree of word order freedom as in most Slavic languages the set of syntactic errors that may be detected by means of simple pattern matching methods is almost negligible. This is probably one of the reasons, why even though the famous paper [CH83] was written as long as 13 years ago, there are still very few articles about this topic, except papers like [K94] or [M96] which appeared only during the last three years. In the present paper we describe the basic ideas behind an implementation of a prototype of a grammar checker for Czech. During the development of this application we had to solve a number of problems concerning the theoretical background, to develop a formalism allowing efficient implementation and of course to create a grammar and define the structure of the lexical data. The last but not least problem was to incorporate the prototype into an existing text editor. How does the system work In order to demonstrate the function of the pivot implementation of our system we decided to connect it to a commercially available text editor. We intended to create a DLL library with the standard grammar checking interface required by a particular text editor. This idea turned out to be unrealistic because the necessary interface is among the classified inside information in most companies. Fortunately there is the possibility to use a concept of Dynamic Data Exchange (DDE) for the communication between programs in the Microsoft Windows environment. This type of connection is of course much slower than the intended one, but for the purpose of this demonstration the difference in speed is not so important. Our system can work with any text editor under Windows that contains a macro language supporting the DDE connection. For the purpose of the pivot implementation of the system we have chosen Microsoft Word 6.0. The grammar checker is implemented as an independent Windows application (GRAMMAR.EXE) which runs on the background of the Word. In order to be able to use GRAMMAR.EXE, we had to create a macro Grammar, assigned to the Grammar Checker item in the Tools menu. This macro selects a current sentence, sends it to GRAMMAR.EXE via DDE, receives the result and indicates the type of the result to the user. This activity is being performed for all sentences in the selection or for all sentences from the position of the cursor till the end of document. Grammar-Checker Grammar Result Options Input sentence: zvoleného při prvních volbách však skončí po třech letech. Syntactic inconsistencies: 6+11 3+6 3+6 5+6 ZVOLENÉHO - SKONČÍ/CASE_DISAGR_IN_THE_F OBDOBÍ - ZVOLENÉHO/CASE_DISAGR_IN_THE_F OBDOBÍ ZVOLENÉHO/ERRCASE! ČLENŮ - ZVOLENÉHO/ERRNUM! Result: Error Error pair: 5 6 Check The user may get several types of messages about the correctness of the text: a) The macro changes the color of words in the text according to the type of the detected error - the unknown words are marked blue, the pairs of words involved in a syntactic error are marked red. b) The macro creates a message box with a warning each time there is an undesired result of grammar checking — either there was no result or the sentence was too complicated. c) In case that the grammar checker identified and localized an error, it creates a message box with a short description of the error(s). Because the grammar checker is running as an independent application, the user may also look at the complete results provided by it. When a message box containing an error message appears on the screen, the user may switch to GRAMMAR and get an additional information. The main window of GRAMMAR is able to provide either the complete list of errors, the statistics concerning for example the number of different syntactic trees built during grammar checking or even the result in the form of a syntactic tree. We do not suppose that the last option is interesting for a typical user, but if we do have all this information, why should we throw it out? 2 Syntactic tree left_sentinel 14 období 28 při volbách 5 zvoleného funkční členů sedmi prvních The architecture of the system The design of the whole system is shown in the Fig. 1. The grammar checker is composed basically of three parts: 1.Morphological and lexical analysis This part is in fact an extended spelling checker. The input text is first checked for spelling errors, then the lexical and morphological analysis creates data, which are combined with the information contained in a separate syntactic dictionary. It would of course be possible to use only one dictionary containing morphosyntactic information about particular words (lemmas), but for the sake of an easier update of information during the development of the system we have decided to keep morphemic and syntactic data in separate files. Morphological dictionary Spelling checker Syntactic dictionary Lex. & morph. analysis Positive projective USER Grammar Neg.project. & pos. nonproj. Negative nonprojective Fig 1: The architecture of the system Evaluation 2.Grammar checking (extended variant of syntactic parsing) This is the main part of the system. It tries to analyze the input sentence. There are three possible results of the analysis: a) The analysis is successful and no syntactic inconsistencies were found (at this stage of processing it is too early to use the term syntactic error, because in our terminology the term error is reserved for something what is being announced to the user after the evaluation) — in this case the sentence is considered to be correct and no message is issued. b) The analysis is successful, but all results contain at least one syntactic inconsistency. In this case it is necessary to pass the results to the evaluation phase. c) The analysis fails and (probably for the reason of the incompleteness of the grammar) it cannot say anything about the input sentence. In such a case no error message is issued. We do not use any partial results for the evaluation of the possible source of an error. Partial results are misleading, because it is often the case that the error is buried somewhere inside the partial tree and no operations performed on partial trees can provide a correct error message. Besides that operations on (hundreds or thousands) partial trees are very ineffective and they can also slow down substantially the processing of the given sentence. 3. Evaluation This phase takes the results of the previous phase in the form of syntactic trees containing markers describing individual syntactic inconsistencies. It tries to locate the source of the error using an algorithm that compares available trees. According to the settings given by the user the evaluation phase issues warnings or error messages. The core of the system is the second, grammar checking phase, therefore we will concentrate on the description of that phase. Process of grammar checking The design of our system was motivated by a simple and natural idea — the grammar checker should not spend too much time on simple correct sentences. The composition of a grammar checking module tries to stick to this idea as much as possible. The processing of an input sentence is divided into three phases: a) Positive projective This phase is in fact a standard parser — it checks if it is possible to represent a given input sentence by means of a projective syntactic tree not containing any negative symbol (these symbols represent the application of a grammar rule with relaxed constraints or an error anticipating rule). If the answer is positive, the sentence is considered to be correct and no error message is issued. As an example we may take the following simple sentence: \"Karlova žena zalévala květiny.\" (Word for word translation: Charles'[fem.sing] wife watered therefore its processing ends here. The system recognizes the structure of this sentence in the following way: LEFT_SENTINEL KARLOVA ZENA ZALEVALA KUETINY b) Positive nonprojective & negative projective This phase tries to find a syntactic tree which either contains negative symbols or nonprojective constructions. A nonprojective subtree is a subtree with discontinuous coverage. It is often the case — for example in wh-sentences — that the sentence may be considered either syntactically incorrect or nonprojective — see examples in [COL94]. If such a syntactic tree exists, the evaluation phase tries to decide if there should be an error message, warning or nothing. Let us present a slightly modified sentence from the previous paragraph: \"Karlovy žena zalévala květiny.\" (Word for word translation: Charles'[fem.pl.] wife watered flowers). This sentence is ambiguous, it is either correct and nonprojective (meaning: Woman watered Charles' flowers) or incorrect (disagreement in number between \"Karlovy\" and \"žena\") and projective. Both results are achieved by this phase of the grammar checker: LEFT_SENTINEL KARLOVY ZENA ZALEVALA KUETINY Projective reading contains an error LEFT_SENTINEL KARLOVY ZENA ZALEVALA KUETINY Nonprojective reading c) Negative nonprojective Both nonprojective constructions and negative symbols are allowed. If this phase succeeds, the evaluation module issues a relevant error message or warning. In case that neither phase provides any result, no error message is issued. In case that the user wants to know which sentences were not analyzed properly, s/he may obtain a warning. Although this division into phases worked fine for short sentences (for the sentences not more than 15 words long the first phase usually took about 1 second on Pentium 75 MHz), long and complicated sentences were unacceptably slow (even tens of seconds). These results turned our attention to the problem how to speed up the processing of correct sentences even further. With the growing length of sentences the parsing will be more complex with respect both to the length of the processing and to the number of resulting syntactic structures. Let us demonstrate the problem on a sample sentence from the corpus of Czech newspaper texts from the newspaper Lidové noviny. Let us take the sentence: \"KDS nepředpokládá spolupráci se stranou pana Sládka a není pravdou, že předseda křesťanských demokratů pan Benda v telefonickém rozhovoru s Petrem Pithartem prosazoval ing. Dejmala do funkce ministra životního prostředí.\" (Word for word translation: \"CDP [does] not suppose cooperation with party [of] Mister Sládek and [it] isn't true, that chairman [of] Christian democrats Mister Benda in telephone discussion with Petr Pithart enforced ing. Dejmal to function [of] minister [of] environment.\") In this basic form of the sentence, which is an exact transcription of the text from the corpus, the processing by the positive projective phase of our parser takes 13,07s and it provides 26 different variants of syntactic trees. During the processing there were 2272 items derived. The testing of this sentence and also of all the following ones was performed on Pentium 75MHz with 16MB RAM. Such a relatively large number of variants is caused by the fact that our syntactic analysis uses only purely syntactic means - we do not take into account either semantics or textual or sentential context. That is the reason why free modifiers at the end of our sample sentence create a great number of variants of syntactic structures and thus make the processing longer and more complicated. In order to demonstrate this problem we will take this sentence and modify it trying to find out what the main source of ineffectiveness of its parsing is. is, of course, also smaller (1817). The gain of speed would be even greater would we have worked with a negative or a nonprojective variant of the parser. The next step is to delete further groups of words from the input sentence. Among the suitable candidates there is, for example, the prepositional phrase \"v telefonickém rozhovoru\" (in [the] telephone discussion). This phrase can be easily checked for grammatical correctness locally, because it has a clear left and right borders (prepositions \"v\"and \"s\"). Here we can easily solve the problem where the nominal group ends on the right hand side. In general, we need to parse the whole sentence in order to get this information, but in some specific cases we can rely only on the surface word order. After we had deleted this phrase, the processing time went down to 8,79s, the same number of syntactic representations as in the previous case was derived (22) and the number of items was slightly lower (1789). This phrase is therefore certainly not the main source of ineffectiveness in parsing. In order to speed up the processing even more we have to use another type of simplification. The first step of simplifying the original input sentence represented almost 50% acceleration although it was only a cosmetic change from an abbreviation to a full word form. From the point of view of localisation of grammatical inconsistencies we can proceed even farther - the group title+surname in fact represents only one item; if we remove titles preceding surnames we do not change syntactic structure of the sentence. It is locally only a tiny bit simpler. When we look more closely at the resulting syntactic representation of the previous variants of the input sentence we may notice that the word \"inženýra\" [engineer[gen.]] figures (inadequately, of course, in this case) also as a right- hand attribute to the word \"Pithartem[instr.]\", as it is shown in the following screenshots (for the sake of simplicity we demonstrate only the relevant part of derivation trees). PROSADIL BENDA PROSTR3ED12 DO FUNKCE PR3EDSEDA PAN ROZHOVORI THARTEM ING. TELEFONICKERSTREM MINISTRA DEMOKRATU? KR3EST3ANSKY2CH DEJMALA Z3IVOTNI2HO If we look more closely at the number of ambiguities present with individual words, we notice that the most ambiguous word is the word (abbreviation) \"ing.\" This word form is the same in all cases, genders and numbers. If we substitute this abbreviation by the full form of the word (\"inženýra\" [engineer - [gen.]]) we get the following results: the sentence is processed 8,95s, the number of variants decreases by four (22) and the number of derived items PROSADIL BENDA PR3EDSEDA PAN DEJMALA DO FUNKCE ROZHOVORI THARTEM / ING. TELEFONICKERSTREM MINISTRA DEMOKRATU? KR3EST3ANSKY2CH PROSADIL S ING. BENDA PR3EDSEDA PAN DEMOKRATU? KR3EST3ANSKY2CH DO PROSTREDI2 Z3IVOTNI 2HO FUNKCE DEJMALA ROZHOVORI THARTEM TELEFONICKERSTREM MINISTRA PROSTREDI2 Z3IVOTNI ZHO Let us remove the word \"inženýra\" from the input sentence altogether. This time the processing time is only 3,74s, only 10 structures are created and 1021 items are derived. Another logical step is to remove all other first names and titles which are placed immediately in front of their governing words. Those words are \"pana\" [mister [gen.]], \"pan\" and \"Petrem\". The claim that the first two words are unambiguous is supported by the fact that the form of the word \"pán\" [mister] is different in Czech in case the word is \"independent\" and in case it is used as a title (pána vs. pana [gen.,acc.], pán vs. pan[nom.]). When we make this change we get more than 50% shorter processing time, namely 1,71s, also the number of resulting structures is a half of the original number (5) and only 587 items are derived. Another change we would like to demonstrate is the deletion of all other free modifiers the result of which is a certain \"backbone\" of the sentence. After having carried out all deletions, we arrive at the following structure: \"KDS nepředpokládá spolupráci a není pravdou, že Benda prosadil Dejmala.\" (Word for word translation: \"CDP [does] not- suppose cooperation and [it] isn't true, that Benda enforced Dejmal.\") The result of the processing is a unique structure and 141 items are derived in 0,22s. The last variant of the input sentence will serve as a contrast to the previous ones. Let us take the last clause of the sentence, namely \"Předseda křesťanských demokratů pan Benda v telefonickém rozhovoru s Petrem Pithartem prosazoval inženýra Dejmala do funkce ministra životního prostředí.\" [\"Chairman [of] Christian democrats Mister Benda in telephone discussion with Petr Pithart enforced ing. Dejmal to function [of] minister [of] environment.\"). If we take into account the results of the previous examples we should not be surprised by the results. The processing time is 2,25s, 10 structures were created and 722 items were derived. This example and also other test data showed that the main source of ineffectivity are clauses with a big number of free modifiers and adjuncts rather than complex sentences with many clauses. These results have led us to a layered design of grammar for positive projective parsing. The core idea of this approach is the following: Syntactic constructions which even in free word order languages may be parsed locally (certain adjectival or prepositional phrases etc.) should be parsed first in order to avoid their mutual unnecessary (from the point of view of grammar checking!) combinations. This means that the grammar should be divided into certain layers of rules (not necessarily disjunctive), which will be applied one after the other (in principle they may be applied even in cycles, but this options is not used in our implementation). In the pivot version of our system we use the following layers: 1st layer: a metarule for processing titles and abbreviations preceding names 2nd layer: a metarule from the first layer together with metarules for processing prepositional and adjectival phrases 3rd layer: metarules from the previous layer together with metarules filling the valency slots and other metarules on the level of one clause 4th layer: metarules from the previous layer together with those processing of complex sentences 5th layer: metarules for processing the left sentinel and the right hand side sentential border The application of layers may slow down the processing of short sentences (it has a fixed cost of opening the description file and consulting it during parsing process), therefore it is applied only to sentences longer than certain threshold (currently 15 words). Another important point is, that the results of parsing in layers provides only positive information (i.e. it is able to sort out sentences which are certainly correct, but the failure of parsing in layers does not necessarily mean that the sentence is incorrect). The same approach may not be used for error localization and identification, although the cases when parsing in layers fails on a correct sentence are quite rare. The implementation The implementation of our system was to a big extent influenced by the demand of effectiveness. For this reason we had to abandon even feature structures as the form of the representation of lexical data. Our data structure is a set of attribute-value pairs with the data about valency frames of particular words as the only complex values (embedded attribute-value pairs). An example of the representation of the Czech wordform \"informoval\" ([he] informed) follows: informoval lexf: informovat wcl: vb syntcl: V v cl: full refl: 0 aspect: prf frameset: ( ( actant: act case: nom prep: 0 ) [ actant: adr case: acc prep: 0 ) [ actant: pat case: clause prep: z3e ]) neg: no v form: pastp gender: ? inan , anim ! num: sg END The grammar of the system is composed of metarules representing whole sets of rules of the background formalism called Robust Free Order Dependency Grammar (RFODG). The limited space of this paper does not allow to present the full description of RFODG here. The definition may be found for example in [TR96]. The RFODG provides a formal base for the description of nonprojective and incorrect syntactic constructions. It introduces three measures by means of which it is possible to classify the degree of nonprojectivness and incorrectness of a particular sentence. In this paper we would like to stress one important feature of this formalism, namely the classification of the set of symbols which are used by RFODG into three types: a) terminals and nonterminals b) deletable and nondeletable symbols c) positive and negative symbols The sets under a) have the usual meaning, the sets under b) serve for the classification of syntactic inconsistencies and the sets under c) serve for their localisation. The union of terminals and nonterminals is exactly the set of all symbols used by RFODG. The same holds about the union of deletable and nondeletable symbols and also about the union of positive and negative symbols. In other words, each symbol used by RFODG belongs to exactly one set from each pair of sets under a), b) and c). This classification therefore allows to handle rules describing both correct and erroneous syntactic constructions in a uniform way and to use a single grammar for the description of both types of syntactic constructions. Whenever a metarule describing syntactic inconsistency is used during the parsing process, a negative symbol is inserted into the tree created according to the grammar. The metarules express a procedural description of the process of checking the applicability of a given metarule to a particular pair of input items A and B (A stands to the left from B in the input). In case that a particular rule may be applied to items A and B, a new item X is created. It is possible to change values of the resulting item X by means of an assignment operator := The constraint relaxation technique is implemented in the form of so called \"soft constraints\" - the constraints with an operator? accompanied by an error marker may be relaxed in phases b) and c) (\"hard constraints\" with an operator = may never be relaxed). The error anticipating rules are marked by a keyword NEGATIVE at the beginning of the rule and are applied only in phases b) and c). The keyword PROJECTIVE indicates that the rule may be applied only in a projective way. An example of a (simplified) metarule describing the attachment of a nominal modifier in genitive case from the right hand side of the noun: PROJECTIVE IF A.SYNTCL = n THEN ELSE IF A.SYNTCL = prep2 THEN ELSE FAIL ENDIF ENDIF B.SYNTCL = n B.case = gen A.RIGHTGEN = yes IF A.TITUL = yes THEN IF A.CASE = gen THEN IF A.GENDER = B.GENDER THEN ENDIF THEN FAIL ELSE ENDIF IF A.NUM = B.NUM ELSE ENDIF ELSE ENDIF ELSE ENDIF X:=A X.RIGHTGEN := no OK END_P The interpretation of the grammar is performed by means of a slightly modified CYK algorithm (a description of this algorithm may be found for example in [S97]. The grammar works with unambiguous input data (ambiguous words are represented as sets of unambiguous items). All partial parses from the first phase are used in the phases b) and c). For the purpose of testing and debugging the system we use full parsing even in the first phase. Speeding up the performance It is often the case that nondeterministic parsers the author of the grammar has to prevent an unnecessary multiplication of results by means of \"tricks\" which are not supported by the linguistic theory -- let us take for example the problem of subject -- predicate -- object construction. If we do not put any additional restriction on the order of application of rules then the rule filling the subcategorization slots for subject and object may be applied in two ways, either first filling the slot for the subject and then the object or vice versa. Both ways create the same syntactic structure. In such a case it is necessary to apply some additional constraints in the grammar -- for example the restriction on the order of subcategorization (an item to the left of a verb should be processed first). This approach makes the grammar more complicated than it is necessary and it may also influence the quality of results (an error on the left hand side of a verb may also prevent an attachment of the items from the right hand side of the verb). The interpreter of our grammar solves these situations itself. Every time a new item is created, the interpreter checks, if such an item with the same structure and coverage already exists. If yes, the new item is deleted. This property of the interpreter is used together with other kinds of pruning techniques in all phases of grammar checking. In addition, there are also some other techniques used especially in phases b) and c). The work with unambiguous input symbols allows fast parsing in the phase a) (CYK is polynomial with respect to the length of the input), but creates some problems in the context of constraint relaxations used in subsequent phases. For example, a typical error in \"free word order\" languages is an error in agreement. Let us suppose that we have the following three input words (the actual lexical value of these words may be neglected): Preposition (accusative or locative) Adjective (animate or inanimate gender, genitive or accusative sing.) Noun (animate, genitive or accusative sing.) These words represent 2 + 4 + 2 = 8 unambiguous items. If we try to create a prepositional phrase without constraint relaxation, we get one resulting item PP(animate, accusative sing.). On the other hand after the relaxation of constraints there are 16 items created. One of them does not contain any syntactic inconsistency, remaining 15 has one or two syntactic inconsistencies. In a nondeterministic parser all 16 variants are used in the subsequent parsing. This causes a combinatorial explosion of mostly incorrect results. There are two ways how to solve this problem. The first possible solution is to relax the constraints in certain order (to apply a hierarchy on constraints). We have chosen the other possible way, which prefers the subtrees with minimal number of errors. Every time a new branch or subtree is created, it is compared with the other branches or subtrees with the same structure and coverage and if it contains more errors than those already existing, it is not parsed further. This technique substantially speeds up the processing of rules with relaxed constraints, but it has also one rather unpleasant side effect: the syntactic inconsistencies may be suppressed and appear later in a different location. This makes the task of the evaluating part of our system a bit more difficult, but nevertheless the gain on effectivity not accompanied by the loss of recall justifies the use of this technique. Conclusion The main purpose of the demo of our system is to demonstrate a method of grammar based grammar checking of a \"free word order\" language. The system is far from being ready for commercial exploitation - the main obstacle is the size of the syntactic dictionary used. Grammar based methods require a complex syntactic information about words. To build a syntactic dictionary of about 150 000 items is a task which exceeds our current capacities with respect both to manpower and funds. It would be interesting to continue the work on our system towards the development of statistical methods for this task. References [COL94] V.Kuboň, M.Plátek: A Grammar Based Approach to Grammar Checking of Free Word Order Languages. In: Proceedings of COLING'94, Kyoto 1994, pp. 906-910 [TR96] T.Holan, V.Kuboň, M.Plátek: Formal Tools Supporting Development of a Grammar Checker, Technical Report No.9/96, Charles University, Prague, December 1996 [CH83] J.Carbonell and P.Hayes: Recovery strategies for parsing extragrammatical language. In: American Journal of Computational Linguistics, 1983 9(3-4) pp.123-146. [K94] Z.Kirschner: CZECKER - a Maquette Grammar- Checker for Czech. In: The Prague Bulletin of Mathematical Linguistics 62, MFF UK Prague, 1994, pp. 5-30. [M96] L.Mitjushin: An Agreement Corrector for Russian. In: Proceedings of COLING'96, Copenhagen 1996, pp. 776-781 [S97] Klaas Sikkel: Parsing Schemata - A Framework for Specification and Analysis of Parsing Algorithms, Texts in Theoretical Computer Science - An EATCS Series, ISBN 3-540-61650-0, Springer Verlag Berlin / Heidelberg / New York, 1997 The work was supported by the following research grants: GAČR 201/96/0195, RSS/HESP No. 85/1995 and JEP PECO 2824,„Language Technologies for Slavic Languages.\""
  },
  {
    "title": "An Automatic Scoring System For Advanced Placement Biology Essays",
    "abstract": "This paper describes a prototype for automatically scoring College Board Advanced Placement (AP) Biology essays. The scoring technique used in this study was based on a previous method used to score sentence-length responses (Burstein, et al, 1996). One hundred training essays were used to build an example-based lexicon and concept grammars. The prototype accesses information from the lexicon and concept grammars to score essays by assigning a classification of Excellent or Poor based on the number of points assigned during scoring. Final computer-based essay scores are based on the system's recognition of conceptual information in the essays. Conceptual analysis in essays is essential to provide a classification based on the essay content. In addition, computer-generated information about essay content can be used to produce diagnostic feedback. The set of essays used in this study had been scored by human raters. The results reported in the paper show 94% agreement on exact or adjacent scores between human rater scores and computer-based scores for 105 test essays. The methods underlying this application could be used in a number of applications involving rapid semantic analysis of textual materials, especially with regard to scientific or other technical text.",
    "content": "INTRODUCTION To replace the conventional multiple choice questions on standardized examinations, ¹Test items in this paper are copyrighted by Educational Testing Service (ETS). No further reproduction is permitted without written permission of ETS Educational Testing Service (ETS) is currently developing computer-based scoring tools for automatic scoring of natural language constructed- responses -- responses that are written, such as a short-answer or an essay. The purpose of this work is to develop computer-based methods for scoring so that computer-administered natural language constructed-response items can be used on standardized tests and scored efficiently with regard to time and cost. Until recently, ETS's automated scoring efforts were primarily devoted to the development of computer programs used to score short-answer constructed-responses of up to 15 words (Burstein and Kaplan, 1995 and Burstein et al., 1996). In this study a classification of Excellent or Poor was automatically assigned to an AP Biology essay. Our initial goal in this study was to develop a prototype scoring system that could reliably assign a classification of Excellent to a set of AP Biology essays. For the evaluation of the scoring method, a small sample of Poor essays were also scored to compare the results.² Human rater scoring of AP Biology essays is based on a highly constrained scoring key, called a rubric, that specifies the criteria human raters use to assign scores to essays. Accordingly, for the test question studied here, the criteria for point ²The Poor classification is not an official AP classification. It was used in this study to distinguish the Excellent essays with scores of 9 and 10 from essays with lower end scores in the 0-3 range. assignment are highly constrained. Essentially, the essay can be treated as a sequence of short-answer responses. Given our preliminary successes with test questions that elicit multiple responses from examinees, similar scoring methods were applied for scoring AP Biology essay. The results show 87% agreement for exact scores between human rater and computer scores, and 94% agreement for exact or adjacent scores between human rater and computer scores. This work is also applicable for other types of assessment as well, such as for employee training courses in corporate and government settings. Since the methods discussed in this paper describe techniques for analysis of semantic information in text, presumably this application could be extended to public informational settings, in which people might key in \"requests for information\" in a number of domains. In particular, these methods could be successfully applied to the analysis of natural language responses for highly constrained domains, such as exist in scientific or technical fields. SYSTEM TRAINING One hundred Excellent essays from the original 200 essays were selected to train the scoring system. The original 200 essays were divided into a training set and test set, selected arbitrarily from the lowest examinee identification number. Only 85 of the original 100 in the test set were included in the study due to illegibility, or use of diagrams instead of text to respond to the question. For convenience during training, and later, for scoring, essays were divided up by section, as specified in the scoring guide (see Figure 1), and stored in directories by essay section. Specifically, the Part A's of the essays were stored in a separate directory, as were Part B's, and Part C's. Examinees typically partitioned the essay into sections that corresponded to the scoring guide. System training involved the following steps that are discussed in subsequent sections: a) manual lexicon development, b) automatic generation of concept-structure representation (CSR), c) manual creation of a computer-based rubric, d) manual CSR \"fine-tuning\", e) automatic rule generation, and f) evaluation of training process. Lexicon Development Example-based approaches to lexicon development have been shown to effectively exemplify word meaning within a domain (Richardson, et al., 1993, and Tsutsumi 1992). It has been further pointed out by Wilks, et al, 1992, that word senses can be effectively captured on the basis of textual material. The lexicon developed for this study used an example-based approach to compile a list of lexical items that characterized the content vocabulary used in the domain of the test question (i.e., gel electrophoresis). The lexicon is composed of words and terms from the relevant vocabulary of the essays used for training. To build the lexicon, all words and terms considered to contribute to the core meaning of each relevant sentence in an essay, were included in the lexicon. The decision with regard to whether or not a sentence was relevant was based on information provided in the scoring guide (in Figure 1). For instance, in the sentence, \"Smaller DNA fragments move faster than larger ones.\", the terms Smaller, DNA, fragments, move, faster, larger are considered to be the most meaningful terms in the sentence. This is based on the criteria for a correct response for the Rate/Size category in the scoring guide. Each lexical entry contained a superordinate concept and an associated list of metonyms. Metonyms are words or terms which are acceptable substitutions for a given word or term (Gerstl, 1991). Metonyms for concepts in the domain of this test question were selected from the example responses in the training data This paradigm was used to identify word similarity in the domain of the essays. For instance, the scoring program needed to recognize that sentences, such as Smaller DNA fragments move faster than larger ones and The smaller segments of DNA will travel more quickly than the bigger ones, contain alternate words with similar meanings in the test question domain. To determine alternate words with similar meanings, metonyms for words, such as fragments and move were established in the lexicon so that the system could identify which words had similar meanings in the test item domain. The example lexical entries in (1) illustrate that the words fragment and segment are metonyms in this domain, as well as the words move and travel. In (1), FRAGMENT and MOVE are the higher level lexical concepts. The associated metonyms for FRAGMENT and MOVE are in adjacent lists illustrated in (1). (1). Sample Lexical Entries FRAGMENT [fragment particle segment ...] MOVE [move travel pass pull repel attract ...] Concept-Structure Representations (CSR) Obviously, no two essays will be identical, and it is unlikely that two sentences in two different essays will be worded exactly alike. Therefore, scoring systems must be able to recognize paraphrased information in sentences across essay responses.. To identify paraphrased information in sentences, the scoring system must be able to identify similar words in consistent syntactic patterns. As, Montemagni and Vanderwende (1993) have also pointed out, structural patterns are more desirable than string patterns for capturing semantic information from text. We have implemented a concept-extraction program for preprocessing of essay data that outputs conceptual information as it exists in the structure of a sentence. The program reads in a parse tree generated by Microsoft's Natural Language Processing Tools (MSNLP) for each sentence in an essay.³ The program substitutes words in the parse tree with superordinate concepts from the lexicon, and extracts the phrasal nodes containing these concepts. (Words in the phrasal node which do not match a lexical concept are not included in the set of extracted phrasal nodes.) The resulting structures are CSRs. Each CSR represents a sentence according to conceptual content and phrasal constituent structure. CSRs characterize paraphrased information in sentences. For example, in the sentences \"The_DNA segment ³ See http://research.microsoft.com/research/nlp for information on MS-NLP. would be digested only once, leaving 2 pieces.\", and \"The DNA fragment would only have 2 segments,\" the phrases DNA segment and DNA fragment are paraphrases of each other, and 2 pieces and 2 segments are paraphrases of each other. These sentences are represented by the CSR in (2a) and in (2b). (2)a. NP: [DNA,FRAGMENT] NP:[TWO,FRAGMENT] In the final version of the CSR, phrasal constituents are reduced to a general XP node, as is illustrated in (2)b..XP:[DNA,FRAGMENT] XP:[TWO,FRAGMENT] Since phrasal category does not have to be specified, the use of a generalized XP node minimizes the number of required lexical entries, as well as the number of concept grammar rules needed for the scoring process. The Computer Rubric Recall that a rubric is a scoring key. Rubric categories are the criteria that determine a correct response. A computer-based rubric was manually created for the purpose of classifying sentences in essays by rubric category during the automated scoring process. Computer rubric categories are created for the bulleted categories listed in the human rater scoring guide illustrated in Figure 1. Part A. Explain how the principles of gel electrophoresis allow for the separation of DNA fragments (4 point maximum). • Electricity. Electrical potential • Charge.............. Negatively charged fragments • Rate/Size..........Smaller fragments move faster • Calibration... DNA's ... used as markers/standards • Resolution........Concentration of gel • Apparatus........ Use of wells, gel material... Part B. Describe the results you would expect from electrophoretic separation of fragments from the following treatments of the DNA segment shown in the question. (4 point maximum). • Treatment I....... Describe 4 bands/fragments • Treatment II...... Describe 2 bands/fragments • Treatment III..... Describe 5 bands/fragments • Treatment IV..... Describe 1 band/fragment Part C1. The mechanism of action of restriction enzymes. (4 point maximum) • Recognition....... Binding of enzyme to target sequence • Cutting.............. Enzyme cuts at every location • Alternate.. Point about enzyme cutting at specific location • Detail Point.......May generate sticky ends Part C2: The different results...if a mutation occurred at the recognition site for enzyme Y. • Change in I.......1 band/fragment • Change in III....4 bands/fragments • Alternate........... Y no longer recognized and cut • Detail Point....... Y site might become an X site Figure 1: Scoring Guide Excerpt Accordingly, the computer-rubric categories were the following. For Part A, the categories were Electricity, Charge, Rate/size, Calibration, Resolution, and Apparatus. For Part B the categories were, Treatment I, Treatment 2, Treatment 3, and Treatment IV. For Part C1, the categories were: Recognition, Cutting, Alternate, and Detail Point. For Part C2, the categories were Change in 1, Change in II, Alternate, and Detail Point. Each computer-rubric category exists as an electronic file and contains the related concept grammar rules used during the scoring process. The concept grammar rules are described later in the paper. Fine-Tuning CSRS CSRs were generated for all sentences in an essay. During training, the CSRs of relevant sentences from the training set were placed into computer- rubric category files. Relevant sentences in essays were sentences identified in the scoring guide as containing information relevant to a rubric category. For example, the representation for the sentence, \"The DNA fragment would only have 2 segments,\" was placed in the computer rubric category file for Treatment II. Typically, CSRs are generated with extraneous concepts that do not contribute to the core meaning of the response. For the purpose of concept grammar rule generation, each CSR from the training data must contain only concepts which denote the core meaning of the sentence. Extraneous concepts had to be removed before the rule generation process, so that the concept- structure information in the concept grammar rules would be precise. The process of removing extraneous concepts from the CSRs is currently done manually. For this study, all concepts in the CSR that were considered to be extraneous to the core meaning of the sentence were removed by hand. For example, in the sentence, The DNA segment would be digested only once, leaving 2 pieces, the CSR in (3) was generated. For Treatment II, the scoring guide indicates that if the sentence makes a reference to 2 fragments that it should receive one point. (The word, piece, is a metonym for the concept, fragment, so these two words may be used interchangably.) The CSR in (3) was generated by the concept-extraction program. The CSR in (4) (in which XP:[DNA, FRAGMENT] was removed) illustrates the fine-tuned version of the CSR in (3). The CSR in (4) was then used for the rule generation process, described in the next section. (3) XP:[DNA,FRAGMENT] XP:[TWO,FRAGMENT] (4) XP:[TWO,FRAGMENT] Concept Grammar Rule Generation At this point in the process, each computer rubric category is an electronic file which contains fine- tuned, CSRs. The CSRs in the computer rubric categories exemplify the information required to receive credit for a sentence in a response. We have developed a program that automatically generates rules from CSRs by generating permutations of each CSR The example rules in (5) were generated from the CSR in (4). The rules in (5) were used during automated scoring (described in the following section). (5)a. XP:[TWO, FRAGMENT] b. XP:[FRAGMENT, TWO] The trade-off for generating rules automatically in this manner is rule overgeneration, but this does not appear to be problematic for the automated scoring process. Automated rule generation is significantly faster and more accurate than writing the rules by hand. We estimate that it would have taken two people about two weeks of full-time work to manually create the rules. Inevitably, there would have been typographical errors and other kinds of \"human error\". It takes approximately 3 minutes to automatically generate the rules. AUTOMATED SCORING The 85 remaining Excellent test essays and a set of 20 Poor essays used in this study were scored. First, all sentences in Parts A, B and C of each essay were parsed using MSNLP. Next, inflectional suffixes were automatically removed from the words in the parsed sentences, since inflectional suffixed forms are not included in the lexicon. CSRs were automatically generated for all sentences in each essay. For each part of the essay, the scoring program uses a searching algorithm which looks for matches between CSRs and/or subsets of CSRs, and concept grammar rules in rubric categories associated with each essay part. Recall that CSRs often have extraneous concepts that do not contribute to the core meaning of the sentence. Therefore, the scoring program looks for matches between concept grammar rules and subsets of CSRs, if no direct match can be found for the complete set of concepts in a CSR. The scoring program assigns points to an essay as rule matches are found, according to the scoring guide (see Figure 1). A total number of points is assigned to the essay after the program has looked at all sentences in an essay. Essays receiving a total of at least 9 points are classified as Excellent, essays with 3 points or less are classified as Poor, and essays with 4 - 8 points are classified as \"Not Excellent.\" The example output in Appendix 1 illustrates matches found between sentences in the essay and the rubric rules from an Excellent essay. RESULTS Table 1 shows the results of using the automatic scoring prototype to score 85 Excellent test essays, and 20 Poor test essays. Coverage (Cov) illustrates how many essays were assigned a score. Accuracy (Acc) indicates percentage of agreement between the computer-based score and the human rater score. Accuracy within 1 (w/i 1) or 2 points (w/i 2) shows the amount of agreement between the computer scores and human raters scores, within 1 or 2 points of human rater scores, respectively. For Excellent essays computer-based scores would be 1 or 2 points below the 9 point minimum, and for Poor essays, they would be 1 or 2 points above the 3 point maximum. Data Set Cov Acc Acc w/i 1 Acc w/i 2 Excellent 100% 89% 95% 100% Poor 100% 75% 90% 95% Total 100% 87% 94% 96% Table 1: Results of Automatic Scoring Prototype ERROR ANALYSIS An error analysis of the data indicated the following two error categories that reflected a methodological problem: a) Lexicon Deficiency and b) Concept Grammar Rule Deficiency. These error categories are discussed briefly below. Both error types could be resolved in future research. Scoring errors can be linked to data entry errors, morphological stripping errors, parser errors, and erroneous rules generated due to misinterpretations of the scoring guide. These errors, however, are peripheral to the underlying methods applied in this study. Lexical Deficiency Recall that the lexicon in this study was built from relevant vocabulary in the set of 100 training essays. Therefore, vocabulary which occurs in the test data, but not in the training data was ignored during the process of concept-extraction. This yielded incomplete CSRs, and degraded scoring resulted. For instance, while the core concept of the commonly occurring phrase one band is more often than not expressed as one band, or one fragment, other equivalent expressions existed in the test data some of which did not occur in the training data. From our 185 essays we extracted possible substitutions of the term one fragment. These are: one spot, one band, one inclusive line, one probe, one group, one bond, one segment, one length of nucleotides, one marking, one strand, one solid clump, in one piece, one bar, one mass, one stripe, one bar, and one blot. An even larger sample of essays could contain more alternate word or phrase substitutions than those are listed here. Perhaps, increased coverage for the test data can be achieved if additional standard dictionary sources are used to create a lexicon, in conjunction with the example based method used in this study (Richardson et al., 1993). Corpus-based techniques using domain-specific texts (e.g., Biology textbooks) might also be helpful (Church and Hanks, 1990). Concept Grammar Rule Deficiency In our error analysis, we found cases in which information in a test essay was expressed in a novel way that is not represented in the set of concept grammar rules. In these cases, essay scores were degraded. For example, the sentence, \"The action of this mutation would nullify the effect of the site, so the enzyme Y would not affect the site of the mutation.\" is expressed uniquely, as compared to its paraphrases in the training set. This response says in a somewhat roundabout way that due to the mutation, the enzyme will not recognize the site and will not cut the DNA at this point. No rule was found to match the CSR generated for this test response. SUMMARY AND CONCLUSIONS This prototype scoring system for AP Biology essays successfully scored the Excellent and Poor essays with 87% exact agreement with human grader scores. For the same set of essays, there was 94% agreement between the computer scores and human rater scores for exact or adjacent scores. The preprocessing steps required for automated scoring are mostly automated. Manual processes, such as lexicon development could be automated in the future using standard context- based, word distribution methods (Smadja, 1993), or other corpus-based techniques. The error analysis from this study suggests that dictionary- based methods, combined with our current example-based approach, might effectively help to expand the lexicon). Such methods could broaden the lexicon and reduce the dependencies on training data vocabulary. The automation of the fine-tuned CSRs will require more research. A fully automated process would be optimal with regard to time and cost savings. Work at the discourse level will have to be done to deal with more sophisticated responses which are currently treated as falling outside of the norm. Perhaps the most attractive feature of this system in a testing environment is that it is defensible. The representation used in the system denotes the content of essay responses based on lexical meanings and their relationship to syntactic structure. The computer-based scores reflect the computer-based analysis of the response content, and how it compares to the scoring guide developed by human experts. Information generated by the system which denotes response content can be used to generate useful diagnostic feedback to examinees. Since our methods explicitly analyze the content of text, these or similar methods could be applied in a variety of testing, training or information retrieval tasks. For instance, these natural language processing techniques could be used for World Wide Web-based queries, especially with regard to scientific subject matter or other material producing constrained natural language text. ACKNOWLEDGMENTS We are grateful to the College Board for support of this project. We are thankful to Altamese Jackenthal for her contributions to this project. We are also grateful to Mary Dee Harris and two anonymous reviewers for helpful comments and suggestions on earlier versions of this paper. References Burstein, Jill C., Randy M. Kaplan, Susanne Wolff and Chi Lu. (1996). Using Lexical Semantic Techniques to Classify Free Responses. Proceedings from the SIGLEX96 Workshop, ACL, University of California, Santa Cruz. Gerstl, P. (1991). A Model for the Interaction of Lexical and Non-Lexical Knowledge in the Determination of Word Meaning. In J. Pustejovsky and S. Bergler (Eds), Lexical Semantics and Knowledge Representation, Springer-Verlag, New York, NY. Church, K and P. Hanks. Word Association Norms, Mutual Information and Lexicography. Computational Linguistics, 16(1), 22-29. Montemagni, Simonetta and Lucy Vanderwende (1993). \"Structural Patterns versus String Patterns for Extracting Semantic Information from Dictionaries,\" In K. Jensen, G. Heidorn and S. Richardson (Eds), Natural Language Processing: the PLNLP Approach, Kluwer Academic Publishers, Boston, MA.. Richardson, Stephen D., Lucy Vandervende, and William Dolan. (1993). Combining Dictionary-Based and Example-Based Methods for Natural Language Analysis. (MSR-TR-93-08). Redmond, WA: Microsoft Corporation. Smadja, Frank. (1993). Retrieving Collocations fromText:Xtract. Computational Linguistics. 19(1), 143-177. Tsutsumi, T. (1992) Word Sense Disambiguation by Examples. In K. Jensen, G. Heidorn and S. Richardson (Eds), Natural Language Processing: the PLNLP Approach, Kluwer Academic Publishers, Boston, MA. Wilks, Y., D. Fass, C. Guo, J. McDonald, T. Plate, and B. Slator. (1992). Providing Machine Tractable Dictionary Tools. In J. Pustejovsky (Ed), Semantics and the Lexicon, Kluwer Academic Publishers, Boston, MA. Appendix 1: Sample Rule Matches for a Scored Essay Part A: \"The cleaved DNA is then placed in a gel electrophoresis box that has a positive and a negative end to it.\" Rubric category: CHARGE Rubric Rule:XP:[DNA],XP:[NEGATIVE] \"The longer, heavier bands would move the least and the smaller lighter bands would move the most and farther from the starting point.\" Rubric category: RATE/SIZE Rubric Rule:XP: [LARGE_SIZE],XP:[MOVE,LESS] Part B: \"If the DNA was digested with only enzyme X then there would be 4 separate bands that would develop.\" Rubric category: Treatment I Rubric Rule:XP:[FOUR] \"If the DNA was digested only with enzyme Y then two fragments or RFLP's would be visible.\" Rubric Category: Treatment II Rubric Rule:XP:[TWO,FRAGMENT] \"If the DNA was digested with both the X and the Y enzyme then there would be 5 RFLP's of 400 base pairs, 500 base pairs, 1,200 base pairs, 1,300 b.p and 1,500 b.p.\" Rubric category: Treatment III Rubric Rule: XP:[FIVE,FRAGMENT] \"If the DNA was undigested then we would find no RFLP's and, as a result, there would be no banding that would occur.\" Rubric category: Treatment IV Rubric Rule:XP:[NOT, FRAGMENT] Parts C1 and C2 \"Restriction enzymes are types of proteins which recognize certain recognition sites along the DNA sequence and cleave the DNA at that end.\" Rubric category RECOGNITION Rule:XP:[CUT,DNA] \"Therefore, there would be no cut at that location and no RFLP produced at the Y recognition site.\" Rubric Category Rule:XP:[NOT],XP:[CUT],XP:[SITE]"
  },
  {
    "title": "Techniques for Accelerating a Grammar-Checker",
    "abstract": "The paper describes several possibilities of using finite-state automata as means for speeding up the performance of a grammar-and-parsing-based (as opposed to pattern-matching-based) grammar-checker able to detect errors from a predefined set. The ideas contained have been successfully implemented in a grammar-checker for Czech, a free-word-order language from the Slavic group.",
    "content": "1 Introduction This paper describes an efficiency-supporting tool for one of the two grammar-checker technologies de- veloped in the framework of the PECO2824 Joint Research Project sponsored by the European Union. The project, covering Bulgarian and Czech, two free- word-order languages from the Slavic family, was performed between January 1993 and mid 1996 by a consortium consisting of both academic and indu- strial partners. The basic philosophy of the technology discussed in this paper¹ is that of linguistic-theoretically sound grammar-and-parsing-based machinery able to de- tect, by constraint relaxation, errors from a predefi- ned set (as opposed to pattern-matching approaches, which do not seem promising for a free word-order language). The core of the system (broad-coverage HPSG-based grammars of Bulgarian and Czech, and a single language-independent parser) was developed in the first three years of the project and was then passed to the industrial partners Bulgarian Business System IMC Sofia and Macron Prague, Ltd. While the Bulgarian system remained in more or less a de- monstrator stage only, the Czech one satisfied Ma- cron's requirements as to syntactic coverage. Ho- wever, Macron expressed serious worries about the speed of the system, should this be really introdu- ced to the market. Following this, several possibili- ¹As for the alternative technology, cf. (Holan, Kubon, and Plátek, 1997) ties of using finite-state automata (FSA) as means for speeding up the performance of the system were designed, developed and implemented, in particular: • for detecting sentences where none of the prede- fined errors can occur (thus ruling out such sent- ences from the procedure of error-search proper) • for detecting which one(s) of the predefined er- ror types might possibly occur in a particular sentence (hence, cutting down the search space of the error-search proper) • for detecting errors which are of such a nature that their occurrence might be discovered by a machinery simpler than full-fledged parsing with constraint relaxation • for splitting (certain cases of) complex sent- ences into independent clauses, allowing thus for the error-detection to be performed on shor- ter strings. 2 Lexicalization of Error Search Very many of the errors to be discovered by the sy- stem can be traced down to mismatches of (values of) features projected into the syntactic structure from the lexicon. Even though the error searching capabilities of the system are not limited in princi- ple to these lexically induced errors, for a practical implementation it turned out to be useful to narrow down the error search of the system to almost only these kinds of errors, for the following reasons: 1. the loss of generality of the system is in fact only minimal, since the majority of errors which the system is able to detect are of this nature: any- way (the only exception being agreement errors involving NPs with complicated internal struc- ture, e.g., ellipses or coordination) 2. this loss of error coverage (almost negligible for a real application) is outweighed by substan- tial gain in overall (statistical) speed of the sy- stem, which is achieved by adding a preproces- sing phase consisting of a finite state automaton passing through the input string and looking for a lexical trigger of a contingent error: • if this automaton does not find any such trigger, the time-consuming grammar- checking process proper (i.e. parsing, pos- sibly also reparsing with relaxed constraints) is not started at all and the sent- ence is immediately marked as one containing no detectable error • if this automaton finds such a lexical trigger of an error, it 'remembers' its nature so that in the following phases, only the respective constraints are relaxed (which helps to cut down the search space, as compared to reparsing with relaxing of all predefined-errors-related constraints) As an example of this idea, let us consider a system dealing with errors in subject-verb agreement in Czech (and taking - for the very purpose of this example - detection of no other errors into account). Since the realistic part of such errors in Czech is the '-I/-Y' dichotomy on homophonic past tense verb endings occurring on plural verbs ('-I' ending standing with plural masculine animate subjects, '-Y' ending with plural masculine inanimate and feminine subjects), the preprocessing finite-state automaton marks all sentences not containing any of these forms (i.e. all sentences containing only singular verbs, or plural verbs but in present tense or in neuter gender, or infinite verb forms) as 'containing no detectable error', without any actual grammar-checking taking place (it is, however, obvious that this does not necessarily mean that the sentences are truly correct - they just do not contain the kind of error the system is able to detect). 3 Alternative Error-Classification and Error Search by Finite Automata Another important step towards the application of FSA to error-detection was developing a new dimension of classification of errors to be detected: apart from the more standard criteria of frequency and performance/competence, we developed a scale based on the complexity of the formal apparatus needed for the detection of the particular error type (as for error typology developed for the purpose of the error detection techniques used in the project. cf. (Rodriguez Selles, Galvez, and Oliva, 1996)). On the one end of this scale were errors recognizable within a strictly local context, such as commas missing in front of a certain kind of complementizers (subordinating conjunctions) or incorrect vocalization of a preposition (in both Bulgarian and Czech, certain prepositions ending normally with a consonant get a supporting vocal in case the word that follows them also starts with a consonant - the parallel in English would be the opposition between the two forms a and an of the indefinite article). On the other end of the scale we put, e.g., the general case of subject- verb agreement errors. Practically more important was the question whether there exists a class of errors with complexity of detection lying between the \"trivial errors\" and the errors for the detection of which a full-fledged analysis is necessary - in other words, the question whether there exist some errors for the recognition of which • on the one hand, a limited local context is insufficient (i.e. it is necessary for this end to process a substring of length which cannot be set in advance, in general the whole input string), • on the other hand, it is not necessary to use the power of the full-fledged parser, and, in particular, it is sufficient to use the power of a finite state automaton or only slight augmentation thereof. Following some linguistic research, two such error types have been selected for implementation, and while one of them is just a marginal subtype of an error in subject-verb agreement, the other is an error type of its own, and in addition one of really crucial importance for practical grammar-checking due to its high frequency of occurrence. The former error to be detected by the finite state machinery is a particular instance of an error where a plural masculine animate subject is conjoined with a verb in a plural feminine form (cf. also the example above). The idea of detecting some particular cases of this error by a finite state automaton results from the combination of the following observations: • the nominative plural form of masculine animate nouns of the declension types pán and předseda is not ambiguous (homonymous) with any other case forms (apart form vocative case, which we shall deal with immediately below); this means that if such a form occurs in a sentence, then this form can be only - either a subject, - or a nominal predicate (with copula) - or a comparison to these, adjoined by means of the conjunctions jako, jakožto or coby - or an exclamative expression (in nominative or vocative case) • due to rules of Czech interpunction, any exclamative expression has to be separated from the rest of the sentence by commas • also, due to rules of Czech interpunction, two finite verbs in Czech must be separated from each other by either a comma or by one of the following coordinating conjunctions: a, i and nebo Hence, if we build up a finite state automaton able to recognize the following substrings: 1. <unambiguous masculine animate noun in nominative plural> followed by any string containing neither a finite verb form nor a comma nor one of the conjunctions a, i and nebo follo- wed by <unambiguous past participle in plural feminine> or (due to free word order) 2. <unambiguous past participle in plu- ral feminine> followed by any string contai- ning neither a finite verb form nor a comma nor one of the conjunctions a, i, nebo followed by <unambiguous masculine animate noun in nominative plural> and combine it with a sim- ple automaton able to detect the absence of the words jako, jakožto and coby as well as the ab- sence of any finite form of the copula být ('to be') in the sentence, then we may conclude that we have built a device able to detect whether a sentence contains a particular instance of a subject-verb agreement violation. The detection of the latter error is also based on the Czech interpunction rule prescribing that there always must occur a comma or a coordinating con- junction between two finite verb forms. Hence, a simple finite state automaton checking whether bet- ween any two finite verb forms a comma or a coor- dinating conjunction occurs is able to detect many cases of the omission of a comma at the end of an embedded subordinated clause, which is one of the most frequent errors at all. (Of course, the word- forms of the verb must be unambiguously identifia- ble as suchi.e. such forms as ženu, jedu, tratím, holi etc., do not qualify due to their part of speech ambiguity, which means that in sentences containing them this strategy cannot be used). 4 Using FSA for Splitting a Sentence into Clauses The last idea how to gain efficiency is that of split- ting the sentence (if possible) into clauses before the processing, which has a two-fold positive effect on the overall process of grammar-checking: 1. it is less time consuming to parse two 'shorter' strings than one longer (assuming that parsing is at least cubic in time, this follows trivially from the inequality A³+ B³ < A³ +3A²B+3AB² + B³ = (A+B)³ for A, B positive - length of strings) 2. it is possible to detect an error in one of the substrings (clauses) irrespective to the results of analysis of (any of) the other one(s); in par- ticular, also in cases where at least one of them was not analyzed and, hence, also the parsing (including the parsing with relaxed constraints) of the whole input could not have been perfor- med on the original string, which would have hindered the error message pertinent to the sub- string successfully parsed during the parsing with constraint relaxation to be issued. In particular, this means that measures are to be found which would allow for splitting the input sent- ence into clauses by purely superficial criteria. Ob- viously, this is not possible in all cases (for all sent- ences), but on the other hand it is also clear that in any language there exists a (statistically) huge sub- set of sentences of this language where such techni- ques are applicable. For Czech, such an approach might be implemented using pattern matching tech- niques which would recognize for example the follo- wing patterns (and use them in an obvious way for splitting the sentence into clauses): 1. <any string> <finite verb> <any string> <conjunctive coordinating conjunction> <any string> <finite verb> <any string> <end of sentence> 2. <any string> <finite verb> <any string> <comma> <non-conjunctive coordinating con- junction or complementizer> <any string> <finite verb> <any string> <end of sentence> 3. <complementizer> <any string> <finite verb> <any string> <comma> <any string> <finite verb> <any string> <end of sentence> where the expressions have the following meaning(s): • <any string> is a variable for any string not containing elements of the following nature: fi- nite verb or word form homonymous with a finite verb, coordinating conjunction (of any kind), complementizer, any interpunction sign • <finite verb> is a variable for a main verb (not for an auxiliary) spe- cified for person, or for a past participle of a main verb; neither of these might be homonymous in part of speech (but they might be ambiguous within the defined class - such verbs as podrobí, proudí do qualify) • <end of sentence> is simply either a full-stop, a question-mark, an exclamation-mark, a colon or a semi-colon. All the remaining expressions have clear mnemonics, and also the classes which they stand for do not contain elements which are ambiguous as to part of speech. 5 Significance and Caveats The techniques to be used for gaining overall per- formance, speed and memory efficiency etc., of a grammar-checking system presented result solely from research concerning relevant properties of the syntax of a particular language (Czech, in part also Bulgarian), and, hence, they are strongly language- dependent. However, it seems to be self-evident that the core idea is transferable to other languages. The introduction of these techniques contributes to the process of ripening of the system into a real indu- strial application at least in the following points: • it speeds up the overall performance of the sy- stem considerably (in the order ranging from one to two magnitudes, depending on the text to be processed) by avoiding full-fledged par- sing to be performed in unnecessary cases or by making this parsing simpler • it extends its coverage, in particular the capa- bilities of the system to recognize as error-free also a large number of sentences which in the original version of the system would be unana- lyzable by the non-relaxed grammar (as well as by the grammar with relaxed constraints, for that matter) due to either incompleteness of the grammar proper or the exhaustion of hardware resources. There is a serious caveat to be issued, however: since they do not employ full analysis of the input sentence, these techniques are albeit probably only rarely on the practical level more likely to issue in- correct error messages, in the sense that their capa- bilities of detecting an erroneous sentence are exactly the same as on the full-fledged approach, but their capabilities of detecting what kind of error occurred in the sentence are slightly reduced. For example, in (the Czech equivalent of) the sentence *Your wife drives very drives fast a grammar-checker based solely on the full-fledged philosophy would correctly recognize that the same verb is repeated twice, while a checker using only fi- nite state automaton detecting the presence/absence of a comma or a coordinating conjunction between two finite verbs issues a message concerning exactly the 'missing comma' - and similar examples can be constructed also for all the other cases. In other words, there is a price to be paid for the speed-up of the error-checking process by means of the tech- niques proposed. References Holan, T., V. Kuboń, and M. Plátek. 1997. A pro- totype of a grammar checker for Czech. In this volume. Rodriguez Selles, Y., M.R. Galvez, and K. Oliva. 1996. Error detection techniques in grammar- checking. Technical report of the PECO2824 pro- ject, Autonomous University of Barcelona and University of Saarland."
  },
  {
    "title": "THE MULTIVOC TEXT-TO-SPEECH SYSTEM",
    "abstract": "In this paper we introduce MULTIVOC, a real-world text-to-speech product geared to the French language. Starting from a ordinary French text, MUL- TIVOC generates in real-time a high quality speech using a synthesis-by-diphone method. The process- ing is divided into 3 main transformations (phoneti- zation, automatic prosody and rhythm marking, and generation of LPC frames). This paper provides a full description of MULTIVOC including not only the technical view but also some applications of the product within the real world.",
    "content": "1. PRESENTATION OF MULTIVOC The text-to-speech MULTIVOC system is the result of a technology transfer from a research insti- tute (CNET Lannion, France), which developed the basis of the system, to an industrial company (Cap Sogeti Innovation, France) which made the system a commercial product. Generating Linear Prediction Coding frames from ordinary text written in French, the goal of MULTIVOC is to give any standard applications the ability to produce (in real time) low-cost and high-quality speech output. MULTIVOC is shipped as a complete software system which aims to provide a sophisti- cated driver enabling applications to directly send French spoken text. The software package consists of the kernel of the driver itself and a set of dic- tionaries used by it. Several tools in the package allow an advanced user to tailor his own MUL- TIVOC driver to specific usage. Beside this static configuration facility, MULTIVOC also provides several run-time features. By submitting specific requests an application can change the following parameters: • The sampling frequency for generated frames. Three different frequencies are available: 8 kHz, 10 kHk and 16 kHz. This parameter will characterize the quality of the output voice, a frequency of 16 kHz providing the best results. • The tone of the output voice can be adjusted in the range 50-350 Hz. • The speech speed may be set from 1 to 10 syll- ables per second. • Two styles of prosody are provided. The \"reading-style\" corresponds to the usual way of reading a text, while the \"advertising-style\" is dedi- cated to short commercial messages like jingles. • One can also choose between a female or a male voice. The method used for the synthesis produces Linear Prediction Coding (LPC) frames generated from a diphone dictionary. Such a dictionary is specific to the sampling frequency used (8, 10 or 16kHz) and also to the style of voice (Female or Male). For this purpose, MULTIVOC provides 6 differents diphone dictionaries. The overall processing is organized as a pipe- lined set of transformations applied to the input text. At the higher level, one can distinguish the follow- ing functions: The pre-processing (or lexical processing) is a text-to-text transformation aiming to expande some non-worded terms like numbers (1987 --> \"Mille Neuf Cent Quatre-Vingt-Sept\"), administra- tive numbers (A4/B5 --> \"A Quatre B Cinq\") or acronyms (CSINN. --> \"Cap Sogeti Innovation\"). The phonetization process transforms the pre-processed text into phonemes according to pre- defined rules stored in a user-modifiable base. The prosody marking process scans the phonetized text and generates appropriate marks to reflect the prosody of the text using built-in rules based on the different punctuation signs and the grammatical type of words. The rhythm marking process computes the duration associated to each phoneme. Last, the frame generation process produces the LPC frames which correspond to the input text according to the different parameters specified and can be sent directly to the output device. In this overall processing, we have deli- berately avoided a time-consuming syntax analysis, to enable MULTIVOC to run in real time. This choice has made MULTIVOC a commercially viable product providing a high-quality speech at low cost and which has been sold to serve as a basic component for several industrial applications. MULTIVOC is available on IBM-PC based systems. 2. THE MULTIVOC PROCESS As explained in the previous section, the input text provided by an application is processed in a \"pipe-line\" through five processes (see figure 1). French Text Pre-processing Phonetization Prosody marking Rhythm Computation Generation of LPC frames \"Reading-Style\" Prosody Generation of LPC frames \"Advertising-Style\" Prosody LPC Frames Figure 1: The MULTIVOC processing Each process takes as input the result of the preceding one and fills specific attributes of the objects composing the internal representation of the text. The final result, a list of LPC frames, is then sent to the LPC interpreter of a speech synthesis device (not described here). PRE-PROCESSING The main purpose of this first step is to decompose the input sentences into a list of words and to set the lexical attributes of each word. In order to allow ordinary-written text to be correctly processed, some patterns are translated into a sequence of words: • numbers are expansed according to the French language rules. The words generated are tagged to permit a correct prosody marking for numbers. • digital dates, time templates (not exhaustive) are matched against corresponding patterns in a set of rules which define the transformation to be applied. Patterns corresponding to the matching part and the transformation format are expressed using a UNIX- scanf/printf-like syntax. • abbreviations and acronyms are translated accord- ing to a user-defined lexicon. The translation part associated each entry of the lexicon can be: - empty to specify that the recognized word is to be spelled ex: 'MIT.' -->. (which will produce 'M ΙΤ' [EM EE TAY in French]) - a full text string which will replace the match- ing word ex: 'MIT.' --> 'Massachusetts Institute of Tech- nology' (in French!...) - a phonetic string if the pronunciation is very different from the lexical form. This function is particularly useful for company or product names ... ex: 'MIT.' --> 'ΑΙ\"ΜΑΥΤΙ'. (better) • mathematic symbols are also translated The process then checks if each word can be pronounced, according to a dictionary of the French sequences of pronounceable letters, and if it cannot the word is spelled. Finally, an attribute is associated to each word describing the grammatical nature of the word (pro- noun, determin, preposition, ...). This dictionary is rather small (300 entries) and does not contain most verbs but does contain the usual auxiliaries. A complete analysis of the sentences would provide a better prosody but, due to the size of the corresponding dictionary, could not be processed in real-time. The resulting prosody is nevertheless judged very natural, albeit in some few cases some- what strange. <LC> and <RC> are the respective Left and Right contexts of the Matching Sequence <PS> is the sequence of Phonetic Symbols to be generated and has the meaning: \"Replace <MS> by <PS> if <MS> is preceded by <LC> and followed by <RC>. Each context specification (<LC> and <RC>) can be empty, in which case the rule is applicable with no conditions, or can be expressed as a logical combination of elementary context: context == elementary.context AND context | elementary.context OR context | elementary.context An elementary context is either a sequence of characters or a class of sequence of characters (e.g. consonants or vowels). During interpretation, if several rules are applicable, the one containing the longest Matching Sequence is chosen: thus, the interpreter goes from the particular case to the general case. If more than one rule satisfies this criterion the first one is chosen and if no rule is applicable, a character is popped from the input and pushed to the output before the process start again. Example of rules: [_LORS_PUIS |_QUOI ] QUE_ [] --> <K><EU>_ . []_QUE [] --> <K><E> . Note: several characters play a special role: - the character'' (underscore) denotes a blank character - the character'' denotes the logical operator OR - the character '&' denotes the logical operator AND One of the set of rules is dedicated to the determi- nation of the correct liaisons between words. ■ PHONETIZATION This process transforms the sentences into a sequence of phonetic symbols. This transformation is carried out by five set of rules. The sets are applied successively to the input text. Each rule has the following form: [<LC>] <MS> [<RC>] --> <PS> . where <MS> is the Matching Sequence of characters in the input text ■ PROSODY MARKING The synthetic speech produced by mere con- catenation of diphones is comprehensible but not very natural. To provide it with an acceptable qual- ity, it is necessary to operate a prosody processing. Prosody facts are of two kinds (Emerard, 1977), (Guidini, 1981), (Sorin, 1984): • macro-prosody, related to the syntactic and seman- tic structure of the sentence, • micro-prosody, treating the interaction between two consecutive phonemes. A study of a set of phrases and the diversity of the voice \"styles\" (reading, advertising, ...) has provided an automatic prosody generation system (Aggoun, 1987). In the first step, this process decomposes the sentences in a set of so-called prosody-groups, and associates to each of them a group category. In the second step, each word within a group is marked and a pause is associated with it. Prosody-Group Categorization A prosody-group is by consecutive words. A set of rules determines the boundaries of a group and its associated category. The main criteria involved in this decomposition are: • the punctuation marks (including the end of a sen- tence), each of them defining a different category • the grammatical natures of two consecutive words. For example, a group ends after a lexical word (noun, non-auxiliary verbal form) followed by a grammatical word (determinant, pre-position, ...). In that case, the category of the group depends on the second word. The resulting sequence of groups is then pro- cessed in order to adjust their categories. Here again, the process is governed by rules based on the following information: • the length of the group (the number of words it contains), • the number of syllables of each word within the group, • the number and the length of non-lexical words, • the category of the adjacent groups As an example of rule: IF there exist a sequence (S) containing 3 groups of category '5' without a pause already established for one of them, AND if one of them (G) begins with one of the following determinant ('AU' or 'AUX') THEN give a category '4' to G and give it a short pause except if its pause is already long. For instance, 50 rules of this kind allow a complete categorization of the groups. [Note: some of them are simpler !] Word Marking According to the category of the group it belongs to, its length, its grammatical nature, each word of a group is then marked and, possibly, a pause is placed at the end of the word. For example: IF the group contains exactly 2 non-lexical consecutive words, AND the first one has one syllable AND the second more than one, THEN give the first word the mark '6+' and give the second the mark '4-' It should be noted that the set of rules used depends on the style of prosody required by the application ('reading' or 'advertising'). Although some attempts have been made to express the prosody-marking rules in a declarative way (Sorin, 1984), (Aggoun, 1987), based on the logic paradigm, the efficiency criteria and the real- time objective we have defined for this product led us to represent them in a procedural way rather than in a production-Srule form. At the end of this process, some words remain unmarked. In the next processes, we consider a sequence of unmarked word terminated by a marked one (a prosody-word) as the basic entity to deal with. ■ RHYTHM COMPUTATION The third process involved in MULTIVOC consists in the computation of the duration to asso- ciate to each phoneme. This duration is computed according to the different attributes attached to each word and to each phoneme, which are: • the kind of phoneme (plosive [bang], fricative [french], liquid [long]), • the mark associated the word • the number of syllabin of the word • the position of the phoneme within the word and a set of rules using this information. As an example of such rules: IF the last phoneme of the word is a vowel AND the mark of the word is '5' OR if a pause is associated with the word, THEN give a duration of '1.4' to this phoneme [Note: the default duration of every phoneme is '1.0'] --- PROSODY GENERATION To every word-mark corresponds a macro- melody schema. This schema enables us to deter- mine the variation of the pitch along the word. Three basic functions are used to express the pitch variation: • constant: the pitch remains unchanged • linear interpolation • exponential variation, namely F(t) = F(to) * e -p(t -t0) where F(t) denotes the value of the pitch at the time 't', to is the initial time and p is a con- stant (p = 0.68) Every macro-melody schema begins at Fdeb' the fundamental frequency of the speaker. Fdeb is set to 240 Hz for a Female voice and 120 Hz for a Male voice. This fundamental is adjusted if the word has a micro-mark '+' or '-'. Then a set of rules determines when these functions should be applied to a word. As an example: For words with mark '1' and containing more than four syllables: - apply constant from the beginning until the middle of the second vowel, - apply exponential with p/2 until the beginning of the first 'voise' phoneme of the last syllable (point A), - apply constant Fdeb/2 from the end of the last vowel (point B) to the end of the word, - interpolate from A to B Then a set of micro-prosody rules is applied on the vowels ('fine tuning'). Example: IF a vowel is not in the last syllable of a word AND followed by an unvoiced consonant THEN the pitch of the last LPC frames of the vowel is adjusted in the following manner: let C = [F(LF-3) - 7/12 * Fdeb] * 100 in F(LF - 2) = F(LF - 3) - 10 * C F(LF - 1) = F(LF - 3) - 15 * C F(LF) = F(LF - 3) - 20 * C At these step in the process, all needed infor- mation has been computed (pitch, duration) and MULTIVOC generates an LPC structure after hav- ing accessed a dictionary of diphones to get the coefficient of the lattice filter for each phoneme. 3. IMPLEMENTATION OF MULTIVOC The MULTIVOC software was developed in Con MS-DOS 3.2 and is compatible with UNIX BSD 4.2. This product is sold either as a running package (binary form) for IBM-PC compatible com- puters or as an adaptable package (source form) for specific usage. On the IBM-PC, the speech synthesis device used comes from the OROS Company (France) and is featured as an IBM-PC pluggable board (OROS- AU20) based on a Texas Instruments TMS320/20 processor. The MULTIVOC driver is implemented as a memory-resident program which application can address using an interrupt mechanism. Doing this, any application can very easily send text to be pronounced in real time. A Microsoft Windows application has been developed to demonstrate the facilities offered by MULTIVOC. Users can enter text using a built-in editor and can send all or mouse-selected text to MULTIVOC. A form (Dialogue-Box) allows the different parameters of MULTIVOC to be set to user specified values. MULTIVOC has also been successfully ported to UNIX BSD 4.2 on a SUN-3 but the driver specific aspects have not yet been developed because of the lack of speech synthesis devices for such machines. 4. APPLICATIONS OF MULTIVOC We give below three examples of concrete and real-world applications of MULTIVOC in an industrial context: • The first one was to use MULTIVOC to pro- nounce TELEX-style messages. This has been real- ized by defining an appropriate lexicon for the numerous abbreviations and acronyms used in such messages. The sources of MULTIVOC have not been modified. • The second application, or class of application, is to adapt MULTIVOC to low cost and small home- computers to develop a new generation of product for this market (Computer aided education software, for example). This is conducted by two customers who bought the sources of MULTIVOC and are now producing a restricted version of the product. --- • The third application is to use MULTIVOC as a basic component in a sophisticated application. We are now running a project for the French Telecom- munications (DGT) to develop phone-based mail services. Using a standard French phone, any user will be able to call the mailing service and dial commands to hear the different messages he has received. Several user-friendly features will enable to hear again part or all of a message or to change MULTIVOC-like parameters (deeper voice, slower, ...). For the purposes of this project MULTIVOC will not be changed. 5. FURTHER WORK The work planned around MULTIVOC is of two kinds: the more research issues and the more commercial/industrial ones. Research issues will include the handling of other languages (English), knowing that some important parts of MULTIVOC have been dedicated to French for reasons of efficiency and therefore will have to be re-written. More valuable results are foreseen by applying our company's experience in natural language processing (Lancel, 1986), (Deci- tre, 1987) to the input phase of MULTIVOC. As a commercial issue, we will continue to sell the MULTIVOC software system and to colla- borate with our customers. In the industrial field we think that a component like MULTIVOC will be a much-appreciated complement to many common applications. To prepare that, we envisage to install MULTIVOC on other machines and other operating systems and this should not cause any trouble. We will also adapt MULTIVOC to different speech synthesis devices based on the linear predic- tion technique. Finally, we will investigate the use of other synthesis technics (synthesis by formants for instance). 6. CONCLUSION Although based on a quite simple mechanism using only a local lexical analysis, avoiding expen- sive syntactic or semantic analysis, the results obtained with MULTIVOC are impressive. In partic- ular, the output speech has very natural prosody. Finally, the performance achieved by MULTIVOC makes it a real-time Text-To-Speech system that will be widely applied in industry. 7. REFERENCES Aggoun A., \"Le système Synthex: Traitement de la prosodie en synthèse de la parole\", Technique et Science Informatiques, vol. 6, no. 3, pp. 217-229, 1987 Decitre P., Grossi T., Jullien C., Solvay J.P., \"Plan- ning for Problem Formulation in Advice-Giving Dialogue\", 3rd Conference of the European Chapter of the Association for Computational Linguistics, Copenhagen (Denmark), 1987. Emerard F., \"Synthèse par Diphones et Traitement de la Prosodie\", Thèse de troisième cycle, Univer- sité de Grenoble, 1977. Guidini A., Choppy C., Dupeyrat B., \"Application de Règles au Calcul Automatique de la Prosodie. Comparaison avec la Prosodie Naturelle\", Sympo- sium Prosodic, Toronto 1981. Lancel J.M., Rousselot F., Simonin N., \"A Grammar Used for Parsing and Generation\", Proceedings of the XIth International Conference on Computational Linguistics, pp. 536-539, Bonn (FR Germany), 1986. Sorin C., Stella M., Aggoun A., Barthkova K., \"Règles Prosodiques et Synthèse de la Parole 'MULTI-STYLE', Symposium Franco-Soviétique sur le Dialogue Homme-Machine, Pouchino, 1984."
  },
  {
    "title": "Software Infrastructure for Natural Language Processing",
    "abstract": "We classify and review current approaches to software infrastructure for research, development and delivery of NLP systems. The task is motivated by a discussion of current trends in the field of NLP and Language Engineering. We describe a system called GATE (a General Architecture for Text Engineering) that provides a software infrastructure on top of which heterogeneous NLP processing modules may be evaluated and refined individually, or may be combined into larger application systems. GATE aims to support both researchers and developers working on component technologies (e.g. parsing, tagging, morphological analysis) and those working on developing end-user applications (e.g. information extraction, text summarisation, document generation, machine translation, and second language learning). GATE promotes reuse of component technology, permits specialisation and collaboration in large-scale projects, and allows for the comparison and evaluation of alternative technologies. The first release of GATE is now available.",
    "content": "1 Introduction This paper reviews the currently available design strategies for software infrastructure for NLP and presents an implementation of a system called GATE — a General Architecture for Text Engineer- ing. By software infrastructure we mean what has been variously referred to in the literature as: soft- ware architecture; software support tools; language engineering platforms; development environments. Our gloss on these terms is: common models for the representation, storage and exchange of data in and between processing modules in NLP systems, along with graphical interface tools for the management of data and processing and the visualisation of data. NLP systems produce information about texts¹, and existing systems that aim to provide software in- frastructure for NLP can be classified as belonging to one of three types according to the way in which they treat this information: additive, or markup-based: information produced is added to the text in the form of markup, e.g. in SGML (Thompson and McKelvie, 1996); referential, or annotation-based: information is stored separately with references back to the original text, e.g. in the TIPSTER architecture (Grishman, 1996); abstraction-based: the original text is preserved in processing only as parts of an integrated data structure that represents information about the text in a uniform theoretically-motivated model, e.g. attribute-value structures in the ALEP system (Simkins, 1994). A fourth category might be added to cater for those systems that provide communication and control infrastructure without addressing the text-specific needs of NLP (e.g. Verbmobil's ICE architecture (Amtrup, 1995)). We begin by reviewing examples of the three ap- proaches we sketched above (and a system that falls into the fourth category). Next we discuss current trends in the field and motivate a set of requirements that have formed the design brief for GATE, which is then described. The initial distribution of the system includes a MUC-6 (Message Understanding Conference 6 (Grishman and Sundheim, 1996)) style information extraction (IE) system and an overview ¹These texts may sometimes be the results of auto- matic speech recognition — see section 2.6. see of these modules is given. GATE is now available for research purposes http://www.dcs.shef.ac.uk/research/groups/ nlp/gate/ for details of how to obtain the system. It is written in C++ and Tcl/Tk and currently runs on UNIX (SunOS, Solaris, Irix, Linux and AIX are known to work); a Windows NT version is in prepa- ration. 2 Managing Information about Text 2.1 Abstraction Approaches The abstraction-based approach to managing in- formation about texts is primarily motivated by theories of the nature of the information to be represented. One such position is that declara- tive, constraint-based representations using feature- structure matrices manipulated under unification are an appropriate vehicle by which \"many techni- cal problems in language description and computer manipulation of language can be solved\" (Shieber, 1992). Information in these models may be charac- terised as abstract in our present context as there is no requirement to tie data elements back to the original text these models represent abstractions from the text. One recent example of an infrastructure project based on abstraction is ALEP the Advanced Lan- guage Engineering Platform (Simkins, 1994). ALEP aims to provide \"the NLP research and engineering community in Europe with an open, versatile, and general-purpose development environment\". ALEP, while in principle open, is primarily an advanced sys- tem for developing and manipulating feature struc- ture knowledge-bases under unification. Also pro- vided are several parsing algorithms, algorithms for transfer, synthesis and generation (Schütz, 1994). As such, it is a system for developing particular types of data resource (e.g. grammars, lexicons) and for doing a particular set of tasks in LE in a particu- lar way. ALEP does not aim for complete gener- icity (or it would need also to supply algorithms for Baum-Welch estimation, fast regular expression matching, etc.). Supplying a generic system to do every LE task is clearly impossible, and prone to instant obsolescence in a rapidly changing field. In our view ALEP, despite claiming to use a theory-neutral formalism (an HPSG-like formalism), is still too committed to a particular approach to lin- guistic analysis and representation. It is clearly of high utility to those in the LE community to whom these theories and formalisms are relevant; but it excludes, or at least does not actively support, all those who are not, including an increasing number of researchers committed to statistical, corpus-based approaches. GATE, as will be seen below, is more like a shell, a backplane into which the whole spec- trum of LE modules and databases can be plugged. Components used within GATE will typically exist already our emphasis is reuse, not reimplementa- tion. Our project is to provide a flexible and efficient way to combine LE components to make LE systems (whether experimental or for delivered applications) not to provide 'the one true system', or even 'the one true development environment'. Indeed, ALEP- based systems might well provide components oper- ating within GATE. Seen this way, the ALEP enter- prise is orthogonal to ours there is no significant overlap or conflict. In our view the level at which we can assume com- monality of information, or of representation of in- formation, between LE modules is very low, if we are to build an environment which is broad enough to support the full range of LE tools and accept that we cannot impose standards on a research com- munity in flux. What does seem to be a highest common denominator is this: modules that process text, or process the output of other modules that process text, produce further information about the text or portions of it. For example, part-of-speech tags, phrase structure trees, logical forms, discourse models can all be seen in this light. It would seem, therefore, that we are on safe common ground if we start only by committing to provide a mechanism which manages arbitrary information about text. There are two methods by which this may be done. First, one may embed the information in the text at the relevant points - the additive approach. Second, one may associate the information with the text by building a separate database which stores this in- formation and relates it to the text using pointers into the text the referential approach. The next two subsections discuss systems that have adopted these two approaches respectively, then we compare the two and indicate why we have chosen a hybrid approached based mainly on the second. Finally we look at a system that falls outside our three cate- gories. 2.2 Additive Approaches Additive architectures for managing information about text add markup to the original text at each successive phase of processing. This model has been adopted by a number of projects including parts of the MULTEXT EC project. The MULTEXT work2 has led to the development of an architecture based 2Note that other partners in the project adopted a different architectural solution http://www.lpl.univ-aix.fr/projects/multext/. see on SGML at the University of Edinburgh called LT- NSL (Thompson and McKelvie, 1996). The architecture is based on a commitment to TEI-style (the Text Encoding Initiative (Sperberg- McQueen and Burnard, 1994)) SGML encoding of information about text. The TEI defines standard tag sets for a range of purposes including many rel- evant to LE systems. Tools in a LT-NSL system communicate via interfaces specified as SGML doc- ument type definitions (DTDs essentially tag set descriptions), using character streams on pipes an arrangement modelled after UNIX-style shell pro- gramming. To obviate the need to deal with some difficult types of SGML (e.g. minimised markup) texts are converted to a normal form before process- ing. A tool selects what information it requires from its input SGML stream and adds information as new SGML markup. An advantage here is a degree of data-structure independence: so long as the necessary information is present in its input, a tool can ignore changes to other markup that in- habits the same stream unknown SGML is simply passed through unchanged (so, for example, a se- mantic interpretation module might examine phrase structure markup, but ignore POS tags). A disad- vantage is that although graph-structured data may be expressed in SGML, doing so is complex (either via concurrent markup, the specification of multi- ple legal markup trees in the DTD, or by rather ugly nesting tricks to cope with overlapping so- called \"milestone tags\"). Graph-structured informa- tion might be present in the output of a parser, for example, representing competing analyses of areas of text. 2.3 Referential Approaches The ARPA-sponsored TIPSTER programme in the US, now entering its third phase, has also produced a data-driven architecture for NLP systems (Grish- man, 1996). Whereas in LT-NSL all information about a text is encoded in SGML, which is added by the modules, in TIPSTER a text remains unchanged while information is stored in a separate database the referential approach. Information is stored in the database in the form of annotations. Anno- tations associate arbitrary information (attributes), with portions of documents (identified by sets of start/end byte offsets or spans). Attributes may be the result of linguistic analysis, e.g. POS tags or tex- tual unit type. In this way the information built up about a text by NLP modules is kept separate from the texts themselves. In place of an SGML DTD, an annotation type declaration defines the informa- tion present in annotation sets. Figure 1 shows an example from (Grishman, 1996). Text Sarah savored the soup. 0...15...10...15...120 Annotations Id Type Span Attributes Start End 1 token 0 5 pos=NP 2 token 6 13 pos=VBD 3 token 14 17 pos=DT 4 token 18 22 pos=NN 5 token 22 23 6 name 0 5 name_type=person 7 sentence 0 23 Figure 1: TIPSTER annotations example The definition of annotations in TIPSTER forms part of an object-oriented model that deals with inter-textual information as well as single texts. Documents are grouped into collections, each with a database storing annotations and document at- tributes such as identifiers, headlines etc. The model also describes elements of information extraction (IE) and information retrieval (IR) systems relating to their use, with classes representing queries and information needs. The TIPSTER architecture is designed to be portable to a range of operating environments, so it does not define implementation technologies. Par- ticular implementations make their own decisions re- garding issues such as parallelism, user interface, or delivery platform. Various implementations of TIP- STER systems are available, including one in GATE. 2.4 Comparison of LT-NSL and TIPSTER Both architectures are appropriate for NLP, but there are a number of significant differences. We discuss five here, then note the possibility of compli- mentary inter-operation of the two. 1. TIPSTER can support documents on read-only media (e.g. Internet material, or CD-ROMS, which may be used for bulk storage by organisa- tions with large archiving needs) without copy- ing each document. 2. From the point of view of efficiency, the original LT-NSL model of interposing SGML between all modules implies a generation and parsing overhead in each module. Later versions have replaced this model with a pre-parsed represen- tation of SGML to reduce this overhead. This representation will presumably be stored in in- termediate files, which implies an overhead from the I/O involved in continually reading and writing all the data associated with a document to file. There would seem no reason why these files should not be replaced by a database im- plementation, however, with potential perfor- mance benefits from the ability to do I/O on subsets of information about documents (and from the high level of optimisation present in modern database technology). 3. A related issue is storage overhead. TIPSTER is minimal in this respect, as there is no inher- ent need to duplicate the source text and all its markup during the nromalisation process. 4. At first thought texts may appear to be one- dimensional, consisting of a sequence of charac- ters. This view breaks down when structures like tables appear — these are inherently two- dimensional and their representation and ma- nipulation is much easier in a referential model like TIPSTER than in an additive model like SGML because a markup-based representation is based on the one-dimensional view. In TIP- STER, the column of a table can be repre- sented as a single object with multiple refer- ences to parts of the text (an annotation with multiple spans). Marking columns in SGML re- quires a tag for each row of the column. Related points are that: TIPSTER avoids the difficul- ties referred to earlier of representing graph- structured information in SGML; LT NSL is inefficient where processing algorithms require non-sequential access to data (McKelvie, Brew, and Thompson, 1997). 5. TIPSTER can easily support multi-level access control via a database's protection mechanisms — this is again not straightforward in SGML. 6. Distributed control is easy to implement in a database-centred system like TIPSTER — the DB can act as a blackboard, and implemen- tations can take advantage of well-understood access control (locking) technology. How to do distributed control in LT-NSL is not obvi- ous. We plan to provide this type of control in GATE via collaboration with the Corelli project at CRL, New Mexico — see (Zajac, 1997) for more details. 2.5 Combining Addition and Reference We believe the above comparison demonstrates that there are significant advantages to the TIPSTER model and it is this model that we have chosen for GATE. We also believe that SGML and the TEI must re- main central to any serious text processing strategy. The points above do not contradict this view, but indicate that SGML should not form the central rep- resentation format of every text processing system. Input from SGML text and TEI conformant output are becoming increasingly necessary for LE appli- cations as more and more publishers adopts these standards. This does not mean, however, that flat- file SGML is an appropriate format for an architec- ture for LE systems. This observation is born out by the facts that TIPSTER started with an SGML architecture but rejected it in favour of the current database model, and that LT-NSL has gone partly towards this style by passing pre-parsed SGML be- tween components. Interestingly, a TIPSTER referential system could function as a module in an LT-NSL additive system, or vice-versa. A TIPSTER storage system could write data in SGML for processing by LT-NSL tools, and convert the SGML results back into native for- mat. Work is underway to integrate the LT-NSL API with GATE and provide SGML I/O for TIP- STER (and we acknowledge valuable assistance from colleagues at Edinburgh in this task). 2.6 ICE ICE, the Intarc Communication Environment (Amtrup, 1995), is an 'environment for the develop- ment of distributed AI systems'. As part of the Verb- mobil real-time speech-to-speech translation project ICE has addressed two key problems for this type of system, viz. distributed processing and incremen- tal interpretation (Gorz et al., 1996): distribution to contribute to processing speed in what is a very compute-intensive application area; incremental in- terpretation both for speed reasons and to facili- tate feedback of results from downstream modules to upstream ones (e.g. to inform the selection of word interpretations from phone lattices using part- of-speech information). ICE provides a distribution and communication layer based on PVM (Parallel Virtual Machine). The infrastructure that ICE delivers doesn't fit into our tripartite classification because the communica- tion channels do not use data structures specific to NLP needs, and because data storage and text col- lection management is left to the individual modules. ICE might well form a useful backbone for an NLP infrastructure, and could operate in any of the three paradigms. 3 NLP Trends and GATE For a variety of reasons NLP has recently spawned a related engineering discipline called language engi- neering (LE), whose orientation is towards the appli- cation of NLP techniques to solving large-scale, real- world language processing problems in a robust and predictable way. These problems include informa- tion extraction, text summarisation, document gen- eration, machine translation, second language learn- ing, amongst others. In many cases, the technologies being developed are assistive, rather than fully auto- matic, aiming to enhance or supplement a human's expertise rather than attempting to replace it. The reasons for the growth of language engineer- ing include: • computer hardware advances which have in- creased processor speeds and memory capacity, while reducing prices; • increasing availability of large-scale, language- related, on-line resources, such as dictionaries, thesauri, and 'designer' corpora lected for representativeness and perhaps anno- tated with descriptive information; corpora se- • the demand for applications in a world where electronic text has grown exponentially in vol- ume and availability, and where electronic com- munications and mobility have increased the importance of multi-lingual communication; • maturing NLP technology which is now able, for some tasks, to achieve high levels of accuracy repeatedly on real data. Aside from the host of fundamental theoretical problems that remain to be answered in NLP, lan- guage engineering faces a variety of problems of its own. Two features of the current situation are of prime importance; they constrain how the field can develop and must be acknowledged and addressed. First, there is no theory of language which is uni- versally accepted, and no computational model of even a part of the process of language understanding which stands uncontested. Second, building intelli- gent application systems, systems which model or reproduce enough human language processing capa- bility to be useful, is a large-scale engineering ef- fort which, given political and economic realities, must rely on the efforts of many small groups of re- searchers, spatially and temporally distributed, with no collaborative master plan. The first point means that any attempt to push researchers into a theoretical or representational straight-jacket is premature, unhealthy and doomed to failure. The second means that no research team alone is likely to have the resources to build from scratch an entire state-of-the-art LE application sys- tem. Note the tension here: the first point identi- fies a centrifugal tendency, pushing researchers into ever greater theoretical diversity; the second, a cen- tripetal tendency forcing them together. Given this state of affairs, what is the best prac- tical support that can be given to advance the field? Clearly, the pressure to build on the efforts of others demands that LE tools or component technologies parsers, taggers, morphological analysers, discourse planning modules, etc, be readily available for ex- perimentation and reuse. But the pressure towards theoretical diversity means that there is no point at- tempting to gain agreement, in the short term, on what set of component technologies should be de- veloped or on the informational content or syntax of representations that these components should re- quire or produce. Our response to these considerations has been to design and implement a software environment called GATE – a General Architecture for Text Engi- neering (Cunningham, Gaizauskas, and Wilks, 1995; Cunningham, Wilks, and Gaizauskas, 1996) – which attempts to meet the following objectives: 1. support information interchange between LE modules at the highest common level possi- ble without prescribing theoretical approach (though it allows modules which share theoret- ical presuppositions to pass data in a mutually accepted common form); 2. support the integration of modules written in any source language, available either in source or binary form, and be available on any common platform; 3. support the evaluation and refinement of LE component modules, and of systems built from them, via a uniform, easy-to-use graphical in- terface which in addition offers facilities for vi- sualising data and managing corpora. The remainder of this paper describes the design of GATE. In section 4 we detail the design of GATE. Section 5 illustrates how GATE can be used by de- scribing how we have taken a pre-existing informa- tion extraction system and embedded it in GATE. Section 6 makes some concluding remarks. 4 GATE Design Corresponding to the three key objectives identified at the end of section 3, GATE comprises three prin- cipal elements: GDM, the GATE Document Man- ager, based on the TIPSTER document manager; CREOLE, a Collection of REusable Objects for Lan- guage Engineering: a set of LE modules integrated with the system; and GGI, the GATE Graphical In- terface, a development tool for LE R&D, providing integrated access to the services of the other compo- nents and adding visualisation and debugging tools. Working with GATE, the researcher will from the outset reuse existing components, and the common APIs of GDM and CREOLE mean only one inte- gration mechanism must be learnt. As CREOLE expands, more and more modules will be available from external sources (including users of other TIP- STER systems). 4.1 GDM The GDM provides a central repository or server that stores all information an LE systein generates about the texts it processes. All communication be- tween the components of an LE system goes through GDM, which insulates these components from direct contact with each other and provides them with a uniform API for manipulating the data they pro- duce and consume. The basic concepts of the data model underlying the GDM have been explained in the discussion of the Tipster model in section 2.3 above. The TIP- STER architecture has been fully specified (Grish- man, 1996) and its specification should be consulted for further details, in particular for definitions of the API. The GDM is fully conformant with the core document management subset of this specification. 4.2 CREOLE All the real work of analysing texts in a GATE-based LE system is done by CREOLE modules or objects (we use the terms module and object rather loosely to mean interfaces to resources which may be pre- dominantly algorithmic or predominantly data, or a mixture of both). Typically, a CREOLE object will be a wrapper around a pre-existing LE module or database a tagger or parser, a lexicon or ngram index, for example. Alternatively, objects may be developed from scratch for the architecture - in ei- ther case the object provides a standardised API to the underlying resources which allows access via GGI and I/O via GDM. The CREOLE APIs may also be used for programming new objects. When the user initiates a particular CREOLE ob- ject via GGI (or when a programmer does the same via the GATE API when building an LE applica- tion) the object is run, obtaining the information it needs (document source, annotations from other ob- jects) via calls to the GDM API. Its results are then stored in the GDM database and become available for examination via GGI or to be the input to other CREOLE objects. GDM imposes constraints on the I/O format of CREOLE objects, namely that all information must be associated with byte offsets and conform to the annotations model of the TIPSTER architecture. The principal overhead in integrating a module with GATE is making the components use byte offsets, if they do not already do so. 4.3 GGI The GGI is a graphical tool that encapsulates the GDM and CREOLE resources in a fashion suitable for interactive building and testing of LE compo- nents and systems. The GGI has functions for creat- ing, viewing and editing the collections of documents which are managed by the GDM and that form the corpora which LE modules and systems in GATE use as input data. The GGI also has facilities to display the results of module or system execution new or changed annotations associated with the doc- ument. These annotations can be viewed either in raw form, using a generic annotation viewer, or in an annotation-specific way, if special annotation view- ers are available. For example, named entity annota- tions which identify and classify proper names (e.g. organization names, person names, location names) are shown by colour-coded highlighting of relevant words; phrase structure annotations are shown by graphical presentation of parse trees. Note that the viewers are general for particular types of annota- tion, so, for example, the same procedure is used for any POS tag set, Named-Entity markup etc. (see section 4.4 below). Thus CREOLE developers reuse GATE data visualisation code with negligible over- head. 4.4 Plug and Play The process of integrating existing modules into GATE (CREOLEising) has been automated to a large degree and can be driven from the interface. The developer is required to produce some C or Tcl code that uses the GDM TIPSTER API to get information from the database and write back re- sults. When the module pre-dates integration, this is called a wrapper as it encapsulates the module in a standard form that GATE expects. When mod- ules are developed specifically for GATE they can embed TIPSTER calls throughout their code and dispense with the wrapper intermediary. The under- lying module can be an external executable written in any language (the current CREOLE set includes Prolog, Lisp and Perl programs, for example). There are three ways to provide the CREOLE wrapper functions. Packages written in C, or in languages which obey C linkage conventions, can be compiled into GATE directly as a Tcl pack- age. This is tight coupling and is maximally efficient but necessitates recompilation of GATE when mod- ules change. On platforms which support shared libraries C-based wrappers can be loaded at run- time – dynamic coupling. This is also efficient (with a small penalty at load time) and allows devel- opers to change CREOLE objects and run them within GATE without recompiling the GATE sys- tem. Wrappers written in Tcl can also be loaded at run-time – loose coupling. There is a performance penalty in comparison with using the C APIs, but for simple cases this is the easiest integration route. In each case the implementation of CREOLE ser- vices is completely transparent to GATE. CREOLE wrappers encapsulate informa- tion about the preconditions for a module to run (data that must be present in the GDM database) and post-conditions (data that will result). This in- formation is needed by GGI, and is provided by the developer in a configuration file, which also details what sort of viewer to use for the module's results and any parameters that need passing to the module. These parameters can be changed from the interface at run-time, e.g. to tell a parser to use a different lexicon. Aside from the information needed for GGI to provide access to a module, GATE compatibility equals TIPSTER compatibility – i.e. there will be very little overhead in making any TIPSTER mod- ule run in GATE. Given an integrated module, all other interface functions happen automatically. For example, the module will appear in a graph of all modules avail- able, with permissible links to other modules auto- matically displayed, having been derived from the module pre- and post-conditions. At any point the developer can create a new graph from a subset of available CREOLE modules to per- form a task of specific interest. 5 VIE: An Application In GATE To illustrate the process of converting pre-existing LE systems into GATE-compatible CREOLE sets we use as an example the creation of VIE (Vanilla Information Extraction system) from LaSIE (Large- Scale Information Extraction system) (Gaizauskas et al., 1995), Sheffield’s entry in the MUC-6 sys- tem evaluations. LaSIE module interfaces were not standardised when originally produced and its CRE- OLEization gives a good indication of the ease of in- tegrating other LE tools into GATE. The resulting system, VIE, is distributed with GATE. 5.1 LaSIE LaSIE was designed as a research system for inves- tigating approaches to information extraction and to be entered into the MUC-6 conference (Grish- man and Sundheim, 1996). As such it was a stand- alone system that was aimed at specific tasks and, while based on a modular design, none of its mod- ules were specifically designed with reuse in mind, nor was there any attempt to standardise data for- mats passed between modules. Modules were writ- ten in a variety of programming languages, includ- ing C, C++, Flex, Perl and Prolog. In this regard LaSIE was probably typical of existing LE systems and modules. The high-level tasks which LaSIE performed include the four MUC-6 tasks (carried out on Wall Street Journal articles) – named entity recognition, coreference resolution and two template filling tasks. The system was a pipelined architec- ture which processes a text sentence-at-a-time and consists of three principal processing stages: lexical preprocessing, parsing plus semantic interpretation, and discourse interpretation. 5.2 The CREOLEisation of LaSIE As described in section 4.2, CREOLEisation of ex- isting LE modules involves providing them with a wrapper so that the modules communicate via the GDM, by accessing TIPSTER-compliant document annotations and updating them with new informa- tion. The major work in converting LaSIE to VIE involved defining useful module boundaries, unpick- ing the connections between them, and then writing wrappers to convert module output into annotations relating to text spans and to convert GDM input from annotations relating to text spans back into the module’s native input format. The complete VIE system comprises ten modules, each of which is a CREOLE object integrated into GATE. The CREOLEisation took approximately two person months. The resulting system has all the functionality of the original LaSIE system. However, the interface makes it much easier to use. And, of course, it is now possible to swap in modules, such as a different parser, with significantly less effort than would have been the case before. For more details of this process see (Cunningham et al., 1996). VIE and its components are being deployed for a number of purposes including IE in French, Ger- man and Spanish. Experience so far indicates that GATE is a productive environment for distributed collaborative reuse-based software development. 6 Concluding Remarks Of course, GATE does not solve all the problems involved in plugging diverse LE modules together. There are three barriers to such integration: • managing storage and exchange of information about texts; • incompatibility of representation of information about texts; • incompatibility of type of information used and produced by different modules. GATE provides a solution to the first two of these, based on the work of the TIPSTER architecture group. Because GATE places no constraints on the linguistic formalisms or information content used by CREOLE modules, the latter problem must be solved by dedicated translation functions e.g. tagset-to-tagset mapping — and, in some cases, by extra processing — e.g. adding a semantic processor to complement a bracketing parser. The recent completion of this work means a full assessment of the strengths and weaknesses of GATE is not yet possible. The implementation of VIE in GATE, however, provides an existence proof that the original conception is workable. We believe that the environment provided by GATE will now allow us to make significant strides in assessing alterna- tive LE technologies and in rapidly assembling LE prototype systems. Thus, to return to the themes of section 3, GATE will not commit us to a particular linguistic theory or formalism, but it will enable us, and anyone who wishes to make use of it, to build, in a pragmatic way, on the diverse efforts of others. 7 Acknowledgements This work was supported by the UK Engineering and Physical Sciences Research Council, grant number GR/K25267, and the EC DG XIII Language Engi- neering programme, grant number LE1-2238. References Amtrup, J.W. 1995. ICE — INTARC Communica- tion Environment User Guide and Reference Man- ual Version 1.4. Technical report, University of Hamburg. Cunningham, H., R.G. Gaizauskas, and Y. Wilks. 1995. A General Architecture for Text En- gineering (GATE) — a new approach to Lan- guage Engineering R&D. Technical Report CS — 95 — 21, Department of Computer Sci- ence, University of Sheffield. Also available as http://xxx.lanl.gov/ps/cmp-1g/9601009. Cunningham, H., K. Humphreys, R. Gaizauskas, and M. Stower, 1996. CREOLE Devel- oper's Manual. Department of Computer Sci- ence, University of Sheffield. Available at http://www.dcs.shef.ac.uk/research/groups/ nlp/gate. Cunningham, H., Y. Wilks, and R. Gaizauskas. 1996. GATE — a General Architecture for Text Engineering. In Proceedings of the 16th Con- ference on Computational Linguistics (COLING- 96), Copenhagen, August. Gaizauskas, R., T. Wakao, K Humphreys, H. Cun- ningham, and Y. Wilks. 1995. Description of the LaSIE system as used for MUC-6. In Proceedings of the Sixth Message Understanding Conference (MUC-6). Morgan Kaufmann. Gorz, G., M. Kessler, J. Spilker, and H. Weber. 1996. Research on Architectures for Integrated Speech/Language Systems in Verbmobil. In Pro- ceedings of COLING-96, Copenhagen. Grishman, R. 1996. TIPSTER Architecture Design Document Version 2.2. Technical report, DARPA. Available at http://www.tipster.org/. Grishman, R. and B. Sundheim. 1996. Message understanding conference — 6: A brief history. In Proceedings of the 16th International Conference on Computational Linguistics, Copenhagen, June. McKelvie, D., C. Brew, and H. Thompson. 1997. Using SGML as a Basis for Data-Intensive NLP. In Proceedings of the fifth Conference on Applied Natural Language Processing (ANLP-97). Schütz, J. 1994. Developing Lingware in ALEP. ALEP User Group News, CEC Luxemburg, 1(1), October. Shieber, S. 1992. Constraint-Based Grammar For- malisms. MIT Press. Simkins, N. K. 1994. An Open Architecture for Language Engineering. In First Language Engi- neering Convention, Paris. Sperberg-McQueen, C.M. and L. Burnard. 1994. Guidelines for Electronic Text Encoding and In- terchange (TEI P3). ACH, ACL, ALLC. Thompson, H.S. and D. McKelvie. 1996. A Software Architecture for Simple, Efficient SGML Applica- tions. In Proceedings of SGML Europe ’96, Mu- nich. Zajac, R. 1997. An Open Distributed Architecture for Reuse and Integration of Heterogenous NLP Components. In Proceedings of the 5th conference on Applied Natural Language Processing (ANLP- 97)."
  },
  {
    "title": "Using Semantic Preferences to Identify Verbal Participation in Role Switching Alternations",
    "abstract": "We propose a method for identifying diathesis alternations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms. The method uses selectional preferences acquired as probability distributions over WordNet. Preferences for the target slots are compared using a measure of distributional similarity. The method is evaluated on the causative and conative alternations, but is generally applicable and does not require a priori knowledge specific to the alternation.",
    "content": "1 Introduction Diathesis alternations are alternate ways in which the arguments of a verb are expressed syntactically. The syntactic changes are sometimes accompanied by slight changes in the meaning of the verb. An ex- ample of the causative alternation is given in (1) be- low. In this alternation, the object of the transitive variant can also appear as the subject of the intransi- tive variant. In the conative alternation, the transi- tive form alternates with a prepositional phrase con- struction involving either at or on. An example of the conative alternation is given in (2). 1. The boy broke the window ↔ The window broke. 2. The boy pulled at the rope ↔ The boy pulled the rope. We refer to alternations where a particular seman- tic role appears in different grammatical roles in al- ternate realisations as \"role switching alternations\" (RSAS). It is these alternations that our method ap- plies to. Recently, there has been interest in corpus-based methods to identify alternations (McCarthy and Ko- rhonen, 1998; Lapata, 1999), and associated verb classifications (Stevenson and Merlo, 1999). These have either relied on a priori knowledge specified for the alternations in advance, or are not suitable for a wide range of alternations. The fully automatic method outlined here is applied to the causative and conative alternations, but is applicable to other RSAS. 2 Motivation Diathesis alternations have been proposed for a number of NLP tasks. Several researchers have sug- gested using them for improving lexical acquisition. Korhonen (1997) uses them in subcategorization frame (SCF) acquisition to improve the performance of a statistical filter which determines whether a SCF observed for a particular verb is genuine or not. They have also been suggested for the recovery of predicate argument structure, necessary for SCF ac- quisition (Briscoe and Carroll, 1997; Boguraev and Briscoe, 1987). And Ribas (1995) showed that selec- tional preferences acquired using alternations per- formed better on a word sense disambiguation task compared to preferences acquired without alterna- tions. He used alternations to indicate where the argument head data from different slots can be com- bined since it occupies the same semantic relation- ship with the predicate. Different diathesis alternations give different em- phasis and nuances of meaning to the same basic content. These subtle changes of meaning are impor- tant in natural language generation (Stede, 1998). Alternations provide a means of reducing redun- dancy in the lexicon since the alternating SCFS need not be enumerated for each individual verb if a marker is used to specify which verbs the alterna- tion applies to. Alternations also provide a means of generalizing patterns of behaviour over groups of verbs, typically the group members are semantically related. Levin (1993) provides a classification of over 3000 verbs according to their participation in alter- nations involving NP and PP constituents. Levin's classification is not intended to be exhaustive. Au- tomatic identification of alternations would be a use- ful tool for extending the classification with new participants. Levin's taxonomy might also be used alongside observed behaviour, to predict unseen be- haviour. Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., <Root> 0.2 0.2 0.04 TCM abstraction human_activity entity event 0.005 0.08 4 A construction war car measure time relation migration meal drum time ceremonial migration A ---0.1- time time_period communication A week month afternoon Figure 1: TCM for the object slot of the transitive frame of start. 1998). Dang et al. (1998) modify it by adding new classes which remove the overlap between classes from the original scheme. Dorr and Jones (1996) extend the classification by using grammatical in- formation in LDOCE alongside semantic information in WordNet. What is missing is a way of classifying verbs when the relevant information is not available in a manmade resource. Using corpora by-passes reliance on the availability and adequacy of MRDS. Additionally, the frequency information in corpora is helpful for estimating alternation productivity (La- pata, 1999). Estimations of productivity have been suggested for controlling the application of alterna- tions (Briscoe and Copestake, 1996). We propose a method to acquire knowledge of alternation partic- ipation directly from corpora, with frequency infor- mation available as a by-product. 3 Method We use both syntactic and semantic information for identifying participants in RSAS. Firstly, syntactic processing is used to find candidates taking the alter- nating SCFs. Secondly, selectional preference models are acquired for the argument heads associated with a specific slot in a specific SCF of a verb. We use the SCF acquisition system of Briscoe and Carroll (1997), with a probabilistic LR parser (Inui et al., 1997) for syntactic processing. The corpus data is Pos tagged and lemmatised before the LR parser is applied. Subcategorization patterns are extracted from the parses, these include both the syntactic cat- egories and the argument heads of the constituents. These subcategorization patterns are then classified according to a set of 161 SCF classes. The SCF en- tries for each verb are then subjected to a statistical filter which removes SCFS that have occurred with a frequency less than would be expected by chance. The resulting SCF lexicon lists each verb with the SCFS it takes. Each SCF entry includes a frequency count and lists the argument heads at all slots. Selectional preferences are automatically acquired for the slots involved in the role switching. We refer to these as the target slots. For the causative al- ternation, the slots are the direct object slot of the transitive SCF and the subject slot of the intransi- tive. For the conative, the slots are the direct object of the transitive and the PP of the np v pp SCF. Selectional preferences are acquired using the method devised by Li and Abe (1995). The pref- erences for a slot are represented as a tree cut model (TCM). This is a set of disjoint classes that partition the leaves of the WordNet noun hypernym hierar- chy. A conditional probability is attached to each of the classes in the set. To ensure the TCM covers all the word senses in WordNet, we modify Li and Abe's original scheme by creating hyponym leaf classes be- low all WordNet's hypernym (internal) classes. Each leaf holds the word senses previously held at the in- ternal class. The nominal argument heads from a target slot are collected and used to populate the WordNet hierarchy with frequency information. The head lemmas are matched to the classes which con- tain them as synonyms. Where a lemma appears as a synonym in more than one class, its frequency count is divided between all classes for which it has direct membership. The frequency counts from hyponym classes are added to the count for each hypernym class. A root node, created above all the WordNet roots, contains the total frequency count for all the argument head lemmas found within WordNet. The minimum description length principle (MDL) (Rissa- nen, 1978) is used to find the best TCM by consid- ering the cost (in bits) of describing both the model and the argument head data encoded in the model. The cost (or description length) for a TCM is cal- culated according to equation 1. The number of parameters of the model is given by k, this is the number of classes in the TCM minus one. S is the sample size of the argument head data. The cost of describing each argument head (n) is calculated us- ing the log of the probability estimate for the classes on the TCM that n belongs to (cn). k description length = xlog |S\\-∑logp(cn) (1) 2 NES A small portion of the TCM for the object slot of start in the transitive frame is displayed in figure 1. WordNet classes are displayed in boxes with a label which best reflects the sense of the class. The prob- ability estimates are shown for the classes along the TCM. Examples of the argument head data are dis- played below the WordNet classes with dotted lines indicating membership at a hyponym class beneath these classes. We assume that verbs which participate will show a higher degree of similarity between the preferences at the target slots compared with non-participating verbs. To compare the preferences we compare the probability distributions across WordNet using a measure of distributional similarity. Since the prob- ability distributions may be at different levels of WordNet, we map the TCMS at the target slots to a common tree cut, a \"base cut\". We experiment with two different types of base cut. The first is simply a base cut at the eleven root classes of WordNet. We refer to this as the \"root base cut\" (RBC). The sec- ond is termed the \"union base cut\" (UBC). This is obtained by taking all classes from the union of the two TCMS which are not subsumed by another class in this union. Duplicates are removed. Probabilities are assigned to the classes of a base cut using the estimates on the original TCM. The probability esti- mate for a hypernym class is obtained by combining the probability estimates for all its hyponyms on the original cut. Figure 2 exemplifies this process for two TCMS (TCM1 and TCM2) in an imaginary hierarchy. The UBC is at the classes B, C and D. To quantify the similarity between the probability distributions for the target slots we use the a-skew divergence (asd) proposed by Lee (1999). 1 This measure, defined in equation 2, is a smoothed version of the Kulback-Liebler divergence. p1(x) and p2(x) are the two probability distributions which are being compared. The a constant is a value between 0 and 1 We also experimented with euclidian distance, the Li norm, and cosine measures. The differences in performance of these measures were not statistically significant. 1 which smooths pl(2) with p2(x) so that aSD is always defined. We use the same value (0.99) for a as Lee. If a is set to 1 then this measure is equivalent to the Kulback-Liebler divergence. asd(p1(x), p2(x)) = D(p2(x)||((a × p1(x)) + ((1 - a) x p2(x)))) (2) 4 Experimental Evaluation We experiment with a SCF lexicon produced from 19.3 million words of parsed text from the BNC (Leech, 1992). We used the causative and conative alternations, since these have enough candidates in our lexicon for experimentation. Evaluation is per- formed on verbs already filtered by the syntactic processing. The SCF acquisition system has been evaluated elsewhere (Briscoe and Carroll, 1997). We selected candidate verbs which occurred with 10 or more nominal argument heads at the target slots. The argument heads were restricted to those which can be classified in the WordNet hypernym hi- erarchy. Candidates were selected by hand so as to obtain an even split between candidates which did participate in the alternation (positive candidates) and those which did not (negative candidates). Four human judges were used to determine the \"gold stan- dard\". The judges were asked to specify a yes or no decision on participation for each verb. They were also permitted a don't know verdict. The kappa statistic (Siegel and Castellan, 1988) was calculated to ensure that there was significant agreement be- tween judges for the initial set of candidates. From these, verbs were selected which had 75% or more agreement, i.e. three or more judges giving the same yes or no decision for the verb. For the causative alternation we were left with 46 positives and 53 negatives. For the conative alter- nation we had 6 of each. In both cases, we used the Mann Whitney U test to see if there was a signifi- cant relationship between the similarity measure and participation. We then used a threshold on the sim- ilarity scores as the decision point for participation to determine a level of accuracy. We experimented with both the mean and median of the scores as a threshold. Seven of the negative causative candi- dates were randomly chosen and removed to ensure an even split between positive and negative candi- dates for determining accuracy using the mean and median as thresholds. The following subsection describes the results of the experiments using the method described in sec- tion 3 above. Subsection 4.2 describes an experiment on the same data to determine participation using a similarity measure based on the intersection of the lemmas at the target slots. --9 E 0.4 F 01- 0.05 0.55 0.95 G H 0.3 0.3 0.1 C E F G H 055- R J New TCM1 TCM1 0.6 New TCM2 R 0.5 0.1 TCM2 I J Figure 2: New TCMS at the union base cut 4.1 Using Syntax and Selectional Preferences The results for the causative alternation are dis- played in table 1 for both the RBC and the UBC. The relationship between participation and asd is highly significant in both cases, with values of p well below 0.01. Accuracy for the mean and median thresholds are displayed in the fourth and fifth columns. Both thresholds outperform the random baseline of 50%. The results for the UBC are slightly improved, com- pared to those for the RBC, however the improve- ment is not significant. The numbers of false negative (FN) and false posi- tive (FP) errors for the mean and median thresholds are displayed in table 2, along with the threshold and accuracy. The outcomes for each individual verb for the experiment using the RBC and the mean thresh- old are as follows: • True negatives: add admit answer believe borrow cost declare de- mand expect feel imagine know notice pay per- form practise proclaim read remember sing sur- vive understand win write • True positives: accelerate bang bend boil break burn change close cook cool crack decrease drop dry end ex- pand fly improve increase match melt open ring rip rock roll shatter shut slam smash snap spill split spread start stop stretch swing tilt turn wake • False negatives: flood land march repeat terminate • False positives: ask attack catch choose climb drink eat help kick knit miss outline pack paint plan prescribe pull remain steal suck warn wash The results for the UBC experiment are very similar. If the median is used, the number of FPs and FNS are evenly balanced. This is because the median threshold is, by definition, taken midway between the test items arranged in order of their similarity scores. There are an even number of items on either side of the decision point, and an even number of positive and negative candidates in our test sample. Thus, the errors on either side of the decision point are equal in number. For both base cuts, there are a larger number of false positives than false negatives when the mean is used. The mean produces a higher accuracy than the median, but gives an increase in false positives. Many false positives arise where the preferences at both target slots are near neighbours in WordNet. For example, this occurred for eat and drink. There verbs have a high probability mass (around 0.7) un- der the entity class in both target slots, since both people and types of food occur under this class. In cases like these, the probability distributions at the RBC, and frequently the UBC, are not sufficiently dis- tinctive. The polysemy of the verbs may provide another explanation for the large quantity of false positives. The SCFs and data of different senses should not ideally be combined, at least not for coarse grained sense distinctions. We tested the false positive and true negative candidates to see if there was a re- lationship between the polysemy of a verb and its misclassification. The number of senses (according to WordNet) was used to indicate the polysemy of a verb. The Mann Whitney U test was performed on Mann Whitney z significance (p) mean median RBC -4.03 0.0003 71 63 UBC -4.3 0.00003 73 70 Table 1: Causative results threshold type threshold accuracy % base cut UBC mean 0.38 73 21 4 UBC median 0.20 70 14 14 RBC mean 0.32 71 22 5 RBC median 0.15 63 17 17 Table 2: Error analysis for the causative experiments num FPS num FNs the verbs found to be true negative and false positive using the RBC. A significant relationship was not found between participation and misclassification. Both groups had an average of 5 senses per verb. This is not to say that distinguishing verb senses would not improve performance, provided that there was sufficient data. However, verb polysemy does not appear to be a major source of error, from our preliminary analysis. In many cases, such as read which was classified both by the judges, and the sys- tem as a negative candidate, the predominant sense of the verb provides the majority of the data. Alter- nate senses, for example, the book reads well, often do not contribute enough data so as to give rise to a large proportion of errors. Finding an appropriate inventory of senses would be difficult, since we would not wish to separate related senses which occur as alternate variants of one another. The inventory would therefore require knowledge of the phenomena that we are endeavouring to acquire automatically. To show that our method will work for other RSAS, we use the conative. Our sample size is rather small since we are limited by the number of positive can- didates in the corpus having sufficient frequency for both SCFs. The sparse data problem is acute when we look at alternations with specific prepositions. A sample of 12 verbs (6 positive and 6 negative) re- mained after the selection process outlined above. For this small sample we obtained a significant re- sult (p = 0.02) with a mean accuracy of 67% and a median accuracy of 83%. On this occasion, the median performed better than the mean. More data is required to see if this difference is significant. 4.2 Using Syntax and Lemmas This experiment was conducted using the same data as that used in the previous subsection. In this ex- periment, we used a similarity score on the argument heads directly, instead of generalizing the argument heads to WordNet classes. The venn diagram in fig- ure 3 shows a subset of the lemmas at the transitive and intransitive SCFs for the verb break. The lemma based similarity measure is termed lemma overlap (LO) and is given in equation 3, where A and B represent the target slots. LO is the size of the intersection of the multisets of argument heads at the target slots, divided by the size of the smaller of the two multisets. The intersection of two mul- tisets includes duplicate items only as many times as the item is in both sets. For example, if one slot contained the argument heads {person, person, person, child, man, spokeswoman}, and the other slot contained {person, person, child, chair, collec- tion}, then the intersection would be {person, per- son, child}, and LO would be. This measure ranges between zero (no overlap) and 1 (where one set is a proper subset of that at the other slot). LO(A, B) = multiset intersection(AB)| smallest set(A, B)| (3) Using the Mann Whitney U test on the Lo scores, we obtained a z score of 2.00. This is significant to the 95% level, a lower level than that for the class- based experiments. The results using the mean and median of the LO scores are shown in table 3. Perfor- mance is lower than that for the class-based experi- ments. The outcome for the individual verbs using the mean as a threshold was:- • True negatives: add admit answer borrow choose climb cost de- clare demand drink eat feel imagine notice out- line pack paint perform plan practise prescribe proclaim read remain sing steal suck survive un- derstand wash win write • True positives: bend boil burn change close cool dry end fly im- prove increase match melt open ring roll shut slam smash start stop tilt wake • False negatives: accelerate bang break cook crack decrease drop expand flood land march repeat rip rock shatter Subjects of Intransitive storm crisis wave war weather hell ... Objects of Intransitive silence jaw neck someone leg back deadlock ... curfew tie cartel ground diet spell ... Figure 3: Lemmas at the causative target slots of break snap spill split spread stretch swing terminate turn • False positives: ask attack believe catch expect help kick knit know miss pay pull remember warn Interestingly, the errors for the LO measure tend to be false negatives, rather than false positives. The LO measure is much more conservative than the ap- proach using the TCMS. In this case the median threshold produces better results. For the conative alternation, the lemma based method does not show a significant relationship be- tween participation and the LO scores. Moreover, there is no difference between the sums of the ranks of the two groups for the Mann Whitney U test. The mean produces an accuracy of 58% whilst the median produces an accuracy of 50%. 5 Related Work There has been some recent interest in observing alternations in corpora (McCarthy and Korhonen, 1998; Lapata, 1999) and predicting related verb classifications (Stevenson and Merlo, 1999). Ear- lier work by Resnik (1993) demonstrated a link be- tween selectional preference strength and participa- tion in alternations where the direct object is omit- ted. Resnik used syntactic information from the bracketing within the Penn Treebank corpus. Re- search into the identification of other diathesis al- ternations has been advanced by the availability of automatic syntactic processing. Most work us- ing corpus evidence for verb classification has re- lied on a priori knowledge in the form of linguistic cues specific to the phenomena being observed (La- pata, 1999; Stevenson and Merlo, 1999). Our ap- proach, whilst being applicable only to RSAS, does not require human input specific to the alternation at hand. Lapata (1999) identifies participation in the dative and benefactive alternations. Lapata's strategy is to identify participants using a shallow parser and vari- ous linguistic and semantic cues, which are specified manually for these two alternations. PP attachments are resolved using Hindle and Rooth's (1993) lexical association score. Compound nouns, which could be mistaken for the double object construction, were filtered using the log-likelihood ratio test. The se- mantic cues were obtained by manual analysis. The relative frequency of a SCF for a verb, compared to the total frequency of the verb, was used for filtering out erroneous SCFs. Lapata does not report recall and precision fig- ures against a gold standard. The emphasis is on the phenomena actually evident in the corpus data. Many of the verbs listed in Levin as taking the al- ternation were not observed with this alternation in the corpus data. This amounted to 44% of the verbs for the benefactive, and 52% for the dative. These figures only take into account the verbs for which at least one of the SCFS were observed. 54% of the verbs listed for the dative and benefactive by Levin were not acquired with either of the target SCFS. Conversely, many verbs not listed in Levin were identified as taking the benefactive or dative alternation using Lapata's criteria. Manual analysis of these verbs revealed 18 false positives out of 52 candidates. Stevenson and Merlo (1999) use syntactic and lex- ical cues for classifying 60 verbs in three verb classes: unergative, unaccusative and verbs with an optional direct object. These three classes were chosen be- --- threshold type threshold accuracy % num FPs num FNS mean 0.26 60 14 23 median 0.23 63 17 17 Table 3: Accuracy and error analysis for lemma based experiments cause a few well defined features, specified a pri- ori, can distinguish the three groups. Twenty verbs from Levin's classification were used in each class. They were selected by virtue of having sufficient fre- quency in a combined corpus (from the Brown and the WSJ) of 65 million words. The verbs were also chosen for having one predominant intended sense in the corpus. Stevenson and Merlo used four linguisti- cally motivated features to distinguish these groups. Counts from the corpus data for each of the four fea- tures were normalised to give a score on a scale of 1 to 100. One feature was the causative non-causative distinction. For this feature, a measure similar to our LO measure was used. The four features were identified in the corpus using automatic POS tagging and parsing of the data. The data for half of the verbs in each class was subject to manual scrutiny, after initial automatic processing. The rest of the data was produced fully automatically. The verbs were classified automatically using the four features. The accuracy of automatic classification was 52% us- ing all four features, compared to a baseline of 33%. The best result was obtained using a combination of three features. This gave an accuracy of 66%. McCarthy and Korhonen (1998) proposed a method for identifying RSAS using MDL. This method relied on an estimation of the cost of us- ing TCMS to encode the argument head data at a target slot. The sum of the costs for the two target slots was compared to the cost of a TCM for encoding the union of the argument head data over the two slots. Results are reported for the causative alterna- tion with 15 verbs. This method depends on there being similar quantities of data at the alternating slots, otherwise the data at the more frequent slot overwhelms the data at the less frequent slot. How- ever, many alternations involve SCFS with substan- tially different relative frequencies, especially when one SCF is specific to a particular preposition. We carried out some experiments using the MDL method and our TCMS. For the causative, we used a sample of 110 verbs and obtained 63% accuracy. For the conative, a sample of 16 verbs was used and this time accuracy was only 56%. Notably, only one negative decision was made because of the disparate frame frequencies, which reduces the cost of combining the argument head data. 6 Conclusion We have discovered a significant relationship be- tween the similarity of selectional preferences at the target slots, and participation in the causative and conative alternations. A threshold, such as the mean or median can be used to obtain a level of accuracy well above the baseline. A lemma based similarity score does not always indicate a significant relation- ship and generally produces a lower accuracy. There are patterns of diathesis behaviour among verb groups (Levin, 1993). Accuracy may be im- proved by considering several alternations collec- tively, rather than in isolation. Complementary techniques to identify alternations, for example (Resnik, 1993), might be combined with ours. Although we have reported results on only two RSAS, our method is applicable to other such alter- nations. Furthermore, such application requires no human endeavour, apart from that required for eval- uation. However, a considerably larger corpus would be required to overcome the sparse data problem for other RSA alternations. 7 Acknowledgements Some funding for this work was provided by UK EP- SRC project GR/L53175 'PSET: Practical Simplifi- cation of English Text'. We also acknowledge Gerald Gazdar for his helpful comments on this paper. References Bran Boguraev and Ted Briscoe. 1987. Large lex- icons for natural language processing: Utilising the grammar coding system of LDOCE. Compu- tational Linguistics, 13(3-4):203-218. Ted Briscoe and John Carroll. 1997. Automatic extraction of subcategorization from corpora. In Fifth Applied Natural Language Processing Con- ference, pages 356-363. Ted Briscoe and Ann Copestake. 1996. Controlling the application of lexical rules. In E Viegas, ed- itor, SIGLEX Workshop on Lexical Semantics - ACL 96 Workshop. Hoa Trang Dang, Karin Kipper, Martha Palmer, and Joseph Rosensweig. 1998. Investigating reg- ular sense extensions based on intersective Levin classes. In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Com- putational Linguistics, volume 1, pages 293-299. --- Bonnie J. Dorr and Doug Jones. 1996. Role of word sense disambiguation in lexical acquisition: Pre- dicting semantics from syntactic cues. In Proceed- ings of the 16th International Conference on Com- putational Linguistics, COLING-96, pages 322- 327. Donald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103-120. Kentaro Inui, Virach Sornlertlamvanich, Hozumi Tanaka, and Takenobu Tokunaga. 1997. A new formalization of probabilistic glr parsing. In 5th ACL/SIGPARSE International Workshop on Parsing Technologies, pages 123-134, Cambridge, MA. Anna Korhonen. 1997. Acquiring Subcategorisation from Textual Corpora. Master's thesis, University of Cambridge. Maria Lapata. 1999. Acquiring lexical generaliza- tions from corpora: A case study for diathe- sis alternations. In Proceedings of the 37th An- nual Meeting of the Association for Computa- tional Linguistics, pages 397-404. Lillian Lee. 1999. Measures of distributional simi- larity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 25-32. Geoffrey Leech. 1992. 100 million words of English: the British National Corpus. Language Research, 28(1):1-13. Beth Levin. 1993. English Verb Classes and Alter- nations: a Preliminary Investigation. University of Chicago Press, Chicago and London. Hang Li and Naoki Abe. 1995. Generalizing case frames using a thesaurus and the MDL principle. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, pages 239-248, Bulgaria. Diana McCarthy and Anna Korhonen. 1998. De- tecting verbal participation in diathesis alterna- tions. In Proceedings of the 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Com- putational Linguists., volume 2, pages 1493-1495. Philip Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania. Francesc Ribas. 1995. On Acquiring Appropriate Selectional Restrictions from Corpora Using a Se- mantic Taxonomy. Ph.D. thesis, University of Catalonia. Jorma. Rissanen. 1978. Modeling by shortest data description. Automatica, 14:465-471. Sidney Siegel and N. John Castellan, editors. 1988. Non-Parametric Statistics for the Behavioural Sciences. McGraw-Hill, New York. Manfred Stede. 1998. A generative perspective on verb alternations. Computational Linguistics, 24(3):401-430. Suzanne Stevenson and Paola Merlo. 1999. Au- tomatic verb classification using distributions of grammatical features. In Proceedings of the Ninth Conference of the European Chapter of the Associ- ation for Computational Linguistics, pages 45-52."
  },
  {
    "title": "Disambiguation of Proper Names in Text",
    "abstract": "Identifying the occurrences of proper names in text and the entities they refer to can be a difficult task because of the many-to-many mapping between names and their referents. We analyze the types of ambiguity — structural and semantic — that make the discovery of proper names difficult in text, and describe the heuristics used to disambiguate names in Nominator, a fully-implemented module for proper name recognition developed at the IBM T.J. Watson Research Center.",
    "content": "1 Proper Name Identification in Natural Language Processing Text processing applications, such as machine trans- lation systems, information retrieval systems or natural-language understanding systems, need to identify multi-word expressions that refer to proper names of people, organizations, places, laws and other entities. When encountering Mrs. Candy Hill in input text, for example, a machine translation system should not attempt to look up the transla- tion of candy and hill, but should translate Mrs. to the appropriate personal title in the target language and preserve the rest of the name intact. Similarly, an information retrieval system should not attempt to expand Candy to all of its morphological variants or suggest synonyms (Wacholder et al. 1994). The need to identify proper names has two as- pects: the recognition of known names and the dis- covery of new names. Since obtaining and maintain- ing a name database requires significant effort, many applications need to operate in the absence of such a resource. Without a database, names need to be discovered in the text and linked to entities they re- fer to. Even where name databases exist, text needs to be scanned for new names that are formed when entities, such as countries or commercial companies, are created, or for unknown names which become important when the entities they refer to become topical. This situation is the norm for dynamic ap- plications such as news providing services or Internet information indexing. The next Section describes the different types of proper name ambiguities we have observed. Sec- tion 3 discusses the role of context and world knowl- edge in their disambiguation; Section 4 describes the process of name discovery as implemented in Nomi- nator, a module for proper name recognition devel- oped at the IBM T.J. Watson Research Center. Sec- tions 5-7 elaborate on Nominator's disambiguation heuristics. 2 The Ambiguity of Proper Names Name identification requires resolution of a subset of the types of structural and semantic ambiguities en- countered in the analysis of nouns and noun phrases (NPs) in natural language processing. Like common nouns, ((Jensen and Binot 1987), (Hindle and Rooth 1993) and (Brill and Resnick 1994)), proper names exhibit structural ambiguity in prepositional phrase (PP) attachment and in conjunction scope. A PP may be attached to the preceding NP and form part of a single large name, as in NP [Midwest Center PP[for NP [Computer Research]]]. Alterna- tively it may be independent of the preceding NP, as in NP[Carnegie Hall] PP[for NP [Irwin Berlin]], where for separates two distinct names, Carnegie Hall and Irwin Berlin. As with PP-attachment of common noun phrases, the ambiguity is not always resolved, even in hu- man sentence parsing (cf. the famous example I saw the girl in the park with the telescope). The loca- tion of an organization, for instance, could be part of its name (City University of New York) or an attached modifier (The Museum of Modern Art in New York City). Without knowledge of the official name, it is sometimes difficult to determine the ex- act boundaries of a proper name. Consider examples such as Western Co. of North America, Commod- ity Exchange in New York and Hebrew University in Jerusalem, Israel. Proper names contain ambiguous conjoined phrases. The components of Victoria and Albert Museum and IBM and Bell Laboratories look identi- cal; however, and is part of the name of the museum in the first example, but a conjunction joining two computer company names in the second. Although this problem is well known, a search of the computa- tional literature shows that few solutions have been proposed, perhaps because the conjunct ambiguity problem is harder than PP attachment (though see (Agarwal and Boggess 1992) for a method of con- junct identification that relies on syntactic category and semantic label). Similar structural ambiguity exists with respect to the possessive pronoun, which may indicate a re- lationship between two names (e.g., Israel's Shimon Peres) or may constitute a component of a single name (e.g., Donoghue's Money Fund Report). The resolution of structural ambiguity such as PP attachment and conjunction scope is required in order to automatically establish the exact bound- aries of proper names. Once these boundaries have been established, there is another type of well-known structural ambiguity, involving the internal struc- ture of the proper name. For example, Professor of Far Eastern Art John Blake is parsed as [[Professor [of Far Eastern Art]] John Blake] whereas Professor Art Klein is [[Professor) Art Klein]. Proper names also display semantic ambiguity. Identification of the type of proper nouns resem- bles the problem of sense disambiguation for com- mon nouns where, for instance, state taken out of context may refer either to a government body or the condition of a person or entity. A name variant taken out of context may be one of many types, e.g., Ford by itself could be a person (Gerald Ford), an organization (Ford Motors), a make of car (Ford), or a place (Ford, Michigan). Entity-type ambiguity is quite common, as places are named after famous people and companies are named after their owners or locations. In addition, naming conventions are sometimes disregarded by people who enjoy creating novel and unconventional names. A store named Mr. Tall and a woman named April Wednesday (McDon- ald 1993) come to mind. Like common nouns, proper nouns exhibit system- atic metonymy: United States refers either to a geo- graphical area or to the political body which governs this area; Wall Street Journal refers to the printed object, its content, and the commercial entity that produces it. In addition, proper names resemble definite noun phrases in that their intended referent may be am- biguous. The man may refer to more than one male individual previously mentioned in the discourse or present in the non-linguistic context; J. Smith may similarly refer to more than one individual named Joseph Smith, John Smith, Jane Smith, etc. Se- mantic ambiguity of names is very common because of the standard practice of using shorter names to stand for longer ones. Shared knowledge and con- text are crucial disambiguation factors. Paris, usu- ally refers to the capital of France, rather than a city in Texas or the Trojan prince, but in a particu- lar context, such as a discussion of Greek mythology, the presumed referent changes. Beyond the ambiguities that proper names share with common nouns, some ambiguities are particular to names: noun phrases may be ambiguous between a name reading and a common noun phrase, as in Candy, the person's name, versus candy the food, or The House as an organization versus a house refer- ring to a building. In English, capitalization usually disambiguates the two, though not at sentence be- ginnings: at the beginning of a sentence, the compo- nents and capitalization patterns of New Coke and New Sears are identical; only world knowledge in- forms us that New Coke is a product and Sears is a company. Furthermore, capitalization does not always dis- ambiguate names from non-names because what constitutes a name as opposed to a non-name is not always clear. According to (Quirk et al. 1972) names, which consist of proper nouns (classified into personal names like Shakespeare, temporal names like Monday, or geographical names like Australia) have 'unique' reference. Proper nouns differ in their linguistic behavior from common nouns in that they mostly do not take determiners or have a plural form. However, some names do take determiners, as in The New York Times; in this case, they \"are perfectly regular in taking the definite article since they are basically premodified count nouns... The difference between an ordinary common noun and an ordinary common noun turned name is that the unique reference of the name has been institution- alized, as is made overt in writing by initial capital letter.\" Quirk et al.'s description of names seems to indicate that capitalized words like Egyptian (an ad- jective) or Frenchmen (a noun referring to a set of individuals) are not names. It leaves capitalized se- quences like Minimum Alternative Tax, Annual Re- port, and Chairman undetermined as to whether or not they are names. All of these ambiguities must be dealt with if proper names are to be identified correctly. In the rest of the paper we describe the resources and heuristics we have designed and implemented in Nominator and the extent to which they resolve these ambiguities. 3 Disambiguation Resources In general, two types of resources are available for disambiguation: context and world knowledge. Each of these can be exploited along a continuum, from 'cheaper' to computationally and manually more ex- pensive usage. 'Cheaper' models, which include no context or world knowledge, do very little dis- ambiguation. More 'expensive' models, which use full syntactic parsing, discourse models, inference and reasoning, require computational and human re- sources that may not always be available, as when massive amounts of text have to be rapidly processed on a regular basis. In addition, given the current state of the art, full parsing and extensive world knowledge would still not yield complete automatic ambiguity resolution. In designing Nominator, we have tried to achieve a balance between high accuracy and speed by adopt- ing a model which uses minimal context and world knowledge. Nominator uses no syntactic contextual information. It applies a set of heuristics to a list of (multi-word) strings, based on patterns of capi- talization, punctuation and location within the sen- tence and the document. This design choice differ- entiates our approach from that of several similar projects. Most proper name recognizers that have been reported on in print either take as input text tagged by part-of-speech (e.g., the systems of (Paik et al. 1993) and (Mani et al. 1993)) or perform syn- tactic and/or morphological analysis on all words, including capitalized ones, that are part of candi- date proper names (e.g., (Coates-Stephens 1993) and (McDonald 1993)). Several (e.g., (McDonald 1993), (Mani et al. 1993), (Paik et al. 1993) and (Cowie et al. 1992)) look in the local context of the candi- date proper name for external information such as appositives (e.g., in a sequence such as Robin Clark, president of Clark Co.) or for human-subject verbs (e.g., say, plan) in order to determine the category of the candidate proper name. Nominator does not use this type of external context. Instead, Nominator makes use of a different kind of contextual information — proper names co- occuring in the document. It is a fairly standard convention in an edited document for one of the first references to an entity (excluding a reference in the title) to include a relatively full form of its name. In a kind of discourse anaphora, other references to the entity take the form of shorter, more ambiguous variants. Nominator identifies the referent of the full form (see below) and then takes advantage of the discourse context provided by the list of names to associate shorter more ambiguous name occurrences with their intended referents. In terms of world knowledge, the most obvious re- source is a database of known names. In fact, this is what many commercially available name identifica- tion applications use (e.g., Hayes 1994). A reliable database provides both accuracy and efficiency, if fast look-up methods are incorporated. A database also has the potential to resolve structural ambigu- ity; for example, if IBM and Apple Computers are listed individually in the database but IBM and Ap- ple Computers is not, it may indicate a conjunction of two distinct names. A database may also con- with no other over-riding information, it may be safe to assume that the string McDonald's refers to an or- ganization. But even if an existing database is reli- able, names that are not yet in it must be discovered and information in the database must be over-ridden when appropriate. For example, if a new name such as IBM Credit Corp. occurs in the text but not in the database, while IBM exists in the database, au- tomatic identification of IBM should be blocked in favor of the new name IBM Credit Corp. If a name database exists, Nominator can take advantage of it. However, our goal has been to de- sign Nominator to function optimally in the absence of such a resource. In this case, Nominator con- sults a small authority file which contains informa- tion on about 3000 special 'name words' and their relevant lexical features. Listed are personal titles (e.g., Mr., King), organizational identifiers (includ- ing strong identifiers such as Inc. and weaker do- main identifiers such as Arts) and names of large places (e.g., Los Angeles, California, but not Scars- dale, N.Y.). Also listed are exception words, such as upper-case lexical items that are unlikely to be single-word proper names (e.g., Very, I or TV) and lower-case lexical items (e.g., and and van) that can be parts of proper names. In addition, the authority file contains about 20,000 first names. Our choice of disambiguation resources makes Nominator fast and robust. The precision and re- call of Nominator, operating without a database of pre-existing proper names, is in the 90's while the processing rate is over 40Mg of text per hour on a RISC/6000 machine. (See (Ravin and Wacholder 1996) for details.) This efficient processing has been achieved at the cost of limiting the extent to which the program can 'understand' the text being ana- lyzed and resolve potential ambiguity. Many word- sequences that are easily recognized by human read- ers as names are ambiguous for Nominator, given the restricted set of tools available to it. In cases where Nominator cannot resolve an ambiguity with rela- tively high confidence, we follow the principle that 'noisy information' is to be preferred to data omit- ted, so that no information is lost. In ambiguous cases, the module is designed to make conservative decisions, such as including non-names or non-name parts in otherwise valid name sequences. It assigns weak types such as ?HUMAN or fails to assign a type if the available information is not sufficient. 4 The Name Discovery Process In this section, we give an overview of the process by which Nominator identifies and classifies proper names. Nominator's first step is to build a list of candidate names for a document. Next, 'splitting' heuristics are applied to all candidate names for the purpose of breaking up complex names into smaller ones. Finally Nominator groups together name vari- ants that refer to the same entity. After information about names and their referents has been extracted from individual documents, an aggregation process combines the names collected from all the documents into a dictionary, or database of names, representa- tive of the document collection. (For more details on the process, see (Ravin and Wacholder 1996)). We illustrate the process of name discovery with an excerpt taken from a Wall Street Journal article in the TIPSTER CD-ROM collection (NIST 1993). Paragraph breaks are omitted to conserve space. …The professional conduct of lawyers in other jurisdictions is guided by American Bar Association rules or by state bar ethics codes, none of which permit non-lawyers to be partners in law firms. The ABA has steadfastly reserved the title of partner and partnership perks (which include getting a stake of the firm's profit) for those with law degrees. But Robert Jordan, a partner at Steptoe & Johnson who took the lead in drafting the new district bar code, said the ABA's rules were viewed as \"too restrictive\" by lawyers here. \"The practice of law in Washing- ton is very different from what it is in Dubuque,\" he said. …Some of these non-lawyer employees are paid at partners' levels. Yet, not having the part- ner title \"makes non-lawyers working in law firms second-class citizens,\" said Mr. Jordan of Steptoe & Johnson… Before the text is processed by Nominator, it is analyzed into tokens—sentences, words, tags, and punctuation elements. Nominator forms a candidate name list by scanning the tokenized document and collecting sequences of capitalized tokens (or words) as well as some special lower-case tokens, such as conjunctions and prepositions. The list of candidate names extracted from the sample document contains: American Bar Association Robert Jordan Steptoe & Johnson ABA Washington Dubuque Mr. Jordan of Steptoe & Johnson Each candidate name is examined for the presence of conjunctions, prepositions or possessive 's. A set of heuristics is applied to determine whether each candidate name should be split into smaller inde- pendent names. For example, Mr. Jordan of Steptoe & Johnson is split into Mr. Jordan and Steptoe & Johnson. Finally, Nominator links together variants that refer to the same entity. Because of standard English-language naming conventions, Mr. Jordan is grouped with Robert Jordan. ABA is grouped with American Bar Association as a possible abbre- categorized by an entity type and assigned a 'canon- ical name' as its identifier. The canonical name is the fullest, least ambiguous label that can be used to refer to the entity. It may be one of the variants found in the document or it may be constructed from components of different ones As the links are formed, each group is assigned a type. In the sample output shown below, each canonical name is followed by its entity type and by the variants linked to it. American Bar Association (ORG) : ABA Steptoe & Johnson (ORG) Washington (PLACE) Dubuque (PLACE) Robert Jordan (PERSON) : Mr. Jordan After the whole document collection has been processed, linked groups are merged across docu- ments and their variants combined. Thus, if in one document President Clinton was a variant of William Clinton, while in another document Gover- nor Clinton was a variant of William Clinton, both are treated as variants of an aggregated William Clinton group. In this minimal sense, Nominator uses the larger context of the document collection to 'learn' more variants for a given name. In the following sections we describe how ambigu- ity is resolved as part of the name discovery process. 5 Resolution of Structural Ambiguity We identify three indicators of potential structural ambiguity, prepositions, conjunctions and possessive pronouns, which we refer to as 'ambiguous oper- ators'. In order to determine whether 'splitting' should occur, a name sequence containing an am- biguous operator is divided into three segments— the operator, the substring to its left and the sub- string to its right. The splitting process applies a set of heuristics based on patterns of capitalization, lexical features and the relative 'scope' of operators (see below) to name sequences containing these op- erators to determine whether or not they should be split into smaller names. We can describe the splitting heuristics as deter- mining the scope of ambiguous operators, by analogy to the standard linguistic treatment of quantifiers. From Nominator's point of view, all three operator types behave in similar ways and often interact when they co-occur in the same name sequence, as in New York's MOMA and the Victoria and Albert Museum in London. The scope of ambiguous operators also interacts with the 'scope' of NP-heads, if we define the scope of NP-heads as the constituents they dominate. For example, in Victoria and Albert Museum, the con- junction is within the scope of the lexical head Museum because Museum is a noun that can take hence pre-modification (Natural History Museum). Since pre-modifiers can contain conjunctions (Japan- ese Painting and Printing Museum), the conjunction is within the scope of the noun, and so the name is not split. Although the same relationship holds between the lexical head Laboratories and the con- junction and in IBM and Bell Laboratories, another heuristic takes precedence, one whose condition re- quires splitting a string if it contains an acronym immediately to the left or to the right of the am- biguous operator. It is not possible to determine relative scope strength for all the combinations of different opera- tors. Contradictory examples abound: Gates of Mi- crosoft and Gerstner of IBM suggests stronger scope of and over of, The Department of German Lan- guages and Literature suggests the opposite. Since it is usually the case that a right-hand operator has stronger scope over a left-hand one, we evalu- ate strings containing operators from right to left. To illustrate, New York's MOMA and the Victoria and Albert Museum in London is first evaluated for splitting on in. Since the left and right substrings do not satisfy any conditions, we proceed to the next operator on the left - and. Because of the strong scope of Museum, as mentioned above, no splitting occurs. Next, the second and from the right is eval- uated. It causes a split because it is immediately preceded by an all-capitalized word. We have found this simple typographical heuristic to be powerful and surprisingly accurate. Ambiguous operators form recursive structures and so the splitting heuristics apply recursively to name sequences until no more splitting conditions hold. New York's MOMA is further split at 's be- cause of a heuristic that checks for place names on the left of a possessive pronoun or a comma. Victo- ria and Albert Museum in London remains intact. Nominator's other heuristics resemble those dis- cussed above in that they check for typographical patterns or for the presence of particular name types to the left or right of certain operators. Some heuris- tics weigh the relative scope strength in the sub- strings on either side of the operator. If the scope strength is similar, the string is split. We have ob- served that this type of heuristic works quite well. Thus, the string The Natural History Museum and The Board of Education is split at and because each of its substrings contains a strong-scope NP-head (as we define it) with modifiers within its scope. These two substrings are better balanced than the sub- strings of The Food and Drug Administration where the left substring does not contain a strong-scope NP-head while the right one does (Administration). Because of the principle that noisy data is prefer- able to loss of information, Nominator does not split names if relative strength cannot be determined. As a result, there occur in Nominator's output certain nications and Houston Industries Inc. or Dallas's MCorp and First RepublicBank and Houston's First City Bancorp. of Texas. 6 Resolution of Ambiguity at Sentence Beginnings Special treatment is required for words in sentence- initial position, which may be capitalized because they are part of a proper name or simply because they are sentence initial. While the heuristics for splitting names are lin- guistically motivated and rule-governed, the heuris- tics for handling sentence-initial names are based on patterns of word occurrence in the document. When all the names have been collected and split, names containing sentence-initial words are compared to other names on the list. If the sentence-initial candi- date name also occurs as a non-sentence-initial name or as a substring of it, the candidate name is as- sumed to be valid and is retained. Otherwise, it is removed from the list. For example, if White occurs at sentence-initial position and also as a substring of another name (e.g., Mr. White) it is kept. If it is found only in sentence-initial position (e.g., White paint is...), White is discarded. A more difficult situation arises when a sentence- initial candidate name contains a valid name that begins at the second word of the string. If the pre- ceding word is an adverb, a pronoun, a verb or a preposition, it can safely be discarded. Thus a sen- tence beginning with Yesterday Columbia yields Co- lumbia as a name. But cases involving other parts of speech remain unresolved. If they are sentence- initial, Nominator accepts as names both New Sears and New Coke; it also accepts sentence-initial Five Reagan as a variant of President Reagan, if the two co-occur in a document. 7 Resolution of Semantic Ambiguity In a typical document, a single entity may be re- ferred to by many name variants which differ in their degree of potential ambiguity. As noted above, Paris and Washington are highly ambiguous out of con- text but in well edited text they are often disam- biguated by the occurrence of a single unambiguous variant in the same document. Thus, Washington is likely to co-occur with either President Washington or Washington, D.C., but not with both. Indeed, we have observed that if several unambiguous variants do co-occur, as in documents that mention both the owner of a company and the company named after the owner, the editors refrain from using a variant that is ambiguous with respect to both. To disambiguate highly ambiguous variants then, we link them to unambiguous ones occurring within the same document. Nominator cycles through the that unambiguously refer to certain entity types. When an anchor is identified, the list of name candi- dates is scanned for ambiguous variants that could refer to the same entity. They are linked to the an- chor. Our measure of ambiguity is very pragmatic. It is based on the confidence scores yielded by heuristics that analyze a name and determine the entity types it can refer to. If the heuristic for a certain entity type (a person, for example) results in a high con- difence score (highly confident that this is a person name), we determine that the name unambiguously refers to this type. Otherwise, we choose the highest score obtained by the various heuristics. A few simple indicators can unambiguously deter- mine the entity type of a name, such as Mr. for a person or Inc. for an organization. More commonly, however, several pieces of positive and negative evi- dence are accumulated in order to make this judge- ment. We have defined a set of obligatory and optional components for each entity type. For a human name, these components include a professional title (e.g., Attorney General), a personal title (e.g., Dr.), a first name, middle name, nickname, last name, and suffix (e.g., Jr.). The combination of the various compo- nents is inspected. Some combinations may result in a high negative score — highly confident that this cannot be a person name. For example, if the name lacks a personal title and a first name, and its last name is listed as an organization word (e.g., Depart- ment) in the authority list, it receives a high negative score. This is the case with Justice Department or Frank Sinatra Building. The same combination but with a last name that is not a listed organization word results in a low positive score, as for Justice Johnson or Frank Sinatra. The presence or absence of a personal title is also important for determining confidence: If present, the result is a high confidence score (e.g., Mrs. Ruth Lake); No personal title with a known first name results in a low positive confi- dence score (e.g., Ruth Lake, Beverly Hills); and no personal title with an unknown first name results in a zero score (e.g., Panorama Lake). By the end of the analysis process, Justice De- partment has a high negative score for person and a low positive score for organization, resulting in its classification as an organization. Beverly Hills, by contrast, has low positive scores both for place and for person. Names with low or zero scores are first tested as possible variants of names with high posi- tive scores. However, if they are incompatible with any, they are assigned a weak entity type. Thus in the absence of any other evidence in the document, Beverly Hills is classified as a ?PERSON. (?PER- SON is preferred over ?PLACE as it tends to be the correct choice most of the time.) This analysis list- Further disambiguation may be possible during aggregation across documents. As mentioned be- fore, during aggregation, linked groups from differ- ent documents are merged if their canonical forms are identical. As a rule, their entity types should be identical as well, to prevent a merge of Boston (PLACE) and Boston (ORG). Weak entity types, however, are allowed to merge with stronger entity types. Thus, Jordan Hills (?PERSON) from one document is aggregated with Jordan Hills (PER- SON) from another, where there was sufficient evi- dence, such as Mr. Hills, to make a firmer decision. 8 Evaluation An evaluation of an earlier version of Nominator, was performed on 88 Wall Street Journal documents (NIST 1993) that had been set aside for testing. We chose the Wall Street Journal corpus because it fol- lows standard stylistic conventions, especially capi- talization, which is essential for Nominator to work. Nominator's performance deteriorates if other con- ventions are not consistently followed. A linguist manually identified 2426 occurrences of proper names, which reduced to 1354 unique to- kens. Of these, Nominator correctly identified the boundaries of 91% (1230/1354). The precision rate was 92% for the 1409 names Nominator identified (1230/1409). In terms of semantic disambiguation, Nominator failed to assign an entity type to 21% of the names it identified. This high percentage is due to a decision not to assign a type if the confi- dence measure is too low. The payoff of this choice is a very high precision rate — 99% — for the as- signment of semantic type to those names that were disambiguated. (See (Ravin and Wacholder 1996) for details. The main reason that names remain untyped is insufficent evidence in the document. If IBM, for example, occurs in a document without Interna- tional Business Machines, Nominator does not type it; rather, it lets later processes inspect the local context for further clues. These processes form part of the Talent tool set under development at the T.J. Watson Research Center. They take as their input text processed by Nominator and fur- ther disambiguate untyped names appearing in cer- tain contexts, such as an appositive, e.g., president of CitiBank Corp. Other untyped names, such as Star Bellied Sneetches or George Melloan's Business World, are neither people, places, organizations nor any of the other legal or financial entities we categorize into. Many of these uncategorized names are titles of ar- ticles, books and other works of art that we currently 9 Conclusion Ambiguity remains one of the main challenges in the processing of natural language text. Efforts to resolve it have traditionally focussed on the devel- opment of full-coverage parsers, extensive lexicons, and vast repositories of world knowledge. For some natural-language applications, the tremendous ef- fort involved in developing these tools is still re- quired, but in other applications, such as informa- tion extraction, there has been a recent trend to- wards favoring minimal parsing and shallow knowl- edge (Cowie and Lehnert 1996). In its minimal use of resources, Nominator follows this trend: it relies on no syntactic information and on a small seman- tic lexicon — an authority list which could easily be modified to include information about new domains. Other advantages of using limited resources are ro- bustness and execution speed, which are important in processing large amounts of text. In another sense, however, development of a mod- ule like Nominator still requires considerable hu- man effort to discover reliable heuristics, particu- larly when only minimal information is used. These heuristics are somewhat domain dependent: dif- ferent generalizations hold for names of drugs and chemicals than those identified for names of people or organizations. In addition, as the heuristics de- pend on linguistic conventions, they are language dependent, and need updating when stylistic con- ventions change. Note, for example, the recent pop- ularity of software names which include exclamation points as part of the name. Because of these dif- ficulties, we believe that for the forseeable future, practical applications to discover new names in text will continue to require the sort of human effort in- vested in Nominator. References Agarwal R. and L. Boggess, 1992. A simple but useful approach to conjunct identification In Pro- ceedings of the 30th Annual Meeting of the ACL, pp.15-21, Newark, Delaware, June. Brill E. and P. Resnick, 1994. A rule-based ap- proach to prepositional phrase disambiguation, URL: http://xxx.lanl.gov/list/cmp.lg/9410026. Coates-Stephens S., 1993. The analysis and acquisi- tion of proper names for the understanding of free text, In Computers and the Humanities, Vol.26, pp.441-456. Cowie J. and W. Lehnert., 1996. Information Extraction In Communications of the ACM, Vol.39(1), pp.83-92. Cowie J., L. Guthrie, Y. Wilks, J. Pustejovsky and S. Waterman, 1992. Description of the Solomon System as used for MUC-4 In Proceedings of the Fourth Message Understanding Conference, pp.223-232. Jensen K. and Binot J-L, 1987. Disambiguating prepositional phrase attachments by using on-line definitions, In Computational Linguistics, Vol. 13, 3-4, pp.251-260. Hayes P., 1994. NameFinder: Software that finds names in text, In Proceedings of RIAO 94, pp.762-774, New York, October. Hindle D. and M. Rooth., 1993. Structural am- biguity and lexical relations, In Computational Linguistics, Vol.19, 1, pp.103-119. Mani I., T.R. Macmillan, S. Luperfoy, E.P. Lusher, and S.J. Laskowski, 1993. Identifying unknown proper names in newswire text. In B. Boguraev and J. Pustejovsky, eds., Corpus Processing for Lexical Acquisition, pp.41-54, MIT Press, Cam- bridge, Mass. McDonald D.D., 1993. Internal and external evi- dence in the identification and semantic catego- rization of proper names. In B. Boguraev and J. Pustejovsky, eds, Corpus Processing for Lexi- cal Acquisition, pp.61-76, MIT Press, Cambridge, Mass. NIST 1993. TIPSTER Information-Retrieval Text Research Collection, on CD-ROM, published by The National Institute of Standards and Technol- ogy, Gaithersburg, Maryland. Paik W., E.D. Liddy, E. Yu, and M. McKenna, 1993. Categorizing and standardizing proper nouns for efficient information retrieval, In B. Boguraev and J. Pustejovsky, eds, Corpus Processing for Lexi- cal Acquisition, pp.44-54, MIT Press, Cambridge, Mass. Quirk R., S. Greenbaum, G. Leech and J. Svar- tik, 1972. A Grammar of Contemporary English, Longman House, Harlow, U.K. Ravin Y. and N. Wacholder, 1996. Extracting Names from Natural-Language Text, IBM Re- search Report 20338. Wacholder N., Y. Ravin and R.J. Byrd, 1994. Re- trieving information from full text using linguis- tic knowledge, In Proceedings of the Fifteenth National Online Meeting, pp.441-447, New York, May."
  },
  {
    "title": "The Experience of Developing a Large-Scale Natural Language Text Processing System: Critique",
    "abstract": "This paper describes our experience in developing the CRITIQUE system. It describes three application areas in which the system is being used and discusses some characteristics of CRITIQUE which we believe are applicable to large-scale natural language systems in general: performance, robustness, flexibility, presentation, and accuracy.",
    "content": "Introduction CRITIQUE is a large-scale natural language text processing system that identifies grammar and style errors in English text. This advanced prototype, which is currently being developed at IBM Research, is based on a broad-coverage na- tural language parser (Richardson, 1985). The parser provides a unique approximate syntactic parse for a large percentage of English text and diagnoses over 100 grammar and style errors. Earlier writing-aid systems, such as Writer's Workbench (Macdonald, et al, 1982), contain functions which identify parts-of-speech of words, perform string-level phrase identification, and generate readability statistics. Similar func- tions are apparent in many systems now com- mercially available, some of which are described in an issue of The Seybold Report (1984). To our knowledge, however, no other system uses a parser that produces complete structural analyses for sentences. CRITIQUE is an extension of the EPISTLE project which began in 1980 (Heidorn, et al, 1982). The parser and grammar are implemented in PLNLP (the Programming Language for Na- tural Language Processing), developed by George Heidorn. PEG (the PLNLP English Grammar) has been written by Karen Jensen, and the style rules were written by Yael Ravin. Today, CRITIQUE is being tested in a variety of appli- cations ranging from office correspondence and technical documentation to student essays. Also, PLNLP and PEG have been incorporated into several other research applications, such as ma- chine translation systems. At the 1986 ACL meeting, Gary Hendrix de- scribed his experience in developing a natural language interface for real users (Hendrix, 1986). In contrast with user interface systems, we con- sider CRITIQUE to be a text processing system. The latter may be distinguished from the former by its broad coverage of texts that were prepared independently to communicate ideas and not strictly to interact with a computer system. Until now, the experience of developing a large-scale natural language text processing system has not been discussed in the literature. This paper first describes the overall process- ing in the CRITIQUE system. Then it describes three application areas in which the system is being used. The remaining sections discuss some characteristics of CRITIQUE which we believe are applicable to large-scale natural language systems in general: performance, robustness, flexibility, presentation, and accuracy. The dis- cussion draws on our experience in all three ap- plication areas. Processing in CRITIQUE CRITIQUE processes text in six steps. The first step determines sentence, heading, and par- agraph boundaries. In the next step, lexical processing identifies unrecognized words and awkward phrases. The on-line dictionary which is used includes more than 100,000 entries and provides information used in syntactic processing as well. After lexical analysis, text is passed to the parser, which produces a parse tree, and in so doing checks for grammar errors. Then stylistic analysis diagnoses potential style prob- lems. CRITIQUE also generates statistical in- formation about documents based on the lexical and syntactic analyses. The final step involves error summarization and display. CRITIQUE has an interactive processing mode that is fully integrated with a text editor, allowing users to update the text as needed. As the text is modified, new sentences are re- analyzed to ensure that no new errors have been introduced. The system provides three levels of on-line help: the first level identifies the error, the second provides a brief explanation, and the third provides a complete tutorial. Figure 1 is an il- lustration of the second level of help. The user can also specify style preferences in an individual profile. Possible errors are filtered through the profile to determine whether or not they should be displayed. Hard-copy output is also available. I am writing to recommend Susan Hayes, who's application you recently received. Confusion of \"who's\" and \"whose\" whose The word \"who's\" (which means \"who is\") and the word \"whose\" (which is possessive) cannot be interchanged. Figure 1. Second level of Help includes name of error, suggested correction, and a brief explanation Application Areas for CRITIQUE During the development of CRITIQUE, we have directed our efforts towards three major application areas: office environments, publica- tions organizations, and educational institutions. Each area has its own particular needs and re- quirements. In the office environment, professionals re- quire quick, succinct feedback on their memos and other documents. They are less interested in maintaining a particular style, but want insurance against obvious grammatical and spelling mis- takes. Our parsing grammar was originally de- veloped using a data base of office correspondence. There has also been an abun- dance of feedback at IBM Research, where the system has been made available to hundreds of users. These users submitted over 3,000 pages of text to CRITIQUE in 1987. Publications organizations usually have strict requirements for style and consistency which ex- ist in the form of tedious style guides. The pro- fessional writers in such organizations also want succinct feedback, but are usually willing to wait longer to receive it, since their documents are typically longer and more involved. An IBM technical writing group and the US government have been our source of experience and feedback in this area. Use by educational institutions has proven to be the most challenging of the three areas. There is a wide range of ill-formed text to deal with, originating from classes in composition, business writing, technical writing, and ESI (English as a Second Language). The professors in these vari- ous arcas also sometimes have differing opinions on grammar and style. Although there may not be such a great need for quick processing time (except by those students who procrastinate), processing cost must be minimized to fit most university budgets. We currently are doing joint studies with three universities to help test and refine CRITIQUE. Performance Broad-coverage natural language processing is computationally expensive. To do it in real time is even more so. Whereas large offices and publications organizations may be able to afford extensive computing power, such is not the case in many of the environments where a system such as CRITIQUE would be most useful. Although CRITIQUE has been developed in a large IBM-mainframe environment, several significant steps have been taken to improve its performance with a view toward running on much smaller machines. In addition, a version of the PLNLP parser on which CRITIQUE is based was successfully ported to an IBM PC in the summer of 1986. Work is continuing on other versions which would run the complete PLNLP English Grammar (PEG) on intelligent work- stations such as the IBM RT PC and PS/2. We have used two complimentary approaches to achieve satisfactory performance. One is to distribute the parts of the system which can run in parallel over multiple processors (where avail- able), and the other is to optimize the perform- ance of the programs themselves. To distribute the processing involved, we have used \"parsing server\" programs which may operate either on the same physical computer, or on several computers connected by a network. When CRITIQUE is invoked by a user, each sentence in the user's document is sent as a sep- arate task to a \"manager server\" which then dis- tributes such tasks to as many parsing servers as are available. After analysis, information about a sentence is returned via the manager server to the user's editing environment. With this scheme, multiple users can access multiple parsing servers that may reside on different computers linked by a network. In this way, several of a user's sen- tences may be processed in parallel and asyn- chronously with respect to other tasks (such as word processing) that the user may be doing. Although this distributed processing system is currently implemented on a network of mainframes, the transition to a workstation- based network like those found in small busi- nesses and university environments will not be difficult. The distributed architecture is also well suited to exploit the power of parallel processor machines currently under development. The granularity of the processing involved, which is now at the sentence level, may also be made smaller or larger, depending on resulting effi- ciency and the possible need to consider larger segments of text for a more complete analysis. The parsing servers referred to above consist of the PLNLP parsing engine, the PLNLP Eng- lish Grammar, and a large set of style rules. PLNLP supports the writing of procedures as well as rules. Consequently, the parsing engine itself is written as a set of PLNLP procedures. In addition to the run-time environment, the trans- lator for the PLNLP language is also written us- ing PLNLP. When an entire programming language system such as this is written in itself, a high degree of portability and language-specific optimization may be achieved, further enhancing overall system performance. The PLNLP translator currently turns PLNLP rules and procedures into LISP or PL.8 (a highly-optimized PL/I variant) code which is then compiled and executed. Work has also been done using C as a base (for the PC version men- tioned earlier), and this work will be extended for portability across computers. Direct compilation into machine code is also being considered. In our experience with various programming languages and environments, we have found it desirable to maintain two versions of the system, which share the same PLNLP source code. One is geared toward grammar and style rule devel- opment, being somewhat slower, but very flexi- ble, and containing a set of specially designed tools and development aids. This version of the system now runs in LISP. The other version, running in PL.8, is optimized for fast execution and is about ten times faster than the develop- ment version. CRITIQUE uses the PL.8 ver- sion, which can analyze a sentence of about 15-20 words in one CPU second on an IBM 3081 computer. This translates into a few sec- onds of elapsed time under an average load. Even as computers become more powerful, there will continue to be a corresponding increase in the complexity and amount of computation involved in natural language processing. Through use of a highly-optimized production run-time environment, PLNLP is able to achieve the re- quired performance without sacrificing flexibility during development. One last performance issue should be men- tioned: the need for a well-integrated dictionary system. As previously stated, CRITIQUE's dic- tionary is able to recognize well over 100,000 words, providing both morphological and syn- tactic information about those words. The trade-offs between keeping the dictionary on disk or in memory are more significant in a very large-scale system. Disk I/O's, including \"hid- den\" paging I/O's when the dictionary is in vir- tual memory, must be carefully considered and minimized. It has been our experience that ex- pensive dynamic morphological processing should also be kept to a minimum, although this may not be possible for other languages. Robustness Any computer system should be robust. This is especially true of natural language systems, and, in particular, those which specialize in han- dling ill-formed input. Robustness should be considered at every level of processing, both for the system in general and for the particulars of dealing with natural language inputs. At the system level, the distributed architec- ture which is used by CRITIQUE for perform- ance reasons requires robust task management mechanisms. The manager server carefully tracks the progress of each task (sentence) and the availability of parser servers on the network. If a parser loses its network connection, exceeds a predetermined time limit, or otherwise fails while processing a task, that task is sent out again to another parser. If a parser fails while processing a task, it automatically restarts itself. Statistics concerning usage and task flow, as well as com- ments recorded by users about the usefulness or accuracy of critique information, are maintained by the manager server and automatically distrib- uted to system developers each day. At the natural language level, robustness first comes into play in handling the various formats of text inputted to the system. Text which has been \"manually\" formatted (using an editor, \"WYSIWYG\" style), as well as text with imbed- ded formatting commands (IBM's SCRIPT and GML commands are currently supported) is scanned by CRITIQUE to identify \"parsable\" segments. This process excludes tables, figures, headings, addresses, etc., and is table driven to accommodate the varying requirements of users in the different application areas. Publications organizations, for example, typically have special additional sets of formatting commands that must be supported. During parsing, words which are not in the dictionary are assigned default morphological and syntactic information so as to avoid a parsing failure. Most such words are generally assumed to be singular nouns, although there are some exceptions. This is usually adequate to obtain a reasonable parse, but can cause problems when it is a verb that is misspelled. Parsing may take place in one pass or two, if necessary. The first pass applies the rules of the grammar with all of the constraints in force. If a parse is not obtained, then a second pass is made, applying the rules with selected constraints being relaxed. Certain lexical substitution rules for easily confused words (e.g., whose/who's, its/it's) are also activated during the second pass. If a porse is still not obtained after the second pass, whether because of an unanticipated error, an unrecognized word, or a possible weakness in the grammar, then the \"parse fitting\" procedure is invoked (Jensen, et al, 1984). This procedure relies on the fact that the parsing algorithm is bottom-up in nature, and therefore intermediate well-formed parse structures are produced for segments of the sentence. These structures may be \"fitted\" together to form a parse for the sen- tence if no other complete structure is found. Even when a fitted parse is obtained, grammar and style error detection is still active within the successfully parsed segments. If multiple parses are obtained, the system selects one based on a parse metric which favors trees in which modifying words and phrases are attached to the closest qualifying constituent (Heidorn, 1982). If the number of parses ob- tained exceeds a certain threshold, CRITIQUE takes advantage of the situation and informs the user that the sentence is probably unclear. If the parser fails for some system reason, the user will receive a message that the segment of text in question was \"too difficult to process.\" No one can foresee all the errors that humans can make. It is for this reason that we have in- cluded these robust mechanisms, and that we continue to enhance the system to catch new er- rors as experience and feedback dictate. Flexibility By virtue of the significantly different needs of each application area listed earlier, flexibility has been a requirement throughout the develop- ment of CRITIQUE. For example, the publi- cation organizations we have dealt with have required large additions of terminology, the han- dling of special input formats and formatting commands, and additional style critiques dictated by organizational style guidelines. Universities, being pedagogically oriented, have been very much concerned with the format and content of the critique information presented in the output. We have attempted to handle the need for this flexibility at the individual, installation, and ap- plication area levels. The basic CRITIQUE system provides pre- determined critiques, intuitively organized into groups, with default thresholds, if applicable, and general help and tutorial information. It handles the formats of files by default according to certain file naming conventions. The vocabulary in the dictionary comes mainly from Webster's 7th Collegiate dictionary, and the grammar and style error rules have been developed according to se- veral widely accepted sources. Every item of in- formation produced by the system is controlled by a switch or threshold contained in a user profile. We have found that a good set of de- faults in this profile is indispensable, since most users often do not bother to change them. Individuals who use the system are free to change any of the settings in the profile according to their own tastes and needs. They may also add words to an addendum which is used solely for the purpose of checking spelling. Knowledgeable users, or, more commonly, installation administrators, may change the de- fault settings in the system profile or create se- veral profiles for different purposes. Such would be the case for university classes of different types or various publications groups, each with its own particular style requirements. This level of customization also includes changing the group- ing of critiques and the associated code (used by the system to flag the occurrence of an error in the output), message, help, and tutorial informa- tion, and making large additions of specialized terminology to the system dictionary. New classes of word- and phrase-level errors may be added to the dictionary as well. Users at some of our test sites have requested the ability to add classes of style errors. This is not currently possible, because they would have to be able to write their own PLNLP rules. For now, further types of customization, for entire application areas, for example, are performed by the system developers, although there is contin- ual re-evaluation of where to draw the line. It is important to point out that the kinds of system customization described above have not, thus far, included tuning the grammar for special handling of the texts common in a particular ap- plication area. Every effort has been made to keep PEG as broad-coverage as possible. In fact, there has been a tendency during the develop- ment of CRITIQUE to move certain types of error detection, where possible, from the gram- mar to the style rule component. Since style rules are applied only after a parse has been obtained by the grammar rules, this lessens the possibility that testing for an error will interfere with gram- mar rule processing. Presentation Systems such as CRITIQUE are generally used to process texts which have been prepared for a human audience often using word process- ing software. Therefore it seems natural, perhaps even necessary, that these systems be tightly in- tegrated with a word processing environment. The CRITIQUE system architecture, which has been described previously from a distributed processing standpoint, may also be viewed as in- corporating a word processing environment as a user interface, with a background natural lan- guage processor. There is nothing in the CRITIQUE system interface that requires that what the parser servers return be grammatical and stylistic information. The \"descriptors\" produced by the parsers are general in nature and could be used to send back any kind of informa- tion, possibly including a content characteriza- tion for information retrieval purposes or even a translation into another language. In this way, the system may be considered as a general pur- pose natural language processing environment. With respect to the presentation of critique information in this integrated environment, the differing needs of the application areas have been evident once again. Several lessons in human factors have been learned and the results imple- mented. In a prior version of CRITIQUE, problems were simply underlined on the screen, and users were required to point to a particular problem and request that a window be opened which contained a description of what was wrong. As a result of studying the usage statistics gathered by the manager server at IBM Research, we de- termined that users were not asking for the de- scriptions of errors. Instead, they seemed to rely on their intuitions, only making use of the fact that CRITIQUE had flagged a particular word or phrase. This led us to replace the underlining with a brief, highlighted code word or phrase which indicates what the problem is. In cases where CRITIQUE suggests a corrected form of a word, that form is now used as the error indi- cator. This new format for displaying errors is shown in Figure 2. Lets contemplate how a president is selected. *Let's In many cases the best candidate in the eyes of the public is the one who has the most exposure. This is no way to chose a president, but *choose unfortunately it is often true. The total package of a candidates political ideas don't really make *doesn't an impression on the public. His appearance \\FRAGMENT and mannerisms and the amount of exposure that make him successful. Figure 2. Example of errors flagged by CRITIQUE From this experience and other similar ones, we concluded that professionals using CRITIQUE in an office environment preferred a quick, interactive review of memos and docu- ments. The amount of feedback on the screen at any one time should be maximized, and the number of keystrokes and overall review time thereby minimized. Publications organizations have proved simi- lar in many respects. However, due to the length and complexity of documents produced in such organizations, users may be more willing to wait for their output, and often make use of overnight batch runs. One feature of CRITIQUE that has proved useful in this respect is called \"interactive review.\" It is based on the fact that the system saves all of the information produced about a given file on disk at the end of a session or run. This in- formation is then read the next time the same document is processed, thereby eliminating the need to reprocess sentences that have not changed. This means that it is possible for very large files to be run overnight, and then be re- viewed interactively the following day, thereby lessening the impact on prime shift computer usage. Publications groups, through their occasional use of sub-contractors that do not have access to on-line information provided part of the moti- vation to optionally produce printed output which is almost identical to what is viewed on the screen. They also required the flexibility of easily integrating the information contained in an or- ganizational style guide with the interactive tuto- rials for each critique. The universities we are working with consid- ered the abbreviated presentation of critique in- formation we developed to be appropriate for their advanced students, but inadequate for oth- ers. They want the ability to lengthen explana- tions where desired and to group critiques by type, only presenting certain types in the output at any one time. Our varied experiences in these application areas have resulted in highly flexible, table-driven presentation modes for both batch and interac- tive output. We continue to experiment and make changes based on feedback. Accuracy Accuracy is perhaps the most important as- pect of a natural language system's overall per- formance. It may be evaluated from two perspectives: the actual \"under-the-covers\" na- tural language processing involved, and the user's perception. Given the state of the art, we may consider it a blessing that it is possible for the latter to be somewhat better than the former. From a processing perspective in CRITIQUE, we reiterate that the PLNLP Eng- lish Grammar produces parses which are ap- proximate. Without recourse to semantics we cannot hope for much better. However, we are quite pleased with the coverage and accuracy that we have obtained, and find them to be adequate for the requirements of a system like CRITIQUE. The semantic ambiguities and in- accuracies which remain in the parses have not been a stumbling block to the usefulness of the system. This demonstrates that some degree of inaccuracy at the natural language processing level can be acceptable as long as it is not readily visible to the user. We do not pretend to be completely satisfied with this situation, however, and we are doing research in the area of \"dictionary-based\" semantic analysis. This will enable us to improve some of the attachments in the parse trees produced by PEG (Binot and Jensen 1987) Even being able to deal with a wide range of ill-formed input, it cannot be expected that a parser without a sophisticated semantic compo- nent can successfully parse \"gobbledegook.\" In the goal to produce a useful and accurate analysis of text, there must also be an assumption in- cluded about the maximum degree of ill- formedness that can be handled. The sentence given below in Figure 3, which was taken from a real student essay, illustrates the kind of ill- formedness which challenges CRITIQUE to its limits. The system did point out the comma splice in this sentence, but nothing else. \"He starts to condemn Nora for her mistake and made as if she is like poison that can be contagious, Trovald was ready to take away the kids and kick Nora out as an outcast as how they did with Mr. Krogstad.\" Figure 3. An ill-formed sentence from a student essay In discussing the robustness of the parser, it was pointed out that error detection is still per- formed within the successfully processed seg- ments of a fitted parse. Our testing to this point indicates that critiques produced in such situ- ations are about as accurate as those produced in non-fitted parses. This is another case where the user's perception may differ from the underlying performance of the system. In general, however, we have found users' perceptions and feedback to be most helpful. The facility that CRITIQUE provides for giving feedback allows users to classify advice provided by the system according to the categories correct, useful, missed, and wrong. These cate- gories are self-explanatory except, perhaps, for the useful category. This refers to the case where a critique is not exactly correct; but, since the user's attention is drawn to a particular phrase or sentence, a real problem is noticed. We tend to include these kinds of critiques with those that are correct in evaluating the usefulness and accu- racy of the system. The most undesirable critiques are those in the wrong category, as they tend to destroy user confidence in the system and are not well toler- ated in educational environments. We have found however that professionals seem much more forgiving of wrong critiques, as long as the time required to disregard them is minimal. This is similar to using spelling checkers, which wrongly highlight many proper names, acro- nyms, etc., but are still considered quite useful. In order to analyze CRITIQUE's current ac- curacy in an educational environment, we re- cently processed a number of student essays provided by the computer-aided writing program at Colorado State University. We randomly se- lected 10 essays from each of four groups: fresh- man composition, business writing, ESL (English as a Second Language), and professional writing. The diagnoses made by CRITIQUE in these es- says were reviewed and classified according to whether they were correct, useful, or wrong. We did not consider errors that were missed, but simply concentrated on the correctness of the critiques actually provided by the system. The reason for this orientation was our concern with the potentially damaging effect of wrong advice. We adjusted the analysis in both directions, in a manner that we believe is fair. On the one hand, we did not count correct critiques of a trivial or mechanical nature, such as misspelled words, superficial punctuation checks, or read- ability scores. On the other hand, we also did not include a particular class of incorrect comma critiques, the handling of which we need to im- prove. All other non-trivial critiques generated by the system were counted. The results are shown in Table 1. Group Fresh Bus ESL Prof # of Essays 10 10 10 10 # of Sentences 108 116 110 401 Avg. Words per Sentence 16 18 21 22 # of Different Critiques 20 9 23 32 # of Critiques (Total) 36 11 63 158 % Correct 72% 73% 54% 39% % Correct and Useful 86% 82% 87% 41% Table 1. Summary of accuracy for non-trivial critiques The analysis confirmed feedback we have re- ceived from users at IBM Research that CRITIQUE is most helpful on straightforward texts before they are significantly revised. The more polished and almost literary style of the professional essays challenged CRITIQUE's ability to provide generally useful advice. The ESL texts, written by native Arabic, Chinese, and Spanish speakers, were also difficult, containing a large percentage of very ill-formed sentences. This is indicated by the higher number of useful critiques for this group, although it could be ar- gued that these critiques may not be as useful to users who lack native intuitions about English. For the ESL group, correcting spelling errors first resulted in significantly better grammar-checking performance. This was not true for the other groups. In general, CRITIQUE also appears to be more accurate on texts with a shorter average sentence length. Conclusions Based on real experience in the application areas of office environments, publications organ- izations and educational institutions, CRITIQUE has been developed to a level of apparent usefulness. Acceptable system per- formance has been achieved through the use of distributed and optimized processing. The sys- tem has achieved a high level of robustness and flexibility in most of its aspects, including pres- entation. The accuracy of the system is currently acceptable for many types of texts and environ- ments, and accuracy continues to improve with exposure in each of the three application areas. CRITIQUE exemplifies a framework for the de- velopment of broad-coverage, large-scale natural language text processing systems. Acknowledgements We would like to thank Professor Charles Smith and his colleagues from Colorado State University, who provided the student essays used in the analysis of accuracy, as well as valuable feedback about CRITIQUE. We also express sincere thanks to George Heidorn for his com- ments and guidance in the preparation of this paper. References Binot, Jean-Louis and Karen Jensen. 1987. \"A semantic expert using an online standard dic- tionary.\" Proceedings of IJCAI-87, Milan, Italy, August 1987. Heidorn, George E. 1982. \"Experience with an Easily Computed Metric for Ranking Alter- native Parses.\" Proceedings of the 20th Annual Meeting of the Association for Computational Linguistics, Toronto, Canada, June 1982. Heidorn, George E., Karen Jensen, Lance Miller, Roy Byrd, and Martin Chodorow. 1982. \"The EPISTLE Text-Critiquing System.\" IBM Systems Journal, 21, 3, 1982. Hendrix, Gary. 1986. \"Bringing Natural Lan- guage Processing to the Micro-Computer Mar- ket: The Story of Q&A.\" Proceedings of the 24th Annual Meeting of the Association for Computa- tional Linguistics, New York, New York, June 1986. Jensen, Karen, George Heidorn, Lance Miller, and Yael Ravin. 1984. \"Parse Fitting and Prose Fixing: Getting a Hold on Ill-formedness.\" American Journal of Computational Linguistics, 9, 3-4, 1984. Macdonald, Nina H., L.T. Frase, P. Gingrich, and S.A. Keenan. 1982. \"The WRIT- ER'S WORKBENCH: Computer aids for text analysis,\" IEEE Transactions on Communication (Special Issue on Communication in the Auto- mated Office), 30, 1982. Richardson, Stephen D. 1985. \"Enhanced Text-Critiquing using a Natural Language Parser.\" Proceedings of the Seventh International Conference on Computers and the Humanities, Provo, Utah, June 1985. Seybold Publications, Inc. 1984. \"Computer Aids for Authors and Editors.\" The Seybold Re- port on Publishing Systems, 13, 10, 1984."
  },
  {
    "title": "A Probabilistic Genre-Independent Model of Pronominalization",
    "abstract": "Our aim in this paper is to identify genre-independent factors that influence the decision to pronominalize. Results based on the annotation of twelve texts from four genres show that only a few factors have a strong influence on pronominalization across genres, i.e., distance from last mention, agreement, and form of the antecedent. Finally, we describe a probabilistic model of pronominalization derived from our data.",
    "content": "1 Introduction Generating adequate referring expressions is an ac- tive research topic in Natural Language Generation. Adequate referring expressions are those that en- able the user to quickly and unambiguously identify the discourse entity that the expression co-specifies with. In this paper, we concentrate on an important aspect of that question, which has received less at- tention than the question of anaphora resolution in discourse interpretation, i.e., when is it feasible to pronominalize? Our aim is to identify the central factors that in- fluence pronominalization across genres. Section 2 motivates and presents the factors that were investi- gated in this study: distance from last mention, par- allelism, ambiguity, syntactic function, agreement, sortal class, syntactic function of the antecedent and form of the antecedent. Our analyses are based on a corpus of twelve texts from four different genres with a total of more than 24,000 words and 7126 referring expressions (Section 3). The results of the statistical analyses are summarized in Section 4. There are strong statistical associations between each of the factors and pronominalization. Only when we combine them into a probabilistic model we can identify those factors whose contribution is really important, i.e. distance from last mention, agreement, and to a certain degree form of the an- tecedent. Since these factors can be annotated rel- atively cheaply, we conclude that it is possible to develop reasonable statistical pronominalization al- gorithms. 2 Factors in Pronoun Generation 2.1 Previous Work Lately, a number of researchers have done corpus- based work on NP generation and pronoun resolu- tion, and a number of studies have found differences in the frequency of both personal and demonstrative pronouns across genres. However, none of these studies compares the influence of different factors on pronoun generation across genres. Recently, Poesio et al. (1999) have described a corpus-based approach to statistical NP generation. While they ask the same question as previous re- searchers (e.g. Dale (1992)), their methods differ from traditional work on NP generation. Poesio et al. (1999) use two kinds of factors: (1) factors related to the NP under consideration such as agree- ment information, semantic factors, and discourse factors, and (2) factors related to the antecedent, such as animacy, clause type, thematic role, proxim- ity, etc. Poesio et al. (1999) report that they were not able to annotate many of these factors reliably. On the basis of these annotations, they constructed de- cision trees for predicting surface forms of referring expressions based on these factors - with good re- sults: all 28 personal pronouns in their corpus were generated correctly. Unfortunately, they do not eval- uate the contribution of each of these factors, so we do not know which ones are important. Work on corpus-based approaches to anaphora resolution is more numerous. Ge et al. (1998) describe a supervised probabilistic pronoun resolu- tion algorithm which is based on complete syntac- tic information. The factors they use include dis- tance from last mention, syntactic function and con- text, agreement information, animacy of the refer- ent, a simplified notion of selectional restrictions, Agree Syn Class SynAnte FormAnte Dist Dist4 Par Ambig Agreement in person, gender, and number Syntactic function Sortal Class (cf. Tab. 2) Syntactic function of antecedent. \"F\" for first mention, \"N\" for deadend Form of antecedent (pers. pron., poss. pron., def. NP, indef. NP, proper name) Distance to last mention in units Dist reduced to 4 values (deadend, Dist=0, Dist=1, Dist>=2) Parallelism (Syn=SynAnte) Number of competing discourse entities Table 1: Overview of factors and the length of the coreference chain. Cardie & Wagstaff (1999) describe an unsupervised algorithm for noun phrase coreference resolution. Their fac- tors are taken from Ge et al. (1998), with two exceptions. First, they replace complete syntactic infor- mation with information about NP bracketing. Sec- ond, they use the sortal class of the referent which they determine on the basis of WordNet (Fellbaum, 1998). There has been no comparison between corpus- based approaches for anaphora resolution and more traditional algorithms based on focusing (Sidner, 1983) or centering (Grosz et al., 1995) except for Azzam et al. (1998). However, their comparison is flawed by evaluating a syntax-based focus algo- rithm on the basis of insufficient syntactic informa- tion. For pronoun generation, the original centering model (Grosz et al., 1995) provides a rule which is supposed to decide whether a referring expression has to be realized as a pronoun. However, this rule applies only to the referring expression which is the backward-looking center (Cb) of the current utterance. With respect to all other referring expression in this utterance centering is underspecified. Yeh & Mellish (1997) propose a set of handcrafted rules for the generation of anaphora (zero and personal pronouns, full NPs) in Chinese. However, the factors which appear to be important in their evaluation are similar to factors described by authors mentioned above: distance, syntactic constraints on zero pronouns, discourse structure, salience and animacy of discourse entities. 2.2 Our Factors The factors we investigate in this paper only rely on annotations of NPs and their co-specification relations. We did not add any discourse structural anno- tation, because (1) the texts are extracts from larger texts which are not available to us, and (2) we have not yet found a labelling scheme for discourse structure that has an inter-coder reliability comparable to the MUC coreference annotation scheme. Based on our review of the literature and relevant work in linguistics (for sortal class, mainly Fraurud (1996) and Fellbaum (1998)), we have chosen the nine factors listed in Table 1. Methodologically, we distinguish two kinds of factors: NP-level factors are independent from co- specification relations. They depend on the semantics of the discourse entity or on discourse information supplied for the NP generation algorithm by the NLG system. Typical examples are NP agreement by gender, number, person and case, the syntactic function of the NP (subject, object, PP adjunct, other), the sortal class of the discourse entity to which an NP refers, discourse structure, or topicality of the discourse entities. In this paper, we focus on the first three factors, agreement (Agree), syntactic function (Syn), and sortal class (Class). Since we are using syntactically annotated data in the Penn Treebank-II format, the syntactic function of an NP was derived from these annotations. Agreement for gender, number, and person was labelled by hand. Since English has almost no nominal case morphemes, case was not annotated. Sortal classes provide information about the discourse entity that a referring expression evokes or accesses. The classes, summarized in Table 2, were derived from EuroWordNet BaseTypes (Vossen, 1998) and are defined extensionally on the basis of WordNet synsets. Their selection was motivated by two main considerations: all classes should occur in all genres, and the number of classes should be as small as possible in order to avoid problems with sparse data. Four classes, State, Event, Action, and Property, cover different types of situations, two cover spatiotemporal characteristics of situations (Loc/Time). The four remaining classes cover the two dimensions \"concrete vs. abstract (Concept)\" and \"human (Pers) vs. non-human (PhysObj) vs. institutionalised groups of humans (Group)\". Since we are only interested in the decision whether to employ pronouns rather than full NPs and less in the form of the NP itself, and since our methodology is based on corpus annotation, we did not take into account more formal semantic categories such as kinds vs. individuals. Co-specification-level factors depend on information about sequences of referring expressions Person one or more human beings Group institutionalized group of human beings PhysObj physical object Concept abstract concept Loc geographical location Time date, time span Event sth. which takes place in space and time Action sth. which is done State state of affairs, feeling,... Property characteristic or attribute of sth. Table 2: Overview of Sortal Classes with rough characterizations of relevant synsets which co-specify with each other. Such a sequence consists of all referring expressions that evoke or ac- cess the same discourse entity. In this paper, we use the following factors from the literature: distance to last mention (Dist and Dist4), ambiguity (Am- big), parallelism (Par), form of the antecedent (For- mAnte), and syntactic function of the antecedent (SynAnte). We also distinguish between discourse entities that are only evoked once, deadend entities, and entities that are accessed repeatedly. Parallelism is defined on the basis of syntactic function: a referring expression and its antecedent are parallel if they have the same syntactic function. For calculating distance and ambiguity, we seg- mented the texts into major clause units (MCUs). Each MCU consists of a major clause C plus any subordinate clauses and any coordinated major clauses whose subject is the same as that of C and where that subject has been elided. Dist provides the number of MCUs between the current and the last previous mention of a discourse entity. When an entity is evoked for the first time, Dist is set to \"D\". Dist4 is derived from Dist by as- signing the fixed distance 2 to all referring expres- sions whose antecedent is more than 1 MCU away. Ambiguity is defined as the number of all discourse entities with the same agreement features that occur in the previous unit or in the same unit before the current referring expression. 3 Data Our data consisted of twelve (plus two) texts from the Brown corpus and the corresponding part-of- speech and syntactic annotations from the Penn Treebank (LDC, 1995). The texts were selected because they contained relatively little or no direct speech; segments of direct speech pose problems for both pronoun resolution and generation because of the change in point of view. Morpho-syntactic in- formation such as markables, part-of-speech labels, grammatical role labels, and form of referring ex- pression were automatically extracted from the ex- isting Treebank annotations. The texts come from four different genres: Popu- lar Lore (CF), Belles Lettres (CG), Fiction/General (CK), and Fiction/Mystery (CL). The choice of genres was dictated by the availability of detailed Treebank-II parses. Table 3 shows that the distri- bution of referring expressions differs considerably between genres. The texts from the two non-narrative types, CF and CG, contain far more discourse entities and far less pronouns than the narrative genres CK and CL. The high number of pronouns in CK and CL is partly due to the fact that in one text from each genre, we have a first person singular narrator. CK patterns with CF and CG in the average number of MCUs; the sentences in the sample from mys- tery fiction are shorter and arguably less complex. CL also has disproportionally few deadend refer- ents. The high percentage of deadend referents in CK is due to the fact that two of the texts deal with relationship between two people. These four dis- course referents account for the 4 longest corefer- ence chains in CK (85, 96, 109, and 127 mentions). Two annotators (the authors, both trained lin- guists), hand-labeled the texts with co-specification information based on the specifications for the Mes- sage Understanding Coreference task (Hirschman & Chinchor (1997); for theoretical reasons, we did not mark reflexive pronouns and appositives as co- specifying). The MCUs were labelled by the sec- ond author. All referring expressions were anno- tated with agreement and sortal class information. Labels were placed using the GUI-based annotation tool REFEREE (DeCristofaro et al., 1999). The annotators developed the Sortal Class anno- tation guidelines on the basis of two training texts. Then, both labellers annotated two texts from each genre independently (eight in total). These eight texts were used to determine the reliability of the sortal class coding scheme. Since sortal class an- notation is intrinsically hard, the annotators looked up the senses of the head noun of each referring NP that was not a pronoun or a proper name in Word- Net. Each sense was mapped directly to one or more of the ten classes given in Table 2. The annotators then chose the adequate sense. The reliability of the annotations were measured Genre words ref. expr. entities sequ. MCUs % pron. % deadend med. len. CF 6097 1725 1223 125 304 19.59% (1.8%, 0.3%, 58.3%) 89.78% 3 CG 6103 1707 1290 120 269 16.17% (9.8%, 1.1%, 4%) 90.70% 2 CK 6020 1848 1071 113 386 36.15% (19.5%, 1.2%, 56.1%) 89.45% 2 CL 6018 1846 954 170 477 35.64% (14.0%, 1.5%, 53.6%) 80.09% 4 Table 3: Relevant quantitative characteristics of the texts. Average length: 2020 words, 120 MCUs. sequ.: number of sequences of co-specifying referring expressions. % deadend: percentage of discourse entities mentioned only once. % pronouns: percentage of all referring expressions realized as pronouns, in brackets: perc. of first person singular pronouns, perc. of second person singular pronouns, perc. of third person singular masculine and feminine pronouns. med. len.: median length of sequences of co-specifying referring expressions with Cohen's κ (Cohen, 1960; Carletta, 1996). Co- hen (1960) shows that a κ between 0.68 and 0.80 al- lows tentative conclusions, while κ > 0.80 indicates reliable annotations. For genres CF (κ = 0.83), CK (κ = 0.84) and CL (κ = 0.83), the sortal class an- notations were indeed reliable, but not for genre CG (κ = 0.63). Nevertheless, overall, the sortal class annotations were reliable (κ = 0.8). Problems are mainly due to the abstract classes Concept, Action, Event, State, and Property. Abstract head nouns sometimes have several senses that fit the context almost equally well, but that lead to different sor- tal classes. Another problem is metaphorical usage. This explains the bad results for CG, which features many abstract discourse entities. 4 Towards a Probabilistic Genre-Independent Model In this section, we investigate to what extent the fac- tors proposed in section 2.2 influence the decision to prominalize. For the purpose of the statistical analy- sis, pronominalization is modelled by a feature Pro. For a given referring expression, that feature has the value \"P\" if the referring expression is a personal or a possessive pronoun, else \"N\". We model this variable with a binomial distribution. 1 4.1 How do the Factors Affect Pronominalization? First, we examine for all nine factors if there is a statistical association between these factors and Pro. Standard non-parametric tests show a strong associ- ation between all nine factors and Pro.2 This holds ¹For all statistical calculations and for the logistic regres- sion analyses reported below, we used R (Ihaka & Gentleman, 1996). ²We used the Kruskal-Wallis test for the ordinal Ambig variable and the χ²-test for the other, nominal, variables. Since first mentions and deadends are coded by the character \"D\" in both for all referring expressions and for those that occur in sequences of co-specifying referring ex- pressions. All of the tests were significant at the p < 0.001-level, with the exception of Par: for ex- pressions that are part of co-specification sequences the effect of that factor is not significant. In the next analysis step, we determine which of the feature values are associated disproportionally often with pronouns, and which values tend to be associated with full NPs. More specifically, we test for each feature-value pair if the pronominalization probability is significantly higher or lower than that computed over (a) the complete data set, (b) all re- ferring expressions in sequences of co-specifying referring expressions, (c) all third person referring expressions in sequences. Almost all feature values show highly significant effects for (a) and (b), but some of these effects vanish in condition (c). Be- low, we report on associations which are significant at p < 0.001 under all three conditions. Unsurprisingly, there is a strong effect of agree- ment values: NPs referring to the first and second person are always pronominalized, and third person masculine or feminine NPs, which can refer to per- sons, are pronominalized more frequently than third person neuter and third person plural. Pronouns are strongly preferred if the distance to the antecedent is 0 or 1 MCUs. Referring expressions are more likely to be pronominalized in subject position than as a PP adjunct, and referring expressions with adjuncts as antecedents are also pronominalized less often than those with antecedents in subject or object po- sition. There is a clear preference for pronouns as possessive determiners, and referring expressions that co-specify with an antecedent possessive pro- noun are highly likely to be pronominalised. We both Dist and Dist4, both are treated as a categorical variable by R. For more on these tests, see (Agresti, 1990). also notice strong genre-independent effects of par- allelism. Although at first glance, Ambig appears to have a significant effect as well, (median ambiguity for nouns is 3, median ambiguity for pronouns 0), closer inspection reveals that this is mainly due to first and second person and third person masculine and feminine pronouns. The sortal classes show a number of interest- ing patterns (cf. Table 4). Not only do the classes differ in the percentage of deadend entities, there are also marked differences in pronominalizabil- ity. There appear to be three groups of sortal classes: Person/Group, with the lowest rate of dead- end entities and the highest percentage of pro- nouns — not only due to the first and second per- son personal pronouns — , Location/PhysObj, with roughly two thirds of all entities not in sequences and a significantly lower pronominalization rate, and Concept/Action/Event/Property/State/Concept, with over 80% deadend entities. Within this group, Action, Event, and Concept are pronominalized more frequently than State and Property. Time is the least frequently pronominalized class. An impor- tant reason for the difference between Loc and Time might be that Times are almost always referred back to by temporal adverbs, while locations, especially towns and countries, can also be accessed via third person neuter personal pronouns. Interactions between the factors and genre were examined by an analysis of deviance run on a fit- ted logistic regression model; significance was cal- culated using the F-test. All factors except for Par show strong (p < 0.001) interactions with Genre. In other words, the influence of all factors but paral- lelism on pronominalization is mediated by Genre. There are two main reasons for this effect: first, some genres contain far more first and second per- son personal pronouns, which adds to the weight of Agree, and second, texts which are about persons and the actions of persons, such as the texts in CK and CL, tend to use more pronouns than texts which are mainly argumentative or expository. 4.2 Which Factors are Important? To separate the important from the unimportant fac- tors, many researchers use decision and regression trees, mostly the binary CART variant (Breiman et al., 1984). We use a different kind of model here, logistic regression, which is especially well suited for categorical data analysis (cf. eg. Agresti (1990) or Kessler et al. (1997)). In this model, the value of the binary target variable is predicted by a lin- ear combination of the predictor variables. Vari- able weights indicate the importance of a variable for classification: the higher the absolute value of the weight, the more important it is. Logistic regression models are not only evaluated by their performance on training and test data. We could easily construct a perfect model of any train- ing data set with n variables, where n is the size of the data set. But we need models that are small, yet predict the target values well. A suitable criterion is the Akaike Information Criterion (AIC, Akaike (1974)), which punishes both models that do not fit the data well and models that have too many pa- rameters. The quality of a factor is judged by the amount of variation in the target variable that it ex- plains. Note that increased prediction accuracy does not necessarily mean an increase in the amount of variation explained. As the model itself is a contin- uous approximation of the categorical distinctions to be modelled, it may occur that the numerical vari- ation in the predictions decreases, but that this de- crease is lost when re-translating numerical predic- tions into categorical ones. The factors for our model were selected based on the following procedure: We start with a model that always predicts the most frequent class. We then de- termine which factor provides the greatest reduction in the AIC, add that factor to the model and retrain. This step is repeated until all factors have been used or adding another factor does not yield any signifi- cant improvements anymore.3 This procedure invariably yields the sequence Dist4, Agree, Class, FormAnte, Syn, SynAnte, Am- big, Par, both when training models on the complete data set and when training on a single genre. Inspec- tion of the AIC values suggests that parallelism is the least important factor, and does not improve the AIC significantly. Therefore, we will discard it from the outset. All other factors are maintained in the initial full model. This model is purely additive; it does not include interactions between factors. This approach allows us to filter out factors which only mediate the influence of other factors, but do not ex- ert any significant influence of their own. Note that this probabilistic model only provides a numerical description of how its factors affect pronominaliza- tion in our corpus. As such, it is not equivalent to a theoretical model, but rather provides data for fur- 3 We excluded Dist from this stepwise procedure, since the relevant information is covered already by Dist4, which fur- thermore has much fewer values. Class Act Concept Event Group Loc Pers PhysObj Prop State Time 84.1 80.0 88.0 46.1 63.3 17.3 65.5 88.5 87.8 92.9 % deadend 6.2 8.5 6.0 28.4 5.7 63.4 10.2 2.5 3.2 0.3 % pronouns % pron. (sequences) 32.5 29.6 33.3 51.6 15.4 73.8 27.2 21.4 23,7 4.5 Table 4: Results for Sortal Classes. % deadend: percentage of deadend entities; % pronouns: percent pronominalised, % pron. (sequences: percent pronominalised relative to all occurrences in co-specification sequences CF CG CK CL all 97.1 93.5 93.6 91.5 93.1 % correct 324.7 654.8 786.1 904.0 2685.8 AIC % variation 83.0 65.4 70.1 65.4 68.7 Table 5: Quality of models fitted to each of the genre-specific corpora (CF, CG, CK, CL) and the complete data set (all). % correct: correctly pre- dicted pronominalization decition, AIC: Akaike In- formation Criterion, % variation: percentage of original variation in the data (as measured by de- viance) accounted for by the model ther theoretical interpretation. Results of a first evaluation of the full model are summarized in Table 5. The model can ex- plain more than two thirds of the variation in the complete data set and can predict pronominalization quite well on the data it was fitted on. The mat- ter becomes more interesting when we examine the genre-specific results. Although overall prediction performance remains stable, the model is obviously suited better to some genres than to others. The best results are obtained on CF, the worst on CL (mys- tery fiction). In the CL texts, MCUs are short, a third of all referring expressions are pronouns, there is no first person singular narrator, and most para- graphs which mention persons are about the inter- action between two persons. The Relative Importance of Factors. All val- ues of Dist4 have very strong weights in all mod- els; this is clearly the most important factor. The same goes for Agree, where the first and second per- son are strong signs of pronominalization, and, to a lesser degree, masculine and feminine third person singular. The most important distinction provided by Class appears to be that between Persons, non- Persons, and Times. This holds as well when the model is only trained on third person referring ex- pressions. For singular referring expressions, Per- sonhood information is reflected in gender, but not for plural referring expressions. Another important influence is the form of the antecedent. The syn- tactic function of the referring expression and of its antecedent are less important, as is ambiguity. In order to examine the importance of the fac- tors in more detail, we refitted the models on the complete data set while omitting one or more of the three central features Dist4, Agree, and Class. The results are summarized in Table 6. The most inter- esting finding is that even if we exclude all three factors, prediction accuracy only drops by 3.2%. This means that the remaining 4 factors also con- tain most of the relevant information, but that this information is coded more \"efficiently\", so to speak, in the first three. Speaking of these factors, ques- tions concerning the effect of sortal class remains. Remarkably enough, when sortal class is omitted, accuracy increases by 0.7%. The increase in AIC can be explained by a decrease in the amount of explained variation. A third result is that informa- tion about the form of the antecedent can substitute for distance information, if that information is miss- ing. Both variables code the crucial distinctions be- tween expressions that evoke entities and those that access evoked entities. Furthermore, a pronominal antecedent tends to occur at a distance of less than 2 MCUs. The contribution of syntactic function re- mains stable and significant, albeit comparatively unimportant. Predictive Power: To evaluate the predictive power of the models computed so far, we determine the percentage of correctly predicted pronouns and NPs. The performance of the trained models was compared to two very simple algorithms: Algorithm A: Always choose the most frequent option (i.e. noun). Algorithm B: If the antecedent is in the same MCU, or if it is in the previous MCU and there is no ambiguity, choose a pronoun; else choose a noun. Table 7 summarises the results of the compari- son. To determine the overall predictive power of excluded fit % explained variation AIC correct Dist4 Agree Class PForm Syn PSyn Ambig none 2686 92.6 54.4 21.1 5.7 3.8 2.3 0.5 1.1 Class Agree 2785 93.3 54.4 21.1 n.a. 4.7 2.8 0.5 1.1 2984 92.6 54.4 n.a. 14.3 6.2 2.7 0.6 1.1 Dist4 3346 90.2 n.a. 35.8 6.1 32 3 0.8 0.1 Dist4 + Class 3443 90.2 n.a. 35.8 n.a. 33.7 3.4 0.8 0.1 Dist4 + Agree 3597 89.6 n.a. n.a. 31.4 35.4 3.1 0.8 0.2 Agree + Class 3098 92.6 54.4 n.a. n.a. 13.11 3.5 0.5 3.6 Dist4 + Agree + Class 3739 89.4 n.a. n.a. n.a. 52.62 4 0.7 1.7 Table 6: Effect of leaving out any one of the three most important factors on model fit. italics: significance is p < 0.05, for all other factors, p < 0.005 or better. test data set CF CG CK CL Alg. A 80.4 83.8 63.8 65.4 all 72.8 Alg. B 91.1 93.0 88.6 84.7 89.4 Model 96.5 92.2 91.8 90.9 92.6 ± 0.02 93.0 ± 0.01 w/o Class 96.8 92.4 91.7 90.7 Table 7: Results of algorithms vs. models on test data in % correct prediction if referring expression is to be pronominalised or not. Setup for genres: model is trained on three genres, tested on the re- maining one the model, we used 10-fold cross-validation. Al- gorithm A always fares worst, while algorithm B, which is based mainly on distance, the strongest fac- tor in the model, performs quite well. Its overall performance is 3.2% below that of the full model, and 3.6% below that of the full model without sor- tal class information. It even outperforms the mod- els on CG, which has the lowest percentage of Per- sons (12.9% vs. 35% for CF and 43.4% and 43.5% for CL and CK). For all other genres, the statistical models outperform the simple heuristics. Excluding sortal class information can boost prediction perfor- mance on unseen data by as much as 0.4% for the complete corpus. The apparent contradiction be- tween this finding and the results reported in the previous section can be explained if we consider that not only were some sortal classes comparatively rare in the data (Property, Event), but that our sortal class definition may still be too fine-grained. We evaluated the genre-independence of the model by training on three genres and testing on the fourth. The results show that the model fares quite well for genre CF, which is also the genre where the overall fit was best (see Table 5). We therefore hy- pothesize that the decrease in performance is mainly due to the model itself, not to the training data. The results presented in both Table 5 and 7 show that although the model we have found is not quite as genre-independent as we would want it to be, it pro- vides a reasonable fit to all the genres we examined. 5 Future Work We have described a probabilistic model of pronom- inalization that is able to correctly predict 93% of all pronouns in a corpus that consists of twelve texts from four different genres. Since the model was de- rived from a limited corpus and a limited number of genres, we cannot guarantee that our results are ap- plicable to all texts without modifications. But since its performance on our sample is consistently above 90% correct, we are reasonably confident that our main findings will hold for a wide variety of texts and text types. In particular, we isolated several fac- tors which are robust predictors of pronominaliza- tion across genres: distance from last mention and agreement, and to a certain extent the form of the antecedent, which appears to be a good substitute if the other two factors are not available. All three fea- tures can be computed on the basis of a chunk parse, a rough morphosyntactic analysis of the resulting NPs, and co-specification sequences. In computa- tional terms, they are comparatively cheap. Large corpora can be annotated relatively quickly with this information, which can then be used for statistical pronoun generation. The comparatively expensive sortal class anno- tation, on the other hand, was not very important in the final model; in fact, prediction accuracy de- creased when sortal class was included. There are two main reasons for this: first, the proposed sortal class annotation scheme needs further work, second, the relationship between sortal class and pronominalization may well be too intricate to be modelled by the factor Class alone. We set out to find a genre-independent model of pronominalization. The model we found per- forms quite well, but genre still considerably affects its performance. Where does the remaining, unex- plained variation come from? The variation might be just that - stylistic variation. It might stem from one of the traditional factors that we did not take into account here, such as thematic role. However, we suspect that the crucial factor at play here is dis- course structure (McCoy & Strube, 1999). Acknowledgements Work on this paper was be- gun while Michael Strube was a postdoctoral fellow at the Institute for Research in Cognitive Science, University of Pennsylvania, and Maria Wolters vis- ited the Institute for a week in summer 1999. We would like to thank Kathleen McCoy, Jonathan De- Cristofaro, and the three anonymous reviewers for their comments on earlier stages of this work. References Agresti, Alan (1990). Categorical Data Analysis. New York, N.Y.: Wiley. Akaike, H. (1974). A new look at statistical model identification. IEEE Transactions Automatic Control, 19:716-722. Azzam, Saliha, Kevin Humphreys & Robert Gaizauskas (1998). Evaluating a focus-based approach to anaphora resolution. In Proceedings of the 17th In- ternational Conference on Computational Linguistics and 36th Annual Meeting of the Association for Com- putational Linguistics, Montréal, Québec, Canada, 10-14 August 1998, pp. 74-78. Breiman, Leo, Jerome H. Friedman, Charles J. Stone & R.A. Olshen (1984). Classification and Regression Trees. Belmont, Cal.: Wadsworth and Brooks/Cole. Cardie, Claire & Kiri Wagstaff (1999). Noun phrase coreference as clustering. In Proceedings of the 1999 SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, Col- lege Park, Md., 21-22 June 1999, pp. 82-89. Carletta, Jean (1996). Assessing agreement on classifi- cation tasks: The kappa statistic. Computational Lin- guistics, 22(2):249-254. Cohen, Jacob (1960). A coefficient of agreement for nominal scales. Educational and Psychological Mea- surement, 20:37-46. Dale, Robert (1992). Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes. Cambridge, Mass.: MIT Press. DeCristofaro, Jonathan, Michael Strube & Kathleen F. McCoy (1999). Building a tool for annotating ref- erence in discourse. In ACL '99 Workshop on the Relationship between Discourse/Dialogue Structure and Reference, University of Maryland, Maryland, 21 June, 1999, pp. 54-62. Fellbaum, Christiane (Ed.) (1998). WordNet: An Elec- tronic Lexical Database. Cambridge, Mass.: MIT Press. Fraurud, Kari (1996). Cognitive ontology and NP form. In T. Fretheim & J. Gundel (Eds.), Reference and Referent Accessibility, pp. 65-87. Amsterdam, The Netherlands: Benjamins. Ge, Niyu, John Hale & Eugene Charniak (1998). A sta- tistical approach to anaphora resolution. In Proceed- ings of the Sixth Workshop on Very Large Corpora, Montréal, Canada, pp. 161-170. Grosz, Barbara J., Aravind K. Joshi & Scott Weinstein (1995). Centering: A framework for modeling the lo- cal coherence of discourse. Computational Linguis- tics, 21(2):203-225. Hirschman, Lynette & Nancy Chinchor (1997). MUC- 7 Coreference Task Definition, http://www. muc.sais.com/proceedings/. Ihaka, Ross & Ross Gentleman (1996). R: A language for data analysis and graphics. Journal of Computa- tional and Graphical Statistics, 5:299-314. Kessler, Brett, Geoffrey Nunberg & Hinrich Schütze (1997). Automatic detection of text genre. In Proceed- ings of the 35th Annual Meeting of the Association for Computational Linguistics and of the 8th Confer- ence of the European Chapter of the Association for Computational Linguistics, Madrid, Spain, 7-12 July 1997, pp. 32-38. LDC (1995). Penn Treebank-II. Linguistic Data Consor- tium. University of Pennsylvania, Philadelphia, Penn. McCoy, Kathleen F. & Michael Strube (1999). Gener- ating anaphoric expressions: Pronoun or definite de- scription? In ACL '99 Workshop on the Relationship between Discourse/Dialogue Structure and Reference, University of Maryland, Maryland, 21 June, 1999, pp. 63-71. Poesio, Massimo, Renate Henschel, Janet Hitzeman & Rodger Kibble (1999). Statistical NP generation: A first report. In R. Kibble & K. van Deemter (Eds.), Proceedings of the Workshop on The Generation of Nominal Expressions, 11th European Summer School on Logic, Language, and Information, Utrecht, 9-13 August 1999. Sidner, Candace L. (1983). Focusing in the compre- hension of definite anaphora. In M. Brady & R.C. Berwick (Eds.), Computational Models of Discourse, pp. 267-330. Cambridge, Mass.: MIT Press. Vossen, Piek (Ed.) (1998). EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Dordrecht, The Netherlands: Kluwer. Yeh, Ching-Long & Chris Mellish (1997). An empiri- cal study on the generation of anaphora in Chinese. Computational Linguistics, 23(1):169-190."
  },
  {
    "title": "An Evaluation of Strategies for Selective Utterance Verification for Spoken Natural Language Dialog",
    "abstract": "As with human-human interaction, spoken human-computer dialog will contain situations where there is miscommunication. In experimental trials consisting of eight different users, 141 problem-solving dialogs, and 2840 user utterances, the Circuit Fix-It Shop natural language dialog system misinterpreted 18.5% of user utterances. These miscommunications created various problems for the dialog interaction, ranging from repetitive dialog to experimenter intervention to occasional failure of the dialog. One natural strategy for reducing the impact of miscommunication is selective verification of the user's utterances. This paper reports on both context-independent and context-dependent strategies for utterance verification that show that the use of dialog context is crucial for intelligent selection of which utterances to verify.",
    "content": "1 Building Robust Spoken Natural Language Interfaces Recent advances in speech recognition technology have raised expectations about the development of practical spoken natural language (NL) interfaces. Such interfaces can provide user flexibility as well as allow users to devote their hands and eyes to the task at hand. In particular, the ability to obtain comput- erized telephone assistance via a robust NL interface could provide ready access to information that cur- rently requires direct human interaction. However, if such interfaces are to be effective with the gen- eral populous, they must be capable of dealing with miscommunication. Miscommunication can arise at several different levels, ranging from discourse struc- ture and speech act misunderstanding (McRoy and Hirst, 1995) to misinterpretation due to misrecog- nition of a speaker's words. We report on a study that focuses on this latter type of miscommunica- tion. While speech recognizer performance in con- trolled environments has improved dramatically in the past decade, recognition errors still occur. Fur- thermore, current speech recognizers cannot perform optimally in uncontrolled environments such as tele- phone interactions. We examine the strategy of verification subdialogs for resolving miscommunications due to misrecog- nition. We first review how verification subdialogs can increase the rate of correct interpretation from 81.5% to 97.4% but at a cost of unnecessary verifica- tions approximately once every five user utterances. However, by adopting a context-dependent strategy for deciding when to use a verification subdialog, we can maintain an understanding rate of 95.3% while reducing the number of unnecessary verifications by over one half. After describing the technique of selective utter- ance verification, this paper gives an overview of the dialog system environment that provides the data used in testing various strategies for selective ut- terance verification, the Circuit Fix-It Shop. The paper concludes with a description of both context- independent and context-dependent strategies for selective utterance verification and reports on the comparative results of dialog simulations using these strategies. The results show the importance of ex- ploiting dialog context for intelligent selection of which utterances to verify. 2 Selective Verification of Questionable User Inputs Every system that uses natural language under- standing will sometimes misunderstand its input. Misunderstandings can arise from speech recognition errors or inadequacies in the language grammar, or they may result from an input that is ungrammati- Spoken: i want to fix this circuit Recognized: power a six a circuit Spoken: the one is flashing for a longer period of time Recognized: one is flashing forth longer in a time Spoken: there is no wire on connector one zero four Recognized: stays know wire i connector one zero for Figure 1: Sample Utterances with Word Misrecognition cal or ambiguous. Here we focus on misunderstand- ings caused by speech recognition errors. Exam- ples of misrecognized inputs from interacting with the Circuit Fix-It Shop are given in figure 1. One method for reducing the number of misunderstand- ings is to require the user to verify each utterance by either speaking every utterance twice, or confirm- ing a word-by-word read back of every utterance (e.g., (Baber and Hone, 1993)). Such verification is effective at reducing errors that result from word misrecognitions, but does nothing to reduce misun- derstandings that result from other causes. Further- more, verification of all utterances can be needlessly wearisome to the user, especially if the system is working well. A better approach is to have the system verify its interpretation of an input only under circumstances where the accuracy of its interpretation is seriously in doubt, or correct understanding is essential to the success of the dialog. The verification is accom- plished through the use of a verification subdialog-a short sequence of utterances intended to confirm or reject the hypothesized interpretation. The follow- ing example of a verification subdialog illustrates the idea. Computer: What is the switch position when the LED is off? User: Up. Computer: Did you mean to say that the switch is up? User: Yes. Notable features of such verification subdialogs in- clude the following. • Verification is selective. A verification subdialog is initiated only if it is believed that the overall performance and accuracy of the dialog system will be improved. In this way, the dialog system responds much as a person would. • Verification is tunable. The propensity of the system to verify can be adjusted so as to pro- vide any required level of speech understanding accuracy. • Verification operates at the semantic level. The system verifies an utterance's meaning, not its syntax. This helps overcome misunderstandings that result from inadequacies in the language model, or ungrammatical or ambiguous inputs. Two important definitions concerning selective verification are the following. An under-verification is defined as the event where the system generates a meaning that is incorrect but not verified. An over-verification occurs when a correct meaning is verified. The example just given is an example of an over-verification. The goal of any algorithm for se- lective utterance verification is to minimize the rate of under-verifications while also holding the rate of over-verifications to as low a value as possible. That is, the goal is to only verify utterances that need verifying, and to verify as many of these as possi- ble. In section 4 we report on the results of tests of various strategies for deciding when to engage in verification subdialogs within a specific dialog envi- ronment, the Circuit Fix-It Shop. In order to un- derstand the strategies used, an overview of this en- vironment must first be presented. 3 Dialog Environment: The Circuit Fix-It Shop 3.1 General Characteristics The data used in this study were collected in ex- perimental trials conducted with \"The Circuit Fix- It Shop,\" a spoken NL dialog system constructed in order to test the effectiveness of an integrated dialog processing model that permits variable ini- tiative behavior as described in (Smith and Hipp, 1994) and (Smith, Hipp, and Biermann, 1995). The implemented dialog system assists users in repair- ing a Radio Shack 160 in One Electronic Project Kit. The particular circuit being used causes the Light-Emitting Diode (LED) to alternately display a one and seven. The system can detect errors caused by missing wires as well as a dead battery. Speech recognition is performed by a Verbex 6000 running on an IBM PC. To improve speech recognition per- formance we restrict the vocabulary to 125 words. A DECtalk DTCO1 text-to-speech converter is used to provide spoken output by the computer. After testing system prototypes with a few vol- unteers, eight subjects used the system during the formal experimental phase. After a warmup ses- sion where the subject trained on the speech recog- nizer and practiced using the system, each subject participated in two sessions where up to ten prob- lems were attempted. Subjects attempted a total of 141 dialogs of which 118 or 84% were completed successfully. The average speech rate by subjects was 2.9 sentences per minute; the average task com- pletion times for successful dialogs were 6.5 minutes. An excerpt from an actual interaction with the system is given in figure 2.2 The words in paren- theses represent the actual sequence of words that the speech recognizer sent to the dialog system for analysis. As can be seen from the example, the sys- tem usually understood the user utterance (but not always). We next describe two features of the sys- tem that were useful in the interpretation process: (1) error-correcting parsing; and (2) dialog expec- tation. In section 4 we will see how these features assist in deciding when to engage the user in a veri- fication subdialog. 3.2 Overcoming Misrecognition by Error-Correcting Parsing The system was able to find the correct meaning for 81.5% of the more than 2800 input utterances even though only 50% of these inputs were correctly rec- ognized word for word by use of an error-correcting parser that uses a dynamic programming approach similar to (Ney, 1991) to compute the best n parses for the input. What constitutes \"best\" is determined by a cost matrix for the possible words in the vocab- ulary and the given grammar. The cost matrix de- fines the cost for inserting or deleting words as well as the cost for a word substitution when such sub- stitutions are allowed. The intent is to permit sub- stitutions for words which sound very similar, such as \"do\" and \"two/to/too,\" words that are likely to be confused by the speech recognizer. The parser performs insertions, deletions, and substitutions in order to transform the input into a grammatical ut- terance. With each \"grammatical\" utterance is as- sociated a parse cost (PC), which is the sum of the costs of each insertion, deletion, and substitution re- quired for the transformation. For each of the best n parses, an expectation cost (EC) is also produced according to how likely the input is to occur accord- ing to the expectations. The total cost of a parse is a weighted sum of PC and EC. The values for these weights and costs have been assigned heuristically. Figure 3 shows some of the utterances successfully handled by the implemented system during the ex- periment. For instance, example 1 is transformed into a grammatical utterance by substituting the phonetically similar word \"six\" for \"fix,\" and \"and\" for \"can.\" This example would have a relatively low parse cost indicating the system has high confidence in its interpretation. On the other hand, example 3 requires a large number of costly insertions and dele- tions, indicating a lack of confidence in the quality of the interpretation. As will be seen in section 4, this difference in scores is an important component in deciding when to engage in verification subdialogs. 3.3 Expectation in Dialog An important phenomenon that is exploited in com- putational models of language is that of expectation (Allen, 1995). Based on the current context, there is an expectation of what is to come next. These ex- pectations can be for phonemes, morphemes, words, or meanings. For example, if we hear the word \"the\" we expect the next word to be either an adjective or noun. When the computer asks, \"Is the knob po- sition at 10?\", we have greatest expectation for a response of either \"yes\" or \"no\", lesser expectation for a sentence answer such as \"The knob position is five.\", and still less expectation for a clarifica- tion question or comment such as \"Which knob?\", \"Where is the knob?\", or \"I do not see it.\" Consider the following dialog sample. 1. C: Is there a wire between connector 84 and connector 99? 2. U: It is not there. 3. C: Add a wire between connector 84 and connector 99. 4. U: I need help. 5. C: Obtain a wire. 6. U: Okay. The system must use the established dialog context in order to properly interpret every user utterance as follows. Of the 23 dialogs which were not completed, misun- derstandings due to misrecognition were the cause in 13 of these failures. There were a variety of causes for the failure in the other 10 dialogs, ranging from inadequate grammar coverage to subject error in connecting wires. 2C denotes utterances spoken by the computer while U denotes utterances spoken by the user. 1. C: This is the Circuit Fix-It Shop. How may I help you? 2. U: I want to fix this circuit. (power a six a circuit) 4. U: I want to fix this circuit (i want a fix a circuit) 3. C: I do not understand. Please repeat what you said. 5. C: What is the ID of the circuit? 6. U: RS111. (rs111) 7. C: I am familiar with that circuit. The LED is supposed to be displaying alternately flashing one and seven. 8. U: The LED is displaying nothing. (be led a displaying nothing) 9. C: What is the switch at when the LED is off? 10. U: The switch is down. (the switch yes down) 11. C: Put the switch up. 12. U: Done. (done) 13. C: Put the knob to one zero. 14. U: The knob is at one zero. (the knob and add one zero) 15. C: What is the LED displaying? 16. U: The LED is displaying a one and seven at the same time (be led a displaying one an seven at the same time) 17. C: Put the knob to zero. 18. U: Done. (done) 19. C: What is the LED displaying? 20. U: The same thing. (the same same) 21. C: Did you mean to say the LED is displaying the same thing? 22. U: Yes. (yes) Figure 2: Sample Dialog Interaction with the Circuit Fix-It Shop • Utterance 2: to determine what \"it\" refers to (i.e., the wire from 84 to 99). • Utterance 4: to determine what the user needs help with (i.e., adding the wire). • Utterance 6: to determine whether \"okay\" de- notes confirmation or comprehension (i.e., con- firmation that the wire has been obtained). Effective use of expectation is necessary for con- straining the search for interpretations and achieving efficient processing of NL inputs. This is particularly crucial in spoken NL dialog, where speakers expect fast response times (Oviatt and Cohen, 1989). The system model of expectations is similar to that of (Young et al., 1989) in that we predict the meanings of possible user responses based on the cur- rent dialog goal. The details of the system model can be found in (Smith and Hipp, 1994). Here we re- view the key aspects that are exploited in a context- dependent strategy for verification. We define ex- pectations based on an abstract representation of the current task goal. For example, goal(user, ach(prop(Obj, PropName, PropValue)))³ ³ This notation is an abbreviated form of the actual denotes the goal that the user achieve the value (PropValue) for a particular property (PropName), of an object (Obj). The specific val- ues for Obj, PropName, and PropValue are filled in according to the current goal. For example, the goal of setting the switch position to up may be rep- resented as goal(user, ach(prop(switch, position, up))) while the goal of observing the knob's color would be goal(user, obs(prop(knob, color, PropValue))) where PropValue is an uninstantiated variable whose value should be specified in the user input. General expectations for the mean- ing of user responses to a goal of the form goal(user, ach(prop(...))) include the following: • A question about the location of Obj. • A question about how to do the action. • An assertion that Obj now has the value PropValue for property PropName. representation used in the system as described in (Smith and Hipp, 1994). Example 1 Computer: There is supposed to be a wire between connector 68 and connector 87. User: Wire connecting six eight and eight seven. Recognized: Wire connecting fix eight can eight seven. Example 2 Computer: User: Putting the knob to one zero is desirable. The knob is at one zero. Recognized: Seven knob use that one zero. Example 3 Computer: Is anything else on the LED on? User: LED is displaying a not flashing seven. Recognized: Be down it be yes displaying be knob flashing seven then. Figure 3: Sample Misrecognitions Correctly Parsed • An acknowledgment that the action has been completed. Even when using error-correcting parsing and dia- log expectations, the Circuit Fix-It Shop misunder- stood 18.5% of user utterances during the experi- mental testing. We now turn our attention to an empirical study of strategies for selective utterance verification that attempt to select for verification as many of the misunderstood utterances as possible while minimizing the selection of utterances that were understood correctly. These strategies make use of information obtainable from dialog expecta- tion and the error-correcting parsing process. 4 Evaluating Verification Strategies 4.1 Strategy 1: Using Parse Cost Only An enhancement to the Circuit Fix-It Shop de- scribed in (Smith and Hipp, 1994) allows for a verifi- cation subdialog only when the hypothesized mean- ing is in doubt or when accuracy is critical for the success of the dialog. The decision of whether or not a particular input should be verified is made by computing for each meaning a parser confidence score (a measure of how plausible the parser's out- put is—this measure is proportional to the inverse of the total cost (section 3.2) normalized for utter- ance length) and a verification threshold (a measure of how important the meaning is toward the suc- cess of the dialog—greater importance is denoted by a higher threshold). The decision rule for deciding when to initiate a verification subdialog is specified as follows: IF the Parser Confidence Score > the Verification Threshold THEN DO NOT engage in a verification subdialog ELSE engage in a verification subdialog This basic capability for verification subdialogs was not available during the 141 dialog experiment. However, simulations run on the collected data raised the percentage of utterances that are correctly understood from 81.5% to 97.4%.4 Unfortunately, besides improving understanding through verifica- tion of utterances initially misinterpreted, the sys- tem also verified 19.2% of the utterances initially interpreted correctly. An example would be ask- ing, \"Did you mean to say the switch is up?\", when that is what the user originally said. These over- verifications result in extraneous dialog, and if ex- cessive, will limit usability. 4.2 Strategy 2: Using Context Only The previous decision rule for utterance verification focused exclusively on the local information about parsing cost and ignores dialog context. In that sit- uation the over-verification rate was 19.2% while the 4 Consequently, the under-verification rate is 2.6%. We say that an utterance is correctly understood if it is either correctly interpreted initially, or is an utterance for which the system will engage the user in a verification subdialog. It is of course possible that the verification subdialog may not succeed, but we have not yet assessed the likelihood of that and thus do not consider this pos- sibility during the evaluation of the various strategies. • obs(prop(Obj, PropName, PropValue)) (PropValue unspecified)-observing a property. Example: a wh-question (e.g., \"What is the switch position?\") Main Expectation: direct answer (e.g., \"The switch is up.\"). • obs(prop(Obj, PropName, PropValue)) (PropValue specified) Example: a yes-no question (e.g., \"Is the switch up?\") Main Expectation: (1) yes/no response and (2) a direct answer as in the above case. • obs(meas(Des, Val))-observing a measurement described by Des where Val is the value. Example: a wh-question (e.g., \"What is the voltage between connectors 121 and 34?\") Main Expectation: direct answer (e.g., \"Seven\" or \"The voltage is seven\"). • obs(behav(Obs, Cond))-observing a behavior where the result of the observation (Obs), depends on the underlying physical conditions present (Cond) when the observation was made. Example: a wh-question (e.g., \"What is the LED displaying when the switch is up?\") Main Expectation: a direct answer (e.g., \"The LED is displaying only a not flashing seven.\"). • ach(prop(Obj, PropName, PropValue))-achieving a property. Example: a command (e.g., \"Put the switch up.\") Main Expectation: (1) completion acknowledgement (e.g., \"Okay\" or \"Done\") and (2) assertion that the desired property now exists (e.g., \"The switch is up.\"). • learn(Fact)-learning a fact. The fact could concern a piece of state information (e.g., that the switch is located in the lower left portion of the circuit), that an action needs completing (e.g., \"Putting the switch up is desirable,\"), or that a certain property should or should not be true (e.g., there should be a wire between connectors 34 and 80). In all cases, one main expectation is an acknowledgment that the Fact is understood. In the case of an action completion or a property status, there is also a main expectation for either that the user completed the action (e.g., \"Done\" or \"The switch is up\"), or that the property status is verified (e.g., \"Wire connecting 34 and 80\"). Figure 4: Summary of Main Expectations for Major Goals under-verification rate was 2.6%. What about using a rule solely based on context? For each abstract task goal, we define a subset of the expectations as the main expectation. This subset consists of the expected meanings that denote a normal continua- tion of the task. Figure 4 lists these expectations for the major task goals of the model. For cooperative task-assistance dialog, making the assumption that the meaning of the user's utterance will belong to a very small subset of the expectations for each ab- stract goal allows us to define the following context- dependent decision rule for utterance verification. IF utterance in the Main Expectation THEN DO NOT engage in a verification subdialog ELSE engage in a verification subdialog Using this decision rule, the over-verification rate rises to 31.8% while the under-verification rate falls to 1.4%. Although it significantly reduces the under- verification rate, this strategy clearly leads to an ex- cessive number of over-verifications. We next con- sider combination strategies that look at both parse cost and context. 4.3 Strategy 3: Parse Cost/Context Combination The Strategy 1 decision rule for utterance verifica- tion says to engage in a verification subdialog if the parser confidence value falls below the verification threshold. With context-dependent verification we additionally require that the utterance meaning can- not be part of the main expectation. Thus, the de- cision rule for verification may be revised as follows: IF the Parser Confidence Score > the Verification Threshold THEN DO NOT engage in a verification subdialog ELSE IF utterance meaning in the Main Expectation THEN DO NOT engage in a verification subdialog ELSE engage in a verification subdialog Using this decision rule and comparing it to Strat- egy 1, the over-verification rate drops from 19.2% to 7.6% while the under-verification rate rises from 2.6% to 4.7% (i.e., the percentage of utterances cor- rectly understood falls from 97.4% to 95.3%). This corresponds to a reduction in over-verifications from once every 5.2 user utterances to once every 13.2 user utterances while under-verifications (i.e., unde- tected misunderstandings) rises from once every 38.5 user utterances to once every 21.3 user utterances. It should be noted that on average, users spoke 20 utterances per dialog. We now examine a context- dependent strategy that takes into account specific domain information. 4.4 Strategy 4: Domain-Dependent Exceptions As previously noted, correctly interpreting certain utterances is crucial for efficient continuation of the dialog. In the Circuit Fix-It Shop, the crucial condi- tion was correct determination of the LED display. Several utterances in each dialog concerned a discus- sion of the LED display. Consequently, assertions about the LED display were often part of the main expectation. However, due to the myriad of possible LED dis- plays and the frequent misrecognition of key words and phrases in these descriptions, an effective di- alog system would want to be careful to ascertain correctness in interpreting these descriptions. Con- sequently, we modify the verification decision rule as follows: IF the Parser Confidence Score > the Verification Threshold THEN DO NOT engage in verification subdialog ELSE IF the utterance meaning is an assertion about the LED display THEN engage in a verification subdialog ELSE IF the utterance meaning is in the Main Expectation THEN DO NOT engage in a verification subdialog ELSE engage in a verification subdialog As a result, the decision rule for verifying utter- ances concerning the LED focuses solely on the local information about parsing cost and does not con- sider dialog context information about expectation.5 Such a modification might also be appropriate in other domains for information deemed essential to continuing progress. For this final decision rule the the over-verification rate is 9.8% while the under-verification rate is 3.7%. 4.5 Strategy Comparsion Table 1 summarizes the results of the four strate- gies for a fixed Verification Threshold. We con- clude that the combination of considering both the local information of the parsing cost and the dia- log context information about expectation provides the best strategy. We also note that inclusion of domain-dependent information does not show any notable improvement in the over-verification/under- verification tradeoff as compared with the context- dependent but domain-independent Strategy 3.6 We believe the results show that for task-oriented do- mains where there are fairly strong expectations for utterances that relate directly to task goals such as those described in figure 4, a context-dependent ver- ification strategy is effective at reducing the over- verification rate to a reasonable amount while keep- ing the number of under-verifications to a near min- imum. Further study is needed to determine the practical usefulness of this strategy in an actual ex- perimental situation and it is an open question as to whether or not such strategies are feasible for less task-specific domains such as advisory dialogs and database query environments. 4.6 Improving Accuracy Obtaining a higher accuracy requires reducing the under-verification rate. For Strategy 1 we explored the impact of raising and lowering the threshold on the over- and under-verification rates. Not sur- prisingly, there was a tradeoff. As the threshold was raised, more utterances are verified, resulting in fewer under-verifications but more over-verifications. Lowering the threshold had the opposite impact. In fact, using just the strategy of lowering the threshold to reduce the over-verification rate to 9.3% causes the under-verification rate to rise to 8.0%. In con- trast, the new context-dependent strategy, Strat- egy 3, achieves an over-verification rate of 7.6%, but the under-verification rate is only 4.7%. Clearly, the use of dialog context in the verification subdialog de- cision rule improves system performance. Neverthe- less, a small set of under-verifications remains. Are there any possibilities for further reductions in the under-verifications without a substantial increase in the over-verification rate? In actuality, a small component of the total parsing cost is the expectation cost based on dialog context, but that weighting is negligible compared to the weighting of the parse cost, the predominant factor in computing total cost. This of course, does not preclude the possibility that domain-dependent interaction may be more useful in other domains. Strategy 1. Parse Cost Only 2. Context Only 3. Parse Cost/Context Combination 4. Domain-Dependent Exceptions Under-verification Rate Over-verification Rate 2.6% 1.4% 4.7% 3.7% 19.2% 31.8% 7.6% 9.8% Table 1: Comparative Performance of Verification Subdialog Decision Strategies An analysis of the 133 under-verifications that oc- cur with the new strategy indicates that while some of the under-verifications are due to deficiencies in the grammar, there is a a core group of under- verifications where misrecognition of the speaker's words is impossible to overcome. Incorrect recogni- tion of digits, lost content words, and misrecognized content words can cause the system to have high confidence in an incorrect interpretation. One ap- proach that may prove helpful with this problem is the use of speech recognition systems that provide alternate hypotheses for the speech signal along with scoring information. Another possibility is word by word verification of the speaker input (see (Baber and Hone, 1993)), but such a strategy is too time- consuming and tedious for general spoken natural language dialog, especially when the user does not have access to a visual display of what the system hypothesizes was spoken. In general, experimental trials to observe subject reaction to verification sub- dialogs are needed. In conclusion, while useful, there appear to be limits to the effectiveness of verification subdi- alogs. Consequently, strategies for delayed detection and resolution of miscommunication (e.g. (McRoy and Hirst, 1995), (Brennan and Hulteen, 1995), and (Lambert and Carberry, 1992)) become nec- essary and remain an area of continued investiga- tion. These include both computer-initiated as well as user-initiated strategies. 5 Acknowledgments The author expresses his appreciation to D. Richard Hipp for his work on the error-correcting parser and for his initial work on context-independent verifica- tion. The author also wishes to express his thanks to Steven A. Gordon and Robert D. Hoggard for their suggestions concerning this work and an earlier draft of this paper. Other researchers who contributed to the development of the experimental system include Alan W. Biermann, Robert D. Rodman, Ruth S. Day, Dania Egedi, and Robin Gambill. This research has been supported by National Science Foundation Grant IRI-9501571. References Allen, J.F. 1995. Natural Language Understand- ing. The Benjamin/Cummings Publishing Com- pany, Inc., Menlo Park, California, 2nd edition. Baber, C. and K.S. Hone. 1993. Modelling error re- covery and repair in automatic speech recognition. Intl. J. Man-Machine Studies, 39:495-515. Brennan, S.E. and E.A. Hulteen. 1995. Interac- tion and feedback in a spoken language system: a theoretical framework. Knowledge-Based Sys- tems, 8:143-151. Lambert, L. and S. Carberry. 1992. Modeling ne- gotiation subdialogues. In Proceedings of the 30th Annual Meeting of the Association for Computa- tional Linguistics, pages 193-200. McRoy, S. and G. Hirst. 1995. The repair of speech act misunderstandings by abductive infer- ence. Computational Linguistics, pages 435-478. Ney, H. 1991. Dynamic programming parsing for context-free grammars in continuous speech recog- nition. IEEE Transactions on Signal Processing, 39(2):336-340. Oviatt, S.L. and P.R. Cohen. 1989. The effects of interaction on spoken discourse. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 126-134. Smith, R.W. and D.R. Hipp. 1994. Spoken Natural Language Dialog Systems: A Practical Approach. Oxford University Press, New York. Smith, R.W., D.R. Hipp, and A.W. Biermann. 1995. An architecture for voice dialog systems based on Prolog-style theorem-proving. Computational Linguistics, pages 281-320. Young, S.R., A.G. Hauptmann, W.H. Ward, E.T. Smith, and P. Werner. 1989. High level knowl- edge sources in usable speech recognition sys- tems. Communications of the ACM, pages 183- 194, February."
  },
  {
    "title": "Trainable Methods for Surface Natural Language Generation",
    "abstract": "We present three systems for surface natural language generation that are trainable from annotated corpora. The first two systems, called NLG1 and NLG2, require a corpus marked only with domain-specific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information. All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation. NLG1 serves as a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase. The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase. We present experiments in which we generate phrases to describe flights in the air travel domain.",
    "content": "1 Introduction This paper presents three trainable systems for sur- face natural language generation (NLG). Surface NLG, for our purposes, consists of generating a grammatical natural language phrase that expresses the meaning of an input semantic representation. The systems take a \"corpus-based\" or \"machine- learning\" approach to surface NLG, and learn to generate phrases from semantic input by statisti- cally analyzing examples of phrases and their cor- responding semantic representations. The determi- nation of the content in the semantic representation, or \"deep\" generation, is not discussed here. Instead, the systems assume that the input semantic repre- sentation is fixed and only deal with how to express it in natural language. This paper discusses previous approaches to sur- face NLG, and introduces three trainable systems for surface NLG, called NLG1, NLG2, and NLG3. Quantitative evaluation of experiments in the air travel domain will also be discussed. 2 Previous Approaches Templates are the easiest way to implement surface NLG. A template for describing a flight noun phrase in the air travel domain might be flight departing from $city-fr at $time-dep and arriving in $city-to at $time-arr where the words starting with \"$\" are actually variables — representing the departure city, and departure time, the arrival city, and the arrival time, respectively, whose values will be extracted from the environment in which the template is used. The approach of writing individual templates is convenient, but may not scale to complex domains in which hundreds or thousands of templates would be necessary, and may have shortcomings in maintainability and text quality (e.g., see (Reiter, 1995) for a discussion). There are more sophisticated surface genera- tion packages, such as FUF/SURGE (Elhadad and Robin, 1996), KPML (Bateman, 1996), MUMBLE (Meteer et al., 1987), and RealPro (Lavoie and Ram- bow, 1997), which produce natural language text from an abstract semantic representation. These packages require linguistic sophistication in order to write the abstract semantic representation, but they are flexible because minor changes to the input can accomplish major changes to the generated text. The only trainable approaches (known to the au- thor) to surface generation are the purely statistical machine translation (MT) systems such as (Berger et al., 1996) and the corpus-based generation sys- tem described in (Langkilde and Knight, 1998). The MT systems of (Berger et al., 1996) learn to gen- erate text in the target language straight from the source language, without the aid of an explicit se- mantic representation. In contrast, (Langkilde and Knight, 1998) uses corpus-derived statistical knowl- edge to rank plausible hypotheses from a grammar- based surface generation component. 3 Trainable Surface NLG In trainable surface NLG, the goal is to learn the mapping from semantics to words that would other- wise need to be specified in a grammar or knowledge base. All systems in this paper use attribute-value --- pairs as a semantic representation, which suffice as a representation for a limited domain like air travel. For example, the set of attribute-value pairs { $city- fr = New York City, $city-to = Seattle, $time-dep = 6 a.m., $date-dep = Wednesday } represent the meaning of the noun phrase \"a flight to Seattle that departs from New York City at 6 a.m. on Wednes- day\". The goal, more specifically, is then to learn the optimal attribute ordering and lexical choice for the text to be generated from the attribute-value pairs. For example, the NLG system should auto- matically decide if the attribute ordering in \"flights to New York in the evening\" is better or worse than the ordering in \"flights in the evening to New York\". Furthermore, it should automatically decide if the lexical choice in \"flights departing to New York\" is better or worse than the choice in \"flights leaving to New York\". The motivation for a trainable surface generator is to solve the above two problems in a way that reflects the observed usage of language in a corpus, but without the manual effort needed to construct a grammar or knowledge base. All the trainable NLG systems in this paper as- sume the existence of a large corpus of phrases in which the values of interest have been replaced with their corresponding attributes, or in other words, a corpus of generation templates. Figure 1 shows a sample of training data, where only words marked with a \"$\" are attributes. All of the NLG systems in this paper work in two steps as shown in Table 2. The systems NLG1, NLG2 and NLG3 all implement step 1; they produce a sequence of words intermixed with attributes, i.e., a template, from the the at- tributes alone. The values are ignored until step 2, when they replace their corresponding attributes in the phrase produced by step 1. 3.1 NLG1: the baseline The surface generation model NLG1 simply chooses the most frequent template in the training data that corresponds to a given set of attributes. Its perfor- mance is intended to serve as a baseline result to the more sophisticated models discussed later. Specifi- cally, nlgı (A) returns the phrase that corresponds to the attribute set A: nlg₁(A) = (argmaxphrase ETA C(phrase, A) TA≠0 [empty string] { TA = 0 where Ta are the phrases that have occurred with A in the training data, and where C(phrase, A) is the training data frequency of the natural language phrase phrase and the set of attributes A. NLG1 will fail to generate anything if A is a novel combi- nation of attributes. 3.2 NLG2: n-gram model The surface generation system NLG2 assumes that the best choice to express any given attribute-value set is the word sequence with the highest probabil- ity that mentions all of the input attributes exactly once. When generating a word, it uses local infor- mation, captured by word n-grams, together with certain non-local information, namely, the subset of the original attributes that remain to be generated. The local and non-local information is integrated with use of features in a maximum entropy prob- ability model, and a highly pruned search procedure attempts to find the best scoring word sequence ac- cording to the model. 3.2.1 Probability Model The probability model in NLG2 is a conditional dis- tribution over VU * stop*, where V is the genera- tion vocabulary and where *stop* is a special \"stop\" symbol. The generation vocabulary V consists of all the words seen in the training data. The form of the maximum entropy probability model is identical to the one used in (Berger et al., 1996; Ratnaparkhi, 1998): p(Wi Wi-1, Wi-2, attri) Z(Wi-1, Wi-2, attri) = = k 1 (Wiwi-1,-2,attri) 1j=1 1α; j Z(Wi-1, Wi-2, attri) k ΣΠα; (w,wi-1,-2,attri) w' j=1 where Wi ranges over VU *stop* and {Wi-1, Wi-2, attr;} is the history, where wi de- notes the ith word in the phrase, and attr; denotes the attributes that remain to be generated at posi- tion i in the phrase. The fj, where fj(a, b) ∈ {0,1}, are called features and capture any information in the history that might be useful for estimating p(Wi Wi-1, Wi--2, attri). The features used in NLG2 are described in the next section, and the feature weights aj, obtained from the Improved Iterative Scaling algorithm (Berger et al., 1996), are set to maximize the likelihood of the training data. The probability of the sequence W = W₁... Wn, given the attribute set A, (and also given that its length is n) is: 3.2.2 n Pr(W = w₁ ... wn|len(W) = n, A) IP(WiWi-1, Wi-2, attri) i=1 Feature Selection = The feature patterns, used in NLG2 are shown in Table 3. The actual features are created by match- ing the patterns over the training data, e.g., an ac- tual feature derived from the word bi-gram template might be: f(Wi, Wi-1, Wi-2, attri) = 1 if wi from and wi-1 = flight and Scity fr∈ attri 0 otherwise flights on $air from $city-fr to $city-to the $time-depint of $date-dep $trip flights on $air from $city-fr to $city-to leaving after $time-depaft on $date-dep flights leaving from $city-fr going to $city-to after $time-depaft on $date-dep flights leaving from $city-fr to $city-to the $time-depint of $date-dep $air flight $fltnum from $city-fr to $city-to on $date-dep $city-fr to $city-to $air flight $fltnum on the $date-dep $trip flights from $city-fr to $city-to Table 1: Sample training data Input to Step 1: { $city-fr, $city-to, $time-dep, $date-dep } Output of Step 1: \"a flight to $city-to that departs from $city-fr at $time-dep on $date-dep\" Input to Step 2: \"a flight to $city-to that departs from $city-fr at $time-dep on $date-dep\", { $city-fr = New York City, $city-to = Seattle, $time-dep = 6 a.m., $date-dep = Wednesday } Output of Step 2: \"a flight to Seattle that departs from New York City at 6 a.m. on Wednesday\" Table 2: Two steps of NLG process Low frequency features involving word n-grams tend to be unreliable; the NLG2 system therefore only uses features which occur K times or more in the training data. 3.2.3 Search Procedure The search procedure attempts to find a word se- quence w₁... wn of any length n ≤ M for the input attribute set A such that 1. wn is the stop symbol *stop* 2. All of the attributes in A are mentioned at least once 3. All of the attributes in A are mentioned at most once and where M is an heuristically set maximum phrase length. The search is similar to a left-to-right breadth- first-search, except that only a fraction of the word sequences are considered. More specifically, the search procedure implements the recurrence: WN,1 WN,i+1 = = top(N, {ww ∈ V}) top(N, next(WN,i)) The set WN,i is the top N scoring sequences of length i, and the expression next(WN,i) returns all sequences w₁... Wi+1 such that w₁... Wi ∈ WN,i, and wi+1 ∈ VU *stop*. The expression top(N, next(WN,i)) finds the top N sequences in next(WN,i). During the search, any sequence that ends with *stop* is removed and placed in the set of completed sequences. If N completed hypotheses are discovered, or if WN,M is computed, the search terminates. Any incomplete sequence which does not satisfy condition (3) is discarded and any com- plete sequence that does not satisfy condition (2) is also discarded. When the search terminates, there will be at most N completed sequences, of possibly differing lengths. Currently, there is no normalization for different lengths, i.e., all sequences of length n ≤ M are equiprobable: Pr(len(W) = n) M = n≤M = 0 n > M NLG2 chooses the best answer to express the at- tribute set A as follows: nlg2(A) = argmaxw∈Wnlg2 Pr(len(W) = n). Pr(W|len(W) = n, A) where Wnig2 are the completed word sequences that satisfy the conditions of the NLG2 search described above. 3.3 NLG3: dependency information NLG3 addresses a shortcoming of NLG2, namely that the previous two words are not necessarily the best informants when predicting the next word. In- stead, NLG3 assumes that conditioning on syntacti- cally related words in the history will result on more accurate surface generation. The search procedure in NLG3 generates a syntactic dependency tree from --- Description Feature f(wi, wi-1, wi-2, attri) = ... No Attributes remaining 1 if wᵢ = ? and attrᵢ = {}, 0 otherwise Word bi-gram with attribute 1 if wᵢ = ? and wᵢ₋₁ = ? and ? ∈ attrᵢ, 0 otherwise Word tri-gram with attribute 1 if wᵢ = ? and wᵢ₋₁wᵢ₋₂ = ?? and ? ∈ attrᵢ, 0 otherwise Table 3: Features patterns for NLG2. Any occurrence of \"?\" will be instantiated with an actual value from training data. top-to-bottom instead of a word sequence from left- to-right, where each word is predicted in the context of its syntactically related parent, grandparent, and siblings. NLG3 requires a corpus that has been an- notated with tree structure like the sample depen- dency tree shown in Figure 1. 3.3.1 Probability Model The probability model for NLG3, shown in Figure 2, conditions on the parent, the two closest siblings, the direction of the child relative to the parent, and the attributes that remain to be generated. Just as in NLG2, p is a distribution over VU *stop*, and the Improved Iterative Scaling algo- rithm is used to find the feature weights aⱼ. The expression chi(w) denotes the ith closest child to the headword w, par(w) denotes the parent of the headword w, dir ∈ {left, right} denotes the direc- tion of the child relative to the parent, and attrw,i denotes the attributes that remain to be generated in the tree when headword w is predicting its ith child. For example, in Figure 1, if w = \"flights\", then ch₁(w) = \"evening\" when generating the left children, and ch₁(w) = \"from\" when generating the right children. As shown in Figure 3, the proba- bility of a dependency tree that expresses an at- tribute set A can be found by computing, for each word in the tree, the probability of generating its left children and then its right children.¹ In this formulation, the left children are generated inde- pendently from the right children. As in NLG2, NLG3 assumes the uniform distribution for the length probabilities Pr(# of left children = n) and Pr(# of right children = n) up to a certain maxi- mum length M' = 10. 3.3.2 Feature Selection The feature patterns for NLG3 are shown in Ta- ble 4. As before, the actual features are created by matching the patterns over the training data. The features in NLG3 have access to syntactic informa- tion whereas the features in NLG2 do not. Low fre- quency features involving word n-grams tend to be unreliable; the NLG3 system therefore only uses fea- tures which occur K times or more in the training data. Furthermore, if a feature derived from Table 4 looks at a particular word chi(w) and attribute a, we only allow it if a has occurred as a descendent of ¹We use a dummy ROOT node to generate the top most head word of the phrase chi(w) in some dependency tree in the training set. As an example, this condition allows features that look at chi(w) = \"to\" and $city-to∈ attru,i but dis- allows features that look at chi(w) = \"to\" and $city- fre attrw,i. 3.4 Search Procedure The idea behind the search procedure for NLG3 is similar to the search procedure for NLG2, namely, to explore only a fraction of the possible trees by con- tinually sorting and advancing only the top N trees at any given point. However, the dependency trees are not built left-to-right like the word sequences in NLG2; instead they are built from the current head (which is initially the root node) in the following order: 1. Predict the next left child (call it x₁) 2. If it is *stop*, jump to (4) 3. Recursively predict children of x₁. Resume from (1) 4. Predict the next right child (call it xᵣ) 5. If it is *stop*, we are done predicting children for the current head 6. Recursively predict children of xᵣ. Resume from (4) As before, any incomplete trees that have generated a particular attribute twice, as well as completed trees that have not generated a necessary attribute are discarded by the search. The search terminates when either N complete trees or N trees of the max- imum length M are discovered. NLG3 chooses the best answer to express the attribute set A as follows: nlg₃(A) = argmax Pr(T|A) T∈Tnlg₃ where Tnlg₃ are the completed dependency trees that satisfy the conditions of the NLG3 search described above. 4 Experiments The training and test sets used to evaluate NLG1, NLG2 and NLG3 were derived semi-automatically from a pre-existing annotated corpus of user queries in the air travel domain. The annotation scheme used a total of 26 attributes to represent flights. --- --- flights evening(-) from(+) in(+) Chicago(+) afternoon(+) the(-) Figure 1: Sample dependency tree for the phrase evening flights from Chicago in the afternoon. - and + signs indicate left or right child, respectively. $p(chi_i(w)|w, chi_{i-1}(w), chi_{i-2}(w), par(w), dir, attr_{w,i}) = \\prod_{j=1}^{n_{i}} \\alpha_{j}^{f_j(chi_i(w),w,chi_{i-1}(w),chi_{i-2}(w),par(w),dir,attr_{w,i})}$ $Z(w, chi_{i-1}(w), chi_{i-2}(w), par(w), dir, attr_{w,i}) = \\sum_{w'} \\prod_{k=1}^{n_i} \\alpha_{k}^{f_k(w',w,chi_{i-1}(w),chi_{i-2}(w),par(w),dir,attr_{w,i})}$ Figure 2: NLG3: Equations for the probability of the ith child of head word w, or chi(w) $\\boxed{Pr(T|A) = \\prod_{w \\in T} Pr_{left}(w|A)Pr_{right}(w|A)}$ $\\boxed{Pr_{left}(w|A) = Pr(\\# \\text{ of left children} = n) \\prod_{i=1}^n p(chi_i(w)|w, chi_{i-1}(w), chi_{i-2}(w), par(w), dir = left, attr_{w,i})}$ $\\boxed{Pr_{right}(w|A) = Pr(\\# \\text{ of right children} = n) \\prod_{i=1}^n p(chi_i(w)|w, chi_{i-1}(w), chi_{i-2}(w), par(w), dir = right, attr_{w,i})}$ Figure 3: NLG3: Equations for the probability of a dependency tree T Description Feature $f(chi_i(w), w, chi_{i-1}(w), chi_{i-2}(w), par(w), dir, attr_{w,i}) = ...$ Siblings 1 if $chi_i(w) = ?$ and $chi_{i-1}(w) = ?$ and $chi_{i-2}(w) = ?$ and $dir = ?$ and $? \\in attr_{w,i}$, 0 otherwise Parent + sibling 1 if $chi_i(w) = ?$ and $chi_{i-1}(w) = ?$ and $w = ?$ and $dir = ?$ and $? \\in attr_{w,i}$, 0 otherwise Parent + grandparent 1 if $chi_i(w) = ?$ and $w = ?$ and $par(w) = ?$ and $dir = ?$ and $? \\in attr_{w,i}$, 0 otherwise Table 4: Features patterns for NLG3. Any occurrence of \"?\" will be instantiated with an actual value from training data. \\begin{tabular}{|l|l|c|c|c|c|c|} \\hline System & Parameters & \\% Correct & \\% OK & \\% Bad & \\% No output & \\% error reduction \\\\ & & & & & & from NLG1 \\\\ \\hline NLG1 & - & 84.9 & 4.9 & 7.2 & 3.0 & \\\\ NLG2 & N=10,M=30,K=3 & 88.2 & 4.7 & 6.4 & 0.7 & 22 \\\\ NLG3 & N=5,M=30,K=10 & 89.9 & 4.4 & 5.5 & 0.2 & 33 \\\\ \\hline \\end{tabular} Table 5: Weighted evaluation of trainable surface generation systems by judge A \\begin{tabular}{|l|l|c|c|c|c|c|} \\hline System & Parameters & \\% Correct & \\% OK & \\% Bad & \\% No output & \\% error reduction \\\\ & & & & & & from NLG1 \\\\ \\hline NLG1 & - & 81.6 & 8.4 & 7.0 & 3.0 & \\\\ NLG2 & N=10,M=30,K=3 & 86.3 & 5.8 & 7.2 & 0.7 & 26 \\\\ NLG3 & N=5,M=30,K=10 & 88.4 & 4.0 & 7.4 & 0.2 & 37 \\\\ \\hline \\end{tabular} Table 6: Weighted evaluation of trainable surface generation systems by judge B --- System Parameters % Correct % OK % Bad % No output % error reduction from NLG1 NLG1 48.4 NLG2 N=10,M=30,K=3 64.7 NLG3 N=5,M=30,K=10 63.1 6.8 12.1 22.6 11.6 24.2 20.5 0.5 23.7 1.6 32 29 Table 7: Unweighted evaluation of trainable surface generation systems by judge A System Parameters % Correct % OK % Bad % No output % error reduction from NLG1 NLG1 41.1 NLG2 N=10,M=30,K=3 62.1 NLG3 N=5,M=30,K=10 65.3 8.9 13.7 23.7 11.1 22.1 29.5 20.5 0.5 1.6 36 41 Table 8: Unweighted evaluation of trainable surface generation systems by judge B The training set consisted of 6000 templates describing flights while the test set consisted of 1946 templates describing flights. All systems used the same training set, and were tested on the attribute sets extracted from the phrases in the test set. For example, if the test set contains the template \"flights to $city-to leaving at $time-dep\", the surface generation systems will be told to generate a phrase for the attribute set { $city-to, $time-dep }. The output of NLG3 on the attribute set { $city-to, $city-fr, $time-dep } is shown in Table 9. There does not appear to be an objective automatic evaluation method for generated text that correlates with how an actual person might judge the output. Therefore, two judges — the author and a colleague — manually evaluated the output of all three systems. Each judge assigned each phrase from each of the three systems one of the following rankings: Correct: Perfectly acceptable OK: Tense or agreement is wrong, but word choice is correct. (These errors could be corrected by post-processing with a morphological analyzer.) Bad: Words are missing or extraneous words are present No Output: The system failed to produce any output While there were a total 1946 attribute sets from the test examples, the judges only needed to evaluate the 190 unique attribute sets, e.g., the attribute set { $city-fr $city-to } occurs 741 times in the test data. Subjective evaluation of generation output is $^2$Measuring word overlap or edit distance between the system's output and a \"reference\" set would be an automatic scoring method. We believe that such a method does not accurately measure the correctness or grammaticality of the text. not ideal, but is arguably superior than an automatic evaluation that fails to correlate with human linguistic judgement. The results of the manual evaluation, as well as the values of the search and feature selection parameters for all systems, are shown in Tables 5, 6, 7, and 8. (The values for N, M, and K were determined by manually evaluating the output of the 4 or 5 most common attribute sets in the training data). The weighted results in Tables 5 and 6 account for multiple occurrences of attribute sets, whereas the unweighted results in Tables 7 and 8 count each unique attribute set once, i.e., { $city-fr $city-to } is counted 741 times in the weighted results but once in the unweighted results. Using the weighted results, which represent testing conditions more realistically than the unweighted results, both judges found an improvement from NLG1 to NLG2, and from NLG2 to NLG3. NLG3 cuts the error rate from NLG1 by at least 33% (counting anything without a rank of Correct as wrong). NLG2 cuts the error rate by at least 22% and underperforms NLG3, but requires far less annotation in its training data. NLG1 has no chance of generating anything for 3% of the data — it fails completely on novel attribute sets. Using the unweighted results, both judges found an improvement from NLG1 to NLG2, but, surprisingly, judge A found a slight decrease while judge B found an increase in accuracy from NLG2 to NLG3. The unweighted results show that the baseline NLG1 does well on the common attribute sets, since it correctly generates only less than 50% of the unweighted cases but over 80% of the weighted cases. 5 Discussion The NLG2 and NLG3 systems automatically attempt to generalize from the knowledge inherent in the training corpus of templates, so that they can generate templates for novel attribute sets. There Probability Generated Text 0.107582 $time-dep flights from $city-fr to $city-to 0.00822441 $time-dep flights between $city-fr and $city-to 0.00564712 $time-dep flights $city-fr to $city-to 0.00343372 flights from $city-fr to $city-to at $time-dep 0.0012465 $time-dep flights from $city-fr to to $city-to Table 9: Sample output from NLG3. (Dependency tree structures are not shown.) Typical values for attributes: $time-dep = \"10 a.m.\", $city-fr = \"New York\", $city-to = \"Miami\" is some additional cost associated with producing the syntactic dependency annotation necessary for NLG3, but virtually no additional cost is associated with NLG2, beyond collecting the data itself and identifying the attributes. The trainable surface NLG systems in this pa- per differ from grammar-based systems in how they determine the attribute ordering and lexical choice. NLG2 and NLG3 automatically determine attribute ordering by simultaneously searching multiple or- derings. In grammar-based approaches, such pref- erences need to be manually encoded. NLG2 and NLG3 solve the lexical choice problem by learning the words (via features in the maximum entropy probability model) that correlate with a given at- tribute and local context, whereas (Elhadad et al., 1997) uses a rule-based approach to decide the word choice. While trainable approaches avoid the expense of crafting a grammar to determine attribute order- ing and lexical choice, they are less accurate than grammar-based approaches. For short phrases, ac- curacy is typically 100% with grammar-based ap- proaches since the grammar writer can either cor- rect or add a rule to generate the phrase of interest once an error is detected. Whereas with NLG2 and NLG3, one can tune the feature patterns, search pa- rameters, and training data itself, but there is no guarantee that the tuning will result in 100% gener- ation accuracy. Our approach differs from the corpus-based surface generation approaches of (Langkilde and Knight, 1998) and (Berger et al., 1996). (Langkilde and Knight, 1998) maps from semantics to words with a concept ontology, grammar, and lexicon, and ranks the resulting word lattice with corpus-based statistics, whereas NLG2 and NLG3 automatically learn the mapping from semantics to words from a corpus. (Berger et al., 1996) describes a statistical machine translation approach that generates text in the target language directly from the source text. NLG2 and NLG3 are also statistical learning ap- proaches but generate from an actual semantic rep- resentation. This comparison suggests that statis- tical MT systems could also generate text from an \"interlingua\", in a way similar to that of knowledge- based translation systems. We suspect that our statistical generation ap- proach should perform accurately in domains of sim- ilar complexity to air travel. In the air travel do- main, the length of a phrase fragment to describe an attribute is usually only a few words. Domains which require complex and lengthy phrase fragments to describe a single attribute will be more challeng- ing to model with features that only look at word n-grams for n∈ {2,3}. Domains in which there is greater ambiguity in word choice will require a more thorough search, i.e., a larger value of N, at the expense of CPU time and memory. Most im- portantly, the semantic annotation scheme for air travel has the property that it is both rich enough to accurately represent meaning in the domain, but simple enough to yield useful corpus statistics. Our approach may not scale to domains, such as freely occurring newspaper text, in which the semantic an- notation schemes do not have this property. Our current approach has the limitation that it ignores the values of attributes, even though they might strongly influence the word order and word choice. This limitation can be overcome by using features on values, so that NLG2 and NLG3 might discover — to use a hypothetical example — that \"flights leaving $city-fr\" is preferred over \"flights from $city-fr\" when $city-fr is a particular value, such as \"Miami\". 6 Conclusions This paper presents the first systems (known to the author) that use a statistical learning approach to produce natural language text directly from a se- mantic representation. Information to solve the attribute ordering and lexical choice problems— which would normally be specified in a large hand- written grammar—is automatically collected from data with a few feature patterns, and is combined via the maximum entropy framework. NLG2 shows that using just local n-gram information can out- perform the baseline, and NLG3 shows that using syntactic information can further improve genera- tion accuracy. We conjecture that NLG2 and NLG3 should work in other domains which have a com- plexity similar to air travel, as well as available an- notated data. 7 Acknowledgements The author thanks Scott McCarley for serving as the second judge, and Scott Axelrod, Kishore Papineni, and Todd Ward for their helpful comments on this work. This work was supported in part by DARPA Contract # MDA972-97-C-0012. References John Bateman. 1996. Kpml development envi- ronment - multilingual linguistic resource devel- opment and sentence generation. Technical re- port, German Centre for Information Technol- ogy (GMD), Institute for Integrated Information and Publication Systems (IPSI), Darmstadt, Ger- many. Adam Berger, Stephen A. Della Pietra, and Vin- cent J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Com- putational Linguistics, 22(1):39-71. Michael Elhadad and Jacques Robin. 1996. An overview of surge: a reusable comprehensive syn- tactic realization component. Technical Report 96-03, Ben Gurion University, Beer Sheva, Israel. Michael Elhadad, Kathleen McKeown, and Jacques Robin. 1997. Floating constraints in lexical choice. Computational Linguistics, pages 195- 239. Irene Langkilde and Kevin Knight. 1998. Genera- tion that exploits corpus-based statistical knowl- edge. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computa- tional Linguistics, University of Montreal, Mon- treal, Quebec, Canada. Benoit Lavoie and Owen Rambow. 1997. A fast and portable realizer for text generation systems. In Proceedings of the Fifth Conference on Ap- plied Natural Language Processing, pages 265-268, Washington D.C., March 31-April 3. M. W. Meteer, D. D. McDonald, S.D. Anderson, D. Forster, L.S. Gay, A.K. Huettner, and P. Si- bun. 1987. Mumble-86: Design and implementa- tion. Technical Report Technical Report COINS 87-87, University of Massachusetts at Amherst. Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolu- tion. Ph.D. thesis, University of Pennsylvania. Ehud Reiter. 1995. Nlg vs. Templates. In Proceed- ings of the 5th European Workshop on Natural Language Generation, Leiden, The Netherlands."
  },
  {
    "title": "From Water to Wine: Generating Natural Language Text from Today's Application Programs",
    "abstract": "In this paper we present a means of compensating for the semantic deficits of linguistically naive underlying application programs without compromising principled grammatical treatments in natural language generation. We present a method for building an interface from today's underlying application programs to the linguistic realization component Mumble-86. The goal of the paper is not to discuss how Mumble works, but to describe how one exploits its capabilities. We provide examples from current generation projects using Mumble as their linguistic component.",
    "content": "INTRODUCTION Work in artificial intelligence has two goals: on the one hand to do concrete work that can be used in actual systems today, on the other to establish strong theoretical foundations that will allow us to build more sophisticated systems tomorrow. Unfortunately, since the field is so young and so few problems are well understood, these two goals are often at odds. Natural language generation is no exception. The years of research in linguistics have made problems in syntax comparatively well understood. Nevertheless, we should not restrict ourselves to just generating single, isolated sentences until the problems of lexical semantics, discourse structure, and conceptual modeling are understood as well. We must find ways to facilitate both efforts, modularizing our systems so that the parts that handle well understood processes need not be compromised to accomodate weaknesses in other parts of the system. This paper is on how to support such modularity in a natural language generator. 1 This work was supported in part by DARPA contracts N00014-87-K0238 at the Univerisity of Massachusetts and DAAA 15-87-C0006 CDRLA002 at BBN Laboratories, and by the Rome Air Development Center contract number AF30602- 81-C-0169, task number 174398 at the University of Massachusetts. In the present case, the well understood process is linguistic realization, and the weaknesses are in the conceptual models and representations of the programs underlying the generator. To bridge this gap, we present a specification language, to be used as input to the linguistic realization component Mumble-86.2 This language provides the designer of a planning component with a vocabulary of linguistic resources (i.e. words, phrases, syntactic constructions) and a straightforward means of directing their composition. The specification language facilitates interfacing Mumble to a wide range of underlying programs and planners. For simple programs not built with language in mind, we show a straightforward means of using predefined templates to map underlying objects to complex linguistic structures. For systems with more sophistication in text planning, we show how the compositionality and flexibility of the specification language can be used to make their task easier. What is template driven at one end of the range can be built compositionally at the other; what is stipulated at one end can be reasoned about at the other. A MOTIVATING EXAMPLE Consider the description \"53rd Mechanized Division\". In most programs today a sufficient representation of the object it names could be just the symbol 53RD-MECHANIZED-DIVISION. The print name of the symbol conveys all the information that a person reading the code needs to know, without it actually playing a role in the program's reasoning. If all we cared about were a single communicative context, we might consider implementing the link between the symbol and the description as though the phrase were one long word without any internal structure. This expedient treatment would severely 2 For a comprehensive description of Mumble-86, see Meteer, McDonald, Anderson, Forster, Gay, Huettner, & Sibun 1987. limit our options, however. Indefinite references, such as \"a mechanized division\", and subsequent references, \"the division\", would have to be handled separately. Pronominalization would not be possible since there are no associated features such as number, gender, and person. Furthermore, since an artificial word would have no internal syntactic structure, a speech production program would have no information on which to base intonation. A better treatment is to introduce into the interface itself some of the generality and structure that the underlying representation is missing. In the ALBM interface being developed at BBN, we associate an object like 53RD-MECHANIZED- DIVISION with the application of a general template to an explicit set of arguments as shown below: (define-default-specification '53rd-mechanized-division :template-name armed-forces-unit-name :arguments (\"53rd\" \"Mechanized\" \"Division\") ) FIGURE 1 By going to this slightly greater effort, we have supplied a hook for handling subsequent reference or other abstractions (\"the 53rd and 42nd mechanized divisions\") without first requiring that the underlying program contain the necessary semantic distinctions and linguistic information. We return to this example later and show how the template named in Figure 1 builds an input specification for Mumble. MUMBLE'S PLACE IN THE GENERATION PROCESS. A key question is what information the input specifications to Mumble represent. This amounts to asking how we take the generation process to divide into subprocesses---what decisions have already been made and are reflected in the specifications, and which ones remain. Since we have positioned the level of the specification language so as to fit the decomposition reflected in our own work and to expedite the use of Mumble-86 by other researchers, the answer can be given quite precisely. For a more complete discussion of our approach and how it contrasts with other work, see (McDonald, Meteer, & Pustejovsky, 1987). Overall we can divide the generation process into three coarse stages: Underlying program -- Developed inde- pendently of the generator per se, this will be the expert diagnostician, cooperative database, ICAI tutor, etc. that the human users want to talk with. Some event within this underlying program will determine the goals the utterances are to achieve and initiate the generation process. Planning -- This process determines how the goals can be achieved in a given context. This includes selecting the information to be communicated (or omitted), determining what perspectives and rhetorical organization the information should be given, and choosing a mapping for the information onto the linguistic resources that the language provides (i.e. open-class words and syntactic constructions). Realization -- This process carries out the planner's specifications to produce an actual text. It has the responsibility for insuring that the text is grammatical, and will handle the bulk if not all of the syntactic and morphological decision making. In these terms, Mumble-86 is a realization component.3 As such, we expect any system that uses it to be able to supply the following kinds of information about each utterance that it wants produced, couching the information in terms of our specification language. Mumble-86 is agnostic as to whether this information was assembled by a theoretically interesting planning component or merely stipulated in predefined templates. (a) The units from which the utterance is to be composed. The mapping for each unit to its intended linguistic resource will either have been already made or will be fully defined for later execution. (b) The functional relationships among the units, e.g. predication, head, modifier, given, theme, etc., that direct or constrain the units' organization within the text. (c) Lexical choice. As the primary means of delimiting what information is or is not communicated and what perspectives and connotations are presented, all open class words are choosen by the planner. 3 We also refer to Mumble as a \"linguistic component\", reflecting the fact that all of the planners and underlying programs that have been used with Mumble to date have concentrated on conceptual issues and left all of the linguistic efforts to it; this designation may have to change in the coming years as the semantic and discourse level contributions of earlier components become more significant. We see our specification language as providing a medium for the results of a planner's decisions. The syntax of the language provides a flexible, compositional notation by which a planner may view the potential linguistic form of the utterance it is constructing without having to understand the myriad details entailed by descriptions at the level of the surface structure. In the next section, we describe the syntax of the specification language. We then look at how predefined templates can be used to abstract away some of the details to make it easier for a planner to construct them. THE INPUT SPECIFICATION LANGUAGE Mumble's input specifications may be seen as expressions over a vocabulary of elementary terms and a syntax for their composition. In defining this language, our choice of terms and compositional operators was driven by what appears to be most useful at the linguistic level. The simplest expressions in the language, kernel specifications, represent the choice of a class of phrases with a lexical head and the specification of its arguments. This reflects our belief that one almost never chooses just to use a certain word, but rather to describe an action with a verb and a specific set arguments for example (see also Kegl, 1987). The result of realizing a kernel is a phrasal unit comparable to an elementary tree of a Tree Adjoining Grammar. (See Joshi, 1987, for a discussion of properties of a TAG which make them well suited to generation.) Formally, a kernel consists of a realization function and a list of arguments which are applied to it, where a realization function is typically a class of phrases distinguished by the characteristics of the syntactic contexts in which they may appear. Executing the realization function consists of choosing among the phrases and instantiating the choice. Larger, more complex utterances are formed by composing kernels: joining them syntactically according to the relationships between them. This process is analogous to adjunction in a TAG. In Mumble, these compositional expressions are called bundles. They have three major parts: (1) The head is either a kernel or a bundle; it is realized first, as an \"initial tree\" into which other specifications are attached; every bundle must have a head. (2) Further-specifications have two parts, a specification (either a kernel or a bundle) and an attachment function, which constrains where the new tree may be adjoined to the surface structure already built; these correspond to the \"auxiliary trees\" of a TAG; a bundle may have any number of further specifications. (3) Accessories contain information about language-specific syntactic details, such as tense and number. Each bundle type has a specific set of obligatory and optional accessories associated with it. Note that bundles are not constrained as to the size of the text they produce: they may produce a single noun phrase or an entire paragraph. Figure 2 shows a representation of the input specification for the description \"53rd Mechanized Division\" discussed at the beginning of the paper. In the next section we describe how this specification could be built from an object in the underlying program."
  },
  {
    "title": "An Unsupervised Method for Detecting Grammatical Errors",
    "abstract": "We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora. The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL). The error-recognition system, ALEK, performs with about 80% precision and 20% recall.",
    "content": "Introduction A good indicator of whether a person knows the meaning of a word is the ability to use it appropriately in a sentence (Miller and Gildea, 1987). Much information about usage can be obtained from quite a limited context: Choueka and Lusignan (1985) found that people can typically recognize the intended sense of a polysemous word by looking at a narrow window of one or two words around it. Statistically-based computer programs have been able to do the same with a high level of accuracy (Kilgarriff and Palmer, 2000). The goal of our work is to automatically identify inappropriate usage of specific vocabulary words in essays by looking at the local contextual cues around a target word. We have developed a statistical system, ALEK (Assessing Lexical Knowledge), that uses statistical analysis for this purpose. A major objective of this research is to avoid the laborious and costly process of collecting errors (or negative evidence) for each word that we wish to evaluate. Instead, we train ALEK on a general corpus of English and on edited text containing example uses of the target word. The system identifies inappropriate usage based on differences between the word's local context cues in an essay and the models of context it has derived from the corpora of well-formed A requirement for ALEK has been that all steps in the process be automated, beyond choosing the words to be tested and assessing the results. Once a target word is chosen, preprocessing, building a model of the word's appropriate usage, and identifying usage errors in essays is performed without manual intervention. ALEK has been developed using the Test of English as a Foreign Language (TOEFL) administered by the Educational Testing Service. TOEFL is taken by foreign students who are applying to US undergraduate and graduate-level programs. 1 Background Approaches to detecting errors by non-native writers typically produce grammars that look for specific expected error types (Schneider and McCoy, 1998; Park, Palmer and Washburn, 1997). Under this approach, essays written by ESL students are collected and examined for errors. Parsers are then adapted to identify those error types that were found in the essay collection. We take a different approach, initially viewing error detection as an extension of the word sense disambiguation (WSD) problem. Corpus-based WSD systems identify the intended sense of a polysemous word by (1) collecting a set of example sentences for each of its various senses and (2) extracting salient contextual cues from these sets to (3) build a statistical model for each sense. They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)). Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they're. He extracted contexts from correct usage of each confusable word in a training corpus and then identified a new occurrence as an error when it matched the wrong context. However, most grammatical errors are not the result of simple word confusions. This complicates the task of building a model of incorrect usage. One approach we considered was to proceed without such a model: represent appropriate word usage (across senses) in a single model and compare a novel example to that model. The most appealing part of this formulation was that we could bypass the knowledge acquisition bottleneck. All occurrences of the word in a collection of edited text could be automatically assigned to a single training set representing appropriate usage. Inappropriate usage would be signaled by contextual cues that do not occur in training. Unfortunately, this approach was not effective for error detection. An example of a word usage error is often very similar to the model of appropriate usage. An incorrect usage can contain two or three salient contextual elements as well as a single anomalous element. The problem of error detection does not entail finding similarities to appropriate usage, rather it requires identifying one element among the contextual cues that simply does not fit. 2 ALEK Architecture What kinds of anomalous elements does ALEK identify? Writers sometimes produce errors that violate basic principles of English syntax (e.g., a desks), while other mistakes show a lack of information about a specific vocabulary item (e.g., a knowledge). In order to detect these two types of problems, ALEK uses a 30-million word general corpus of English from the San Jose Mercury News (hereafter referred to as the general corpus) and, for each target word, a set of 10,000 example sentences from North American newspaper text¹ (hereafter referred to as the word-specific corpus). ¹ The corpora are extracted from the ACL-DCI corpora. In selecting the sentences for the word ALEK infers negative evidence from the contextual cues that do not co-occur with the target word - either in the word specific corpus or in the general English one. It uses two kinds of contextual cues in a ±2 word window around the target word: function words (closed-class items) and part-of-speech tags (Brill, 1994). The Brill tagger output is post-processed to \"enrich” some closed class categories of its tag set, such as subject versus object pronoun and definite versus indefinite determiner. The enriched tags were adapted from Francis and Kučera (1982). After the sentences have been preprocessed, ALEK counts sequences of adjacent part-of- speech tags and function words (such as determiners, prepositions, and conjunctions). For example, the sequence a/AT full-time/JJjob/NN contributes one occurrence each to the bigrams AT+JJ, JJ+NN, a+JJ, and to the part-of-speech tag trigram AT+JJ+NN. Each individual tag and function word also contributes to its own unigram count. These frequencies form the basis for the error detection measures. From the general corpus, ALEK computes a mutual information measure to determine which sequences of part-of-speech tags and function words are unusually rare and are, therefore, likely to be ungrammatical in English (e.g., singular determiner preceding plural noun, as in *a desks). Mutual information has often been used to detect combinations of words that occur more frequently than we would expect based on the assumption that the words are independent. Here we use this measure for the opposite purpose - to find combinations that occur less often than expected. ALEK also looks for sequences that are common in general but unusual in the word specific corpus (e.g., the singular determiner a preceding a singular noun is common in English but rare when the noun is specific corpora, we tried to minimize the mismatch between the domains of newspapers and TOEFL essays. For example, in the newspaper domain, concentrate is usually used as a noun, as in orange juice concentrate but in TOEFL essays it is a verb 91% of the time. Sentence selection for the word specific corpora was constrained to reflect the distribution of part-of-speech tags for the target word in a random sample of TOEFL essays. knowledge). These divergences between the two corpora reflect syntactic properties that are peculiar to the target word. 2.1 Measures based on the general corpus: The system computes mutual information comparing the proportion of observed occurrences of bigrams in the general corpus to the proportion expected based on the assumption of independence, as shown below: MI = log2 P(AB) P(A)×P(B) Here, P(AB) is the probability of the occurrence of the AB bigram, estimated from its frequency in the general corpus, and P(A) and P(B) are the probabilities of the first and second elements of the bigram, also estimated from the general corpus. Ungrammatical sequences should produce bigram probabilities that are much smaller than the product of the unigram probabilities (the value of MI will be negative). Trigram sequences are also used, but in this case the mutual information computation compares the co-occurrence of ABC to a model in which A and C are assumed to be conditionally independent given B (see Lin, 1998). P(ABC) MI = log2 P(B)×P(A|B)×P(C|B) Once again, a negative value is often indicative of a sequence that violates a rule of English. 2.2 Comparing the word-specific corpus to the general corpus: ALEK also uses mutual information to compare the distributions of tags and function words in the word-specific corpus to the distributions that are expected based on the general corpus. The measures for bigrams and trigrams are similar to those given above except that the probability in the numerator is estimated from the word- specific corpus and the probabilities in the denominator come from the general corpus. To return to a previous example, the phrase a knowledge contains the tag bigram for singular determiner followed by singular noun (AT NN). This sequence is much less common in the word-specific corpus for knowledge than would be expected from the general corpus unigram probabilities of AT and NN. In addition to bigram and trigram measures, ALEK compares the target word's part-of- speech tag in the word-specific corpus and in the general corpus. Specifically, it looks at the conditional probability of the part-of-speech tag given the major syntactic category (e.g., plural noun given noun) in both distributions, by computing the following value. log2 Pspecific_corpus(tag | category) Pgeneral_corpus(tag|category) For example, in the general corpus, about half of all noun tokens are plural, but in the training set for the noun knowledge, the plural knowledges occurs rarely, if at all. The mutual information measures provide candidate errors, but this approach overgenerates — it finds rare, but still quite grammatical, sequences. To reduce the number of false positives, no candidate found by the MI measures is considered an error if it appears in the word-specific corpus at least two times. This increases ALEK's precision at the price of reduced recall. For example, a knowledge will not be treated as an error because it appears in the training corpus as part of the longer a knowledge of sequence (as in a knowledge of mathematics). ALEK also uses another statistical technique for finding rare and possibly ungrammatical tag and function word bigrams by computing the x² (chi square) statistic for the difference between the bigram proportions found in the word-specific and in the general corpus: x² = (Pword_specific - Pgeneral_corpus)² Pgeneral_corpus(1 - Pgeneral_corpus) Nword_specific The x² measure faces the same problem of overgenerating errors. Due to the large sample sizes, extreme values can be obtained even though effect size may be minuscule. To reduce false positives, ALEK requires that effect sizes be at least in the moderate-to-small range (Cohen and Cohen, 1983). Direct evidence from the word specific corpus can also be used to control the overgeneration of errors. For each candidate error, ALEK compares the larger context in which the bigram appears to the contexts that have been analyzed in the word-specific corpus. From the word- specific corpus, ALEK forms templates, sequences of words and tags that represent the local context of the target. If a test sentence contains a low probability bigram (as measured by the x² test), the local context of the target is compared to all the templates of which it is a part. Exceptions to the error, that is longer grammatical sequences that contain rare sub- sequences, are found by examining conditional probabilities. To illustrate this, consider the example of a knowledge and a knowledge of. The conditional probability of of given a knowledge is high, as it accounts for almost all of the occurrences of a knowledge in the word- specific corpus. Based on this high conditional probability, the system will use the template for a knowledge of to keep it from being marked as an error. Other function words and tags in the +1 position have much lower conditional probability, so for example, a knowledge is will not be treated as an exception to the error. 2.3 Validity of the n-gram measures TOEFL essays are graded on a 6 point scale, where 6 demonstrates \"clear competence\" in writing on rhetorical and syntactic levels and 1 demonstrates \"incompetence in writing\". If low probability n-grams signal grammatical errors, then we would expect TOEFL essays that received lower scores to have more of these n- grams. To test this prediction, we randomly selected from the TOEFL pool 50 essays for each of the 6 score values from 1.0 to 6.0. For each score value, all 50 essays were concatenated to form a super-essay. In every super-essay, for each adjacent pair and triple of tags containing a noun, verb, or adjective, the bigram and trigram mutual information values were computed based on the general corpus. Table 1 shows the proportions of bigrams and trigrams with mutual information less than -3.60. As predicted, there is a significant negative correlation between the score and the proportion of low probability bigrams (r= -.94, n=6, p<.01, two-tailed) and trigrams (r=-.84, n=6, p<.05, two-tailed). 2.4 System development ALEK was developed using three target words that were extracted from TOEFL essays: concentrate, interest, and knowledge. These words were chosen because they represent different parts of speech and varying degrees of polysemy. Each also occurred in at least 150 sentences in what was then a small pool of TOEFL essays. Before development began, each occurrence of these words was manually labeled as an appropriate or inappropriate usage — without taking into account grammatical errors that might have been present elsewhere in the sentence but which were not within the target word's scope. Critical values for the statistical measures were set during this development phase. The settings were based empirically on ALEK's performance so as to optimize precision and recall on the three development words. Candidate errors were those local context sequences that produced a mutual information value of less than -3.60 based on the general corpus; mutual information of less than -5.00 for the specific/general comparisons; or a x² value greater than 12.82 with an effect size greater than 0.30. Precision and recall for the three words are shown below. Score % of bigrams % of trigrams 1.0 3.6 1.4 2.0 3.4 0.8 3.0 2.6 0.6 4.0 1.9 0.3 5.0 1.3 0.4 6.0 1.5 0.3 Table 1: Percent of n-grams with mutual information < -3.60, by score point Target word n Precision Recall Concentrate 169 .875 .280 Interest 416 .840 .330 Knowledge 761 .918 .570 Table 2: Development Words Test Word Precision Recall Total Recall (estimated) Affect .848 .762 .343 Area .752 .846 .205 Aspect .792 .717 .217 Benefit .744 .709 .276 Career .736 .671 .110 Communicate .784 .867 .274 Concentrate .848 .791 .415 Conclusion .944 .756 .119 Culture .704 .656 .083 Economy .816 .666 .235 Test Word Precision Recall Total Recall (estimated) Energy .768 .666 .104 Function .800 .714 .168 Individual .576 .742 .302 Job .728 .679 .103 Period .832 .670 .102 Pollution .912 .780 .310 Positive .784 .700 .091 Role .728 .674 .098 Stress .768 .578 .162 Technology .728 .674 .093 Mean .779 .716 .190 3 Experimental Design and Results ALEK was tested on 20 words. These words were randomly selected from those which met two criteria: (1) They appear in a university word list (Nation, 1990) as words that a student in a US university will be expected to encounter and (2) there were at least 1,000 sentences containing the word in the TOEFL essay pool. To build the usage model for each target word, 10,000 sentences containing it were extracted from the North American News Corpus. Preprocessing included detecting sentence boundaries and part-of-speech tagging. As in the development system, the model of general English was based on bigram and trigram frequencies of function words and part-of- speech tags from 30-million words of the San Jose Mercury News. For each test word, all of the test sentences were marked by ALEK as either containing an error or not containing an error. The size of the test set for each word ranged from 1,400 to 20,000 with a mean of 8,000 sentences. 3.1 Results To evaluate the system, for each test word we randomly extracted 125 sentences that ALEK classified as containing no error (C-set) and 125 sentences which it labeled as containing an error (E-set). These 250 sentences were presented to a linguist in a random order for blind evaluation. The linguist, who had no part in ALEK's development, marked each usage of the target word as incorrect or correct and in the case of incorrect usage indicated how far from the target one would have to look in order to recognise that there was an error. For example, in the case of \"an period\" the error occurs at a distance of one word from period. When the error is an omission, as in “lived in Victorian period\", the distance is where the missing word should have appeared. In this case, the missing determiner is 2 positions away from the target. When more than one error occurred, the distance of the one closest to the target was marked. Table 3 lists the precision and recall for the 20 test words. The column labelled \"Recall\" is the proportion of human-judged errors in the 250- sentence sample that were detected by ALEK. \"Total Recall\" is an estimate that extrapolates from the human judgements of the sample to the entire test set. We illustrate this with the results for pollution. The human judge marked as incorrect usage 91.2% of the sample from ALEK's E-set and 18.4% of the sample from its C-set. To estimate overall incorrect usage, we computed a weighted mean of these two rates, where the weights reflected the proportion of sentences that were in the E-set and C-set. The E-set contained 8.3% of the pollution sentences and the C-set had the remaining 91.7%. With the human judgements as the gold standard, the estimated overall rate of incorrect usage is (.083 x .912 + .917 × .184) = .245. ALEK's estimated recall is the proportion of sentences in the E-set times its precision, divided by the overall estimated error rate (.083 × .912) / .245 = .310. The precision results vary from word to word. Conclusion and pollution have precision in the low to middle 90's while individual's precision is 57%. Overall, ALEK's predictions are about 78% accurate. The recall is limited in part by the fact that the system only looks at syntactic information, while many of the errors are semantic. 3.2 Analysis of Hits and Misses Nicholls (1999) identifies four error types: an unnecessary word (*affect to their emotions), a missing word (*opportunity of job.), a word or phrase that needs replacing (*every jobs), a word used in the wrong form (*pollutions). ALEK recognizes all of these types of errors. For closed class words, ALEK identified whether a word was missing, the wrong word was used (choice), and when an extra word was used. Open class words have a fourth error category, form, including inappropriate compounding and verb agreement. During the development stage, we found it useful to add additional error categories. Since TEOFL graders are not supposed to take punctuation into account, punctuation errors were only marked when they caused the judge to \"garden path\" or initially misinterpret the sentence. Spelling was marked either when a function word was misspelled, causing part-of- speech tagging errors, or when the writer's intent was unclear. The distributions of categories for hits and misses, shown in Table 4, are not strikingly different. However, the hits are primarily syntactic in nature while the misses are both semantic (as in open-class:choice) and syntactic (as in closed-class:missing). ALEK is sensitive to open-class word confusions (affect vs effect) where the part of speech differs or where the target word is confused with another word (*In this aspect, ... instead of In this respect, ...). In both cases, the system recognizes that the target is in the wrong syntactic environment. Misses can also be syntactic - when the target word is confused with another word but the syntactic environment fails to trigger an error. In addition, ALEK does not recognize semantic errors when the error involves the misuse of an open-class word in Category Closed-class - choice -extra % Hits % Misses 22.5 15.5 15.5 13.0 -missing 8.0 8.5 Open-class - choice 12.0 19.0 -extra .5 1.0 -missing .5 1.5 -form 28.0 28.5 Punctuation 5.5 1.5 Sentence fragment 1.5 2.0 Spelling/typing error 5.5 8.5 Word order .5 1.0 Table 4: Hits and misses based on a random sample of 200 hits and 200 misses combination with the target (for example, make in \"*they make benefits\"). Closed class words typically are either selected by or agree with a head word. So why are there so many misses, especially with prepositions? The problem is caused in part by polysemy -- when one sense of the word selects a preposition that another sense does not. When concentrate is used spatially, it selects the preposition in, as \"the stores were concentrated in the downtown area\". When it denotes mental activity, it selects the preposition on, as in \"Susan concentrated on her studies\". Since ALEK trains on all senses of concentrate, it does not detect the error in \"*Susan concentrated in her studies\". Another cause is that adjuncts, especially temporal and locative adverbials, distribute freely in the word- specific corpora, as in \"Susan concentrated in her room.\" This second problem is more tractable than the polysemy problem -- and would involve training the system to recognize certain types of adjuncts. 3.3 Analysis of False Positives False positives, when ALEK \"identifies\" an error where none exists, fall into six major categories. The percentage of each false positive type in a random sample of 200 false positives is shown in Table 5. Domain mismatch: Mismatch of the newspaper-domain word-specific corpora and essay-domain test corpus. One notable difference is that some TOEFL essay prompts call for the writer's opinion. Consequently, Error Type % Occurrence Domain mismatch 12.5 Tagger 17.0 Syntactic 14.5 Free distribution 16.5 Punctuation 12.0 Infrequent tags 9.0 Other 18.5 Table 5. Distribution of false positive types TOEFL essays often contain first person references, whereas newspaper articles are written in the third person. We need to supplement the word-specific corpora with material that more closely resembles the test corpus. Tagger: Incorrect analysis by the part-of-speech tagger. When the part-of-speech tag is wrong, ALEK often recognizes the resulting n-gram as anomalous. Many of these errors are caused by training on the Brown corpus instead of a corpus of essays. Syntactic analysis: Errors resulting from using part-of-speech tags instead of supertags or a full parse, which would give syntactic relations between constituents. For example, ALEK false alarms on arguments of ditransitive verbs such as offer and flags as an error \"you benefits\" in \"offers you benefits\". Free distribution: Elements that distribute freely, such as adverbs and conjunctions, as well as temporal and locative adverbial phrases, tend to be identified as errors when they occur in some positions. Punctuation: Most notably omission of periods and commas. Since these errors are not indicative of one's ability to use the target word, they were not considered as errors unless they caused the judge to misanalyze the sentence. Infrequent tags. An undesirable result of our \"enriched\" tag set is that some tags, e.g., the post-determiner last, occur too infrequently in the corpora to provide reliable statistics. Solutions to some of these problems will clearly be more tractable than to others. 4 Comparison of Results Comparison of these results to those of other systems is difficult because there is no generally accepted test set or performance baseline. Given this limitation, we compared ALEK's performance to a widely used grammar checker, the one incorporated in Microsoft's Word97. We created files of sentences used for the three development words concentrate, interest, and knowledge, and manually corrected any errors outside the local context around the target before checking them with Word97. The performance for concentrate showed overall precision of 0.89 and recall of 0.07. For interest, precision was 0.85 with recall of 0.11. In sentences containing knowledge, precision was 0.99 and recall was 0.30. Word97 correctly detected the ungrammaticality of knowledges as well as a knowledge, while it avoided flagging a knowledge of. In summary, Word97's precision in error detection is impressive, but the lower recall values indicate that it is responding to fewer error types than does ALEK. In particular, Word97 is not sensitive to inappropriate selection of prepositions for these three words (e.g., *have knowledge on history, *to concentrate at science). Of course, Word97 detects many kinds of errors that ALEK does not. Research has been reported on grammar checkers specifically designed for an ESL population. These have been developed by hand, based on small training and test sets. Schneider and McCoy (1998) developed a system tailored to the error productions of American Sign Language signers. This system was tested on 79 sentences containing determiner and agreement errors, and 101 grammatical sentences. We calculate that their precision was 78% with 54% recall. Park, Palmer and Washburn (1997) adapted a categorial grammar to recognize \"classes of errors [that] dominate\" in the nine essays they inspected. This system was tested on eight essays, but precision and recall figures are not reported. 5 Conclusion The unsupervised techniques that we have presented for inferring negative evidence are effective in recognizing grammatical errors in written text. Preliminary results indicate that ALEK's error detection is predictive of TOEFL scores. If ALEK accurately detects usage errors, then it should report more errors in essays with lower scores than in those with higher scores. We have already seen in Table 1 that there is a negative correlation between essay score and two of ALEK's component measures, the general corpus n-grams. However, the data in Table 1 were not based on specific vocabulary items and do not reflect overall system performance, which includes the other measures as well. Table 6 shows the proportion of test word occurrences that were classified by ALEK as containing errors within two positions of the target at each of 6 TOEFL score points. As predicted, the correlation is negative (r = -1.00, n = 6, p < .001, two-tailed). These data support the validity of the system as a detector of inappropriate usage, even when only a limited number of words are targeted and only the immediate context of each target is examined. Score ALEK Human 1 .091 --- 2 .085 .375 3 .067 .268 4 .057 .293 5 .048 .232 6 .041 .164 Table 6: Proportion of test word occurrences, by score point, classified as containing an error by ALEK and by a human judge For comparison, Table 6 also gives the estimated proportions of inappropriate usage by score point based on the human judge's classification. Here, too, there is a negative correlation: rs = -.90, n = 5, p < .05, two-tailed. Although the system recognizes a wide range of error types, as Table 6 shows, it detects only about one-fifth as many errors as a human judge does. To improve recall, research needs to focus on the areas identified in section 3.2 and, to improve precision, efforts should be directed at reducing the false positives described in 3.3. ALEK is being developed as a diagnostic tool for students who are learning English as a foreign language. However, its techniques could be incorporated into a grammar checker for native speakers. Acknowledgments We thank Susanne Wolff for evaluating the test sentences, and Robert Kantor, Ken Sheppard and 3 anonymous reviewers for their helpful suggestions. References Brill, E. 1994. Some advances in rule-based part-of- speech tagging. Proceedings of the Twelfth National Conference on Artificial Intelligence, Seattle, AAAI. Choueka, Y. and S. Lusignan. 1985. Disambiguation by short contexts. Computers and the Humanities, 19:147-158. Cohen, J. and P. Cohen. 1983. Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences. Hillsdale, NJ: Erlbaum. Francis, W. and H. Kučera. 1982. Frequency Analysis of English Usage: Lexicon and Grammar. Boston, Houghton Mifflin. Golding, A. 1995. A Bayesian hybrid for context- sensitive spelling correction. Proceedings of the 3rd Workshop on Very Large Corpora. Cambridge, ΜΑ. 39-53. Kilgarriff, A. and M. Palmer. 2000. Introduction to the special issue on SENSEVAL. Computers and the Humanities, 34:1-2. Leacock, C., M. Chodorow and G.A. Miller. 1998. 1998. Using corpus statistics and WordNet's lexical relations for sense identification. Computational Linguistics, 24:1. Lin, D. 1998. Extracting collocations from text corpora. First Workshop on Computational Terminology. Montreal, Canada. Miller, G.A. and P. Gildea. 1987. How children learn words. Scientific American, 257. Nation, I.S.P. 1990. Teaching and learning vocabulary. New York: Newbury House. Nicholls, D. 1999. The Cambridge Learner Corpus Error coding and analysis. Summer Workshop on Learner Corpora. Tokyo Park, J.C., M. Palmer and G. Washburn. 1997. Checking grammatical mistakes for English-as-a- second-language (ESL) students. Proceedings of KSEA-NERC. New Brunswick, NJ. Schneider, D.A. and K.F. McCoy. 1998. Recognizing syntactic errors in the writing of second language learners. Proceedings of Coling-ACL-98, Montréal. Yarowsky, D. 1993. One sense per collocation. Proceedings of the ARPA Workshop on Human Language Technology. San Francisco. Morgan Kaufman."
  },
  {
    "title": "QuickSet: Multimodal Interaction for Simulation Set-up and Control",
    "abstract": "This paper presents a novel multimodal system applied to the setup and control of distributed interactive simulations. We have developed the QuickSet prototype, a pen/voice system running on a hand-held PC, communicating through a distributed agent architecture to NRaD's¹ LeatherNet system, a distributed interactive training simulator built for the US Marine Corps (USMC). The paper briefly describes the system and illustrates its use in multimodal simulation setup. KEYWORDS: multimodal interfaces, agent architecture, gesture recognition, speech recognition, natural language processing, distributed interactive simulation.",
    "content": "1. INTRODUCTION In order to train personnel more effectively, the US military is developing large-scale distributed simulation capabilities. Begun as SIMNET in the 1980's [23], these distributed, interactive environments attempt to provide a high degree of fidelity in simulating combat, including simulations of the individual combatants, the equipment, entity movements, atmospheric effects, etc. There are four general phases of user interaction with these simulations: Creating entities, supplying their initial behavior, interacting with the entities during a running simulation, and reviewing the results. The present research concentrates on the first two of these stages. Our contribution to the distributed interactive simulation (DIS) effort is to rethink the nature of the user interaction. As with most modern simulators, DISs are controlled via graphical user interfaces (GUIs). However, the simulation GUI is showing signs of strain, since even for a small-scale scenario, it requires users to choose from hundreds of entities in order to select the desired ones to place on a map. To compound these interface problems, the military is intending to increase the scale of the simulations ¹ NRaD = US Navy Command and Control Ocean Systems Center Research Development Test and Evaluation (San Diego). dramatically, while at the same time, for reasons of mobility and affordability, desiring that simulations should be creatable from small devices (e.g., PDAs). This impending collision of trends for smaller screen size and for more entities requires a different paradigm for human- computer interaction. We have argued generically that GUI technologies offer advantages in allowing users to manipulate objects that are on the screen, in reminding users of their options, and in minimizing errors [7]. However, GUIs are often weak in supporting interactions with many objects, or objects not on the screen. In contrast, it was argued that linguistically- based interface technologies offer the potential to describe large sets of objects, which may not all be present on a screen, and can be used to create more complex behaviors through specification of rule invocation conditions. Simulation is one type of application for which these limitations of GUIs, as well as the strengths of natural language, especially spoken language, are apparent [6]. It has become clear, however, that speech-only interaction is not optimal for spatial tasks. Using a high-fidelity \"Wizard-of-Oz\" methodology [20], recent empirical results demonstrate clear language processing and task performance advantages for multimodal (pen/voice) input over speech- only input for map-based systems [17,18]. 3. QUICKSET To address these simulation interface problems, and motivated by the above results, we have developed QuickSet (see Figure 1) a collaborative, handheld, multimodal system for configuring military simulations based on LeatherNet [5], a system used in training platoon leaders and company commanders at the USMC base at 29 Palms, California. LeatherNet simulations are created using the ModSAF simulator [10] and can be visualized in a CAVE-based virtual reality environment [11, 26] called CommandVu (see Figure 2 QuickSet systems are on the soldiers' tables). In addition to LeatherNet, QuickSet is being used in a second effort called ExInit (Exercise Initialization), that will enable users to create division-sized exercises. Because of the use of OAA, QuickSet can interoperate with agents from CommandTalk [14], which provides a speech-only interface to ModSAF. QuickSet runs on both desktop and hand-held PC's, communicating over wired and wireless LAN's, or modem links. The system combines speech and pen-based gesture input on multiple 3-lb hand-held PCs (Fujitsu Stylistic 1000), which communicate via wireless LAN through the Open Agent Architecture (OAA)<sup>2</sup> [8], to ModSAF, and also to CommandVu. With this highly portable device, a user can create entities, establish \"control measures\" (e.g., objectives, checkpoints, etc.), draw and label various lines and areas, (e.g., landing zones) and give the entities behavior. Figure 1: QuickSet running on a wireless handheld PC. In the remainder of the paper, we illustrate the system briefly, describe its components, and discuss its application. SPOKEN AND GESTURAL INTERACTION Figure 2: Artist's rendition of QuickSet used with CommandVu virtual display of distributed interactive simulation. 4. SYSTEM ARCHITECTURE Architecturally, QuickSet uses distributed agent technologies based on the Open Agent Architecture for interoperation, information brokering and distribution. An <sup>2</sup> Open Agent Architecture is a trademark of SRI International. agent-based architecture was chosen to support this application because it offers easy connection to legacy applications, and the ability to run the same set of software components in a variety of hardware configurations, ranging from stand-alone on the handheld PC, to distributed operation across numerous workstations and PCs. Additionally, the architecture supports mobility in that lighter weight agents can run on the handheld, while more computationally-intensive processing can be migrated elsewhere on the network. The agents may be written in any programming language (here, Quintus Prolog, Visual C++, Visual Basic, and Java), as long as they communicate via an interagent communication language. The configuration of agents used in the Quickset system is illustrated in Figure 3. A brief description of each agent follows: QuickSet interface: On the handheld PC is a geo- referenced map of the region such that entities displayed on the map are registered to their positions on the actual terrain, and thereby to their positions on each of the various user interfaces connected to the simulation. The map interface agent provides the usual pan and zoom capabilities, multiple overlays, icons, etc. The user can draw directly on the map, in order to create points, lines, and areas. The user can create entities, give them behavior, and watch the simulation unfold from the handheld. When the pen is placed on the screen, the speech recognizer is activated, thereby allowing users to speak and gesture simultaneously. Speech recognition agent: The speech recognition agent used in QuickSet employs either IBM's VoiceType Application Factory or VoiceType 3.0 recognizers. The recognizers use an HMM-based continuous speaker- independent speech recognition technology for PC's under Windows 95/NT. Currently, the system has a vocabulary of 450 words. It produces a single most likely interpretation of an utterance. Gesture recognition agent: OGI's gesture recognition agent processes all pen input from a PC screen or tablet. The agent weights the results of both HMM and neural net recognizers, producing a combined score for each of the possible recognition results. Currently, 45 gestures can be recognized, resulting in the creation of 21 military symbols, irregular shapes, and various types of lines. Natural language agent: The natural language agent currently employs a definite clause grammar and produces typed feature structures as a representation of the utterance meaning. Currently, for this task, the language consists of noun phrases that label entities, as well as a variety of imperative constructs for supplying behavior. Multimodal integration agent: The multimodal interpretation agent accepts typed feature structure meaning representations from the language and gesture recognition agents, and produces a unified multimodal interpretation. QuickSet Brokered Architecture Other Facilitator Corba bridge Other Interfaces Databases CommandVu VR Interface Gesture Speech Mediation Facilitator ModSAF Simulator More detail on the architecture and the individual agents are provided in [12, 22]. 5. EXAMPLE Holding QuickSet in hand, the user views a map from the ModSAF simulation, and with spoken language coupled with pen gestures, issues commands to ModSAF. In order to create a unit in QuickSet, the user would hold the pen at the desired location and utter (for instance): \"red T72 platoon\" resulting in a new platoon of the specified type being created at that location. Simulation (Mediation -- Horn Clauses) Natural Language Multimodal Integration User Interface Application Bridge Figure 3: A blackboard is used by a facilitator agent, who routes queries to appropriate agents for solution. Simulation agent: The simulation agent, developed primarily by SRI International, but modified by us for multimodal interaction, serves as the communication channel between the OAA-brokered agents and the ModSAF simulation system. This agent offers an API for ModSAF that other agents can use. Web display agent: The Web display agent can be used to create entities, points, lines, and areas. It posts queries for updates to the state of the simulation via Java code that interacts with the blackboard and facilitator. The queries are routed to the running ModSAF simulation, and the available entities can be viewed over a WWW connection using a suitable browser. Other user interfaces: When another user interface connected to the facilitator subscribes to and produces the same set of events as others, it immediately becomes part of a collaboration. One can view this as human-human collaboration mediated by the agent architecture, or as agent- agent collaboration. CommandVu agent: Since the CommandVu virtual reality system is an agent, the same multimodal interface on the handheld PC can be used to create entities and to fly the user through the 3-D terrain. For example, the user can ask \"CommandVu, fly me to this platoon <gesture on the map>.\" Application bridge agent: The bridge agent generalizes the underlying applications' API to typed feature structures, thereby providing an interface to the various applications such as ModSAF, CommandVu, and Exinit. This allows for a domain-independent integration architecture in which constraints on multimodal interpretation are stated in terms of higher-level constructs such as typed feature structures, greatly facilitating reuse. CORBA bridge agent: This agent converts OAA messages to CORBA IDL (Interface Definition Language) for the Exercise Initialization project. M1A1 PLATOON FOLLOW THIS ROUTE Figure 4: The QuickSet interface as the user establishes two platoons, a barbed-wire fence, a breached minefield, and then issues a command to one platoon to follow a traced route. The user then adds a barbed-wire fence to the simulation by drawing a line at the desired location while uttering \"barbed wire.\" Similarly a fortified line is added. A minefield of an amorphous shape is drawn and is labeled verbally, and finally an M1A1 platoon is created as above. Then the user can assign a task to the new platoon by saying \"M1A1 platoon follow this route\" while drawing the route with the pen. The results of these commands are visible on the QuickSet screen, as seen in Figure 4, in the ModSAF simulation, and in the CommandVu 3D rendering of the scene. In addition to multimodal input, unimodal spoken language and gestural commands can be given at any time, depending on the user's task and preference. 6. MULTIMODAL INTEGRATION Since any unimodal recognizer will make mistakes, the output of the gesture recognizer is not accepted as a simple unilateral decision. Instead the recognizer produces a set of probabilities, one for each possible interpretation of the gesture. The recognized entities, as well as their recognition probabilities, are sent to the facilitator, which forwards them to the multimodal interpretation agent. In combining the meanings of the gestural and spoken interpretations, we attempt to satisfy an important design consideration, namely that the communicative modalities should compensate for each other's weaknesses [7, 16]. This is accomplished by selecting the highest scoring unified interpretation of speech and gesture. Importantly, the unified interpretation might not include the highest scoring gestural (or spoken language) interpretation because it might not be semantically compatible with the other mode. The key to this interpretation process is the use of a typed feature structure [1, 3] as a meaning representation language that is common to the natural language and gestural interpretation agents. Johnston et al. [12] present the details of multimodal integration of continuous speech and pen-based gesture, guided by research in users' multimodal integration and synchronization strategies [19]. Unlike many previous approaches to multimodal integration (e.g, [2, 9, 12, 15, 25]) speech is not \"in charge,\" in the sense of relegating gesture a secondary and dependent role. This mutually-compensatory interpretation process is capable of analyzing multimodal constructions, as well as speech-only and pen-only constructions when they occur. Vo and Wood's system [24] is similar to the one reported here, though we believe the use of typed feature structures provides a more generally usable and formal integration mechanism than their frame-merging strategy. Cheyer and Julia [4] sketch a system based on Oviatt's [17] results and the OAA [8], but do not discuss the integration strategy nor multimodal compensation. 7. CONCLUDING REMARKS QuickSet has been delivered to the US Navy (NRaD) and US Marine Corps. for use at 29 Palms, California, where it is primarily used to set up training scenarios and to control the virtual environment. It is also installed at NRaD's Command Center of the Future. The system was used by the US Army's 82nd Airborne Corps. at Ft. Bragg during the Royal Dragon Exercise. There, QuickSet was deployed in a tent, where it was subjected to an extreme noise environment, including explosions, low-flying jet aircraft, generators, and the like. Not surprisingly, spoken interaction with QuickSet was not feasible, although users gestured successfully. Instead, users wanted to gesture. Although we had provided a multimodal interface for use in less hostile conditions, nevertheless we needed to provide,and in fact have provided, a complete overlap in functionality, such that any task can be accomplished just with pen or just with speech when necessary. Finally, QuickSet is now being extended for use in the ExInit simulation initialization system for DARPA'S STOW-97 Advanced Concept Demonstration that is intended for creation of division-sized exercises. Regarding the multimodal interface itself, QuickSet has undergone a \"proactive\" interface evaluation in that the studies that were performed in advance of building the system predicted the utility of multimodal over unimodal speech as an input to map-based systems [17, 18]. In particular, it was discovered in this research that multimodal interaction generates simpler language than unimodal spoken commands to maps. For example, to create a \"phase line\" between two three-digit <x,y> grid coordinates, a user would have to say: \"create a line from nine four three nine six one to nine five seven nine six eight and call it phase line green\" [14]. In contrast, a QuickSet user would say \"phase line green\" while drawing a line. Creation of area features with unimodal speech would be more complex still, if not infeasible. Given that numerous difficult-to-process linguistic phenomena (such as utterance disfluencies) are known to be elevated in lengthy utterances, and also to be elevated when people speak locative constituents [17, 18], multimodal interaction that permits pen input to specify locations and that results in brevity offers the possibility of more robust recognition. Further development of QuickSet's spoken, gestural, and multimodal integration capabilites are continuing. Research is also ongoing to examine and quantify the benefits of multimodal interaction in general, and our architecture in particular. ACKNOWLEDGMENTS This work is supported in part by the Information Technology and Information Systems offices of DARPA under contract number DABT63-95-C-007, in part by ONR grant number N00014-95-1-1164, and has been done in collaboration with the US Navy's NCCOSC RDT&E Division (NRaD), Ascent Technologies, Mitre Corp., MRJ Corp., and SRI International. REFERENCES 1. Calder, J. Typed unification for natural language processing. In E. Klein and J. van Benthem (Eds.), Categories, Polymorphisms, and Unification. Centre for Cognitive Science, University of Edinburgh, Edinburgh, 1987, 65-72. 2. Brison, E. and N. Vigouroux. (unpublished ms.). Multimodal references: A generic fusion process. URIT-URA CNRS. Université Paul Sabatier, Toulouse, France. 3. Carpenter, R. The logic of typed feature structures. Cambridge University Press, Cambridge, 1992. 4. Cheyer, A., and L. Julia. Multimodal maps: An agent- based approach. International Conference on Cooperative Multimodal Communication (CMC/95), May 1995. Eindhoven, The Netherlands, 1995, 24-26. 5. Clarkson, J. D., and Yi., J., LeatherNet: A synthetic forces tactical training system for the USMC commander. Proceedings of the Sixth Conference on Computer Generated Forces and Behavioral Representation. Institute for simulation and training. Technical Report IST-TR-96-18, 1996, 275-281. 6. Cohen, P. R. Integrated Interfaces for Decision Support with Simulation, Proceedings of the Winter Simulation: Conference, Nelson, B. and Kelton, W. D. and Clark, G. M., (eds.), ACM, New York, December, 1991, 1066-1072. 7. Cohen, P. R. The Role of Natural Language in a Multimodal Interface. Proceedings of UIST'92, ACM Press, New York, 1992, 143-149. 8. Cohen, P.R., Cheyer, A., Wang, M., and Baeg, S.C. An Open Agent Architecture. Working notes of the AAAI Spring Symposium Series on Software Agents Stanford Univ., CA, March, 1994, 1-8. 9. Cohen, P. R., Dalrymple, M., Moran, D.B., Pereira, F. C. N., Sullivan, J. W., Gargan, R. A., Schlossberg, J. L., and Tyler, S.W. Synergistic Use of Direct Manipulation and Natural Language, Human Factors in Computing Systems: CHI'89 Conference Proceedings, ACM, Addison Wesley Publishing Co New York, 227-234, 1989. 10. Courtemanche, A.J. and Ceranowicz, A. ModSAF Development Status. Proceedings of the Fifth Conference on Computer Generated Forces and Behavioral Representation, Univ. Central Florida, Orlando, 1995, 3-13. 11. Cruz-Neira, C. D.J. Sandin, T.A. DeFanti, \"Surround- Screen Projection-Based Virtual Reality: The Design and Implementation of the CAVE,\" Computer Graphics (Proceedings of SIGGRAH'93), ACM SIGGRAPH, August 1993, 135-142. 12. Johnston, M., Cohen, P. R., McGee, D., Oviatt, S. L., Pittman, J., and Smith, I.. Unification-based multimodal integration, in submission. 13. Koons, D.B., C.J. Sparrell and K.R. Thorisson. 1993. Integrating simultaneous input from speech, gaze, and hand gestures. In Mark T. Maybury (ed.) Intelligent Multimedia Interfaces. AAAI Press/ MIT Press, Cambridge, MA, 257-276. 14. Moore, R., Dowding, J. Bratt, H. Gawron, J. M., and Cheyer, A., CommandTalk: A Spoken-Language Interface for Battlefield Simulations, 1997, (this volume). 15. Neal, J.G. and Shapiro, S.C. Intelligent multi-media interface technology. In J.W. Sullivan and S.W. Tyler, editors, Intelligent User Interfaces, chapter 3, pages 45- 68. ACM Press Frontier Series, Addison Wesley Publishing Co., New York, New York, 1991. 16. Oviatt, S. L., Pen/Voice: Complementary multimodal communication, Proceedings of SpeechTech'92, New York, February, 1992, 238-241. 17. Oviatt, S.L. Multimodal interfaces for dynamic interactive maps. Proceedings of CHI'96 Human Factors in Computing Systems (April 13-18, Vancouver, Canada), ACM Press, NY, 1996, 95-102. 18. Oviatt, S. L., Multimodal interactive maps: Designing for human performance, Human-Computer Interaction, in press. 19. Oviatt, S. L, A. DeAngeli, and K. Kuhn. In press. Integration and synchronization of input modes during multimodal human-computer interaction. Proceedings of the Conference on Human Factors in Computing Systems (CHI '97), ACM Press, New York. 20. Oviatt, S. L., Cohen, P. R, Fong, M. W. and Frank, M. P., A rapid semi-automatic simulation technique for interactive speech and handwriting, Proceedings of the 1992 International Conference Spoken Language Processing, vol. 2, University of Alberta, J. Ohala (ed.), October, 1992, 1351-1354. 21. Oviatt, S. L., Cohen, P. R., Wang, M. Q.,Toward interface design for human language technology: Modality and structure as determinants of linguistic complexity, Speech Communication, 15 (3-4), 1994. 22. Pittman, J.A., Smith, I.A., Cohen, P.R., Oviatt, S.L., and Yang, T.C. QuickSet: A Multimodal Interface for Military Simulation. in Proceedings of the Sixth Conference on Computer-Generated Forces and Behavioral Representation, Orlando, Florida, 1996. 23. Thorpe, J. A., The new technology of large scale simulator networking: Implications for mastering the art of warfighting. Proceedings of the 9th Interservice/industry Training Systems Conference, Orlando, Florida, December, 1987, 492-501. 24. Vo, M. T. and C. Wood. Building an application framework for speech and pen input integration in multimodal learning interfaces. International Conference on Acoustics, Speech, and Signal Processing, Atlanta, GA, 1996. 25. Wauchope, K. Eucalyptus: Integrating natural language input with a graphical user interface. Naval Research Laboratory, Report NRL/FR/5510--94-9711, 1994. 26. Zyda, M. J., Pratt, D. R., Monahan, J. G., and Wilson, K. P., NPSNET: Constructing a 3-D virtual world, Proceedings of the 1992 Symposium on Interactive 3-D Graphics, March, 1992."
  },
  {
    "title": "Building a Generation Knowledge Source using Internet-Accessible Newswire",
    "abstract": "In this paper, we describe a method for automatic creation of a knowledge source for text generation using information extraction over the Internet. We present a prototype system called PROFILE which uses a client-server architecture to extract noun-phrase descriptions of entities such as people, places, and organizations. The system serves two purposes: as an information extraction tool, it allows users to search for textual descriptions of entities; as a utility to generate functional descriptions (FD), it is used in a functional-unification based generation system. We present an evaluation of the approach and its applications to natural language generation and summarization.",
    "content": "1 Introduction In our work to date on news summarization at Columbia University (McKeown and Radev, 1995; Radev, 1996), information is extracted from a se- ries of input news articles (MUC, 1992; Grishman et al., 1992) and is analyzed by a generation com- ponent to produce a summary that shows how per- ception of the event has changed over time. In this summarization paradigm, problems arise when in- formation needed for the summary is either miss- ing from the input article(s) or not extracted by the information extraction system. In such cases, the information may be readily available in other current news stories, in past news, or in online databases. If the summarization system can find the needed information in other online sources, then it can produce an improved summary by merging information from multiple sources with information extracted from the input articles. In the news domain, a summary needs to refer to people, places, and organizations and provide descriptions that clearly identify the entity for the reader. Such descriptions may not be present in the original text that is being summarized. For ex- ample, the American pilot Scott O'Grady, downed in Bosnia in June of 1995, was unheard of by the American public prior to the incident. If a reader tuned into news on this event days later, descrip- tions from the initial articles may be more useful. A summarizer that has access to different descrip- tions will be able to select the description that best suits both the reader and the series of articles be- ing summarized. In this paper, we describe a system called PROFILE that tracks prior references to a given entity by extracting descriptions for later use in summarization. In contrast with previous work on information extraction, our work has the following features: • It builds a database of profiles for entities by storing descriptions from a collected corpus of past news. • It operates in real time, allowing for connec- tions with the latest breaking, online news to extract information about the most recently mentioned individuals and organizations. • It collects and merges information from dis- tributed sources thus allowing for a more com- plete record of information. • As it parses and identifies descriptions, it builds a lexicalized, syntactic representation of the description in a form suitable for in- put to the FUF/SURGE language generation system (Elhadad, 1993; Robin, 1994). The result is a system that can combine de- scriptions from articles appearing only a few min- utes before the ones being summarized with de- scriptions from past news in a permanent record for future use. Its utility lies in its potential for representing entities, present in one article, with descriptions found in other articles, possibly com- ing from another source. Since the system constructs a lexicalized, syn- tactic functional description (FD) from the ex- tracted description, the generator can re-use the description in new contexts, merging it with other descriptions, into a new grammatical sentence. This would not be possible if only canned strings were used, with no information about their inter- nal structure. Thus, in addition to collecting a knowledge source which provides identifying fea- tures of individuals, PROFILE also provides a lex- icon of domain appropriate phrases that can be in- tegrated with individual words from a generator's lexicon to flexibly produce summary wording. We have extended the system by semantically categorizing descriptions using WordNet (Miller et al., 1990), so that a generator can more easily de- termine which description is relevant in different contexts. PROFILE can also be used in a real-time fash- ion to monitor entities and the changes of descrip- tions associated with them over the course of time. In the following sections, we first overview re- lated work in the area of information extraction. We then turn to a discussion of the system com- ponents which build the profile database, followed by a description of how the results are used in gen- eration. We close with our current directions, de- scribing what parameters can influence a strategy for generating a sequence of anaphoric references to the same entity over time. 2 Related Work Research related to ours falls into two main cate- gories: extraction of information from input text and construction of knowledge sources for genera- tion. 2.1 Information Extraction Work on information extraction is quite broad and covers far more topics and problems than the in- formation extraction problem we address. We restrict our comparison here to work on proper noun extraction, extraction of people descriptions in various information extraction systems devel- oped for the message understanding conferences (MUC, 1992), and use of extracted information for question answering. Techniques for proper noun extraction include the use of regular grammars to delimit and iden- tify proper nouns (Mani et al., 1993; Paik et al., 1994), the use of extensive name lists, place names, titles and \"gazetteers\" in conjunction with par- tial grammars in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored the use of both lo- cal context surrounding the hypothesized proper nouns (McDonald, 1993; Coates-Stephens, 1991) and the larger discourse context (Mani et al., 1993) to improve the accuracy of proper noun extrac- tion when large known word lists are not available. Like this research, our work also aims at extract- ing proper nouns without the aid of large word lists. We use a regular grammar encoding part-of- speech categories to extract certain text patterns and we use WordNet (Miller et al., 1990) to pro- vide semantic filtering. Our work on extracting descriptions is quite similar to the work carried out under the DARPA message understanding program for extracting de- scriptions (MUC, 1992). The purpose for and the scenario in which description extraction is done is quite different, but the techniques are very simi- lar. It is based on the paradigm of representing patterns that express the kinds of descriptions we expect; unlike previous work we do not encode se- mantic categories in the patterns since we want to capture all descriptions regardless of domain. Research on a system called Murax (Kupiec, 1993) is similar to ours from a different perspec- tive. Murax also extracts information from a text to serve directly in response to a user question. Murax uses lexico-syntactic patterns, collocational analysis, along with information retrieval statis- tics, to find the string of words in a text that is most likely to serve as an answer to a user's wh- query. In our work, the string that is extracted may be merged, or regenerated, as part of a larger textual summary. 2.2 Construction of Knowledge Sources for Generation The construction of a database of phrases for re- use in generation is quite novel. Previous work on extraction of collocations for use in genera- tion (Smadja and McKeown, 1991) is related in that full phrases are extracted and syntactically typed so that they can be merged with individual words in a generation lexicon to produce a full sen- tence. However, extracted collocations were used only to determine realization of an input concept. In our work, stored phrases would be used to pro- vide content that can identify a person or place for a reader, in addition to providing the actual phrasing. 3 Creation of a Database of Profiles Figure 1 shows the overall architecture of PRO- FILE and the two interfaces to it (a user interface on the World-Wide Web and an interface to a nat- ural language generation system). In this section, we describe the extraction component of PRO- FILE, the following section focuses on the uses of PROFILE for generation, and Section 7 describes the Web-based interface. News retrieval News Entity Extraction Description Extraction Description Categorization Description Storage Descriptions Entities PROFILE Web Interface Conversion to FDs User Surface Generation Figure 1: Overall Architecture of PROFILE. 3.1 Extraction of entity names from old newswire To seed the database with an initial set of descrip- tions, we used a 1.4 MB corpus containing Reuters newswire from March to June of 1995. The pur- pose of such an initial set of descriptions is twofold. First, it allows us to test the other components of the system. Furthermore, at the time a descrip- tion is needed it limits the amount of online full text, Web search that must be done. At this stage, search is limited to the database of retrieved de- scriptions only, thus reducing search time as no connections will be made to external news sources at the time of the query. Only when a suitable stored description cannot be found will the sys- tem initiate search of additional text. • Extraction of candidates for proper nouns. After tagging the corpus using the POS part-of-speech tagger (Church, 1988), we used a CREP (Duford, 1993) regular grammar to first extract all possible candi- dates for entities. These consist of all se- quences of words that were tagged as proper nouns (NP) by POS. Our manual analysis showed that out of a total of 2150 entities recovered in this way, 1139 (52.9%) are not names of entities. Among these are n-grams such as \"Prime Minister\" or \"Egyptian Pres- ident\" which were tagged as NP by POS. Ta- ble 1 shows how many entities we retrieve at this stage, and of them, how many pass the semantic filtering test. The numbers in the left-hand column refer to two-word noun phrases that identify entities (e.g., \"Bill Clin- ton\"). Counts for three-word noun phrases are shown in the right-hand column. We show counts for multiple and unique occurrences of the same noun phrase. • Weeding out of false candidates. Our system analyzed all candidates for entity names using WordNet (Miller et al., 1990) and removed from consideration those that contain words appearing in WordNet's dictio- nary. This resulted in a list of 421 unique entity names that we used for the automatic description extraction stage. All 421 entity names retrieved by the system are indeed proper nouns. 3.2 Extraction of descriptions There are two occasions on which we extract de- scriptions using finite-state techniques. The first case is when the entity that we want to describe was already extracted automatically (see Subsec- tion 3.1) and exists in PROFILE's database. The second case is when we want a description to be re- trieved in real time based on a request from either a Web user or the generation system. There exist many live sources of newswire on the Internet that can be used for this second case. Some that merit our attention are the ones that can be accessed remotely through small client pro- grams that don't require any sophisticated proto- cols to access the newswire articles. Such sources include HTTP-accessible sites such as the Reuters site at www.yahoo.com and CNN Interactive at www.cnn.com, as well as others such as ClariNet which is propagated through the NNTP protocol. All these sources share a common characteristic in that they are all updated in real time and all contain information about current events. Hence, they are therefore likely to satisfy the criteria of pertinence to our task, such as the likelihood of the sudden appearance of new entities that couldn't possibly have been included a priori in the gener- ation lexicon. Our system generates finite-state representa- tions of the entities that need to be described. An example of a finite-state description of the entity \"Yasser Arafat\" is shown in Figure 2. These full expressions are used as input to the description finding module which uses them to find candidate sentences in the corpus for finding descriptions. Since the need for a description may arise at a later time than when the entity was found and may require searching new text, the description finder must first locate these expressions in the text. These representations are fed to CREP which extracts noun phrases on either side of the en- tity (either pre-modifiers or appositions) from the news corpus. The finite-state grammar for noun 223 Stage POS tagging only After WordNet checkup Two-word entities Entities Unique Entities 9079 1546 1509 395 Three-word entities Entities Unique Entities 2617 604 81 26 Table 1: Two-word and three-word entities retrieved by the system. SEARCH_STRING = (({NOUN_PHRASE}{SPACE})+{SEARCH_0})|({SEARCH_0}{SPACE}{COMMA}{SPACE}{NOUN_PHRASE}) SEARCH_109 = [Yy]asser{T_NOUN}{SPACE}[Aa]rafat{T_NOUN} SEARCH_0 = {SEARCH_1}|{SEARCH_2}|...|{SEARCH_109}|... Figure 2: Finite-state representation of \"Yasser Arafat\". phrases that we use represents a variety of differ- ent syntactic structures for both pre-modifiers and appositions. Thus, they may range from a simple noun (e.g., \"president Bill Clinton\") to a much longer expression (e.g., \"Gilberto Rodriguez Ore- juela, the head of the Cali cocaine cartel\"). Other forms of descriptions, such as relative clauses, are the focus of ongoing implementation. Table 2 shows some of the different patterns retrieved. 3.3 Categorization of descriptions We use WordNet to group extracted descriptions into categories. For all words in the description, we try to find a WordNet hypernym that can re- strict the semantics of the description. Currently, we identify concepts such as \"profession\", \"nation- ality\", and \"organization\". Each of these concepts is triggered by one or more words (which we call \"triggers\") in the description. Table 2 shows some examples of descriptions and the concepts under which they are classified based on the WordNet hy- pernyms for some \"trigger\" words. For example, all of the following \"triggers\" in the list \"minister\", \"head\", \"administrator\", and \"commissioner\" can be traced up to \"leader\" in the WordNet hierarchy. 3.4 Organization of descriptions in a database of profiles For each retrieved entity we create a new profile in a database of profiles. We keep information about the surface string that is used to describe the entity in newswire (e.g., \"Addis Ababa\"), the source of the description and the date that the entry has been made in the database (e.g., \"reuters95_06_25\"). In addition to these pieces of meta-information, all retrieved descriptions and their frequencies are also stored. Currently, our system doesn't have the capa- bility of matching references to the same entity that use different wordings. As a result, we keep separate profiles for each of the following: \"Robert Dole\", \"Dole\", and \"Bob Dole\". We use each of these strings as the key in the database of descrip- tions. Figure 3 shows the profile associated with the key \"John Major\". KEY: john major SOURCE: reuters95_03-06_.nws DESCRIPTION: british prime minister FREQUENCY: 75 DESCRIPTION: prime minister FREQUENCY: 58 DESCRIPTION: a defiant british prime minister FREQUENCY: 2 DESCRIPTION: his british counterpart FREQUENCY: 1 Figure 3: Profile for John Major. The database of profiles is updated every time a query retrieves new descriptions matching a cer- tain key. 4 Generation We have made an attempt to reuse the descrip- tions, retrieved by the system, in more than a triv- ial way. The content planner of a language gener- ation system that needs to present an entity to the user that he has not seen previously, might want to include some background information about it. However, in case the extracted information doesn't contain a handy description, the system can use some descriptions retrieved by PROFILE. 4.1 Transformation of descriptions into Functional Descriptions Since our major goal in extracting descriptions from on-line corpora was to use them in gener- ation, we have written a utility which converts finite-state descriptions retrieved by PROFILE into functional descriptions (FD) (Elhadad, 1991) that we can use directly in generation. A descrip- tion retrieved by the system from the article in 4 is shown in Figure 5. The corresponding FD is shown in Figure 6. We have implemented a TCP/IP interface to Surge. The FD generation component uses this interface to send a new FD to the surface realiza- tion component of Surge which generates an En- glish surface form corresponding to it. 224 Example Trigger Term Semantic Category Addis Ababa, the Ethiopian capital capital location South Africa's main black opposition leader, Mangosuthu Buthelezi leader occupation Boerge Ousland, 33 33 age maverick French ex-soccer boss Bernard Tapie boss occupation Italy's former prime minister, Silvio Berlusconi minister occupation Sinn Fein, the political arm of the Irish Republican Army arm organization Table 2: Examples of retrieved descriptions. MILAN - A judge ordered Italy's former prime minister Silvio Berlusconi to stand trial in Jan- uary on corruption charges in a ruling that could destroy the media magnate's hope of returning to high office. Figure 4: Source article. Italy@NPNP 's@$ former@JJ prime@JJ minister@NN Silvio@NPNP Berlusconi@NPNP Figure 5: Retrieved description for Silvio Berlus- coni. ((cat np) (complex apposition) (restrictive no) (distinct (((cat common) (possessor ((cat common) (determiner none) (lex \"Italy\"))) (classifier ((cat noun-compound) (classifier ((lex \"former\"))) (head ((lex \"prime\"))))) (head ((lex \"minister\")))) ((cat person-name) (first-name ((lex \"Silvio\"))) (last-name ((lex \"Berlusconi\")))))))) Figure 6: Generated FD for Silvio Berlusconi. 4.2 Lexicon creation We have identified several major advantages of using FDs produced by the system in generation compared to using canned phrases. • Grammaticality. The deeper representa- tion allows for grammatical transformations, such as aggregation: e.g., \"president Yeltsin\" + \"president Clinton\" can be generated as \"presidents Yeltsin and Clinton\". • Unification with existing ontologies. E.g., if an ontology contains information about the word \"president\" as being a realiza- tion of the concept \"head of state\", then un- der certain conditions, the description can be replaced by one referring to \"head of state\". • Generation of referring expressions. In the previous example, if \"president Bill Clin- ton\" is used in a sentence, then \"head of state\" can be used as a referring expression in a subsequent sentence. • Enhancement of descriptions. If we have retrieved \"prime minister\" as a description for Silvio Berlusconi, and later we obtain knowl- edge that someone else has become Italy's primer minister, then we can generate \"for- mer prime minister\" using a transformation of the old FD. • Lexical choice. When different descrip- tions are automatically marked for semantics, PROFILE can prefer to generate one over an- other based on semantic features. This is useful if a summary discusses events related to one description associated with the entity more than the others. • Merging lexicons. The lexicon generated automatically by the system can be merged with a domain lexicon generated manually. These advantages look very promising and we will be exploring them in detail in our work on summarization in the near future. 5 Coverage and Limitations In this section we provide an analysis of the capa- bilities and current limitations of PROFILE. 5.1 Coverage At the current stage of implementation, PROFILE has the following coverage. • Syntactic coverage. Currently, the sys- tem includes an extensive finite-state gram- mar that can handle various pre-modifiers and appositions. The grammar matches arbi- trary noun phrases in each of these two cases to the extent that the POS part-of-speech tag- ger provides a correct tagging. • Precision. In Subsection 3.1 we showed the precision of the extraction of entity names. Similarly, we have computed the precision of retrieved 611 descriptions using randomly se- lected entities from the list retrieved in Sub- section 3.1. Of the 611 descriptions, 551 (90.2%) were correct. The others included a roughly equal number of cases of incorrect NP attachment and incorrect part-of-speech 5.2 Limitations Our system currently doesn't handle entity cross- referencing. It will not realize that \"Clinton\" and \"Bill Clinton\" refer to the same person. Nor will it link a person's profile with the profile of the organization of which he is a member. At this stage, the system generates functional descriptions (FD), but they are not being used in a summarization system yet. 6 Current Directions One of the more important current goals is to increase coverage of the system by providing in- terfaces to a large number of on-line sources of news. We would ideally want to build a compre- hensive and shareable database of profiles that can be queried over the World-Wide Web. We need to refine the algorithm to handle cases that are currently problematic. For example, pol- ysemy is not properly handled. For instance, we would not label properly noun phrases such as \"Rice University\", as it contains the word \"rice\", which can be categorized as a food. Another long-term goal of our research is the generation of evolving summaries that continu- ously update the user on a given topic of inter- est. In that case, the system will have a model containing all prior interaction with the user. To avoid repetitiveness, such a system will have to re- sort to using different descriptions (as well as refer- ring expressions) to address a specific entity¹. We will be investigating an algorithm that will select a proper ordering of multiple descriptions referring to the same person. After we collect a series of descriptions for each possible entity, we need to decide how to select ¹Our corpus analysis supports this proposition – a large number of threads of summaries on the same topic from the Reuters and UPI newswire used up to 10 different referring expressions (mostly of the type of descriptions discussed in this paper) to refer to the same entity. among all of them. There are two scenarios. In the first one, we have to pick one single descrip- tion from the database which best fits the sum- mary that we are generating. In the second sce- nario, the evolving summary, we have to generate a sequence of descriptions, which might possibly view the entity from different perspectives. We are investigating algorithms that will decide the order of generation of the different descriptions. Among the factors that will influence the selec- tion and ordering of descriptions, we can note the user's interests, his knowledge of the entity, the fo- cus of the summary (e.g., \"democratic presidential candidate\" for Bill Clinton, vs. \"U.S. president\"). We can also select one description over another based on how recent they have been included in the database, whether or not one of them has been used in a summary already, whether the summary is an update to an earlier summary, and whether another description from the same category has been used already. We have yet to decide under what circumstances a description needs to be gen- erated at all. We are interested in implementing existing al- gorithms or designing our own that will match dif- ferent instances of the same entity appearing in different syntactic forms e.g., to establish that \"PLO\" is an alias for the \"Palestine Liberation Organization\". We will investigate using cooccur- rence information to match acronyms to full or- ganization names and alternative spellings of the same name with each other. An important application that we are consid- ering is applying the technology to text available using other protocols – such as SMTP (for elec- tronic mail) and retrieve descriptions for entities mentioned in such messages. We will also look into connecting the current interface with news available to the Internet with an existing search engine such as Lycos (www.- lycos.com) or AltaVista (www.altavista.digital.- com). We can then use the existing indices of all Web documents mentioning a given entity as a news corpus on which to perform the extraction of descriptions. Finally, we will investigate the creation of KQML (Finin et al., 1994) interfaces to the differ- ent components of PROFILE which will be linked to other information access modules at Columbia University. 7 Contributions We have described a system that allows users to retrieve descriptions of entities using a Web-based search engine. Figure 7 shows the Web interface to PROFILE. Users can select an entity (such as \"John Major\"), specify what semantic classes of descriptions they want to retrieve (e.g., age, posi- tion, nationality) as well as the maximal number of queries that they want. They can also specify which sources of news should be searched. Cur- rently, the system has an interface to Reuters at www.yahoo.com, The CNN Web site, and to all local news delivered via NNTP to our local news domain. PROFILE Welcome to PROFILE Please formulate a query using the form below. Search engoruthu Buthalesi Search category: person Description type: Maximum number of descriptions retrieved: PROFILE Taboo Reuters US News source: Yahoo/Reute Yahoo/Reuters Would Threshold (V) 2 Figure 7: Web-based interface to PROFILE. The Web-based interface is accessible publicly (currently within Columbia University only). All queries are cached and the descriptions retrieved can be reused in a subsequent query. We believe that such an approach to information extraction can be classified as a collaborative database. The FD generation component produces syn- tactically correct functional descriptions that can be used to generate English-language descriptions using FUF and Surge, and can also be used in a general-purpose summarization system in the do- main of current news. All components of the system assume no prior domain knowledge and are therefore portable to many domains - such as sports, entertainment, and business. 8 Acknowledgments This work was partially supported by NSF grant GER-90-2406 and by a grant from Columbia Uni- versity's Strategic Initiative Fund sponsored by the Provost's Office. References John Aberdeen, John Burger, Dennis Connolly, Susan Roberts, and Marc Vilain. 1992. Mitre- bedford: Description of the alembic system as used for muc-4. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 215-222, McLean, Virginia, June. Damaris Ayuso, Sean Boisen, Heidi Fox, Herb Gish, Robert Ingria, and Ralph Weischedel. 1992. Bbn: Description of the plum system as used for muc-4. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 169-176, McLean, Virginia, June. Kenneth W. Church. 1988. A stochastic parts program and noun phrase parser for unre- stricted text. In Proceedings of the Second Conference on Applied Natural Language Pro- cessing (ANLP-88), pages 136-143, Austin, Texas, February. Association for Computa- tional Linguistics. Sam Coates-Stephens. 1991. Automatic lexi- cal acquisition using within-text descriptions of proper nouns. In Proceedings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research, pages 154-169. Jim Cowie, Louise Guthrie, Yorick Wilks, James Pustejovsky, and Scott Waterman. 1992. Crl/nmsu and brandeis: Description of the mucbruce system as used for muc-4. In Pro- ceedings of the Fourth Message Understanding Conference (MUC-4), pages 223-232, McLean, Virginia, June. Darrin Duford. 1993. Crep: a regular expression- matching textual corpus tool. Technical Re- port CUCS-005-93, Columbia University. Michael Elhadad. 1991. Fuf: The universal unifier user manual, version 5.0. Technical Report CUCS-038-91, Columbia University. Michael Elhadad. 1993. Using argumentation to control lexical choice: a unification-based im- plementation. Ph.D. thesis, Computer Science Department, Columbia University. Tim Finin, Rich Fritzson, Don McKay, and Robin McEntire. 1994. KQML - a language and pro- tocol for knowledge and information exchange. Technical Report CS-94-02, Computer Science Department, University of Maryland and Val- ley Forge Engineering Center, Unisys Corpora- tion. R. Grishman, C. Macleod, and J. Sterling. 1992. New york university: Description of the pro- teus system as used for muc-4. In Proceedings of the Fourth Message Understanding Confer- ence, June. Julian M. Kupiec. 1993. Murax: A robust lin- guistic approach for question answering using an on-line encyclopedia. In Proceedings, 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. W. Lehnert, J. McCarthy, S. Soderlan, E. Riloff, C. Cardie, J. Peterson, and F. Feng. 1993. Umass/hughes: Description of the circus sys- tem used for muc-5. In Proceedings of the Fifth Message Understanding Conference (MUC-5), pages 277-291, Baltimore, Md., August. Inderjeet Mani, Richard T. Macmillan, Su- sann Luperfoy, Elaine Lusher, and Sharon Laskowski. 1993. Identifying unknown proper names in newswire text. In Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text, pages 44-54, Columbus, Ohio, June. Special Interest Group on the Lexicon of the Association for Computational Linguistics. David D. McDonald. 1993. Internal and exter- nal evidence in the identification and semantic cateogrization of proper names. In Proceed- ings of the Workshop on Acquisition of Lexical Knowledge from Text, pages 32-43, Columbus, Ohio, June. Special Interest Group on the Lex- icon of the Association for Computational Lin- guistics. Kathleen R. McKeown and Dragomir R. Radev. 1995. Generating summaries of multiple news articles. In Proceedings, 18th Annual In- ternational ACM SIGIR Conference on Re- search and Development in Information Re- trieval, pages 74-82, Seattle, Washington, July. George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography (special issue), 3(4):235-312. Message Understanding Conference MUC. 1992. Proceedings of the Fourth Message Under- standing Conference (MUC-4). DARPA Soft- ware and Intelligent Systems Technology Of- fice. Woojin Paik, Elizabeth D. Liddy, Edmund Yu, and Mary McKenna. 1994. Interpretation of proper nouns for information retrieval. In Pro- ceedings of the Human Language Technology Workshop, pages 309-313, Plainsboro, New Jersey, March. ARPA Software and Intelli- gent Systems Technology Office, Morgan Kauf- mann. Dragomir R. Radev. 1996. An architecture for dis- tributed natural language summarization. In Proceedings of the 8th International Workshop on Natural Language Generation: Demonstra- tions and Posters, pages 45-48, Herstmon- ceaux, England, June. Jacques Robin. 1994. Revision-Based Genera- tion of Natural Language Summaries Provid- ing Historical Background. Ph.D. thesis, Com- puter Science Department, Columbia Univer- sity. Frank Smadja and Kathleen R. McKeown. 1991. Using collocations for language generation. Computational Intelligence, 7(4), December. Ralph Weischedel, Damaris Ayuso, Sean Boisen, Heidi Fox, Robert Ingria, Tomoyoshi Mat- sukawa, Constantine Papageorgiou, Dawn MacLaughlin, Masaichiro Kitagawa, Tsutomu Sakai, June Abe, Hiroto Hosihi, Yoichi Miyamoto, and Scott Miller. 1993. Bbn: De- scription of the plum system as used for muc- 5. In Proceedings of the Fifth Message Under- standing Conference (MUC-5), pages 93-108, Baltimore, Md., August."
  },
  {
    "title": "An English to Turkish Machine Translation System Using Structural Mapping",
    "abstract": "This paper describes the design and implementation of an English-Turkish machine translation (MT) system developed as a part of the TU-Language project supported by a NATO Science for Stability Project grant. The system uses a structural transfer approach in translating the domain of IBM computer manuals. The general design of the translation system and a detailed description of the transfer component is presented in this paper.",
    "content": "1 Introduction The TU-Language project sponsored by the NATO Science for Stability Programme was started in 1994 to establish computational foundations for the natu- ral language processing research on the Turkish lan- guage with the collaboration of the Computer Engi- neering Department of Middle East Technical Uni- versity, the Computer Science Department of Bilkent University and Halici Computing, Inc. The project attempts to perform extensive research on Turkish which will eventually lead to the development of an English to Turkish machine translation system, Turkish language tutorial system, a Turkish dictio- nary and other software tools to be used in further research. In this paper, some issues in translating from En- glish to Turkish languages, the translation domain, the outline of the machine translation system un- der development, and a detailed description of the transfer component will be presented. 2 Turkish Language Morphology and syntax of Turkish are very differ- ent from English, therefore, the formalism used to represent English texts has to be altered signifi- cantly for Turkish text representation. The Turk- ish language is characterized as a head final lan- guage where the modifier/specifier always precedes the modified/specified. This characteristic also af- fects the word order of the sentences which can be described as SOV where the verb is positioned at the end. Also, when compared to other languages, Turkish relies more on overt case markings which mark the role of the argument in a sentence. The case mark- ings enables Turkish to have a relatively free word- order property where every variation in the word order in a sentence results in a different meaning. In the MT system being developed, these and other different characteristics of the Turkish lan- guage are handled in the transfer and generation components. 3 Translation Domain As more and more computer companies enter the Turkish market, a growing demand for English to Turkish translation of computer manuals has emerged. Other machine translation systems have also chosen the domain of computer manuals for their translation systems because of the relatively unambiguous and narrow sublanguage used (Tsut- sumi, 1986). Also, in his research, Nasukawa (Na- sukawa, 1993) concluded that the statistical anal- ysis of the text in IBM computer manuals showed that 92.6 percent of the words in a computer man- ual are used in the same word sense which would significantly reduce the problem of lexical ambiguity resolution. Another advantage is that the material in a computer manual is observed to be written as clearly as possible in a relatively narrow area which will hopefully ease the difficult job of understanding and representing the input sentence. As a result of these observations, the TU- Language project team has chosen the IBM com- puter manuals as their translation domain.. 4 Machine Translation System The English to Turkish MT system under develop- ment uses a structural transfer approach which has the following components. First, the English sen- tence retrieved from the IBM manual is analyzed by the CLE parser (Alshawi and Moore. 1992) to generate an intermediate representation. This rep- resentation is mapped onto a recursively embedded case frame which is then input to the transfer mod- ule. The transfer module maps the input case frame into the target case frame which is then filtered to be transformed into the required input format of the target language generator. Lastly, the gener- ator maps the Turkish case frame into the Turkish sentence which is then post-edited by a human trans- lator to get an intelligible and accurate translation. 4.1 Analysis Phase For analyzing the English input, the Core Language Engine developed by the SRI Cambridge Computer Science Research Centre was used (Alshawi and Moore, 1992). The CLE system has been trained to meet the lexical, syntactic and semantic demands of the IBM corpus. In CLE, explicit intermediate levels of linguistic representation are used in the dif- ferent phases of the analysis. Following the syntac- tic and semantic analysis/synthesis which uses the unification-based approach, the quasi logical form (QLF) is developed. QLF can be described as a con- textually sensitive logical form. Since the CLE sys- tem produces various parses for an input sentence, the best parse is filtered by the system which con- veys the intended meaning of the sentence. Then the chosen representation is mapped into a case frame. 4.2 Transfer Phase Experience with previous systems using the interlin- gua technique showed the significant complexity of extracting and representing deep meaning of a natu- ral language text (Goodman and Nirenburg, 1991). Another major difficulty encountered with this ap- proach is that the language specific attributes nec- essary to define the translation equivalents in the lexical and structural levels are neutralized in the interlingual representation thereby complicating the task of generation considerably. A similar problem occurred with systems using the transfer approach with deep semantic analysis such as the EUROTRA project (Johnson et al., 1985). Such systems were observed to be difficult to develop and maintain. To avoid these problems, the MT systems developed recently generally chose to use the straightforward transfer approach which relies on various types of lexical, syntactic information and a limited use of semantic analysis (Tsutsumi, 1986). The system being developed as a part of the TU- Language project also chose the structural transfer approach with a minimal amount of semantic anal- ysis. The transfer phase of our MT system per- forms structural transfer between the respective case frames of the analysed English sentence and target- ted Turkish output. In a top-down manner, the transfer module transforms the English case frame or adds new information to the Turkish case frame in order to generate the equivalent Turkish noun phrase, clause or sentence with the aid of a trans- fer dictionary, and the transfer rules. The English and Turkish case frames for clauses/sentences are generally similar to each other with differences seen in the sentence's mood and the verb's aspect and modality. Some information not extracted in the analysis phase such as the sentence form, clause type, role, etc. have to be determined in the beginning of the transfer phase and added to the Turkish case frame. An example sentence and parts of the corresponding English and Turkish case frames can be seen below: (1) John must show the program works. John goster+tns program+gen calis+tns 'John programin calistigini gostermeli'. English Case Frame: [mood declarative voice active verb [root show [mod must [subj [nom John] [mood declarative [voice active args [obj [verb [args [Turkish Case Frame: [sform finite [ctype pred [mood necess [voice active [verb [root goster [tens pres [subj [John] [root work [tense pres [subj [nom program] [role fact [sform part [ctype pred [mood decl [voice active [obj [verb [args [subj [program] The case frames representing the noun phrases of the English and Turkish sentences vary from each other in a number of ways because the generator requires additional information to form an equiv- alent Turkish representation. For example, in the sentences below, (2) That man writes programs. 0 adam yaz+pres program '0 adam program yazar.' (3) Programs were written for the project. Program+pl yaz+pass+pst icin proje 'Proje icin programlar yazildi.' Even though the word program is used in the plu- ral form in both of the English sentences, the trans- fer module needs to determine the specificity of the noun phrase in question and send it to the generator which will accordingly output either the singular or plural form of the noun. Some of the complex transfer issues presented by Lindop and Tsujii (Lindop and Tsujii, 1991) also arise in our machine translation system. These is- sues are handled with special transfer rules and transfer lexicon entries. In the beginning of the transfer phase, the exception rules are tested and eventually a checklist containing the problematic components of the input is generated. Some ex- amples of these components are verbs which change meaning when used with different attributes, pas- sive, existential or conditional sentences, relative clauses, idiomatic use of prepositional phrases, etc. As the transfer process continues, the checklist is referenced in order to block the default translation and handle the exceptions. The rest of the mapping proceeds in a straightforward fashion until all of the information in the source case frame is mapped onto the target case frame. Some of the complex transfer issues handled in the transfer phase will be presented in this section. First, a significant amount of head-switching is performed to resolve the lexical and structural differences in the English and Turkish languages. In the example below, (4) attempted execution tesebbus calistirma 'calistirma tesebbusu : execution attempt' execution is the head noun of the English phrase whereas tesebbus (attempt) becomes the head noun in the target phrase. Another problem encountered in the transfer mod- ule is complex lexical transfer with category changes such as the example given below: (5) John gave a weak cough. John oksur+pst hafifce 'John hafifce oksurdu.:John coughed weakly.' The adjective weak has to be mapped onto an ad- verb hafifce and the verb give's default translation into the verb ver has to be blocked when it is used with the dependent noun cough. Consequently, the fitting target verb is found to be oksurmek. Also, dependent on the verb, an object of an English sentence may be mapped to different case markings in Turkish. (6) I hit the man. vur+pst+pers adam+dat 'Adama vurdum' (7) I shot the man. vur+pst+pers adam+acc 'Adami vurdum' As seen above, the object of the sentence the man is mapped either to an accusative marked object adami or a dative marked indirect object adama in the tar- get sentence. There are also some complex structural changes encountered during transfer. An English clause might be mapped into a Turkish gerund: (8) While he was working +ken calis+tns 'Calisirken' Another example of a structural transformation encountered can be seen in active/passive forms of sentences. In the English passive form, the surface subject can correspond to both the direct object or the indirect object of the active form. Yet in Turk- ish, the surface subject of a passive sentence can only be the direct object of the active form. The differ- ence between the two sentences is distinguished by the order of the phrases in the target sentence as seen in the example below: (9) This program was given to the user. Bu program ver+pas+pst kullanici+dat 'Bu program kullaniciya verildi.' (10) The user was given the program. Kullanici+dat ver+pas+pst program 'Kullaniciya program verildi' In both of the Turkish translations, the sur- face subject is program whereas the surface subject changes in the English inputs. The order of the words in the output sentences are determined by the topic and focus features of the tar- get case frame which are mapped during the transfer phase. In the first sentence, the topic is found to be program and the focus is kullanici, whereas in the second sentence the topic and the focus are kullanici and program, respectively. The transfer module also attacks problems related to sentential transformation such as the ones re- quired in the example below: (11) There are programs in the disk. Disk program+pl disk+loc var 'Diskte programlar var' Parts of the case frames for the sentences above are as follows: English Case Frame: 「mood decl voice active root be verb tens pres nom subj det entity there args arg2 nom program m adjs place nom disk Turkish Case Frame: 「sform finite ctype exist mood decl voice active verb args 「root to-bel tens pres asp aor subj [program] [disk]] adjs place disk Other problems encountered in the transfer phase are the lexical gaps, idiomatic uses of phrases, and lexical disambiguation by syntactic or semantic con- tent. With all the complex transfer issues resolved in the transfer phase, the corresponding Turkish case frame is generated which is then translated from its Prolog notation into the Lisp notation required by the generation module. 4.3 Generation Module The generation component of the system is based on the GenKit environment developed at the Carnegie Mellon University - Center for Machine Transla- tion which provides facilities for a unification-based generation grammar environment (Hakkani et al., 1996). As input, the generator receives a recursively em- bedded target case frame representation where all the lexical choices have been made, and produces the Turkish sentence conveying the same meaning. Since Turkish has complex agglutinative word forms, a separate morphological generator handles the proper morpheme selection, vowel harmony, etc. to produce the surface form of the generated words. The Turkish sentence output by the generator is post-edited by a human translator to ensure accu- racy and intelligibility of the target sentence. 5 Conclusion In this paper, an English to Turkish MT system us- ing the structural transfer approach with a limited amount of semantic analysis has been described. The structural transfer method which uses the re- cursively embedded case frames as intermediate rep- resentation proved to be very suitable in the applica- tion of English to Turkish machine translation. The greatest difficulty encountered with this approach is handling the complex transfer issues that arise due to the differences between the two languages. Hopefully, the introduction of the English to Turk- ish MT software into the Turkish market will meet the growing demands for accurate, fast and high- quality translations in the field of computer manu- als. Depending on the success of the system, the lexicons and the transfer module might be modified to tackle other translation domains in the future. 6 Acknowledgements Helpful comments of Asst.Prof Cem Bozsahin and Assoc. Prof.Mehmet R.Tolun are gratefully acknowl- edged. This work has been supported by the NATO Science for Stability Project TU-LANGUAGE. References Hiyan Alshawi and Robert C. Moore. 1992. In- troduction to the CLE. In Hiyan Alshawi, editor, The Core Language Engine. The MIT Press, Cam- bridge, Massachusetts. Kenneth Goodman and Sergei Nirenburg ed. 1991. The KBMT Project: A Case Study in Knowledge- Based Machine Translation. Morgan Kaufmann, San Mateo, California. Dilek Z. Hakkani, Kemal Oflazer and Ilyas Cicekli. 1996. Tactical Generation in a Free Constituent Order Language. In Proceedings of 8th Inter- national Workshop on Natural Language Gener- ation, Sussex, UK, June. Rod Johnson, Maghi King and Lois Tombe. 1985. EUROTRA: A Multilingual System Under Devel- opment. In Computational Linguistics, 11:155- 169. Jeremy Lindop and Jun-ichi Tsujii. 1991. Com- plex Transfer in MT: A Survey of Examples. CCL/UMIST Report No:91/5. Tetsuya Nasukawa. 1993. Discourse Constraint in Computer Manuals. In Proceedings of the TMI 1993, pages 183-193. Taijiro Tsutsumi. 1986. A Prototype English- Japanese Machine Translation System for Trans- lating IBM Computer Manuals. In Proceedings of Coling 1986, pages 646-648."
  },
  {
    "title": "Acknowledgments in Human-Computer Interaction",
    "abstract": "Acknowledgments are relatively rare in human-computer interaction. Are people unwilling to use this human convention when talking to a machine, or is their scarcity due to the way that spoken-language interfaces are designed? We found that, given a simple spoken-language interface that provided opportunities for and responded to acknowledgments, about half of our subjects used acknowledgments at least once and nearly 30% used them extensively during the interaction.",
    "content": "1 Introduction As our ability to build robust and flexible spoken- language interfaces increases, it is worthwhile to ask to what extent we should incorporate various human-human discourse phenomena into our dia- logue models. Many studies have shown that peo- ple alter their dialogue techniques when they believe that they are talking to a computer (e.g., Brennan, 1991), so it is not clear that observations of human-human conversation will provide us with the guidance we need. At the same time, we cannot always look to current systems to determine which discourse phenomena should be supported. Cur- rent-generation interfaces are still relatively fragile, and so designers of spoken-language systems go to some effort to structure dialogues and create prompts that guide the user toward short, content- ful, in-vocabulary responses (e.g., Basson et al, 1996; Cole, et al, 1997; Oviatt et al, 1994). One result of this approach is the suppression of meta- dialogue acts such as acknowledgement and repeti- tion. The term \"acknowledgment\" is from Clark and Schaefer (1989), who describe a hierarchy of meth- ods by which one conversant may signal that another's contribution has been understood well enough to allow the conversation to proceed. Acknowledgments often appear in American English conversation as an \"okay\" or \"uh-huh\" that signals understanding but not necessarily agree- ment. These are also called \"back channels\" or \"prompts\" (e.g., Chu-Carroll & Brown, 1997), Closely related to acknowledgments are repeti- tions, in which the conversant provides a stronger signal that a contribution has been understood by repeating part or all of the other's contribution. Repetitions are also referred to as \"paraphrases\" (Traum & Heeman, 1996), \"echoing\" (Swerts et al, 1998), and \"demonstration\" (Clark & Schaefer, 1989). Repetitions are often seen when one is con- veying complex information, such as when one copies an address or telephone number. Neither acknowledgments nor repetitions con- tribute new domain information to the conversa- tion, but they serve to assure the speaker that information has been conveyed successfully. Acknowledgments also play a role in managing turn-taking in mixed-initiative dialogue; although acknowledgments may preface a new contribution by the same speaker (Novick & Sutton, 1994), often they occur alone as a single-phrase turn that appears to serve the purpose of explicitly declining an opportunity to take a turn (Sacks et al, 1974). Acknowledgments and repetitions are ubiqui- tous in many types of human-human conversation. In a corpus of problem-solving spoken dialogues, for example, Traum and Heeman (1996) found that 51% of turns began with or consisted of an explicit acknowledgment. Given this, one would expect that acknowledgments should be modeled in dia- logue models for spoken-language systems, and indeed some research models are beginning to incorporate acknowledgments, e.g., Kita et al (1996), Aist, (1998), Iwase & Ward (1998). Typical human-computer dialogue models are structured in ways that suppress the use of acknowledgments. In many systems turn-taking is completely controlled by one conversant, e.g., the user responds to system prompts, which tends to eliminate the need for acknowledgments as a turn- taking mechanism. In other systems, the use of barge-in defeats the common interpretation of an acknowledgment; if the user speaks, the system contribution is cut off before the user utterance is interpreted. If that utterance was intended to signal that the contribution should continue, the effect is exactly the opposite of the one desired. Thus, current design practices both discourage and render meaningless the standard uses of acknowledgments. If these impediments were removed, would people choose to use acknowledg- ments when interacting with a computer interface? 2 Experiment This study was designed as a pilot to our larger investigation into the effects of incorporating acknowledgement behavior in dialogue models for spoken-language interfaces. Before we attempted to compare interfaces with and without acknowl- edgement behavior, we wanted to understand whether people are willing to use this sort of meta- dialogue behavior when interacting with a com- puter. 2.1 Approach In this study we hypothesized that subjects will choose to use acknowledgments in human-com- puter interaction if they are given an interface that provides opportunities for and responds to acknowledgments. In designing the study, we assumed that it would not immediately occur to subjects that they could use acknowledgments to a computer. At the same time, we did not want to explicitly instruct or require subjects to use acknowledgment behavior, as that would tell us nothing about their prefer- ences. We therefore decided against a comparison/ control-group experimental design for this initial study and instead focused on creating a situation in which subjects would have a reason to use acknowledgments, perhaps even gain an advantage from doing so, while still keeping the behavior optional. We decided to focus on a somewhat narrow use of acknowledgments. Conversants are especially likely to offer acknowledgments and repetitions when complex information is being presented, especially when the conversant is copying the information. While this is certainly explainable in terms of mutuality of understanding, this particular use of acknowledgment may be viewed from a more mechanical standpoint as regulating the pace at which information is presented. This insight sug- gested to us that a fruitful task for this study might be one in which the subject is asked to write down verbally-presented information, as when taking messages over the telephone. 2.2 Task We selected the domain of telephone interface to email and designed a task in which subjects were asked to transcribe items of information from the messages. Writing is slow in comparison to speak- ing, so we anticipated that subjects would require a slower pace of information presentation when they were writing. The messages included information not asked for on the question list to simulate \"unin- teresting\" material that the subject would want to move through at a faster pace. In this way we hoped to motivate subjects to try to control the pace at which information was presented. The email was presented in segments roughly corresponding to a long phrase. After each seg- ment, the system paused to give the subject time to make notes. If the subject said nothing, the system would continue by presenting the next message segment. Subjects could accept and perhaps make use of this delay, or they could reduce it by acknowledging the contribution, e.g., \"okay,\" or by commanding the system to continue, e.g., \"go on.\" The system signalled the possibility of controlling the delay by prompting the subject \"Are you ready to go on?\" after the first pause. This prompting was repeated for every third pause in which the subject said nothing. In this way we hoped to suggest to the subjects that they could control the wait time if desired without explicitly telling them how to do so. On the surface, there is no functional differ- ence in system behavior between a subject's use of a command to move the system onward (e.g., \"go on,\" \"next\", \"continue\") and the use of an acknowl- edgment (\"okay,\" \"uh-huh\", or a repetition). In either case, the system responds by presenting the next message segment, and in fact it eventually presents the next segment even if the subject says nothing at all. Thus, the design allows the subject to choose freely between accepting the system's pace (system initiative), or commanding the system to continue (user initiative), or acknowledging the presentations in a fashion more typical of mixed- initiative human conversation. In this way, we hoped to understand how the subject preferred to interact with the computer. 2.3 Subjects Subjects were told that the study's purpose was to assess the understandability and usability of the interface, and that their task was to find the answers to a list of questions. They were given no instructions in the use of the program beyond the information that they could talk to it using normal, everyday speech. The 14 volunteers were native speakers of North American English, and most were staff at a research university. Ten were female, four were male. Ages ranged from 13 to 57. All used comput- ers, typically office software and games, but none had significant programming experience. Each ses- sion lasted about 45 minutes total, and each subject was paid $10.00. One subject declined payment. 2.4 Interface As mentioned earlier, one difficulty with recogniz- ing acknowledgements in spoken-language inter- faces is that the use of barge-in tends to defeat the purpose of acknowledgments when they occur in overlapped speech. We used a Wizard of Oz proto- col as a simple way to allow the system to respond to such utterances and to provide robustness in handling repetitions. The wizard's interface was constructed using the Rapid Application Developer in the Center for Spoken Language Understanding Toolkit (Sutton, et al, 1998). A simple button panel allowed the wizard to select the appropriate response from the actions supported by the application. The applica- tion functionality was deliberately limited to sug- gest realistic abilities for a current spoken- language interface. Using messages pre-recorded in a synthesized voice, the wizard was able to direct the system to: • Read a list of all messages. • Begin reading a particular message. • Read the next message segment. • Repeat the current message segment. • Repeat the previous message segment. • Ask the subject whether the program should continue reading the current message. • Ask the subject to what to do next. • End the program. • Play one of several error and help messages. The texts of the email messages were pre- sented in phrases of varying lengths, with each phrase followed by a pause of about five seconds. Preliminary tests showed that the combined response time of the wizard and the interface was between one and two seconds, and that pauses of less than five seconds were not obviously different from the normal pace of system response. Five sec- onds is a long response time, uncomfortably so for human-human conversation, so we hoped that this lengthy pause would encourage the subjects to take the initiative in controlling the pace of the interac- tion. The messages were divided into segments by hand. The divisions were intended to simulate a phrase-level presentation, although some short phrases were combined to make the presentation less choppy. An example of one message and its division into phrases may be seen in Figure 1. Synthesized speech from the Festival speech synthesizer (Taylor, et al, 1998) was used through- out the interface. The message texts were presented in a synthesized male voice, while the control por- tions of the interface used a synthesized female voice. Default pronunciations were used except when the default was incorrect, e.g., \"read\" defaulted to the past-tense pronunciation in all con- texts. Also, there was minor use of the SABLE markup language (Wouters, et al, 1999) to flatten the pitch range at the end of phrases in list items; the intent was to suggest the prosody of list contin- uation rather than the default sentence-final drop. Message six is from Jo at teleport dot com, about, please stop by store on your way home. I'm going to be late getting home tonight, so would you please stop by the store on your way home? We need milk, eggs, a bunch of spinach, fresh ginger, green onions, maple syrup, a pound of coos-coos, mild curry powder, a pound of coffee, and a package of seventy five watt light bulbs. Thanks! See you tonight. Figure 1. Text of a sample message. The subject's list of questions included \"What items are you supposed to pick up at the store?\" To improve the understandability, both voices were slowed slightly to 90% of the default speaking rate. 2.5 Measures The central question to be answered is: will the subject use acknowledgments in interacting with the program? A subject can show one of several patterns of response: • The subject may make no attempt to control the pacing of the interface, instead allowing the interaction to proceed via time-outs. • The subject may use only commands to control the pacing. • The subject may use only acknowledgments to control the pacing. • The subject may use a mixture of commands and acknowledgments. The determination as to whether a particular utter- ance constituted an acknowledgment or a com- mand was based primarily on word choice and dialogue context; this approach is consistent with definitions of this behavior, e.g., Chu-Carroll and Brown (1997). For example, \"yes\" in the context of a system inform (a segment of an email message) was considered an acknowledgment, but \"yes\" in the context of a system question was not. The words \"okay,\" \"uh-huh,\" and \"yes\" (immediately following an inform) were taken as evidence of acknowledgments, and phrases such as \"go on,\" \"continue,\" \"next\" following an inform were taken as evidence of commands. The interpretation was confirmed during the post-experiment interview by questioning the subjects about their word choice. 2.6 Post-Experiment Interview A post-experiment interview was conducted to gather subject feedback and to answer subjects' questions. The experimenter took notes and thus could have introduced bias in the record of responses. No tape recording was made. The subject was first invited to comment on the interface and the interaction in an open-ended fash- ion. When the subject had finished, the experi- menter asked several specific questions to assess their understanding of the interface functionality. During this time, the experimenter reminded the subjects of the words that they had used most fre- quently to prompt the system to continue during pauses and asked the subjects why they had selected those words. Finally, the experimenter explained the true purpose and hypothesis of the experiment, verified that the subject was unaware that they had been interacting with a Wizard-of-Oz interface, and asked the subject to comment on the notion of using acknowledgments when interacting with a computer. The responses to this question, espe- cially, must be assumed to be somewhat optimistic, as it is likely that at least some subjects would be reluctant to disagree with the experimenter. 3 Results Results are summarized in Table 1. Because the subject pool was not balanced for gender, results for male and female subjects are reported sepa- rately. Due to the small number of male subjects in Table 1. Summary of Acknowledgment Behavior Subjects Total Behavior Female 10 subjects Male. 4 subjects (14) Used acknowledgment/repetition at least once 4 (40%) 4 (100%) 8 (57%) Used acknowledgment/repetition more than command 3 (30%) 1 (25%) 4 (29%) Used acknowledgment but no commands 1 (10%) 0 1 (7%) Described acknowledgment to computer as strange 2 (20%) 0 2 (14%) this pilot study, no tests of statistical significance of differences in the rates of acknowledgment behav- ior were made. Eight of the fourteen subjects used an acknowledgment or repetition at least once, and four used acknowledgment/repetitions more fre- quently than they used commands. Only one sub- ject used acknowledgments exclusively, while five subjects never used acknowledgments. No subject relied exclusively on time-outs to allow the system to proceed at its own pace, although one subject did use that as her predominant method (42 times, while using acknowledgments only six times and commands three times). Only one subject used rep- etition, and he reported during the interview that he was unaware of having done so. It is interesting to note that while all of the male subjects in this sample exhibited acknowledg- ment behavior at least once, only one preferred acknowledgment over command. One of the male subjects used acknowledgments only three times, in all cases as prefaces to commands. Conversely, although a lower percentage of women used acknowledgments (40%), a higher percentage of them (30%) used acknowledgments in preference to commands. Because of the small numbers of subjects, however, we do not conclude that these differences are significant. During the post-experiment interview, two sub- jects (both female) described the idea of using acknowledgments to the computer as strange and stated that they didn't feel that they would do this unless directed to—and even then, they would regard it as simply an alternate command. Two other subjects, both females who had used acknowledgments 2-6 times during the task, each reported that she had felt silly when she had caught herself saying \"please\" and \"okay\" to a computer but had been pleased when it had worked. The remainder of the subjects either expressed no strong opinion (two, both female) or expressed a positive attitude toward being able to use acknowl- edgments when interacting with a computer. Two subjects who had not used acknowledgments com- mented that they would probably be more likely to use human-like conversation if the synthesized voice were more human-like. Again, this report of the subjects' attitudes should be interpreted with caution; at this point in the interview they knew the experimenter's hypoth- esis and so may have been reluctant to disagree. 3.1 Other Dialogue Behaviors Although we had not formed any hypothesis about other dialogue behaviors, we noticed several inter- esting dialogue behaviors that we had not antici- pated. We were surprised at the number of subjects who exhibited politeness behavior toward the inter- face, either saying \"please\" when issuing com- mands to the computer or responding to the program's \"good-bye\" at the end of the session. One subject used \"please\" throughout the interac- tion, but a more common pattern was to use \"please\" at the beginning of the session and to drop the behavior as the interface became more familiar. Politeness did not seem to be strongly associated with a willingness to use acknowledgments, how- ever; four of the nine subjects who exhibited politeness did not use any acknowledgments in their interaction. Despite the deliberately-artificial interface, several subjects responded at least once to the mes- sage content as if they were talking to the message System: I could come to your office now or at any of the following times. one thirty SUBJECT: continue System: three o clock SUBJECT: continue System: or five fifteen SUBJECT: continue System: thank you. I look forward to your prompt reply SUBJECT: thank you- uh ((laugh)) continue Figure 2. Excerpt of transcript. Subject thanks the interface. The system is reading the text of one of the messages. sender. In the excerpt shown in Figure 2., for exam- ple, the subject replied \"Thank you\" to the message text's \"thank you.\" This did not appear to be a mat- ter of misunderstanding the capabilities of the interface; the subject later reported that despite the synthesized voices she had briefly forgotten that she wasn't talking to her secretary. Three subjects also made one or more meta- comments, e.g., \"ah, there it is\" when finding a par- ticular piece of information. These may have been at least partially an artifact of the \"treasure hunt\" nature of the task. When questioned in the post- experiment interview, subjects didn't seem aware that they'd made these comments. All but one of these instances were followed immediately by a command, so the wizard responded to the com- mand and ignored the meta-comment. The one stand-alone meta-comment was treated as an unrecognized command (an error message was played). 4 Discussion Subjects were provided with three methods for controlling the pace at which information was pre- sented: silence, command, or acknowledgment/ repetition. The majority of the subjects used com- mands more than they used acknowledgments, but over one half used an acknowledgment or repeti- tion at least once during their interaction and nearly 30% used acknowledgments in preference to commands. This occurred despite the fact that subjects were given no reason to think that this behavior would be effective: the interface was deliberately limited in functionality, and voice syn- thesis was used instead of recorded voice to emphasize the artificial nature of the interaction. Furthermore, the interface did not offer acknowl- edgments to the subjects, and the subjects were given no instructions suggesting that the interface understood acknowledgments. In fact two subjects who did use acknowledgments expressed surprise that they had worked, and two who had not used acknowledgments reported that they would proba- bly have used them if they had known it would work. It is interesting to consider these results in light of those reported by Okato et al (1998). They describe a Japanese-language Wizard-of-Oz study in which the subjects were given some instruction on using the system, and in which the system both presented and accepted back-channel feedback. They found that even when the interface offered back channels itself the rate of subject back-chan- nels was somewhat lower in human-computer interaction than in comparable human-human con- versation. This makes the fact that our interface elicited acknowledgments without offering them even more encouraging. Clearly, some people are willing to utilize this human conversational con- vention in human-computer dialogue. Our post- experiment interviews suggest, however, that some people will find the use of acknowledgements strange or uncomfortable in human-computer inter- action. While self-reports of attitudes toward hypo- thetical situations must be treated with some caution, it seems reasonable to assume that even when such interfaces become available there will be users who will prefer to interact with computers using commands. Will attitudes and conversational behavior change as people gain experience with more advanced spoken-language interfaces? Despite the relatively short duration of this test-most subjects completed the task itself in 15-20 minutes-some changes in behavior could be observed over the course of the dialogue. In particular, politeness behaviors were likely to be seen early in the dia- logues and then diminish as the subjects became more comfortable with their interaction. We specu- late that the use of politeness words did not reflect a strong underlying politeness toward the computer so much as a falling back on human conventions when faced with an unfamiliar dialogue situation. One subject who had used \"please\" 21 times dur- ing the interaction, for example, simply hung up without warning when she had finished. This con- trasts, however, with the findings of Nass et al (1999) that people do offer socially-desirable behavior to computers. Would a better voice increase the incidence of acknowledgment behavior? Several subjects thought it would, and even with the current synthe- sized voices we saw several examples of subjects seemingly forgetting briefly that they were not talking to a human. We plan to explore this ques- tion in future work. 4.1 Conclusions and Future Work We conducted a preliminary study to examine the willingness of subjects to use a particular dialogue act, acknowledgment, in human-computer interac- tion. Although the number of subjects was small, we saw that about half of our subjects used acknowledgements or repetition at least occasion- ally to control the pace at which information was presented, and about 29% used acknowledgments more frequently than they used commands for that purpose. Our immediate plans include extending this study to a larger and gender-balanced group of sub- jects so that we can draw firmer quantitative con- clusions about the percentage of people who are likely to prefer this style of interaction. In particu- lar, we cannot conclude from the current study's small sample how strong the preference for using acknowledgment might be, especially among male subjects. Also, in our current study the subject achieved no functional benefit in using acknowl- edgments. With better estimates of subject prefer- ences, we can then proceed to our larger goal of comparing the usefulness and user acceptability of spoken language dialogue models with and without acknowledgment behavior (c.f. Walker, 1993). We also plan to explore the effect of the quality of the synthesized voice on the incidence of acknowledg- ment behavior. Acknowledgments This work was partially supported by a grant from Intel Research Council. The authors gratefully acknowledge and thank David G. Novick and the anonymous reviewers for their helpful comments and suggestions. References Gregory Aist. 1998. Expanding a Time-Sensitive Conversational Architecture for Turn-Taking to Handle Content-Driven Interruption. In Pro- ceedings of ICSLP 98 Fifth International Con- ference on Spoken Language Processing, pages 413-417. Sara Basson, Stephen Springer, Cynthia Fong, Hong Leung, Ed Man, Michele Olson, John Pitrelli, Ranvir Singh, and Suk Wong. 1996. User Participation and Compliance in Speech Automated Telecommunications Applications. In Proceedings of ICSLP 96 Fourth Interna- tional Conference on Spoken Language Pro- cessing, pages 1676-1679. Susan E. Brennan. 1991. Conversation With and Through Computers. User Modeling and User- Adapted Interaction. 1:67-86. Jennifer Chu-Carroll and Michael K. Brown. 1997. Tracking Initiative in Collaborative Dialogue Interactions. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 262-270. Herbert H. Clark and Edward F. Schaefer. 1989. Contributing to Discourse. Cognitive Science, 13:259-294. Ronald A. Cole, David G. Novick, Pieter J. E. Ver- meulen, Stephen Sutton, Mark Fanty, L. F. A. Wessels, Jacques H. de Villiers, Johan Schalk- wyk, Brian Hansen and Daniel Burnett. 1997. Experiments with a Spoken Dialogue System for Taking the U.S. Census. Speech Communi- cations, Vol. 23. Peter A. Heeman, Michael Johnston, Justin Den- ney and Edward Kaiser. 1998. Beyond Struc- tured Dialogues: Factoring out Grounding. In Proceedings of ICSLP 98 Fifth International Conference on Spoken Language Processing, pages 863-867. Tatsuya Iwase, and Nigel Ward. 1998. Pacing Spo- ken Directions to Suit the Listener. In Proceed- ings of ICSLP 98 Fifth International Conference on Spoken Language Processing, Vol. 4, page 1203-1207. K. Kita, Y. Fukui, M. Jagata and T. Morimoto. 1996. Automatic Acquisition of Probabilistic Dialogue Models. In Proceedings of ICSLP 96 Fourth International Conference on Spoken Language Processing, pages 196-199. Clifford Nass, Youngme Moon and Paul Carney. 1999. Are Respondents Polite to Computers? Social Desirability and Direct Responses to Computers. Journal of Applied Social Psychol- ogy, 29:5, pages 1093-1110. H. Noguchi and Yasuharu Den. 1998. Prosody- Based Detection of the Context of Backchan- nel Responses. In Proceedings of ICSLP 98 Fifth International Conference on Spoken Lan- guage Processing, Vol. 2, pages 487-490. David G. Novick and S. Sutton. 1994. An Empiri- cal Model of Acknowledgment for Spoken- Language Systems. In Proceedings of the 32nd Annual Meeting of the Association for Compu- tational Linguistics, pages 96-101. Yohei Okato, Keiji Kato, Mikio Yamamoto and Shuichi Itahashi. 1998. System-User Interac- tion and Response Strategy in Spoken Dia- logue System. Proceedings of ICSLP 98 Fifth International Conference on Spoken Lan- guage Processing, Vol. 2, pages 495-498. S. L. Oviatt and P. R. Cohen and M. Wang. 1994. Toward Interface Design for Human Language Technology: Modality and Structure as Deter- minants of Linguistic Complexity. In Speech Communication, 15:3-4, pages 283-300. H. Sacks, E. Schegloff and G. Jefferson. 1974. A Simplest Systematics for the Organization of Turn-Taking in Conversation. Language, 50:696-735. Stephen Sutton, Ronald Cole, Jacques de Villiers, Johan Schalkwyk, Pieter Vermeulen, Mike Macon, Yonghong Yan, Ed Kaiser, Brian Run- dle, Khaldoun Shobaki, Paul Hosom, Alex Kain, Johan Wouters, Dominic Massaro and Michael Cohen. 1998. Universal Speech Tools: the CSLU Toolkit. In Proceedings of the Inter- national Conference on Spoken Language Pro- cessing, pages 3221-3224. Marc Swerts, Hanae Koiso, Atsushi Shimojima and Yasuhiro Katagiri. 1998. On Different Functions of Repetitive Utterances. In Pro- ceedings of ICSLP 98 Fifth International Con- ference on Spoken Language Processing, Vol. 2, pages 483-487. Paul Taylor, Alan W. Black and Richard Caley. 1998. The Architecture of the Festival Speech Synthesis System. In The Third ESCA/ COCOSDA Workshop on Speech Synthesis, pages 147-151. David R. Traum and Peter A. Heeman. 1996. Utterance Units and Grounding in Spoken Dia- logue. In Proceedings of ICSLP 96 Fourth International Conference on Spoken Lan- guage Processing, pages 1884-1887. Marilyn A. Walker. 1993. Informational Redun- dancy and Resource Bounds in Dialogue. Doc- toral dissertation, University of Pennsylvania. Johan Wouters, Brian Rundle and Michael W. Macon. 1996. Authoring Tools for Speech Synthesis using the Sable Markup Standard. In Proceedings of Eurospeech '99. Nigel Ward. 1996. Using Prosodic Clues to Decide When to Produce Back-Channel Utterances. In Proceedings of ICSLP 96 Fourth International Conference on Spoken Language Processing, page 1724-1727."
  },
  {
    "title": "Name pronunciation in German text-to-speech synthesis",
    "abstract": "We describe the name analysis and pronunciation component in the German version of the Bell Labs multilingual text-to-speech system. We concentrate on street names because they encompass interesting aspects of geographical and personal names. The system was implemented in the framework of finite-state transducer technology, using linguistic criteria as well as frequency distributions derived from a database. In evaluation experiments, we compared the performances of the general-purpose text analysis and the name-specific system on training and test materials. The name-specific system significantly outperformed the generic system. The error rates compare favorably with results reported in the research literature. Finally, we discuss areas for future work.",
    "content": "1 Introduction The correct pronunciation of names is one of the biggest challenges for text-to-speech (TTS) conver- sion systems. At the same time, many current or en- visioned applications, such as reverse directory sys- tems, automated operator services, catalog ordering or navigation systems, to name just a few, crucially depend upon an accurate and intelligible pronuncia- tion of names. Besides these specific applications, any kind of well-formed text input to a general- purpose TTS system is extremely likely to contain names, and the system has to be well equipped to process these names. This requirement was the main motivation to develop a name analysis and pronunci- ation component for the German version of the Bell Labs multilingual text-to-speech system (GerTTS) (Möbius et al., 1996). Names are conventionally categorized into per- sonal names (first and surnames), geographical names (place, city and street names), and brand names (organization, company and product names). In this paper, we concentrate on street names be- cause they encompass interesting aspects of geo- graphical as well as of personal names. Linguistic de- scriptions and criteria as well as statistical considera- tions, in the sense of frequency distributions derived from a large database, were used in the construction of the name analysis component. The system was implemented in the framework of finite-state trans- ducer (FST) technology (see (Sproat, 1992) for a discussion focussing on morphology). For evalua- tion purposes, we compared the performances of the general-purpose text analysis and the name-specific systems on training and test materials. As of now, we have neither attempted to deter- mine the etymological or ethnic origin of names, nor have we addressed the problem of detecting names in arbitrary text. However, due to the integration of the name component into the general text analysis system of GerTTS, the latter problem has a reason- able solution. 2 Some problems in name analysis What makes name pronunciation difficult, or spe- cial, in comparison to words that are considered as regular entries in the lexicon of a given language? Various reasons are given in the research literature (Carlson, Granström, and Lindström, 1989; Macchi and Spiegel, 1990; Vitale, 1991; van Coile, Leys, and Mortier, 1992; Coker, Church, and Liberman, 1990; Belhoula, 1993): • Names can be of very diverse etymological ori- gin and can surface in another language without undergoing the slow linguistic process of assim- ilation to the phonological system of the new language. • The number of distinct names tends to be very large: For English, a typical unabridged colle- giate dictionary lists about 250,000 word types, whereas a list of surnames compiled from an address database contains 1.5 million types (72 million tokens) (Coker, Church, and Liberman, 1990). It is reasonable to assume similar ratios for German, although no precise numbers are currently available. • There is no exhaustive list of names; and in German and some related Germanic lan- guages, street names in particular are usu- ally constructed like compounds (Rheinstraße, Kennedyallee) which makes decomposition both practical and necessary. • Name pronunciation is known to be idiosyn- cratic; there are many pronunciations contra- dicting common phonological patterns, as well as alternative pronunciations for certain grapheme strings. • In many languages, general-purpose grapheme- to-phoneme rules are to a significant extent inappropriate for names (Macchi and Spiegel, 1990; Vitale, 1991). • Names are not equally amenable to morpho- logical processes, such as word formation and derivation or to morphological decomposition, as regular words are. That does not render such an approach unfeasible, though, as we show in this paper. • The large number of different names together with a restricted morphological structure leads to a coverage problem: It is known that a rel- atively small number of high-frequency words can cover a high percentage of word tokens in arbitrary text; the ratio is far less favorable for names (Carlson, Granström, and Lindström, 1989; van Coile, Leys, and Mortier, 1992). We will now illustrate some of the idiosyncra- cies and peculiarities of names that the analysis has to cope with. Let us first consider morphological issues. Some German street names can be mor- phologically and lexically analyzed, such as Kur- fürst-en-damm ('electorial prince dam'), Kirche-n- weg ('church path'). Many, however, are not de- composable, such as Heumerich ('?') or Rimpar- straße ('?Rimpar street'), at least not beyond ob- vious and unproblematic components (Straße, Weg, Platz, etc.). Even more serious problems arise on the phono- logical level. As indicated above, general-purpose pronunciation rules often do not apply to names. For instance, the grapheme <e> in an open stressed syllable is usually pronouned [e:]; however, in many first names (Stefan, Melanie) it is pronounced [e]. Or consider the word-final grapheme string <ie> in Batterie [batər'i:] 'battery', Materie [mat'e:riǝ] 'matter', and the name Rosemarie [r'o:zəmari:]. And word-final <us>: Mus [m'u:s] 'mush, jam' vs. Eras- mus [er'asmus]. A more special and yet typical ex- ample: In regular German words the morpheme- initial substring <chem> as in chemisch is pro- nounced [çe:m], whereas in the name of the city Chemnitz it is pronounced [kem]. Generally speaking, nothing ensures correct pro- nunciation better than a direct hit in a pronuncia- tion dictionary. However, for the reasons detailed above this approach is not feasible for names. In short, we are not dealing with a memory or storage problem but with the requirement to be able to ap- proximately correctly analyze unseen orthographic strings. We therefore decided to use a weighted finite-state transducer machinery, which is the tech- nological framework for the text analysis compo- nents of the Bell Labs multilingual TTS system. FST technology enables the dynamic combination and recombination of lexical and morphological sub- strings, which cannot be achieved by a static pronun- ciation dictionary. We will now describe the proce- dure of collecting lexically or morphologically mean- ingful graphemic substrings that are used produc- tively in name formation. 3 Productive name components 3.1 Database Our training material is based on publically avail- able data extracted from a phone and address di- rectory of Germany. The database is provided on CD-ROM (D-Info, 1995). It lists all customers of Deutsche Telekom by name, street address, city, phone number, and postal code. The CD-ROM contains data retrieval and export software. The database is somewhat inconsistent in that informa- tion for some fields is occasionally missing, more than one person is listed in the name field, busi- ness information is added to the name field, first names and street names are abbreviated. Yet, due to its listing of more than 30 million customer records it provides an exhaustive coverage of name-related phenomena in German. 3.2 City names The data retrieval software did not provide a way to export a complete list of cities, towns, and vil- lages; thus we searched for all records listing city halls, township and municipality administrations and the like, and then exported the pertinent city names. This method yielded 3,837 city names, ap- proximately 15% of all the cities (including urban districts) covered in the database. It is reasonable to assume, however, that this corpus provided suffi- cient coverage of lexical and morphological subcom- ponents of city names. We extracted graphemic substrings of different lengths from all city names. The length of the strings varied from 3 to 7 graphemes. Useful substrings were selected using frequency analysis (automati- cally) and native speaker intuition (manually). The final list of morphologically meaningful substrings consisted of 295 entries. In a recall test, these 295 strings accounted for 2,969 of the original list of city names, yielding a coverage of 2,969/3,837 = 77.4%. München Berlin Hamburg Köln Total (south) (east) (north) (west) component types 7,127 7,291 8,027 4,396 26,841 morphemes 922 574 320 124 1,940 recall 2,387 2,538 4,214 2,102 11,241 residuals (abs.) 4,740 4,753 3,813 2,294 15,600 residuals (rel.) 66.5% 65.0% 47.5% 52.2% 58.1% Table 1: Extraction of productive street name components: quantitative data. 3.3 First names The training corpus for first names and street names was assembled based on data from the four largest cities in Germany: Berlin, Hamburg, Köln (Cologne) and München (Munich). These four cities also pro- vide an approximately representative geographical and regional/dialectal coverage. The size and ge- ography criteria were also applied to the selection of the test material which was extracted from the cities of Frankfurt am Main and Dresden (see Evaluation). We retrieved all available first names from the records of the four cities and collected those whose frequency exceeded 100. To this corpus we added the most popular male and female (10 each) names given to newborn children in the years 1995/96, in both the former East and West Germany, according to an official statistical source on the internet. The cor- pus also contains interesting spelling variants (Hel- mut/Hellmuth) as well as peculiarities attributable to regional tastes and fashions (Maik, Maia). The total number of first names in our list is 754. No attempt was made to arrive at some form of morphological decomposition despite several obvious recurring components, such as <-hild>, <-bert>, <-fried>; the number of these components is very small, and they are not productive in name-forming processes anymore. 3.4 Streets We retrieved all available street names from the records of the four cities. The street names were split up into their individual word-like components, i.e., a street name like Konrad-Adenauer-Platz cre- ated three separate entries: Konrad, Adenauer, and Platz. This list was then sorted and made unique. The type inventory of street name components was then used to collect lexically and semantically meaningful components, which we will henceforth conveniently call 'morphemes'. In analogy to the procedure for city names, these morphemes were used in a recall test on the original street name com- ponent type list. This approach was successively ap- plied to the street name inventory of the four cities, starting with München, exploiting the result of this first round in the second city, Berlin, applying the combined result of this second round on the third city, and so on. Table 1 gives the numbers corresponding to the steps of the procedure just described. The number of morphemes collected from the four cities is 1,940. The selection criterion was frequency: Component types occurring repeatedly within a city database were considered as productive or marginally produc- tive. The 1,940 morphemes recall 11,241 component types out of the total of 26,841 (or 41.9%), leaving 15,600 types (or 58.1%) that are unaccounted for ('residuals') by the morphemes. Residuals that occur in at least two out of four cities (2,008) were then added to the list of 1,940 morphemes. The reasoning behind this is that there are component types that occur exactly once in a given city but do occur in virtually every city. To give a concrete example: There is usually only one Hauptstraße ('main street') in any given city but you almost certainly do find a Hauptstraße in every city. After some editing and data clean-up, the final list of linguistically motivated street name morphemes contained 3,124 entries. 4 Compositional model of street names In this section we will present a compositional model of street names that is based on a morphological word model and also includes a phonetic syllable model. We will also describe the implementation of these models in the form of a finite-state transducer. 4.1 Naming schemes for streets in German Evidently, there is a finite list of lexical items that almost unambiguously mark a name as a street name; among these items are Straße, Weg, Platz, Gasse, Allee, Markt and probably a dozen more. These street name markers are used to construct street names involving persons (Stephan- Lochner-Straße, Kennedyallee), geographical places (Tübinger Allee), or objects (Chrysanthemenweg, Containerbahnhof); street names with local, regional or dialectal peculiarities (Söbendieken, Höglstieg); and finally intransparent street names (Krüsistraße, Damaschkestraße). Some names of the latter type may actually refer to persons' names but the origin is not transparent to the native speaker. START ROOT {Eps} ROOT FIRST SyllModel ROOT FIRST 'Alfons{firstname}<0.2> ROOT FIRST D'irk{firstname}<0.2> ROOT FIRST D'ominik{firstname}<0.2> ROOT FIRST b'urg{city} ROOT FIRST br'uck{city}<0.2> ROOT FIRST d'orf{city} ROOT FIRST fl'eet{city}<0.2> ROOT FIRST d'ach{street}<0.2> ROOT FIRST h'ecke{street}<0.2> ROOT FIRST kl'ar{street}<0.2> ROOT FIRST kl'ee{street}<0.2> ROOT FIRST kl'ein{street}<0.2> ROOT FIRST st'ein{street}<0.2> ROOT FIRST all'ee{marker} ROOT FIRST g'arten{marker} ROOT FIRST pl'atz{marker} ROOT FIRST w'eg{marker} FIRST ROOT {++}<0.1> FIRST FUGE {Eps}<0.2> FIRST FUGE s<0.2> FIRST FUGE n<0.2> FIRST SUFFIX {Eps}<0.2> FIRST SUFFIX s<0.2> FIRST SUFFIX n<0.2> FUGE FIRST {++}<10.0> FUGE ROOT {++}<0.5> SUFFIX END {name} END Figure 1: Parts of a grammar (in arclist format) for street name decomposition in German. 4.2 Building a generative transducer for street names The component types collected from the city, first name and street databases were integrated into a combined list of 4,173 productive name components: 295 from city names, 754 from first names, 3,124 from street names. Together with the basic street name markers, these components were used to con- struct a name analysis module. The module was im- plemented as a finite-state transducer using Richard Sproat's lextools (Sproat, 1995), a toolkit for creat- ing finite-state machines from linguistic descriptions. The module is therefore compatible with the other text analysis components in the German TTS sys- tem (Möbius, 1997) that were all developed in the same FSM technology framework. One of the lextools, the program arclist, is par- ticularly well suited for name analysis. The tool facilitates writing a finite-state grammar that de- scribes words of arbitrary morphological complexity and length (Sproat, 1995). In the TTS system it is also applied to the morphological analysis of com- pounds and unknown words. Figure 1 shows parts of the arclist source file for street name decomposition. The arc which describes the transition from the initial state \"START\" to the state \"ROOT\" is labeled with e (Epsilon, the empty string). The transition from \"ROOT\" to the state \"FIRST\" is defined by three large families of arcs which represent the lists of first names, productive city name components, and productive street name components, respectively, as described in the previ- ous section. The transition from \"ROOT\" to \"FIRST\" which is labeled SyllModel is a place holder for a pho- netic syllable model. This syllable model reflects the phonotactics and the segmental structure of syl- lables in German, or rather their correlates on the orthographic surface. This allows the module to an- alyze substrings of names that are unaccounted for by the explicitly listed name components (see 'resid- uals' in the previous section) in arbitrary locations in a complex name. A detailed discussion of the syl- lable model is presented elsewhere (Möbius, 1997). From the state \"FIRST\" there is a transition back to \"ROOT\", either directly or via the state \"FUGE\", thereby allowing arbitrarily long con- catenations of name components. Labels on the arcs to \"FUGE\" represent infixes ('Fugen') that German word forming grammar requires as inser- tions between components within a compounded word in certain cases, such as Wilhelm+s+platz or Linde+n+hof. The final state \"END\" can only be reached from \"FIRST\" by way of \"SUFFIX\". This transition is defined by a family of arcs which repre- sents common inflectional and derivational suffixes. On termination the word is tagged with the label 'name' which can be used as part-of-speech informa- tion by other components of the TTS system. Most arc labels are weighted by being assigned a cost. Weights are a convenient way to describe and predict linguistic alternations. In general, such a description can be based on an expert's analy- sis of linguistic data and his or her intuition, or on statistical probabilities derived from annotated cor- pora. Works by Riley (Riley, 1994) and Yarowsky (Yarowsky, 1994) are examples of inferring models of linguistic alternation from large corpora. How- ever, these methods require a database that is anno- tated for all relevant factors, and levels on these fac- tors. Despite our large raw corpus, we lack the type of database resources required by these methods. Thus, all weights in the text analysis components of GerTTS are currently based on linguistic intuition; they are assigned such that after integration of the name component in the general text analysis system, direct hits in the general-purpose lexicon will be less expensive than name analyses (see Discussion). No weights or costs are assigned to the most frequently occurring street name components, previously intro- SyllModel/10 d'ach/0.2 st'ein/0.2 {##}/0 h'ecke/0.2 START ROOT name(##}/0 FIRST END all'ee/0.2 n/0.2 pl'atz/0 ++/0.1 ++/0.5 FUGE Figure 2: The transducer compiled from the sub-grammar that performs the decomposition of the fictitious street name Dachsteinhohenheckenalleenplatz. duced as street name markers, making them more likely to be used during name decomposition. The orthographic strings are annotated with symbols for primary (') and secondary (\") lexical stress. The symbol {++} indicates a morpheme boundary. The finite-state transducer that this grammar is compiled into is far too complex to be usefully dia- grammed here. For the sake of exemplification, let us instead consider the complex fictitious street name Dachsteinhohenheckenalleenplatz. Figure 2 shows the transducer corresponding to the sub-grammar that performs the decomposition of this name. The path through the graph is as follows: The arc between the initial state \"START\" and \"ROOT\" is labeled with a word boundary {##} and zero cost (0). From here we take the arc with the label d'ach and a cost of 0.2 to state \"FIRST\". The next name component that can be found in the grammar is stein; we have to return to \"ROOT\" by way of an arc that is labeled with a morph bound- ary and a cost of 0.1. The next known component is hecke, leaving a residual string hohen which has to be analyzed by means of the syllable model. Applying the syllable model is expensive because we want to cover the name string with as many known compo- nents as possible. The costs actually vary depending upon the number of syllables in the residual string and the number of graphemes in each syllable; the string hohen would thus have be decomposed into a root hohe and the 'Fuge' n. For the sake of simplic- ity we assign a flat cost of 10.0 in our toy example. In the transition between hecke and allee a 'Fuge' (n) has to be inserted. The cost of the following morph boundary is higher (0.5) than usual in order to favor components that do not require infixation. Another Fuge has to be inserted after allee. The cost of the last component, platz, is zero because this is one of the customary street name markers. Finally, the completely analyzed word is tagged as a name, and a word boundary is appended on the way to the final state \"END\". The morphological information provided by the name analysis component is exploited by the phono- logical or pronunciation rules. This component of the linguistic analysis is implemented using a mod- ified version of the Kaplan and Kay rewrite rule al- gorithm (Kaplan and Kay, 1994). 5 Evaluation We evaluated the name analysis system by compar- ing the pronunciation performance of two versions of the TTS system, one with and one without the name-specific module. We ran both versions on two lists of street names, one selected from the training material and the other from unseen data. 5.1 General-purpose vs. name-specific analysis Two versions of the German TTS system were in- volved in the evaluation experiments, differing in the structure of the text analysis component. The first system contained the regular text analysis mod- ules, including a general-purpose module that han- dles words that are not represented in the system's lexicon: typically compounds and names. This ver- sion will be refered to as the old system. The second version purely consisted of the name grammar trans- ducer discussed in the previous section. It did not have any other lexical information at its disposal. This version will be refered to as the new system. number of names at least one system wrong both systems wrong total error rate (no correct solution) Training Data 631 Test Data 206 250/631 (39.6%) 82/206 (39.8%) 72/250 (28.8%) 26/82 (31.7%) 72/631 (11.1%) 26/206 (12.6%) Table 2: Performance of the general-purpose and the name-specific text analysis systems on training and test data sets. new system correct && old system wrong Training Data Test Data 138/163 (84.7%) 35/50 (70.0%) old system correct && new system wrong net improvement 25/163 (15.3%) 15/50 (30.0%) 113/163 (69.4%) 20/50 (40.0%) Table 3: Comparison between the general-purpose and the name-specific text analysis systems on training and test data sets. 5.2 Training vs. test materials The textual materials used in the evaluation exper- iments consisted of two sets of data. The first set, henceforth training data, was a subset of the data that were used in building the name analysis gram- mar. For this set, the street names for each of the four cities Berlin, Hamburg, Köln and München were randomized. We then selected every 50th entry from the four files, yielding a total of 631 street names; thus, the training set also reflected the respective size of the cities. The second set, henceforth test data, was ex- tracted from the databases of the cities Frankfurt am Main and Dresden. Using the procedure described above, we selected 206 street names. Besides be- ing among the ten largest German cities, Frankfurt and Dresden also meet the requirement of a bal- anced geographical and dialectal coverage. These data were not used in building the name analysis system. 5.3 Results The old and the new versions of the TTS system were run on the training and the test set. Pronun- ciation performance was evaluated on the symbolic level by manually checking the correctness of the re- sulting transcriptions. A transcription was consid- ered correct when no segmental errors or erroneous syllabic stress assignments were detected. Multiple mistakes within the same name were considered as one error. Thus, we made a binary decision between correct and incorrect transcriptions. Table 2 summarizes the results. On the training data, in 250 out of a total of 631 names (39.6%) at least one of the two systems was incorrect. In 72 out of these 250 cases (28.8%) both systems were wrong. Thus, for 72 out of 631 names (11.4%) no correct transcription was obtained by either system. On the test data, at least one of the two sys- tems was incorrect in 82 out of a total of 206 names (39.8%), an almost identical result as for the training data. However, in 26 out of these 82 cases (31.7%) both systems were wrong. In other words, no cor- rect transcription was obtained by either system for 26 out of 206 names (12.6%), which is only slightly higher than for the training data. Table 3 compares the performances of the two text analysis systems. On the training data, the new system outperforms the old one in 138 of the 163 cases (84.7%) where one of the systems was cor- rect and the other one was wrong; we disregard here all cases where both systems were correct as well as the 87 names for which no correct transcription was given by either system. But there were also 25 cases (15.3%) where the old system outperformed the new one. Thus, the net improvement by the name-specific system over the old one is 69.4%. On the test data set, the old system gives the cor- rect solution in 15 of 50 cases (30.0%), compared to 35 names (70.0%) for which the new system gives the correct transcription; again, all cases were excluded in which both systems performed equally well or poorly. The net improvement by the name-specific system over the generic one on the test data is thus 40%. A detailed error analysis yielded the following types of remaining problems: • Syllabic stress: Saarbrücken [za:ebR'ykən] but Zweibrücken [tsv'aıbRykən]. • Vowel quality: Soest [zo:st], not [zøst] or [zo:əst]. • Consonant quality: Chemnitz [kemnits], not [çe:mnits] in analogy to chemisch [çe:mı∫]. • Morphology: Erroneous decomposition of sub- strings (hyper-correction over old system); e.g., Rim+par+straße [ri:mpae] instead of Rim- par+straße [rımpae]. • Pronunciation rules: \"Holes\" in the general- purpose pronunciation rule set were revealed by orthographic substrings that do not occur in the regular lexicon. It has been shown for English (van Santen, 1992) that the frequency distribu- tion of triphones in names is quite dissimilar to the one found in regular words. • Idiosyncrasies: Peculiar pronunciations that cannot be described by rules and that even na- tive speakers quite often do not know or do not agree upon; e.g., Oeynhausen [ø:nhauzən], Duisdorf [dy:sdorf] or [du:sdorf] or [du:isdorf]. 6 Discussion and future work After the evaluation, the name analysis transducer was integrated into the text analysis component of the German TTS system. The weights were adjusted in such a way that for any token, i.e., word or word form, in the input text an immediate match in the lexicon is always favored over name analysis which in turn is prefered to unknown word analysis. Even though the evaluation experiments reported in this paper were performed on names in isolation rather than in sentential contexts, the error rates obtained in these experiments (Table 2) correspond to the per- formance on names by the integrated text analysis component for arbitrary text. There are two ways of interpreting the results. On the one hand, despite a significant improvement over the previous general-purpose text analysis we have to expect a pronunciation error rate of 11-13% for unknown names. In other words, roughly one out of eight names will be pronounced incorrectly. On the other hand, this performance compares rather favorably with the results reported for the German branch of the European Onomastica project (Onomastica, 1995). Onomastica was funded by the European Community from 1993 to 1995 and aimed to produce pronunciation dictionaries of proper names and place names in eleven languages. The final report describes the performance of grapheme- to-phoneme rule sets developed for each language. For German, the accuracy rate for quality band III- names which were transcribed by rule only was 71%; in other words, the error rate in the same sense as used in this paper was 29%. The grapheme-to- phoneme conversion rules were written by experts, based on tens of thousands of the most frequent names that were manually transcribed by an expert phonetician. However, the Onomastica results can only serve as a qualitative point of reference and should not be compared to our results in a strictly quantitative sense, for the following reasons. First, the percent- age of proper names is likely to be much higher in the Onomastica database (no numbers are given in the report), in which case higher error rates should be expected due to the inherent difficulty of proper name pronunciation. In our study, proper names were only covered in the context of street names. Second, Onomastica did not apply morphological analysis to names, while morphological decomposi- tion, and word and syllable models, are the core of our approach. Third, Onomastica developed name- specific grapheme-to-phoneme rule sets, whereas we did not augment the general-purpose pronunciation rules. How can the remaining problems be solved, and what are the topics for future work? For the task of grapheme-to-phoneme conversion, several ap- proaches have been proposed as alternatives to ex- plicit rule systems, particularly self-learning meth- ods (van Coile, 1990; Torkkola, 1993; Andersen and Dalsgaard, 1994) and neural networks (Sejnowski and Rosenberg, 1987; An et al., 1988). None of these methods were explored and applied in the present study. One reason is that it is difficult to construct or select a database if the set of factors that in- fluence name pronunciation is at least partially un- known. In addition, even for an initially incomplete factor set the corresponding feature space is likely to cause coverage problems; neural nets, for instance, are known to perform rather poorly at predicting unseen feature vectors. However, with the results of the error analysis as a starting point, we feel that a definition of the factor set is now more feasible. One obvious area for improvement is to add a name-specific set of pronunciation rules to the general-purpose one. Using this approach, Belhoula (Belhoula, 1993) reports error rates of 4.3% for Ger- man place names and 10% for last names. These re- sults are obtained in recall tests on a manually tran- scribed training corpus; it remains unclear, however, whether the error rates are reported by letter or by word. The addition of name-specific rules presupposes that the system knows which orthographic strings are names and which are regular words. The prob- lem of name detection in arbitrary text (see (Thie- len, 1995) for an approach to German name tagging) has not been addressed in our study; instead, it was by-passed for the time being by integrating the name component into the general text analysis system and by adjusting the weights appropriately. Other areas for future work are the systematic treatment of proper names outside the context of street names, and of brand names, trademarks, and company names. One important consideration here is the recognition of the ethnic origin of a name and the application of appropriate specific pronunciation rules. Heuristics, such as name pronunciation by analogy and rhyming (Coker, Church, and Liber- man, 1990) and methods for, e.g., syllabic stress as- signment (Church, 1986) can serve as role models for this ambitious task. 7 Acknowledgments We wish to acknowledge Richard Sproat who devel- oped and provided the lextools toolkit; this work also benefited from his advice. We also wish to thank an anonymous reviewer for constructive suggestions. References Z. An, S. Mniszewski, Y. Lee, G. Papcun, and G. Doolen. 1988. Hiertalker: A default hierarchy of high order neural networks that learns to read English aloud. In Proceedings of the IEEE In- ternational Conference on Neural Networks, vol- ume 2, pages 221-228, San Diego, CA. Ove Andersen and Paul Dalsgaard. 1994. A self-learning approach to transcription of Dan- ish proper names. In Proceedings of the Inter- national Conference on Spoken Language Process- ing, ICSLP-94, volume 3, pages 1627-1630, Yoko- hama, Japan. Karim Belhoula. 1993. A concept for the synthesis of names. In ESCA Workshop on Applications of Speech Technology, Lautrach, Germany. Rolf Carlson, Björn Granström, and Anders Lind- ström. 1989. Predicting name pronunciation for a reverse directory service. In Proceedings of the European Conference on Speech Communication and Technology, Eurospeech-89, volume 1, pages 113-116, Paris, France. Kenneth Church. 1986. Stress assignment in let- ter to sound rules for speech synthesis. In Pro- ceedings of the IEEE International Conference on Acoustics and Speech Signal Processing, ICASSP- 86, volume 4, pages 2423-2426, Tokyo, Japan. Cecil H. Coker. 1990. Morphology and rhyming: Two powerful alternatives to letter-to-sound rules for speech synthesis. In Proceedings of the ESCA Workshop on Speech Synthesis, pages 83-86, Au- trans, France. D-Info. 1995. D-Info—Adress- und Telefonauskunft Deutschland. CD-ROM. TopWare, Mannheim, Germany. Ronald Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computa- tional Linguistics, 20:331-378. Marian Macchi and Murray Spiegel. 1990. Us- ing a demisyllable inventory to synthesize names. Speech Technology, pages 208-212. Bernd Möbius. 1997. Text analysis in the Bell Labs German TTS system. Technical report, Bell Lab- oratories. Bernd Möbius, Juergen Schroeter, Jan van Santen, Richard Sproat, and Joseph Olive. 1996. Recent advances in multilingual text-to-speech synthesis. In Fortschritte der Akustik—DAGA '96, Bad Hon- nef, Germany. DPG. Onomastica. 1995. Multi-language pronunciation dictionary of proper names and place names. Technical report, European Community, Ling. Res. and Engin. Prog. Project No. LRE-61004, Final Report, 30 May 1995. Michael Riley. 1994. Tree-based models of speech and language. In Proceedings of the IEEE-IMS Workshop on Information Theory and Statistics, Alexandria, VA. T. Sejnowski and C.R. Rosenberg. 1987. Parallel networks that learn to pronounce English text. Complex Systems, 1:144-168. Richard Sproat. 1992. Morphology and computa- tion. MIT Press, Cambridge, MA. Richard Sproat. 1995. Lextools: Tools for finite- state linguistic analysis. Technical report, AT&T Bell Laboratories. Christine Thielen. 1995. An approach to proper name tagging in German. In Proceedings of the EACL-95 SIGDAT Workshop: From Text to Tags, Dublin, Ireland. Kari Torkkola. 1993. An efficient way to learn En- glish grapheme-to-phoneme rules automatically. In Proceedings of the IEEE International Confer- ence on Acoustics and Speech Signal Processing, ICASSP-93, volume 2, pages 199-202. Bert van Coile. 1990. Inductive learning of grapheme-to-phoneme rules. In Proceedings of the International Conference on Spoken Language Processing, ICSLP-90, volume 2, pages 765-768, Kobe, Japan. Bert van Coile, Steven Leys, and Luc Mortier. 1992. On the development of a name pronunciation sys- tem. In Proceedings of the International Confer- ence on Spoken Language Processing, ICSLP-92, volume 1, pages 487-490, Banff, Alberta. Jan van Santen. 1992. Personal communication. Tony Vitale. 1991. An algorithm for high accuracy name pronunciation by parametric speech synthe- sizer. Computational Linguistics, 17:257-276. David Yarowsky. 1994. Homograph disambiguation in speech synthesis. In Proceedings of the Second ESCA Workshop on Speech Synthesis, pages 244- 247, New Paltz, NY."
  },
  {
    "title": "Identifying Topics by Position",
    "abstract": "This paper addresses the problem of identifying likely topics of texts by their position in the text. It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure. This method can be used in applications such as information retrieval, routing, and text summarization.",
    "content": "1 Introduction: Topic Spotting by Position In an increasingly information-laden world, the problem of automatically finding the major topics of texts acquires new urgency. A module that can suggest likely locations of topics in texts, robustly and with an acceptable degree of correctness, would be useful for a number of important applications, including information retrieval, gisting, and auto- mated summarization. Several methods have been tried to perform Topic Identification. Some involve parsing and semantic analysis of the text, and are therefore less robust over arbitrary input. Others, such as the Cue Phrase and Position methods, are more robust, though gen- erally somewhat less accurate. Of these, the Position Method, identified in the late 1950's, remains among the best; it can outperform newer methods such as those based on word counting (Salton et al., 1994). The Position Method springs from the recognition that texts in a genre generally observe a predictable discourse structure, and that sentences of greater topic centrality tend to occur in certain specifiable locations. The text's title, for example, is a very informative position in most genres, as is the Ab- stract paragraph in scientific articles. Edmundson (Edmundson, 1969) defined the Position Method as follows: \"...the machine-readable cues are cer- tain general characteristics of the corpus provided by the skeletons of documents, i.e. headings and format. The Location method is based on the hypothesis that: (1) sentences occurring under certain headings are positively relevant; and (2) topic sen- tences tend to occur very early or very late in a document and its paragraphs.\" However, since the paradigmatic discourse structure differs significantly over text genres and subject do- mains, the Position Method cannot be defined as straightforwardly as Baxendale's (Baxendale, 1958) title plus first and last sentences of each paragraph; it has to be tailored to genre and domain. Can one develop ways of tailoring this method? Furthermore, since the resolution power of the Position Method is the sentence, while the de- sired output-topics-generally appear at the word or phrase level, the most accurate results of this method may still include too much spurious mate- rial to be really useful. How useful is the method in general? By what measure(s) can one evaluate it? Basic questions about how the Position Method can be tailored for optimality over a genre and how it can be evaluated for effectiveness remain unan- swered. To our knowledge, no systematic study has yet been performed, though some variant of it has been used in computational studies (see for example (Edmundson, 1969; Luhn, 1958; Baxendale, 1958)), writing-education classes (for example, (Sjostrom and Hare, 1984)), and has been the subject of cog- nitive psychological verification (Kieras, 1985). This paper contains an analysis of the Position Method. We first discuss previous work, then in Sec- tion 3 describe the background studies and training of an Optimal Position Policy for a genre of texts, and in Section 4 describe its evaluation. 2 Related Work Edmundson's (Edmundson, 1969) laid the ground- work for the Position Method. He introduced four clues for identifying significant words (topics) in a text. Among them, Title and Location are related to the Position Method. Edmundson assigned pos- itive weights to sentences according to their ordi- nal position in the text, giving most weight to the first sentence in the first paragraph and the last sen- tence in the last paragraph. He conducted seven- teen experiments to verify the significance of these methods. According to his results, the Title and Location methods respectively scored around 40% and 53% accuracy, where accuracy was measured as the coselection rate between sentences selected by Edmundson's program and sentences selected by a human. Although Edmundson's work is fundamental, his experiments used only 200 documents for training and another 200 documents for testing. Further- more, he did not trying out other possible combina- tions, such as the second and third paragraphs or the second-last paragraph. In order to determine where the important words are most likely to be found, Baxendale (Baxendale, 1958) conducted an inves- tigation of a sample of 200 paragraphs. He found that in 85% of paragraphs the topic sentence was in the first sentence and in 7% the final one. Donlan (Dolan, 1980) stated that a study of topic sentences in expository prose showed that only 13% of para- graphs of contemporary professional writers began with topic sentences (Braddock, 1974). Singer and Donlan (Singer and Dolan, 1980) maintain that a paragraph's main idea can appear anywhere in the paragraph, or not be stated at all. Arriving at a negative conclusion, Paijmans (Pai- jmans, 1994) conducted experiments on the relation between word position in a paragraph and its signif- icance, and found that \"words with a high informa- tion content according to the tf.idf-based weighting schemes do not cluster in the first and the last sen- tences of paragraphs or in paragraphs that consist of a single sentence, at least not to such an extent that such a feature could be used in the prepara- tion of indices for Information Retrieval purposes.\" In contrast, Kieras (Kieras, 1985) in psychological studies confirmed the importance of the position of a mention within a text. 3 Training the Rules 3.1 Background The purposes of our study are to clarify these contra- dictions, to test the abovementioned intuitions and results, and to verify the hypothesis that the impor- tance of a sentence in a text is indeed related to its ordinal position. Furthermore, we wish to discover empirically which textual positions are in fact the richest ones for topics, and to develop a method by which the optimal positions can be determined au- tomatically and their importance evaluated. To do all this, one requires a much larger docu- ment collection than that available to Edmundson and Baxendale. For the experiments described here, we used the Ziff-Davis texts from the corpus pro- duced for DARPA's TIPSTER program (Harman, 1994). Volume 1 of the Ziff corpus, on which we trained the system, consists of 13,000 newspaper texts about new computers and related hardware, computer sales, etc., whose genre can be character- ized as product announcements. The average text length is 71 sentences (34.4 paragraphs). Each text is accompanied by both a set of three to eight topic keywords and an abstract of approx. 6 sentences (both created by a human). In summary, we did the following: To determine the efficacy of the Position Method, we empirically determined the yield of each sentence position in the corpus, measuring against the topic keywords. We next ranked the sentence positions by their average yield to produce the Optimal Position Policy (OPP) for topic positions for the genre. Finally, now com- paring to the abstracts accompanying the texts, we measured the coverage of sentences extracted from the texts according to the policy, cumulatively in the position order specified by the policy. The high degree of coverage indicated the effectiveness of the position method. 3.2 Sentence Position Yields and the Optimal Position Policy We determined the optimal position for topic oc- currence as follows. Given a text T and a list of topics keywords t; of T, we label each sentence of T with its ordinal paragraph and sentence number (Pm,Sn). We then removed all closed-class words from the texts. We did not perform morphological restructuring (such as canonicalization to singular nouns, verb roots, etc.) or anaphoric resolution (re- placement of pronouns by originals, etc.), for want of robust enough methods to do so reliably. This makes the results somewhat weaker than they could be. What data is most appropriate for determining the optimal position? We had a choice between the topic keywords and the abstracts accompanying each text in the corpus. Both keywords and abstracts contain phrases and words which also appear in the original texts; on the assumption that these phrases or words are more important in the text than other ones, we can assign a higher importance to sentences with more such phrases or words (or parts of them).¹ Since a topic keyword has a fixed boundary, using it to rank sentences is easier than using an abstract. For this reason we defined sentence yield as the av- erage number of different topic keywords mentioned in a sentence. We computed the yield of each sen- tence position in each text essentially by counting ¹ How many topic keywords would be taken over ver- batim from the texts, as opposed to generated para- phrastically by the human extractor, was a question for empirical determination-the answer provides an upper bound for the power of the Position Method. the number of different topic keywords contained in the appropriate sentence in each text, and averag- ing over all texts. Sometimes, however, keywords consist of multiple words, such as \"spreadsheet soft- ware\". In order to reward a full-phrase mention in a sentence over just a partial overlap with a multi- word keyword/phrase, we used a formula sensitive to the degree of overlap. In addition, to take into account word position, we based this formula on the Fibonacci function; it monotonically increases with longer matched substrings, and is normalized to pro- duce a score of 1 for a complete phrase match. Our hit function H measures the similarity between topic keyword t; and a window wij that moves across each sentence (Pm, Sn) of the text. A window matches when it contains the same words as a topic keyword ti. The length of the window equals the length of the topic keyword. Moving the window from the be- ginning of a sentence to the end, we computed all the H, scores and added them together to get the total score H, for the whole sentence. We acquired the H, scores for all sentences in T and repeated the whole process for the each text in the corpus. After obtaining all the H, scores, we sorted all the sentences according to their paragraph and sentence numbers. For each paragraph and sentence number position, we computed the average Haug score. These average yields for each position are plotted in Figure 1, which shows the highest-yield sentence position to be (P2,S1), followed by (P3,S1), followed by (P4,S1), etc. Finally, we sorted the paragraph and sentence po- sition by decreasing yield Haug scores. For positions with equal scores, different policies are possible: one can prefer sentence positions in different paragraphs on the grounds that they are more likely to contains distinctive topics. One should also prefer sentence positions with smaller Sm, since paragraphs are gen- erally short. Thus the Optimal Position Policy for the Ziff-Davis corpus is the list [(T) (P2,S1) (P3,S1) (P2,S2) {(P4,S1) (P5,S1) (P3,S2)} {(P1,S1) (P6,S1) (P7,S1) (P1,S3) (P2,S3)} ...] 3.3 Additional Measures and Checks Throughout the above process, we performed addi- tional measures and checks in order to help us pre- vent spurious or wrong rules. We collected facts about the training corpus, including the average number of paragraphs per text (PPT), the average number of sentences per paragraph (SPP), and the average number of sentences per human-made sum- mary (SPS). PPT and SPP prevent us from forming a rule such as 25th sentence in the 100th paragraph when PPT is 15 and SPP is 5. SPS suggests how many sentences to extract. For the ZIFF Vol. 1 cor- pus, PPT is 34.43, SPP is 2.05, and SPS is 5.76. Most texts have under 30 paragraphs; 97.2% of para- SENTENCE POSITION IN A PARAGRAPH TIPSTER ZIFF VOL1 POLICY DETERMINATION MAP 10 0.5 0.3 43 20 25 30 10 15 PARAGRAPH POSITION IN A TEXT Figure 1: Average yield by paragraph and sentence position; lightest shade shows highest yield. graphs have fewer than 5 sentences. 47.7% of para- graphs have only one sentence (thus the first sen- tence is also the last), and 25.2% only two. With regard to the abstracts, most have 5 sentences and over 99.5% have fewer than 10. We also counted how many different topic key- words each specific text unit contains, counted once per keyword. This different hit measure dhit played an important role, since the OPP should be tuned to sentence positions that bear as many different topic keywords as possible, instead of positions with very high appearances of just a few topic keywords. We can compute dhit for a sentence, several sentences, or several paragraphs. Sentenceyield is dhit score of a sentence. Figure 2 shows dhit scores for the first 50 paragraph positions, and Figure 3 dhit scores for the last 50 positions (counting backward from the end of each text). Since PPT=34.43, the first and last 50 positions fully cover the majority of texts. The former graph illustrates the immense importance of the title sentence (dhit = 1.96), and the importance of the second (dhit = 0.75) and third (dhit = 0.64) paragraphs relative to the first (dhit = 0.59). Para- graphs close to the beginning of texts tend to bear more informative content; this is borne out in Fig- ure 3, which clearly indicates that paragraph posi- tions close to the end of texts do not show particu- larly high values, while the peak occurs at position P-14 with dhit = 0.42. This peak occurs precisely where most texts have their second or third para- graphs (recall that the average text length is 13 to 16 paragraphs). To examine Baxendale's first/last sentence hy- pothesis, we computed the average dhit scores for the first and the last 10 sentence positions in a para- graph as shown in Figure 4 and Figure 5 respectively. The former indicates that the closer a sentence lies DHIT DHIT 0.4 02 68 1.8 1.6 1,4 1.2 1.36 2 TIPSTER ZIFF VOL1 AVERAGE DHIT DISTRIBUTION OF THE FIRST 10 SENTENCE POSITIONS 0.45 0.4 0.35 TIPSTER ZIFF VOL1 AVERAGE DHIT DISTRIBUTION OF THE TITLE SENTENCE AND THE FIRST 50 PARAGRAPH POSITIONS 0.3 0.25 DHIT 0.2 0.15 0.1 0.05 0 S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 SENTENCE POSITION 0.75 T P2 P4 P5 P8 P10 P12 P14 P18 P18 P20 P22 P24 P28 P28 P30 P32 P34 P36 P38 P40 P42 P44 P48 P48 P50 P1 P2 PS P7 P9 P11 P13 P15 P17 P19 P21 P23 P25 P27 P29 P31 P33 P36 P37 P38 P41 P43 P45 P47 P49 PARAGRAPH POSITION Figure 4: Vol. 1 dhit distribution of the first 10 sen- tence positions in a paragraph. Figure 2: Vol. 1 dhit distribution for the title sen- tence and the first 50 paragraph positions. 0.45 TIPSTER ZIFF VOL1 AVERAGE DHIT DISTRIBUTION OF THE LAST 50 PARAGRAPH POSITIONS 0.42 DHIT TIPSTER ZIFF VOL1 AVERAGE DHIT DISTRIBUTION OF THE LAST 10 SENTENCE POSITIONS 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 S-1 S-2 S-3 S-4 S-5 S-6 S-7 S-8 S-9 S-10 SENTENCE POSITION 0.4 0.36 2 23 0.25 02 0.15 0.1 0.05 P-1 P-3 P-6 P-7 P P-11 P-13 P-15 P-17 P-19 P-21 P-23 P-25 P-27 P-29 P-31 P-33 P-36 P-37 P-38 P-41 P-43 P-46 P-47 P-49 P2 P4 Pa Pa P-10 P-12 P-14 P-16P-18 P-20 P-22 P-24 P-26 P-28 P-30 P-32 P-34 P-38 P-38 P-40 P-42 P-44 P-48 P-48 P-50 PARAGRAPH POSITION Figure 3: Vol. 1 dhit distribution for the last 50 paragraph positions, counting backward. Figure 5: Vol. 1 dhit distribution of the last 10 sen- tence positions in a paragraph. to the beginning of a paragraph, the higher its dhit score is. This confirms the first sentence hypothe- sis. On the other hand, the latter figure does not support the last sentence hypothesis; it suggests in- stead that the second sentence from the end of a paragraph contains the most information. This is explained by the fact that 47.7% of paragraphs in the corpus contain only one sentence and 25.2% of the paragraphs contain two sentences, and the SPP is 2.05: the second-last sentence is the first! 4 Evaluation The goal of creating an Optimal Position Policy is to adapt the position hypothesis to various domains or genres in order to achieve maximal topic coverage. Two checkpoints are required: --- SENTENCE POSITION IN A PARAGRAPH 10 5 TIPSTER ZIFF VOL2 ZF_251 TO ZF_300 POLICY DETERMINATION MAP 0 1 10 15 20 25 PARAGRAPH POSITION IN A TEXT 30 Figure 6: Vol. 2 optimal position Policy Determina- tion Map in contour view. 1. applying the procedure of creating an OPP to another collection in the same domain should result in a similar OPP, and 2. sentences selected according to the OPP should indeed carry more information than other sen- tences. Two evaluations were conducted to confirm these points. In both cases, we compared the sentences ex- tracted according to the OPP to the sentences con- tained in the human-generated abstracts. Though we could have used topic keywords for both training and evaluation, we decided that the abstracts would provide a more interesting and practical measure for output, since the OPP method extracts from the text full sentences instead of topic phrases. Accord- ingly, we used as test corpus another, previously un- seen, set of 2,907 texts from Vol. 2 of the Ziff-Davis corpus, which contained texts of the same nature and genre as Vol. 1. 4.1 Evaluation I This evaluation established the validity of the Po- sition Hypothesis, namely that the OPP so deter- mined does in fact provide a way of identifying high- yield sentences, and is not just a list of average high- yield positions of the corpus we happened to pick. following the same steps as before, we therefore de- rived a new OPP on the test corpus. The result of the average scores of 300 positions (Pm, Sn) shown in Figure 6, with 1 ≤ m ≤ 30 and 1 ≤ n ≤ 10, was a contour map highly similar to Figure 1. Both peak at position (P₂, S₁) and decrease grad- ually in the X direction and more rapidly in the Y direction. The similarity between the policy de- termination maps of the training and test sets con- firms two things: First, correspondences exist be- tween topics and sentence positions in texts such as the ZIFF-Davis collection. Second, the regularity between topics and sentence positions can be used to identify topic sentences in texts. 4.2 Evaluation II In the evaluation, we measured the word overlap of sentences contained in the abstracts with sentence(s) extracted from a text according to the OPP. For each measure, we recorded scores cumulatively, choosing first the most promising sentence according to the OPP, then the two most promising, and so on. We measured word overlap as follows: first, we re- moved all function (closed-class) words from the ab- stract and from the text under consideration. Then, for the first 500 sentence positions (the top 1, 2, 3,..., taken according to the OPP), we counted the number of times a window of text in the extracted sentences matched (i.e., exactly equalled) a window of text in the abstract. (Again we performed no mor- phology manipulations or reference resolution, steps which would improve the resulting scores.) We per- formed the counts for window lengths of 1, 2, 3, 4, and 5 words. If a sentence in an abstract matched more than one sentence extracted by the OP, only the first match was tallied. For each number of sen- tences extracted, and for each window size, we aver- aged the counts over all 2,907 texts. We define some terms and three measures used to assess the quality of the OPP-selected extracts. For an extract E and a abstract A: wmi: a window i of size m in E. wmi: a window i of size m in A. [W]: total number of windows of size m in E. A [W]: total number of different windows of size m in A, i.e., how many wmi ≠ wmj. A hit : wmi = wmj, i.e., words and word se- quences in wmi and wmj are exactly the same. Precision of windows size m: Pm = #hits WE Recall of windows size m: Rm = # different hits WA Coverage of windows size m: Cm = # sentences in A with at least one hit # sentences in A --- --- PRECISION/RECALL 6.25 2 4 TIPSTER ZIFF VOL1 POLICY SELECTION PRECISION/RECALL WINDOWS SIZE 1 6.44 0.14 6.15 6.05 64 2 4 024 OPP SELECTED POSITION 0.42 0.4 0.46 6.37 0.37 6.37 0.36 Precision Recall PRECISON 6.35 611 6.25 12 6.15 GLOS TIPSTER ZIFF VOL1 OPP SELECTED POSITION PRECSION SCORE WITH INDICATION OF INDIVIDUAL CONTRIBUTION FROM WINDOW SIZE OF 1 TO 5 PO P1 P2 Ps P4 PS PS PS Ps OPP SELECTED POSITION Window D Figure 7: Cumulative precision/recall scores of top ten OPP-selected sentence positions of window size 1. PRECISION/RECALL 62 TIPSTER ZIFF VOL1 POLICY SELECTION PRECISION/RECALL WINDOWS SIZE 2 6.18 617 8.18 6.14 0.14 6.12 0.06 0.04 0.04 2 0.12 OPP SELECTED POSITION 6.16 7 10 Precision Recall Figure 8: Cumulative precision/recall scores of top ten OPP-selected sentence positions of window size 2. 4.2.1 Precision and Recall Precision, Pm, measures what percentage of win- dows of size min E can also be found in A (that is, Pm indicates what percentage of E is considered important with regard to A). Recall, Rm, measures the diversity of E. A high Pm does not guarantee recovery of all the possible topics in A, but a high Rm does ensure that many different topics in A are covered in E. However, a high Rm alone does not warrant good performance either. For example, an OPP that selects all the sentences in the original text certainly has a very high Rm, but this extract duplicates the original text and is the last thing we want as a summary! Duplicate matches (the same word(s) in different windows) were counted in P but not in R. Figure 9: Precision scores show individual contribu- tion from window size 1 to 5. Figure 7 and Figure 8 show the precision/recall graphs of window sizes 1 and 2 respectively. Fig- ure 7 indicates that the precision score decreases slowly and the recall score increases more rapidly as we choose more sentences according to the OPP. Selecting 7 sentences (is 10% of the average length of a ZIFF text), the precision is 0.38 and the re- call 0.35. Considering that the matching process requires exact match and morphological transfor- mation is not used, this result is very encouraging. However, with window size 2, precision and recall scores drop seriously, and more so with even larger windows. This suggests using variable-length win- dows, sizing according to maximal match. So doing would also avoid counting matches on window size 1 into matches of larger window sizes. The contri- butions of precision, P, and recall, Rom, from each m-word window alone, can be approximated by: PPm- Pm+1 RmRm - Rm+1 Figure 9 and Figure 10 show precision and recall scores with individual contributions from window sizes 1 to 5. Precision P, and recall R₁ of variable- length windows can be estimated as follows: P 1 ~ ΣΡ m=1 1 Σ m R ≈ Rm m=1 The performance of variable-length windows com- pared with windows of size 1 should have a differ- ence less than the amount shown in the segments of window size > 5. --- TIPSTER ZIFF VOL1 OPP SELECTED POSITION RECALL SCORE WITH INDICATION OF INDIVIDUAL CONTRIBUTION FROM WINDOW SIZE OF 1 TO 5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.05 0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 OPP SELECTED POSITION Figure 10: Recall scores show individual contribu- tion from window size 1 to 5. 4.2.2 Coverage Coverage, Cm, tests similarity between E and A in a very loose sense. It counts the number of sen- tences in A with at least one hit in E (i.e., there exists at least one pair of windows wAi and wmj such that wAi = wmj). Cm estimates the potential of the OPP procedure. Figure 11 shows the cumula- tive average coverage scores of the top ten sentence positions of the training set following the OPP. Fig- ure 11 indicates that 68% of sentences in A shared with the title sentence at least one word, 25% two words, 10% three words, 4% four words, and 2% five words. The amount of sharing at least one word goes up to 88% if we choose the top 5 positions ac- cording to the OPP and 95% if we choose the top 10 positions! The contribution of coverage score, Cm, solely from m-word match between E and A can be com- puted as follows: Cm = Cm - Cm-1 The result is shown in Figure 12. Notice that the topmost segment of each column in Figure 12 repre- sents the contribution from matches of at least five words long, since we only have Cm up to m = 5. The average number of sentences per summary (SPS) is 5.76. If we choose the top 5 sentence positions ac- cording to the OPP, Figure 12 tells us that these 5-sentences extracts E (the average length of an ab- stract), cover 88% of A in which 42% derives solely from one-word matches, 22% two words, 11% three words, and 6% four words. The average number of sentences per text in the corpus is about 70. If we produce an extract of about 10% of the average length of a text, i.e. 7 sentences, the coverage score is 0.91. This result is extremely promising and con- firms the OPP-selected extract bearing important contents. COVERAGE 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 OPP SELECTED POSITION Figure 11: Cumulative coverage scores of top ten sentence positions according to the OPP, with win- dow sizes 1 to 5. COVERAGE 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 TIPSTER ZIFF VOL1 CUMULATIVE AVERAGE COVERAGE OF TOP 10 OPP-SELECTED POSITIONS R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 OPP SELECTED POSITION Figure 12: Cumulative coverage scores of top ten sentence positions with contribution marked for each window size. DHIT/TOPIC 0.8 0.7 0.6 0.5 0.42 0.4 0.3 TIPSTER ZIFF VOL1 OPP CUMULATIVE DHIT PER TOPIC OF THE OPP SELECTED TOP 18 POSITIONS 0.50 8.54 0.40 6.61 862 8.63 0.88 0.68 0.88 0.70 0.70 0.70 0.70 0.06 future work. Ultimately, the Position Method can only take one a certain distance. Because of its limited power of resolution--the sentence-and its limited method of identification-ordinal positions in a text-it has to be augmented by additional, more precise tech- niques. But the results gained from what is after all a fairly simple technique are rather astounding nonetheless. 02 0.1 Tase P2/51 P3/31 P2/52 P4461 PS/$1 P3/52 P1/51 P51 P7/31 PS1 P51 P10/51 P1/52 P4/52 P1/53 P2/33 P2/34 OPP SELECTED TOP 18 POSITIONS Figure 13: Cumulative dhit per topic for the top 18 OPP selected positions. 5 Conclusion This study provides empirical validation for the Po- sition Hypothesis. It also describes a method of de- riving an Optimal Position Policy for a collection of texts within a genre, as long as a small set of topic keywords is defined with each text. The Precision and Recall scores indicate the selective power of the Position method on individual topics, while the Cov- erage scores indicate a kind of upper bound on topics and related material as contained in sentences from human-produced abstracts. The results displayed in Figure 13 are especially promising. It is clear that only about 30% of topic keywords are not mentioned in the text directly. This is excellent news: it means that as an upper bound, only about 30% of the humans' abstracts in this domain derive from some inference processes, which means that in a computational implementa- tion only about the same amount has to be derived by processes yet to be determined. Second, the ti- tle contains about 50% of the topic keywords; the title plus the two most rewarding sentences provide about 60%, and the next five or so add another 6%. Thus, a fairly small number of sentences provides 2/3 of the keyword topics. It must be remembered that our evaluations treat the abstract as ideal--they rest on the assumption that the central topic(s) of a text are contained in the abstract made of it. In many cases, this is a good assumption; it provides what one may call the author's perspective of the text. But this assump- tion does not support goal-oriented topic search, in which one wants to know whether a text pertains to some particular prespecified topics. For a goal- oriented perspective, one has to develop a different method to derive an OPP; this remains the topic of References P. B. Baxendale. 1958. Machine-made index for technical literature an experiment. IBM Jour- nal, pages 354-361, October. Richard Braddock. 1974. The frequency and place- ment of topic sentences in expository prose. In Research in The Teaching of English, volume 8, pages 287-302. Dan Dolan. 1980. Locating main ideas in history textbooks. In Journal of Reading, pages 135-140. H. P. Edmundson. 1969. New methods in automatic extracting. Journal of the ACM, 16(2):264-285. Donna Harman. 1994. Data preparation. In R. Mer- chant, editor, The Proceedings of the TIPSTER Text Program Phase I, San Mateo, California. Morgan Kaufmann Publishing Co. D.E. Kieras, 1985. Thematic Process in the Com- prehension of Technical Prose, pages 89-108. Lawrence Elrbaum Association, Hillsdale, New Jersey. H. P. Luhn. 1958. The automatic creation of lit- erature abstracts. IBM Journal, pages 159-165, April. J.J. Paijmans. 1994. Relative weights of words in documents. In L.G.M. Noordman and W.A.M. de Vroomen, editors, Conference Proceedings of STINFON. StinfoN. Gerard Salton, James Allan, Chris Buckley, and Amit Singhal. 1994. Automatic analysis, theme generation, and summarization of machine- readable texts. Science, 264:1421-1426, June. Harry Singer and Dan Dolan. 1980. Reading And Learning from Text. Little Brown, Boston, Mass. Colleen Langdon Sjostrom and Victoria Chou Hare. 1984. Teaching high school students to identify main ideas in expository text. Journal of Educa- tional Research, 78(2):114-118."
  },
  {
    "title": "Using SGML as a Basis for Data-Intensive NLP",
    "abstract": "This paper describes the LT NSL system (McKelvie et al, 1996), an architecture for writing corpus processing tools. This system is then compared with two other systems which address similar issues, the GATE system (Cunningham et al, 1995) and the IMS Corpus Workbench (Christ, 1994). In particular we address the advantages and disadvantages of an SGML approach compared with a non-SGML database approach. Finally, in order to back up our claims about the merits of SGML-based corpus processing, we present a number of case studies of the use of the LT NSL system for corpus preparation and linguistic analysis.",
    "content": "1 Introduction The theme of this paper is the design of software and data architectures for natural language process- ing using corpora. Two major issues in corpus-based NLP are: how best to deal with medium to large scale corpora often with complex linguistic annota- tions, and what system architecture best supports the reuse of software components in a modular and interchangeable fashion. In this paper we describe the LT NSL system (McK- elvie et al, 1996), an architecture for writing corpus processing tools, which we have developed in an at- tempt to address these issues. This system is then compared with two other systems which address some of the same issues, the GATE system (Cun- ningham et al, 1995) and the IMS Corpus Work- bench (Christ, 1994). In particular we address the advantages and disadvantages of an SGML approach compared with a non-SGML database approach. Fi- nally, in order to back up our claims about the merits of SGML-based corpus processing, we present a num- ber of case studies of the use of the LT NSL system for corpus preparation and linguistic analysis. 2 The LT NSL system LT NSL is a tool architecture for SGML-based pro- cessing of (primarily) text corpora. It generalises the UNIX pipe architecture, making it possible to use pipelines of general-purpose tools to process an- notated corpora. The original UNIX architecture al- lows the rapid construction of efficient pipelines of conceptually simple processes to carry out relatively complex tasks, but is restricted to a simple model of streams as sequences of bytes, lines or fields. LT NSL lifts this restriction, allowing tools access to streams which are sequences of tree-structured text (a repre- sentation of SGML marked-up text). The use of SGML as an I/O stream format between programs has the advantage that SGML is a well de- fined standard for representing structured text. Its value is precisely that it closes off the option of a proliferation of ad-hoc notations and the associated software needed to read and write them. The most important reason why we use SGML for all corpus lin- guistic annotation is that it forces us to formally de- scribe the markup we will be using and provides soft- ware for checking that these markup invariants hold in an annotated corpus. In practise this is extremely useful. SGML is human readable, so that interme- diate results can be inspected and understood. It also means that it is easy for programs to access the information which is relevant to them, while ignor- ing additional markup. A further advantage is that many text corpora are available in SGML, for exam- ple, the British National Corpus (Burnage&Dunlop, 1992). The LT NSL system is released as C source code. The software consists of a C-language Application Program Interface (API) of function calls, and a num- ber of stand-alone programs which use this API. The current release is known to work on UNIX (SunOS 4.1.3, Solaris 2.4 and Linux), and a Windows-NT version will be released during 1997. There is also an API for the Python programming language. One question which arises in respect to using SGML as an I/O format is: what about the cost of parsing SGML? Surely that makes pipelines too in- efficient? Parsing SGML in its full generality, and providing validation and adequate error detection is indeed rather hard. For efficiency reasons, you wouldn't want to use long pipelines of tools, if each tool had to reparse the SGML and deal with the full language. Fortunately, LT NSL doesn't require this. The first stage of processing normalises the input, producing a simplified, but informationally equivalent form of the document. Subsequent tools can and often will use the LT NSL API which parses normalised SGML (henceforth NSGML) approximately ten times more efficiently than the best parsers for full SGML. The API then returns this parsed SGML to the calling program as data-structures. NSGML is a fully expanded text form of SGML in- formationally equivalent to the ESIS output of SGML parsers. This means that all markup minimisation is expanded to its full form, SGML entities are ex- panded into their value (except for SDATA entities), and all SGML names (of elements, attributes, etc) are normalised. The result is a format easily readable by humans and programs. The LT NSL programs consist of mknsg, a program for converting arbitrary valid SGML into normalised SGML¹, the first stage in a pipeline of LT NSL tools; and a number of programs for manipulating nor- malised SGML files, such as sggrep which finds SGML elements which match some query. Other of our soft- ware packages such as LT POS (a part of speech tag- ger) and LT WB (Mikheev&Finch, 1997) also use the LT NSL library. In addition to the normalised SGML, the mknsg program writes a file containing a compiled form of the Document Type Definition (DTD)2, which LT NSL programs read in order to know what the structure of their NSGML input or output is. How fast is it? Processes requiring sequential ac- cess to large text corpora are well supported. It is unlikely that LT NSL will prove the rate limiting step in sequential corpus processing. The kinds of re- peated search required by lexicographers are more of a problem, since the system was not designed for that purpose. The standard distribution is fast enough for use as a search engine with files of up to several million words. Searching 1% of the British National Corpus (a total of 700,000 words (18 Mb)) is currently only 6 times slower using LT NSL sggrep than using fgrep, and sggrep allows more complex structure-sensitive queries. A prototype indexing mechanism (Mikheev&McKelvie, 1997), not yet in ¹Based on James Clark's SP parser (Clark, 1996). ²SGML's way of describing the structure (or grammar) of the allowed markup in a document the distribution, improves the performance of LT NSL to acceptable levels for much larger datasets. Why did we say \"primarily for text corpora\"? Be- cause much of the technology is directly applicable to multimedia corpora such as the Edinburgh Map Task corpus (Anderson et al, 1991). There are tools which interpret SGML elements in the corpus text as offsets into files of audio-data, allowing very flexi- ble retrieval and output of audio information using queries defined over the corpus text and its annota- tions. The same could be done for video clips, etc. 2.1 Hyperlinking We are inclined to steer a middle course between a monolithic comprehensive view of corpus data, in which all possible views, annotations, structurings etc. of a corpus component are combined in a sin- gle heavily structured document, and a massively decentralised view in which a corpus component is organised as a hyper-document, with all its informa- tion stored in separate documents, utilising inter- document pointers. Aspects of the LT NSL library are aimed at supporting this approach. It is neces- sary to distinguish between files, which are storage units, (SGML) documents, which may be composed of a number of files by means of external entity ref- erences, and hyper-documents, which are linked en- sembles of documents, using e.g. HyTime or TEI (Sperberg-McQueen&Burnard, 1994) link notation. The implication of this is that corpus compo- nents can be hyper-documents, with low-density (i.e. above the token level) annotation being expressed in- directly in terms of links. In the first instance, this is constrained to situations where element content at one level of one document is entirely composed of elements from another document. Suppose, for example, we already had segmented a file resulting in a single document marked up with SGML headers and paragraphs, and with the word segmentation marked with <w> tags: <p id=p4> <w id=p4.w1>Time</w> <w id=p4.w2>flies</w> <w id=p4.w3>.</w> </p> The output of a phrase-level segmentation might then be stored as follows: <p id=p4> <phr id=p4.ph1 type=n doc=file1 from='id p4.w1'> <phr id=p4.ph2 type=v from='id p4.w2'> </p> Linking is specified using one of the available TEI mechanisms. Details are not relevant here, suffice it to say that doc=file1 resolves to the word level file and establishes a default for subsequent links. At a minimum, links are able to target single elements or sequences of contiguous elements. LT NSL imple- ments a textual inclusion semantics for such links, in- serting the referenced material as the content of the element bearing the linking attributes. Although the example above shows links to only one document, it is possible to link to several documents, e.g. to a word document and a lexicon document: <word> <source doc=file1 from='id p4.w1'> <lex doc=lex1 from='id lex.40332'> </word> Note that the architecture is recursive, in that e.g. sentence-level segmentation could be expressed in terms of links into the phrase-level segmentation as presented above. The data architecture needs to address not only multiple levels of annotation but also alternative ver- sions at a given level. Since our linking mechanism uses the SGML entity mechanism to implement the identification of target documents, we can use the entity manager's catalogue as a means of managing versions. For our example above, this means that the connection between the phrase encoding document and the segmented document would be in two steps: the phrase document would use a PUBLIC identi- fier, which the catalogue would map to the particular file. Since catalogue entries are interpreted by tools as local to the directory where the catalogue itself is found, this means that binding together groups of alternative versions can be easily achieved by storing them under the same directory. Subdirectories with catalogue fragments can thus be used to represent both increasing detail of anno- tation and alternatives at a given level of annotation. Note also that with a modest extension of func- tionality, it is possible to use the data architecture described here to implement patches, e.g. to the to- kenisation process. If alongside an inclusion seman- tics, we have a special empty element <repl> which is replaced by the range it points to, we can produce a patch file, e.g. for a misspelled word, as follows (irrelevant details omitted): <nsl> <!-- to get the original header--> <repl doc=original from='id hdr1'> <text> <!-- the first swatch of unchanged text --> <repl from='id p1' to='id p324'> <!-- more unchanged text --> <p id=p325> <repl from='id p325.t1' to='id p325.t15'> <!-- the correction itself --> <corr sic='procede' resp='ispell'> <token id=p325.t16>proceed</token> </corr> <!-- more unchanged text--> <repl from='id p325.t17' to='id p325.t96'> </p> <!-- the rest of the unchanged text--> <repl from='id p326' to='id p402'> </text> </nsl> Whether such a patch would have knock-on effects on higher levels of annotation would depend, inter alia, on whether a change in tokenisation crossed any higher-level boundaries. 2.2 sggrep and the LT NSL query language The API provides the program(mer) with two alter- native views of the NSGML stream: an object stream view and a tree fragment view. The first, lower level but more efficient, provides data structures and ac- cess functions such as GetNextBit and PrintBit, where there are different types of Bits for start (or empty) tags with their attributes, text content, end tags, and a few other bits and pieces. The alternative, higher level, view, lets one treat the NSGML input as a sequence of tree- fragments. The API provides functions GetNextItem and PrintItem to read and write the next com- plete SGML element. It also provides functionality GetNextQueryElement(infile,query,subquery, regexp, outfile) where query is an LT NSL query which allows one to specify particular elements on the basis of their position in the document struc- ture and their attribute values. The subquery and regexp allow one to specify that the matching ele- ment has a subelement matching the subquery with text content matching the regular expression. El- ements which do not match the query are passed through unchanged to outfile. Under both mod- els, processing is essentially a loop over calls to the API, in each case choosing to discard, modify or out- put unchanged each Bit or Element. Rather than define the query language here (de- tails can be found in (McKelvie et al, 1996)), we will just provide an example. The call GetNextQueryElement(inf,\".*/TEXT/.*/P\", \"P/.*/S\",\"th(eilie)r\", outf) would return the next <P> element dominated anywhere by <TEXT> at any depth, with the <P> element satisfying the additional requirement that it contain at least one <S> element at any depth with text containing at least one instance of 'their' (possibly misspelt). 3 Comparisons with other systems The major alternative corpus architecture which has been advocated is a database approach, where anno- tations are kept separately from the base texts. The annotations are linked to the base texts either by means of character offsets or by a more sophisticated indexing scheme. We will discuss two such systems and compare them with the LT NSL approach. 3.1 GATE The GATE system (Cunningham et al, 1995), currently under development at the University of Sheffield, is a system to support modular language engineering. 3.1.1 System components It consists of three main components: • GDM - an object oriented database for stor- ing information about the corpus texts. This database is based on the TIPSTER document architecture (Grishman, 1995), and stores text annotations separate from the texts. Annota- tions are linked to texts by means of character offsets³. • Creole - A library of program and data resource wrappers, that allow one to interface externally developed programs/resources into the GATE architecture. • GGI - a graphical tool shell for describing pro- cessing algorithms and viewing and evaluating the results. A MUC-6 compatible information extraction sys- tem, VIE, has been built using the GATE architec- ture. 3.1.2 Evaluation Separating corpus text from annotations is a gen- eral and flexible method of describing arbitrary structure on a text. It may be less useful as a means of publishing corpora and may prove inefficient if the underlying corpus is liable to change. Although TIPSTER lets one define annotations and their associated attributes, in the present ver- sion (and presumably also in GATE) these defini- tions are treated only as documentation and are not validated by the system. In contrast, the SGML parser validates its DTD, and hence provides some check that annotations are being used in their in- tended way. SGML has the concept of content mod- els which restrict the allowed positions and nesting ³More precisely, by inter byte locations. of annotations. GATE allows any annotation any- where. Although this is more powerful, i.e. one is not restricted to tree structures, it does make validation of annotations more difficult. The idea of having formalised interfaces for exter- nal programs and data is a good one. The GGI graphical tool shell lets one build, store, and recover complex processing specifications. There is merit in having a high level language to specify tasks which can be translated automatically into executable programs (e.g. shell scripts). This is an area that LT NSL does not address. 3.1.3 Comparison with LT NSL In (Cunningham et al, 1996), the GATE archi- tecture is compared with the earlier version of the LT NSL architecture which was developed in the MULTEXT project. We would like to answer these points with reference to the latest version of our soft- ware. It is claimed that using normalised SGML implies a large storage overhead. Normally however, nor- malised SGML will be created on the fly and passed through pipes and only the final results will need to be stored. This may however be a problem for very large corpora such as the BNC. It is stated that representing ambiguous or over- lapping markup is complex in SGML. We do not agree. One can represent overlapping markup in SGML in a number of ways. As described above, it is quite possible for SGML to represent 'stand-off' annotation in a similar way to TIPSTER. LT NSL provides the hyperlinking semantics to interpret this SGML. The use of normalised SGML and a compiled DTD file means that the overheads of parsing SGML in each program are small, even for large DTDs, such as the TEI. LT NSL is not specific to particular applications or DTDs. The MULTEXT architecture was tool- specific, in that its API defined a predefined set of ab- stract units of linguistic interest, words, sentences, etc. and defined functions such as ReadSentence. That was because MULTEXT was undecided about the format of its I/O. LT NSL in contrast, since we have decided on SGML as a common format, provides functions such as GetNextItem which read the next SGML element. Does this mean the LT NSL architec- ture is application neutral? Yes and no. Yes, because there is in principle no limit on what can be encoded in an SGML document. In the TIP- STER architecture there is an architectural require- ment that all annotations be ultimately associated with spans of a single base text, but LT NSL imposes no such requirement. This makes it easier to be clear about what happens when a different view is needed on fixed-format read-only information, or when it turns out that the read-only information should be systematically corrected. The details of this are a matter of ongoing research, but an important moti- vation for the architecture of LT NSL is to allow such edits without requiring that the read-only informa- tion be copied. No, because in practice any corpus is encoded in a way which reflects the assumptions of the corpus de- velopers. Most corpora include a level of representa- tion for words, and many include higher level group- ings such as breath groups, sentences, paragraphs and/or documents. The sample back-end tools dis- tributed with LT NSL reflect this fact. It is claimed that there is no easy way in SGML to differentiate sets of results by who or what produced them. But, to do this requires only a convention for the encoding of meta-information about text cor- pora. For example, SGML DTDs such as the TEI include a 'resp' attribute which identifies who was responsible for changes. LT NSL does not require tools to obey any particular conventions for meta- information, but once a convention is fixed upon it is straightforward to encode the necessary informa- tion as SGML attributes. Unlike TIPSTER, LT NSL is not built around a database, so we cannot take advantage of built-in mechanisms for version control. As far as corpus annotation goes, UNIX rcs, has proved an adequate solution to our version control needs. Alternatively, version control can be provided by means of hyper- linking. The GATE idea of providing formal wrappers for interfacing programs is a good one. In LT NSL the corresponding interfaces are less formalised, but can be defined by specifying the DTDs of a pro- gram's input and output files. For example a part- of-speech tagger would expect <W> elements inside <S> elements, and a 'TAG' attribute on the output <W> elements. Any input file whose DTD satisfied this constraint could be tagged. SGML architectural forms (a method for DTD subsetting) could provide a method of formalising these program interfaces. As Cunningham et. al. say, there is no reason why there could not be an implementation of LT NSL which read SGML elements from a database rather than from files. Similarly, a TIPSTER architecture like GATE could read SGML and convert it into its internal database. In that case, our point would be that SGML is a suitable abstraction for programs rather than a more abstract (and perhaps more lim- ited) level of interface. We are currently in discus- sion with the GATE team about how best to allow the interoperability of the two systems. 3.2 The IMS Corpus Workbench The IMS Corpus Workbench (Christ, 1994) includes both a query engine (CQP) and a Motif-based user visualisation tool (zkwic). CQP provides a query lan- guage which is a conservative extension of famil- iar UNIX regular expression facilities. XKWIC is a user interface tuned for corpus search. As well as providing the standard keyword-in-context facilities and giving access to the query language it gives the user sophisticated tools for managing the query his- tory, manipulating the display, and storing search results. The most interesting points of comparison with LT NSL are in the areas of query language and underlying corpus representation. 3.2.1 The CQP model CQP treats corpora as sequences of attribute-value bundles. Each attributes can be thought of as a total function from corpus positions to attribute values. Syntactic sugar apart, no special status is given to the attribute word. 3.2.2 The query language The query language of IMS-CWB, which has the usual regular expression operators, works uniformly over both attribute values and corpus positions. This regularity is a clear benefit to users, since only one syntax must be learnt. Expressions of considerable sophistication can be generated and used successfully by beginners. Con- sider: [pos=\"DT\" & word !=\"the\"] [pos=\"JJ.*\"]? [pos=\"N.+\"] This means, in the context of the Penn treebank tagset, \"Find me sequences beginning with deter- miners other than the, followed by optional adjec- tives, then things with nominal qualities\". The in- tention is presumably to find a particular sub-class of noun-phrases. The workbench has plainly achieved an extremely successful generalisation of regular expressions, and one which has been validated by extensive use in lexicography and corpus-building. There is only limited access to structural infor- mation. While it is possible, if sentence boundaries are marked in the corpus, to restrict the search to 4Like LT NSL IMS-CWB is built on top of Henry Spencer's public domain regular expression package In CQP terminology these are the \"positional attributes\". within-sentence matches, there are few facilities for making more refined use of hierarchical structure. The typical working style, if you are concerned with syntax, is to search for sequences of attributes which you believe to be highly correlated with particular syntactic structures. 3.2.3 Data representation CQP requires users to transform the corpora which will be searched into a fast internal format. This format has the following properties: • Because of the central role of corpus position it is necessary to tokenise the input corpus, map- ping each word in the raw input to a set of at- tribute value pairs and a corpus position. • There is a logically separate index for each at- tribute name in the corpus. • CQP uses an integerised representation, in which corpus items having the same value for an at- tribute are mapped into the same integer de- scriptor in the index which represents that at- tribute. This means that the character data cor- responding to each distinct corpus token need only be stored once. • For each attribute there is an item list contain- ing the sequence of integer descriptors corre- sponding to the sequence of words in the corpus. Because of the presence of this list the storage cost of adding a new attribute is linear in the size of the corpus. If the new attribute were sparse, it would be possible to reduce the space cost by switching (for that attribute) to a more space efficient encoding 3.2.4 Evaluation The IMS-CWB is a design dominated by the need for frequent fast searches of a corpus with a fixed an- notation scheme. Although disk space is now cheap, the cost of preparing and storing the indices for IMS- CWB is such that the architecture is mainly appro- priate for linguistic and lexicographic exploration, but less immediately useful in situations, such as obtain in corpus development, where there is a re- curring need to experiment with different or evolving attributes and representational possibilities. Some support is provided for user-written tools, but as yet there is no published API to the poten- tially very useful query language facilities. The in- dexing tools which come with IMS-CWB are less flexi- ble than those of LT NSL since the former must index $^6$IMS-CWB already supports compressed index files, and special purpose encoding formats would presumably save even more space. on words, while the latter can index on any level of the corpus annotation. The query language of IMS-CWB is an elegant and orthogonal design, which we believe it would be ap- propriate to adopt or adapt as a standard for corpus search. It stands in need of extension to provide more flexible access to hierarchical structure. The query language of LT NSL is one possible template for such extensions, as is the opaque but powerful tgrep program (Pito, 1994) which is provided with the Penn Treebank. 4 Case studies 4.1 Creation of marked-up corpora One application area where the paradigm of sequen- tial adding of markup to an SGML stream fits very closely, is that of the production of annotated cor- pora. Marking of major sections, paragraphs and headings, word tokenising, sentence boundary mark- ing, part of speech tagging and parsing are all tasks which can be performed sequentially using only a small moving window of the texts. In addition, all of them make use of the markup created by earlier steps. If one is creating an annotated corpus for pub- lic distribution, then SGML is (probably) the format of choice and thus an SGML based NLP system such as LT NSL will be appropriate. Precursors to the LT NSL software were used to an- notate the MLCC corpora used by the MULTEXT project. Similarly LT NSL has been used to recode the Edinburgh MapTask corpus into SGML markup, a process which showed up a number of inconsis- tencies in the original (non-SGML) markup. Because LT NSL allows the use of multiple I/O files (with dif- ferent DTDs), in (Brew&McKelvie, 1996) it was pos- sible to apply these tools to the task of finding trans- lation equivalencies between English and French. Using part of the MLCC corpus, part-of-speech tagged and sentence aligned using LT NSL tools, they explored various techniques for finding word align- ments. The LT NSL programs were useful in eval- uating these techniques. See also (Mikheev&Finch, 1995), (Mikheev&Finch, 1997) for other uses of the LT NSL tools in annotating linguistic structures of interest and extracting statistics from that markup. 4.2 Transformation of corpus markup Although SGML is human readable, in practice once the amount of markup is of the same order of magni- $^7$This may be a specialised need of academic linguists, and for many applications it is undoubtedly more im- portant to provide clean facilities for non-hierarchical queries but it seems premature to close off the option of such access. tude as the textual content, reading SGML becomes difficult. Similarly, editing such texts using a normal text editor becomes tedious and error prone. Thus if one is committed to the use of SGML for corpus- based NLP, then one needs to have specialised soft- ware to facilitate the viewing and editing of SGML. A similar problem appears in the database approach to corpora, where the difficulty is not in seeing the original text, but in seeing the markup in relation- ship to the text. 4.2.1 Batch transformations To address this issue LT NSL includes a num- ber of text based tools for the conversion of SGML: textonly, sgmltrans and sgrpg. With these tools it is easy to select portions of text which are of inter- est (using the query language) and to convert them into either plain text or another text format, such as LATEX or HTML. In addition, there are a large num- ber of commercial and public domain software pack- ages for transforming SGML. In the future, however, the advent of the DSSSL transformation language will undoubtably revolutionise this area. 4.2.2 Hand correction Specialised editors for SGML are available, but they are not always exactly what one wants, because they are too powerful, in that they let all markup and text be edited. What is required for markup correction are specialised editors which only allow a specific subset of the markup to be edited, and which provide an optimised user interface for this limited set of edit operations. In order to support the writing of specialised ed- itors, we have developed a Python (vanRossum, 1995) API for LT NSL, (Tobin&McKelvie, 1996). This allows us to rapidly prototype editors using the Python/Tk graphics package. These editors can fit into a pipeline of LT NSL tools allowing hand cor- rection or disambiguation of markup automatically added by previous tools. Using this API we are devel- oping a generic SGML editor. It is an object-oriented system where one can flexibly associate display and interaction classes to particular SGML elements. Al- ready, this generic editor has been used for a number of tasks; the hand correction of part-of-speech tags in the MapTask, the correction of turn boundaries in the Innovation corpus (Carletta et al, 1996), and the evaluation of translation equivalences between aligned multilingual corpora. We found that using this generic editor framework made it possible to quickly write new editors for new tasks on new corpora. 5 Conclusions SGML is a good markup language for base level an- notations of published corpora. Our experience with LT NSL has shown that: • It is a good system for sequential corpus pro- cessing where there is locality of reference. • It provides a modular architecture which does not require a central database, thus allowing distributed software development and reuse of components. • It works with existing corpora without extensive pre-processing. • It does support the Tipster approach of sep- arating base texts from additional markup by means of hyperlinks. In fact SGML (HyTime) allows much more flexible addressing, not just character offsets. This is of benefit when work- ing with corpora which may change. LT NSL is not so good for: • Applications which require a database ap- proach, i.e. those which need to access markup at random from a text, for example lexico- graphic browsing or the creation of book in- dexes. • Processing very large plain text or unnormalised SGML corpora, where indexing is required, and generation of normalised files is a large over- head. We are working on extending LT NSL in this direction, e.g. to allow processing of the BNC corpus in its entirety. In conclusion, the SGML and database approaches are optimised for different NLP applications and should be seen as complimentary rather than as con- flicting. There is no reason why one should not at- tempt to use the strengths of both the database and the SGML stream approaches. It is recommended that future work should include attention to allow- ing interfacing between both approaches. 6 Acknowledgements This work was carried out at the Human Commu- nication Research Centre, whose baseline funding comes from the UK Economic and Social Research Council. The LT NSL work began in the context of the LRE project MULTEXT with support from the European Union. It has benefited from discussions with other MULTEXT partners, particularly ISSCO Geneva, and drew on work at our own institution by Steve Finch and Andrei Mikheev. We also wish to thank Hamish Cunningham and Oliver Christ for useful discussions. References A. H. Anderson, M. Bader, E. G. Bard, E. H. Boyle, G. M. Doherty, S. C. Garrod, S. D. Isard, J. C. Kowtko, J. M. McAllister, J. Miller, C. F. Sotillo, H. S. Thompson, and R. Weinert. The HCRC Map Task Corpus. Language and Speech, 34(4):351-366, 1991. C. Brew and D. McKelvie. 1996. \"Word-pair extrac- tion for lexicography\". In Proceedings of NeM- LaP'96, pp 45-55, Ankara, Turkey. G. Burnage and D. Dunlop. 1992. \"Encoding the British National Corpus\". In 13th Interna- tional Conference on English Language research on computerised corpora, Nijmegen. Available at http://www.sil.org/sgml/bnc-encoding2.html See also http://info.ox.ac.uk/bnc/ J. Carletta, H. Fraser-Krauss and S. Garrod. 1996. \"An Empirical Study of Innovation in Manufac- turing Teams: a preliminary report\". In Proceed- ings of the International Workshop on Commu- nication Modelling (LAP-96), ed. J. L. G. Dietz, Springer-Verlag, Electronic Workshops in Com- puting Series. O. Christ. 1994. \"A modular and flexible archi- tecture for an integrated corpus query system\". In Proceedings of COMPLEX '94: 3rd Conference on Computational Lexicography and Text Research (Budapest, July 7-10, 1994), Budapest, Hungary. CMP-LG archive id 9408005 J. Clark. 1996 \"SP: An SGML System Conforming to International Standard ISO 8879 - Standard Generalized Markup Language\". Available from http://www.jclark.com/sp/index.htm. H. Cunningham, Y. Wilks and R. J. Gaizauskas. 1996. \"New Methods, Current Trends and Soft- ware Infrastructure for NLP\". In Proceedings of the Second Conference on New Methods in Lan- guage Processing, pages 283-298, Ankara, Turkey, March. H. Cunningham, R. Gaizauskas and Y. Wilks. 1995. \"A General Architecture for Text Engineering (GATE) - a new approach to Language Engineer- ing R&D\". Technical Report, Dept of Computer Science, University of Sheffield. Available from http://www.dcs.shef.ac.uk/research/groups /nlp/gate/ R. Grishman. 1995. \"TIPSTER Phase II Architecture Design Document Version 1.52\". Technical Report, Dept. of Computer Sci- ence, New York University. Available at http://www.cs.nyu.edu/tipster D. McKelvie, H. Thompson and S. Finch. 1996. \"The Normalised SGML Library LT NSL ver- sion 1.4.6\". Technical Report, Language Technol- ogy Group, University of Edinburgh. Available at http://www.ltg.ed.ac.uk/software/nsl A. Mikheev and S. Finch. 1995. \"Towards a Work- bench for Acquisition of Domain Knowledge from Natural Language\". In Proceedings of the Seventh Conference of the European Chapter of the Asso- ciation for Computational Linguistics (EACL'95). Dublin, Ireland. A. Mikheev and S. Finch. 1997. \"A Workbench for Finding Structure in Texts\". in these proceedings. A. Mikheev and D. McKelvie. 1997. \"Indexing SGML files using LT NSL\". Technical Report, Language Technology Group, University of Edin- burgh. R. Pito. 1994. \"Tgrep Manual Page\". Available from http://www.ldc.upenn.edu/ldc/online/treebank/man/ G. van Rossum. 1995. \"Python Tutorial\". Available from http://www.python.org/ C. M. Sperberg-McQueen & L. Burnard, eds. 1994. \"Guidelines for Electronic Text Encoding and In- terchange\". Text Encoding Initiative, Oxford. R. Tobin and D. McKelvie. 1996. \"The Python Interface to the Normalised SGML Li- brary (PythonNSL)\". Technical Report, Language Technology Group, University of Edinburgh."
  },
  {
    "title": "Mixed-Initiative Development of Language Processing Systems",
    "abstract": "Historically, tailoring language processing systems to specific domains and languages for which they were not originally built has required a great deal of effort. Recent advances in corpus-based manual and automatic training methods have shown promise in reducing the time and cost of this porting process. These developments have focused even greater attention on the bottleneck of acquiring reliable, manually tagged training data. This paper describes a new set of integrated tools, collectively called the Alembic Workbench, that uses a mixed-initiative approach to \"bootstrapping\" the manual tagging process, with the goal of reducing the overhead associated with corpus development. Initial empirical studies using the Alembic Workbench to annotate \"named entities\" demonstrates that this approach can approximately double the production rate. As an added benefit, the combined efforts of machine and user produce domain-specific annotation rules that can be used to annotate similar texts automatically through the Alembic NLP system. The ultimate goal of this project is to enable end users to generate a practical domain-specific information extraction system within a single session.",
    "content": "1. Introduction In the absence of complete and deep text understanding, implementing information extraction systems remains a delicate balance between general theories of language processing and domain-specific heuristics. Recent developments in the area of corpus-based language processing systems indicate that the successful application of any system to a new task depends to a very large extent on the careful and frequent evaluation of the evolving system against training and test corpora. This has focused increased attention on the importance of obtaining reliable training corpora. Unfortunately, acquiring such data has usually been a labor-intensive and time-consuming exercise. The goal of the Alembic Workbench is to dramatically accelerate the process by which language processing systems are tailored to perform new tasks. The philosophy motivating our work is to maximally reuse and re-apply every kernel of knowledge available at each step of the tailoring process. In particular, our approach applies a bootstrapping procedure to the development of the training corpus itself. By re-investing the knowledge available in the earliest training data to pre- tag subsequent un-tagged data, the Alembic Workbench can transform the process of manual tagging to one dominated by manual review. In the limit, if the pre- tagging process performs well enough, it becomes the domain-specific automatic tagging procedure itself, and can be applied to those new documents from which information is to be extracted. As we and others in the information extraction arena have noticed, the quality of text processing heuristics is influenced critically not only by the power of one's linguistic theory, but also by the ability to evaluate those theories quickly and reliably. Therefore, building new information extraction systems requires an integrated environment that supports: (1) the development of a domain-specific annotated corpus; (2) the multi-faceted analysis of that corpus; (3) the ability to quickly generate hypotheses as to how to extract or tag information in that corpus; and (4) the ability to quickly evaluate and analyze the performance of those hypotheses. The Alembic Workbench is our attempt to build such an environment. As the Message Understanding Conferences move into their tenth year, we have seen a growing recognition of the value of balanced evaluations against a common test corpus. What is unique in our approach is to integrate system development with the corpus annotation process itself. The early indications are that at the very least this integration can significantly increase the productivity of the corpus annotator. We believe that the benefits will flow in the other direction as well, and that a concomitant increase in system performance will follow as one applies the same mixed-initiative development environment to the problem of domain- specific tailoring of the language processing system. Alembic Workbench /NFS/ai/systems/awb/data/demo-10-18/muc6-ft-1-5.key.sgml File Tag: Person Options Utilities Help Alembic Workbench Language: English The MITRE Corporation Washington, an Exchange Ally, Seems To Be Strong Candidate to Head SEG Tag: Person Person <Mete-p> Organization <Mete-x> @ By Jeffrey H. Bimbaum and David Wessel Staff Reporters of The Wall Street Journal 01/19/93 WALL STREET JOURNAL (J), PAGE A2 WASHINGTON Consuela Washington, a longtime house staffer and an expert in securities laws, is a leading candidate to be chairwoman of the Securities and Exchange Commission in administration. Date <Meta-d> Time Percent <Control-p> Money <Mela-m> Ms. Washington, 44 years old, would be the first woman and first black to head the five-member commission that oversees securities markets. Utilities Find in current document... Score files Process Text... Learn Alembic Phrase Finding Rules... Perform Error Analysis of Rules... Extract Phrases... Compare Alternate Annotation... Ms. Washington's candidacy is being championed by several powerful lawmakers including her bess, Chainnan John Dingell (D) of the House Energy and Commerce Committee. She was a counsel to the committee. Ms. Washington and Mr. Dingell have considered allies of the securities exchanges, while banks and <HL> <DOC> <MSGSEQ> Figure 1. Screen dump of a typical Alembic Workbench session. 2. Alembic Workbench: A brief description The Alembic Workbench provides a graphical user interface by which texts can be annotated using the mouse and user-defined key bindings. The Workbench mouse interface is engineered specifically to minimize hand motion. This allows text markup to proceed very quickly. Once a text has been marked up, the user's annotations are highlighted in colors specified by the user. A \"mouse line\" at the bottom of the text window provides further visual feedback indicating all of the annotations associated with the location under the mouse cursor, including document structure markup, if available. An example screen image from a typical session with the Workbench is shown above. Our focus in building the Alembic Workbench is to provide a natural but powerful environment for annotating texts in the service of developing natural language processing systems. To this end we have incorporated a growing number of analysis and reporting features. The current set of utilities includes: • A string-matching mechanism that can automatically replicate new markup to identical instances elsewhere in the document. • A rule language for constructing task-specific phrase tagging and/or pre-tagging rule sets. • A tool that generates phrase-based KWIC (\"key- word in context\") reports to help the user identify common patterns in the markup. • A procedure that generates word lists based on their frequency. This tool also measures the degree to which a word occurs in different markup contexts. • A visualization component for viewing inter- annotator (or key/answer) agreement. • A scorer that allows arbitrary SGML markup to be selected for scoring. • A full-featured interface to the multi-stage architecture of the Alembic text processing system. • An interface to Alembic's phrase-rule learning system for generating new application-specific rule sets. • The Alembic Workbench also provides specialized interfaces for supporting more complex, linked markup such as that needed for coreference. Another interface is geared towards capturing arbitrary n-ary relations between tagged elements in a text (these have been called \"Scenario Templates\" in MUC). More details about the implementation of the Workbench are provided in Section 7. The development of the Alembic Workbench environ- ment came about as a result of MITRE's efforts at refining and modifying our natural language processing system, Alembic [1,7], to new tasks: the Message Understanding Conferences (MUC5 and MUC6), and the TIPSTER Multi-lingual Entity Task (MET1). (See [6] for an overview and history of MUC6 and the 'Named Entity Task\".) The Alembic text processing system applies Eric Brill's notion of rule sequences [2,3] at almost every one of its processing stages, from part-of- speech tagging to phrase tagging, and even to some portions of semantic interpretation and inference. While its name indicates its lineage, we do not view the Alembic Workbench as wedded to the Alembic text processing system alone. We intend to provide a well- documented API in the near future for external utilities to be incorporated smoothly into the corpus/system development environment. We envision two classes of external utilities: tagging utilities and analysis utilities. By integrating other tagging modules (including complete NLP systems), we hope those systems can be more efficiently customized when the cycle of analysis, hypothesis generation and testing is tightened into a well-integrated loop. The current version of the tool supports viewing, annotating and analyzing documents in 7-bit, 8-bit and 2-byte character sets. Current support includes the Latin-1 languages, Japanese (JIS), Chinese (GB1232), Russian, Greek and Thai. 3. Increasing manual annotation productivity through pre-tagging A motivating idea in the design of the Alembic Workbench is to apply any available information as early and as often as possible to reduce the burden of manual tagging. In addition to careful interface design and support for user-customization, a core mechanism for enhancing this process is through pre-tagging. The generation of reliably tagged text corpora requires that a human annotator read and certify all of the annotations applied to a document. This is especially true if the annotations are to be used for subsequent manual or automatic training procedures. However, much of the drudgery of this process can be removed if the most obvious and/or oft-repeated expressions can be tagged prior to the annotator's efforts. One way of doing this is to apply tags to any and all strings in a document that match a given string. This is the nature of the \"auto-tagging\" facility built-in to the Workbench interface. For example, in annotating journalistic document collections with 'Named Entity\" tags, one might want to simply pre-tag every occurrence of \"President Clinton\" with Person.. Of course, these actions should be taken with some care, since mis- tagging entities throughout a document might actually lead to an increase in effort required to accurately fix or remove tags in the document. A more powerful approach is to allow patterns, or rules, to form the basis for this pre-tagging. The Alembic phrase-rule interpreter provides the basis for developing rule-based pre-tagging heuristics in the Workbench. In the current version of the Workbench, the user is free to compose these \"phraser\" rules and group them into specialized rule sets. Figure 2 shows an example sequence of rules that could be composed for pre-tagging a corpus with Person tags. The Brill control regime interprets these rules strictly sequentially: rule n is applied wherever in the text it can be; it is then discarded and rule n+1 is consulted. There is no unconstrained forward chaining using a \"soup\" of rules as in a standard production (or rule-based) system. The Alembic \"phraser\" rule interpreter has been applied to tagging named entities, sentence chunks, simple entity relations (\"template element\" in the parlance of MUC6), and other varieties of phrases. (def-phraser-rule :anchor :lexeme :conditions (:left-1 :lex (\"Mr.” “Ms.\" \"Dr.\" ...)) :actions (:create-phrase :person)) (def-phraser-rule :conditions (:phrase :phrase-label :person) :actions (:right-1 :pos :NNP) (:expand :right-1)) Figure 2. An example Alembic rule sequence that (1) produces Person phrases around any word immediately to the right of a title and/or honorific, and then (2) grows the extent of the phrase to the right one lexeme, if that word is a proper noun. 4. Mixed-initiative text annotation In addition to allowing users to define pre-tagging rules, we have developed a learning procedure that can be used to induce these rules from small training corpora. Operationally, an annotator starts by generating a small initial corpus and then invokes the learner to derive a set of pre-tagging rules. These rules can then be applied to new, unseen texts to pre-tag them. Figure 3 illustrates this bootstrapping cycle. ¹ The Named Entity task from MUC6 consists of adding tags to indicate expressions of type Person, Location, Organization. Date. Time and Money, see [6] The earlier we can extract heuristic rules on the basis of manually tagged data, the earlier the user can be relieved from some portion of the chore of physically marking up the text—the user will need to edit and/or add only a fraction of the total phrases in a given document. In our experience of applying the Alembic phrase rule learner to named-entity and similar problems, our error- reduction learning method requires only modest amounts of training data. (We present performance details in Section 6.) Unprocessed material Alembic Workbench Domain-specific Tagging Rules Machine Learning Domain 1 User Training/Testing corpora Evaluation & Analysis Domain 2 Figure 3. The Alembic Workbench seeks to involve the user in a corpus development cycle, making use of pre-tagging facilities, analysis facilities, and the automatic generation of pre-tagging rule sets through machine learning. As the human annotator continues generating reliable training data, she may, at convenient intervals, re- invoke the learning process. As the amount of training data increases, the performance of the learned rules tends to increase, and so the amount of labor saved in pre- tagging subsequent training data is further increased. The bootstrapping effect tends to increase over time. For the \"named entity\" task in MUC6 approximately 25,000 words were provided as annotated training data by the conference organizers (\"formal training\" and \"dryrun\" data sets). Prior to developing the Alembic Workbench, we were able to use this amount of data in Alembic to generate a system performing at 85.2 P&R on unseen test data. Based on the tagging rates we have measured thus far using the Workbench, it would take somewhere between 1.5 to 2.5 hours to tag these 25,000 words of data. There is a limit on how much one can reduce the time- requirements for generating reliable training data—this is the rate required by a human domain expert to carefully read and edit a perfectly pre-annotated training corpus. Training data cannot be generated without this $^2$P&R (or F-measure) is a weighted combination of recall and precision. $^3$human investment. Indeed, in situations where the quality of the data is particularly important (as it is in, say a multi-system evaluation such as MUC), it is typical that multiple reviews of the same corpus is performed by various annotators, especially given the known ambiguity of any annotation task definition. 5. Manual refinement of automatically derived pre-tagging heuristics In the previous section we presented our approach to mixed-initiative corpus development and tagging heuristics without assuming any sophistication on the part of the human user beyond a clear understanding of the information extraction task being addressed. Usually, however, even a lay end-user is likely to have a number of intuitions about how the un-annotated data could be pre-tagged to reduce the burden of manual tagging. Hand-coded rules can be applied in concert with the machine-derived rules mentioned earlier. One way this can be done is by invoking the rule learning subsequent to the application of the hand-coded pre- tagging rules. On the other hand, if the user notices a consistent mistake being made by the machine-learned rules early in the bootstrapping process, the user can augment the machine-derived rule sequence with manually composed rules. In fact, every rule composed by the learning procedure is completely inspectable by the user, and so some users may want to modify individual machine-derived rules, perhaps to expand their generality beyond the particular data available in the emerging corpus. This is another way, then, that the Alembic Workbench environment enables and encourages the mixed, or cooperative, application of human and machine skills to the combined task of developing a domain-specific corpus and set of extraction heuristics. Of course, composing rules is somewhat akin to programming, and not all users will be inclined, or well-equipped, to become involved in this process. One impediment to end-users composing their own rules is the particular syntax of Alembic's phraser rules, so we anticipate exploring other, simpler rule languages that will encourage end-user participation. Another approach that we are interested in exploring involves supporting more indirect feedback or directives from the user that are rooted more closely to examples in the data. $^3$This is not to say that high-quality machine-tagged data cannot be generated faster than this, and that these data may indeed be helpful in the learning procedure of some other systems. But all such data will remain suspect as far as being considered part of an annotated training corpus until inspected by a human, given the vagaries of genre and style that can easily foil the most sophisticated systems. Similarities and differences between manual and automatic rule formation The automatic rule-learning procedure uses a generate- and-test approach to learn a sequence of rules. A set of rule schemata, defining a set of possible rule instances determines the rule space that the learning procedure explores. The learner uses indexing based on the actual data present in the corpus to help it explore the rule space efficiently. The learning process is initiated by deriving and applying an initial labeling function based on the differences between an un-annotated version and a correctly annotated version of the corpus. Then, during each learning cycle, the learner tries out applicable rule instances and selects the rule that most improves the score when applied to the corpus. The score is determined by evaluating the corpus as currently annotated against the correctly annotated version, using some evaluation function (generally precision, recall or F-measure). The corpus annotation is updated by applying the chosen rule, and the learning cycle repeats. This cycle is continued until a stopping criterion is reached, which is usually defined as the point where performance improvement falls below a threshold, or ceases. Other alternatives include setting a strict limit on the number of rules, and testing the performance improvement of a rule on a corpus distinct from the training set. Of course, there are two important advantages that a human expert might have over the machine algorithm: linguistic intuition and world knowledge. Rules that include references to a single lexeme can be expanded to more general applicability by the human expert who is able to predict alternatives that lie outside the current corpus available to the machine. By supporting multiple ways in which rules can be hypothesized, refined and tested, the strengths of both sources of knowledge can be brought to bear. 6. Experimental Results We are still in the early stages of evaluating the performance of the Alembic Workbench along a number of different dimensions. However, the results from early experiments are encouraging. Figure 4 compares the productivity rates using different corpus development utilities. These are indicated by the four categories on the X-axis: (1) using SGML-mode in emacs (by an expert user); (2) using the Workbench interface and \"auto-tag\" string-matching utility only; (3) using the Workbench following the application of learned tagging rules derived from 5 short documents-approximately 1,500 words, and (4) using the Workbench following the application of learned tagging rules again, but this time with the learned rules having trained on 100 documents (approximately 48,000 words), instead of only five documents. As can be seen in these experiments, there is a clear increase in the productivity as a function of both the user interface (second column) and the application of pre-tagging rules (third and fourth columns). The large step in performance between columns three and four indicate that repeated invocation of the learning process during the intermediate stages of the corpus development cycle will likely result in acceleration of the annotation rate. (As it happens, these results are probably underestimating the pre-tagging productivity. The reason for this is that the version of the Workbench used was not yet able to incorporate date and time annotations generated by a separate pre-processing step; this date and time tagger performs at an extremely high level of precision for this genre in the high nineties P&R.) These initial experiments involved a single expert annotator on a single tagging task (MUC6 named entity). The annotator was very familiar with the tagging task. Words/Minute Annotated 280 260 240 220 200 180 160 140 120 100 Emacs Productivity Gains AWB No learning ரு Corpus Development Tools Used ■Words/Minute ■Tags/Minute measures 25 -20 -15 +10 +5 Tags/Minute Figure 4. Two of corpus annotation productivity using the Alembic Workbench. The X-axis indicates what kind of corpus-development utilities were used: (1) SGML-mode of emacs text editor; (2) Workbench (AWB) manual interface only, (3) AWB rule-learning bootstrap method with 5-document training set; (4) AWB rule-learning bootstrap method with 100-document training set. See discussion in text. To place this in the perspective of the human annotator, after only about 15 minutes of named entity tagging, having annotated some 1,500 words of text with approximately 150 phrases, the phrase rule learner can derive heuristic rules that produce a pre-tagging performance rate (P&R) of between 50 and 60 percent. Of course, this performance is far short of what is needed for a practical extraction system, but it already constitutes a major source for labor savings, since 50 to 60 percent of the annotations that need to be moused (or clicked) in are already there. Since the precision at this early stage is only around 60 percent, there will be extra phrases that need (1) to be removed, (2) their assigned category changed (from, say, organization to person), or (3) their boundaries adjusted. It turns out that for the first two of these kinds of precision errors, the manual corrections are extremely quick to perform. (Boundaries are not really difficult to modify, but the time required is approximately the same as inserting a tag from scratch.) In addition, making these corrections removes both a precision and a recall error at the same time. Therefore, it turns out that even at this very early stage, the modest pre-tagging performance gained from applying the learning procedure provides measurable performance improvement. In order to obtain more detailed results on the effect of pre-tagging corpora, we conducted another experiment in which we made direct use of the iterative automatic generation of rules from a growing manually-tagged corpus. Using the same skilled annotator, we introduced a completely new corpus for which named- entity tagging happened to be needed within our company. We randomly divided approximately 50 documents of varying sizes into five groups. The word counts for these five groups were: Group1: 19,300; Group2: 13,800; Group3: 6,300; Group4: 15,800; Group5: 8,000; for a total of 63,000 words. After manually tagging the first group, we invoked the rule learning procedure. Applying the learning procedure on each training set required two to three hours of elapsed time on a Sun Sparc Ultra. The new tagging rules were then applied to the next ten documents prior to being manually tagged/edited. This enlarged corpus was then used to derive a new rule set to be applied to the next group of documents, and so on. A summarization of the results are presented in Figure 5. Productivity by Group 30 Clearly, more experiments are called for-we plan to conduct these across different annotators, task types, and languages, to better evaluate productivity, quality and other aspects of the annotation process. It is extremely difficult to control many of the features that influence the annotation process, such as the intrinsic complexity of the topic in a particular document, the variation in tag-density (tags per word) that may occur, the user's own training effect as the structure and content of documents become more familiar, office distractions, etc. In order to gain a better understanding of the underlying tagging performance of the rule learner, and so separate out some of these human factors issues, we ran an automated experiment in which different random subsets of sentences were used to train rule sets, which were then evaluated on a static test corpus. The results shown in Figure 6 give some indication of the ability of the rule-sequence learning procedure to glean useful generalizations from meager amounts of training data. Performance of Learned Rule Set as a Function of Training Set Size Performance on Test Set 80 70 60 50 40 30 20 10 Training Set Size (Named Entities) 1008 F-measure --Recall Precision Figure 6. Performance of learned rules on independent test set of 662 sentences. Tag/Minute 25 20 15 10 5 0 1 2 3 4 5 Group -Tag/Minute Performance on Test Set 80 70 60 50 40 30 20 10 0 67.67 85.33 Average Performance of Learned Rules as a Function of Training Set Size Figure 5. Tagging productivity gains with the incremental application of automatically acquired rule sets. The first observation we make is that there is a clear and obvious direction of improvement-by the time 30 documents have been tagged, the annotation rate on Group 4 has increased considerably. It is important to note, however, that there is still noise in the curve. In addition, the granularity is perhaps still too coarse to measure the incremental influences of pre-tagging rules. Training Entities Figure 7. Averaged F-measure performance figures. One clear effect of increasing training set size is a reduction in the sensitivity of the learning procedure to particular training sets. We hypothesize that this effect is partly indicative of the generalization behavior on which the learning procedure is based, which amplifies the effects of choosing more or less representative training sentences by chance. Since the learning process is not merely memorizing phrases, but generating contextual rules to try to predict phrase types and extents, the rules are very sensitive to extremely small selections of training sentences. Figure 7 shows the F-measure performance smoothed by averaging neighboring data points, to get a clearer picture of the general tendency. We should note that the Alembic Workbench, having been developed only recently in our laboratory, was not available to us in the course of our effort to apply the Alembic system to the MUC6 and MET tasks. Therefore we have not been able to measure its influence in preparing for a particular new text processing task. We intend to use the system to prepare for future evaluations (including MUC7 and MET2) and to carefully evaluate the Alembic Workbench as an environment for the mixed-initiative development of information extraction systems in multiple languages. 7. Implementation The Alembic Workbench interface has been written in Tcl/Tk. Some of the analysis and reporting utilities (available from within the interface as well as Unix command-line utilities) are written in Perl, C or Lisp. The separate Alembic NLP system consists of C pre- processing taggers (for dates, word and sentence tokenization and part-of-speech assignments) and a Lisp image that incorporates the rest of Alembic: the phrase- rule interpreter, the phrase rule learner, and a number of discourse-level inference mechanisms described in [8]. This code currently runs on Sun workstations running Sun OS 4.1.3 and Solaris 2.4 (Sun OS 5.4) and greater; we have begun porting the system to Windows NT/Windows 95. We anticipate providing an API for integrating other NLP systems in the near future. The Workbench reads and saves its work in the form of SGML-encoded files, though the original document need not contain any SGML mark-up at all. These files are parsed with the help of an SGML normalizer. During the course of the annotation process the Workbench uses a \"Parallel Tag File\" (PTF) format, which separates out the embedded annotations from the source text, and organizes user-defined sets of annotations within distinct \"tag files.\" While these files are generally hidden from the user, they provide a basis for the combination and separation of document annotations (\"tagsets\") without needing to modify or otherwise disturb the base document. This allows the user to view. 4 In cases where documents use some of the more complex aspects of SGML, the user supplies a Document Type Description (DTD) file for use in normalization. For simple SGML documents, or documents with no original SGML only Named Entity tags, or only tokenization tags, or any desired subset of tagsets. Thus, the Workbench is written to be TIPSTER-compliant, though it is not itself a document manager as envisioned by that architecture (see [5]). We anticipate integrating the Workbench with other TIPSTER compliant modules and document managers via the exchange of SGML- formatted documents. The Parallel Tag File (PTF) format used by the Workbench provides another means by which a translator could be written. 8. Future Work Broadly defined, there are two distinct types of users who we imagine will find the Workbench useful: NLP researchers and information extraction system end-users. While our dominant focus so far has been on supporting the language research community, it is important to remember that new domains for language processing generally, and information extraction in particular, will have their own domain experts, and we want the text annotation aspects of the tool to be quite usable by a wide population. In this vein we would like to enable virtually any user to be able to compose new patterns (rules) for performing pre-tagging on the data. While the current rule language has a simple syntax, as well as an extremely simple control regimen, we do not imagine all users will want to engage directly in an exploration for pre-tagging rules. A goal for our future research is to explore new methods for incorporating end-user feedback to the learning procedure. This feedback might include modifying a very simplified form of a single rule for greater generality by integrating thesauri to construct word-list suggestions. We also would like to give users immediate feedback as to how a single rule applies (correctly and incorrectly) to many different phrases in the corpus. In this paper we have concentrated on the named entity task as a generic case of corpus annotation. Of course, there are many different ways in which corpora are being annotated for many different tasks. Some of the specific extensions to the user interface that we have already begun building include part-of-speech tagging (and \"dense\" markup more generally), and full parse syntactic tagging (where we believe reliable training data can be obtained much more quickly than heretofore). In these and other instances the tagging process can be accelerated by applying partial knowledge early on, transforming the task once again into that of editing and correcting. Most of these tagging tasks would be improved by making use of methods that preferentially select ambiguous data for manual annotation-for example, as described in [4]. There are a number of psychological and human factors issues that arise when one considers how the pre- annotated data in a mixed-initiative system may affect the human editing or post-processing. If the pre- tagging process has a relatively high recall, then we hypothesize that the human will tend increasingly to trust the pre-annotations, and thereby forget to read the texts carefully to discover any phrases that escaped being annotated. A similar effect seems possible for relatively high precision systems, though proper interface design (to highlight the type assigned to a particular phrase) should be able to mitigate these tendencies. A more subtle interaction is \"theory creep,\" where the heuristics induced by the machine learning component begin to be adopted by the human annotator, due, in many cases, to the intrinsic ambiguity of defining annotation tasks in the first place. In all of these cases the most reliable method for detecting these human/machine interactions is probably to use some representative sub-population of the corpus documents to measure and analyze the inter-annotator agreement between human annotators who have and who have not been exposed to the machine derived heuristics for assigning annotations. 9. Conclusions On the basis of observing our own and others' experiences in building and porting natural language systems for new domains, we have come to appreciate the pivotal role played in continuous evaluation throughout the system development cycle. But evaluation rests on an oracle, and for text processing, that oracle is the training and test corpora for a particular task. This has led us to develop a tailoring environment which focuses all of the available knowledge on accelerating the corpus development process. The very same learning procedure that is used to bootstrap the manual tagging process leads eventually to the derivation of tagging heuristics that can be applied in the operational setting to unseen documents. Rules derived manually, automatically, and through a combination of efforts have been applied successfully in a variety of languages, including English, Spanish, Portuguese, Japanese and Chinese. The tailoring environment, known as the Alembic Workbench, has been built and used within our organization, and we are making it available to other organizations involved in the development of language processing systems and/or annotated corpora. Initial experiments indicate an significant improvement in the rate at which annotated corpora can be generated using the Alembic Workbench methodology. Earlier work has shown that with the training data obtained in the course of only a couple of hours of text annotation, an information extraction system can be induced purely automatically that achieves a very competitive level of performance. References [1] John Aberdeen, John Burger, David Day, Lynette Hirschman, David Palmer, Patricia Robinson, and Marc Vilain. 1996. The Alembic system as used in MET. In Proceedings of the TIPSTER 24 Month Workshop, May. [2] Eric Brill. 1992. A simple rule-based part of speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, Trento. [3] Eric Brill. 1993. A Corpus-Based Approach to Language Learning. Ph.D. thesis, University of Pennsylvania, Philadelphia, Penn. [4] Sean P. Engelson and Ido Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. Computation and Linguistic E-Print Service (cmp-lg/9606030), June. [5] Ralph Grishman. 1995. TIPSTER phase II architecture design. World Wide Web document. URL=http://cs.nyu.edu/cs/faculty/grishman/tipster.html [6] Ralph Grishman and Beth Sundheim. 1996. Message Understanding Conference-6: A Brief History. In International Conference on Computational Linguistics, Copenhagen, Denmark, August. The International Committee on Computational Linguistics. [7] Marc Vilain and David Day. 1996. Finite-state parsing by rule sequences. In International Conference on Computational Linguistics, Copenhagen, Denmark, August. The International Committee on Computational Linguistics. [8] Marc Vilain. 1993. Validation of terminological inference in an information extraction task. In Proceedings of the ARPA Workshop on Human Language Technology, Plainsboro, New Jersey."
  },
  {
    "title": "Computational Techniques for Improved Name Search",
    "abstract": "This paper describes enhancements made to techniques currently used to search large databases of proper names. Improvements included use of a Hidden Markov Model (HMM) statistical classifier to identify the likely linguistic provenance of a surname, and application of language-specific rules to generate plausible spelling variations of names. These two components were incorporated into a prototype front-end system driving existing name search procedures. HMM models and sets of linguistic rules were constructed for Farsi, Spanish and Vietnamese surnames and tested on a database of over 11,000 entries. Preliminary evaluation indicates improved retrieval of 20-30% as measured by number of correct items retrieved.",
    "content": "1.0 INTRODUCTION This paper describes enhancements made to current name search techniques used to access large databases of proper names. The work focused on improving name search algorithms to yield better matching and retrieval performance on data-bases containing large numbers of non-European 'foreign' names. Because the linguistic mix of names in large computer-supported databases has changed due to recent immigration and other demographic factors, current name search procedures do not provide the accurate retrieval required by insurance companies, state motor vehicle bureaus, law enforcement agencies and other institutions. As the potential consequences of incorrect retrieval are so severe (e.g., loss of benefits, false arrest), it is necessary that name name search techniques be improved to handle the linguistic variability reflected in current databases. Our specific approach decomposed the name search problem into two main components: • Language classification techniques to identify the source language for a given query name, and • Name association techniques, once a source language for a name is known, to exploit language-specific rules to generate variants of a name due to spelling variation, bad transcrip- tions, nicknames, and other name conventions. A statistical classification technique based on the use of Hidden Markov Models (HMM) was used as a language discriminator. The test database contained about 11,000 names, including about 2,000 each from three target languages, Vietnamese, Farsi and Spanish, and 5,000 termed 'other' to broadly represent general European names. The decision procedures assumed a closed-world situation in which a name must be assigned to one of the four classes. Language-specific rules in the form of context-sensitive, string rewrite rules were used to generate name variants. These were based on linguistic analysis of naming conventions, pronunciations and common misspellings for each target language. These two components were incorporated into a front-end system driving existing name search procedures. The front-end system was implemented in the C language and runs on a VAX-11/780 and Sun 3 workstations under Unix 4.2. Preliminary tests indicate improved retrieval (number of correct items retrieved) by as much as 20-30% over standard SOUNDEX and NYSIIS (Taft 1970) techniques. 2.0 CURRENT NAME SEARCH PROCEDURES In current name search procedures, a search request is reduced to a canonical form which is then matched against a database of names also reduced to their canonical equivalents. All names having the same canonical form as the query name will be retrieved. The intent is that similar names (e.g., Cole, Kohl, Koll) will have identical canonical forms and dissimilar names (e.g., Cole, Smith, Jones) will have different canonical forms. Retrieval should then be insensitive to simple transformations such as spelling variants. Techniques of this type have been reviewed by Moore et al. (1977). However, because of spelling variation in proper names, the canonical reduction algorithm may not always have the desired characteristics. Sometimes similar names are mapped to different canonical forms and dissimilar names mapped to the same forms. This is especially true when 'foreign' or non-European names are included in the database, because the canonical reduction techniques such as SOUNDEX and NYSIIS are very language-specific and based largely on Western European names. For example, one of the SOUNDEX reduction rules assumes that the characteristic shape of a name is embodied in its consonants and therefore the rule deletes most of the vowels. Although reasonable for English and certain other languages, this rule is less applicable to Chinese surnames which may be distinguished only by vowel (e.g., Li, Lee, Lu). In large databases with diverse sources of names, other name conventions may also need to be handled, such as the use of both matronymic and patronymic in Spanish (e.g., Maria Hernandez Garcia) or the inverted order of Chinese names (e.g., Li-Fang-Kuei, where Li is the surname). 3.0 LANGUAGE CLASSIFICATION As mentioned in section 1.0, the approach taken to improve existing name search techniques was to first classify the query name as to language source and then use language-specific rewrite rules to generate plausible name variants. A statistical classifier based on Hidden Markov Models (HMM) was developed for several reasons. Similar models have been used successfully in language identification based on phonetic strings (House and Neuburg 1977, Li and Edwards 1980) and text strings (Ferguson 1980). Also, HMMs have a relatively simple structure that make them tractable, both analytically and computationally, and effective procedures already exist for deriving HMMs from a purely statistical analysis of representative text. HMMs are useful in language classification because they provide a means of assigning a probability distribution to words or names in a specific language. In particular, given an HMM, the probability that a given word would be generated by that model can be computed. Therefore, the decision procedure used in this project is to compute that probability for a given name against each of the language models, and to select as the source language that language whose model is most likely to generate the name. 3.1 EXAMPLE OF HMM MODELING TEXT The following example illustrates how HMMs can be used to capture important information about language data. Table 1 contains training data representing sample text strings in a language corpus. Three different HMMs of two, four and six states, were built from these data and are shown in Tables 2- 4, respectively. (The symbol CR in the tables corresponds to the blank space between words and is used as a word delimiter.) These HMMs can also be represented graphically, as shown in Figures 1-3. The numbered circles correspond to states; the arrows represent state transitions with non-zero probability and are labeled with the transition probability. The boxes contain the probability distribution of the output symbols produced when the model is in the state to which the box is connected. The process of generating the output sequence of a model can then be seen as a random traversal of the graph according to the probability weights on the arrows, with an output symbol generated randomly each time a state is visited, according to the output distribution associated with that state. For example, in the two-state model shown in Table 2 (and graphically in Figure 1), letter (non- delimiter) symbols can be produced only in state two, and the output probability distribution for this state is simply the relative frequency with which each letter appears in the training data. That is, in the training data in Table 1 there are 15 letter symbols: [ERROR: Failed to process this page - Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 4. Meaning that the model was reciting from copyrighted material.] Table 4. Six State HMM Based on Sample Data Hidden Markov Model Parameters Six State, State Output Model Output Probabilities State Symbol 1 2 3 4 5 6 CR 1 0 0 0 0 0 a 0 0 1 0 0 0 b 0 0 0 1 0 0 c 0 0 0 0 1 0 d 0 1 0 0 0 0 e 0 0 0 0 0 1 State Transition Probabilities To From 1 2 3 4 5 6 1 0 0 1 0 0 0 2 0.5 0 0 0 0 0.5 3 0.2 0 0 0 0.8 0 4 0.333 0.667 0 0 0 0 5 0.25 0 0 0.75 0 0 6 1 0 0 0 0 0 a 1 .8 3 5 .75 CR 1 1 .25 .2 .333 1 1 e 1 4. .667 .5 6 2 .5 c 1 d 1 Figure 2 Graphic Representation of Six State HMM for Sample Data Table 5. Output from Two, Four and Six State HMM for Sample Data Outputs from Hidden Markov Models Two States Four States Six States aadcc ab abcde be ab abc abcacaa abcc abcd dcacc abd abcde aaedb abd a c ab abcde caea abc abc c ab abcd cbc ab ab ec ab abc b abc ab cbbcbcaebd abcd abc a a a ca ab abcd b abc abcd cb abccdcc abc cde abcc ab bccbabebd abc abc bc ab abcd dd ab abcd dca abe abcde ad abed a e ab abcde c abe abcd ba ab ab baea abc ab b abe ab ba a abcde cabbd ab a b ab ab ac abe ab --- five \"a\", four \"b\", three \"c\", etc., and the model assigns a probability of 5/15 = 0.333 to \"a\", 4/15 = 0.267 to \"b\", and so on. Similarly, the state transition probabilities for state two reflect the relative frequency with which letters follow letters and word delimiters follow letters. These parameters are derived strictly from an iterative automatic procedure and do not reflect human analysis of the data. In the four state model shown in Table 3 (and Figure 2), it is possible to model the training data with more detail, and the iterations converge to a model with the two most frequently occuring symbols, \"a\" and \"b\", assigned to unique states (states two and four, respectively) and the remaining letters aggregated in state three. State one contains the word delimiter and transitions from state one occur only to state two, reflecting the fact that \"a\" is always word-initial in the training data. In the six state model shown in Table 4 (and Figure 3), the training data is modeled exactly. Each state corresponds to exactly one output symbol (a letter or word delimiter). For each state, transitions occur only to the state corresponding to the next allowable letter or to the word delimiter. The outputs generated by these three models are shown in Table 5. The six state model can be used to model the training data exactly, and in general, the faithfulness with which the training data are represented increases with the number of states. 3.2 HMM MODEL OF SPANISH NAMES The simple example in the preceding section illustrates the connection between model parameters and training data. It is more difficult to interpret models derived from more complex data such as natural language text, but it is possible to provide intuitive interpretations to the states in such models. Table 6 shows an eight state HMM derived from Spanish surnames. State transition probabilities are shown at the bottom of the table, and it can be seen that the transition probability from state eight to state one (word delimiter) is greater than 95. That is, state eight can be considered to represent a \"word final\" state. The top part of the table shows that the highest output probabilities for state eight are assigned to the letters \"a,o,s,z\", correctly reflecting the fact that these letters commonly occur word final in Spanish Garcia, Murillo, Fuentes, Diaz. This HMM also \"discovers\" linguistic categories, such as the class of non-word-final vowels represented by state seven with the highest output probabilities assigned to the vowels \"a,e,i,o,u\". 3.3 LANGUAGE CLASSIFICATION In order to use HMMs for language classification, it was first necessary to construct a model for each language category based on a representative sample. A maximum likelihood (ML) estimation technique was used because it leads to a relatively simple method for iteratively generating a sequence of successively better models for a given set of words. HMMs of four, six and eight states were generated for each of the language categories, and an eight state HMM was selected for the final configuration of the classifier. Higher dimensional models were not evaluated because the eight state model performed well enough for the application. With combined training and test data, language classification accuracy was 98% for Vietnamese, 96% for Farsi, 91% for Spanish, and 88% for Other. With training data separate from test data, language classification accuracy was 96% for Vietnamese, 90% for Farsi, 89% for Spanish, and 87% for Other. The language classification results are shown in Tables 7 and 8. 4.0 LINGUISTIC RULE COMPONENT For each of the three language groups, Vietnamese, Farsi and Spanish, a set of linguistic rules could be applied using a general rule interpreter. The rules were developed after studying naming conventions and common transcription variations and also after performing protocol analyses to see how native English speakers (mis)spelled names pronounced by native Vietnamese (and Farsi and Spanish) speakers and (mis)pronounced by other English speakers. Naming conventions included word order (e.g., surnames coming first, or parents' surnames both used); common transcription variations included Romanization issues (e.g., Farsi character that is written as either 'v' or 'w'). The general form of the rules is lhs --> rhs / leftContext_rightContext where the left-hand-side (lhs) is a character string and the right-hand-side is a string with a possible Table 6. Eight State HMM for Spanish Hidden Markov Model Parameters Eight State, State Output Model for Spanish Output Probabilities State Symbol 1 2 3 4 5 6 7 8 CR 1 0 0 0 0 0 0 0 - 0 0.0479 0.00427 0 0.0042 0.0753 0.324 0.219 a 0 0.00208 0.0133 0 0.00158 0.0427 0 0 b 0 0.0193 0 0.127 0.00222 0.0864 0 0 c 0 0.0755 0.0207 0.0601 0.229 0.0408 0 0 d 0 0.567 0.032 0.00169 0.00477 0.00368 0.196 0.0268 e 0 0 0 0.00875 0 0.0612 0 0 f 0 0.0207 0 0.174 0 0.052 0 0.00161 g 0 0 0 0 0 0.0825 0.0109 0 h 0 0.00432 0.0495 0 0.013 0.00193 0.164 0.00442 i 0 0.0104 0 0.0233 0 0.00295 0 0 j 0 0.00252 0 0 0 0.00123 0 0 k 0 0.0048 0.189 0.066 0.0626 0.0565 0.00559 0.0118 l 0 0.00484 0 0.118 0.00448 0.0917 0 0 m 0 0.0743 0.262 0.0697 0.0593 0 0 0.0252 n 0 0.00784 0.00968 0 0 0.0122 0.186 0.189 o 0 0.0121 0.00825 0.0132 0.0138 0.122 0 0 p 0 0 0 0.0149 0.0199 0.00551 0 0 q 0 0.0528 0.346 0.0794 0.273 0.141 0.0129 0.00279 r 0 0.0393 0.0442 0.00992 0.00899 0.0872 0 0.123 s 0 0.0339 0 0.0726 0.155 0.00288 0 0.0131 t 0 0.00162 0.00476 0 0 0 0.1 0.00671 u 0 0.015 0 0.0884 0 0.0177 0 0 v 0 0 0 0.00103 0 0.00213 0 0 w 0 0 0 0 0 0 0 0.00183 x 0 0.00198 0.013 0.0031 0.00465 0.00149 0 0.00534 y 0 0.00175 0.00287 0 0.14 0.00727 0 0.368 z State Transition Probabilities To From 1 2 3 4 5 6 7 8 1 0 0 0 0.339 0.00323 0.602 0.0548 0 2 0.00968 0.075 0.00561 0 0.0869 0.00212 0.00665 0.814 3 0.0615 0.269 0.0353 0.259 0.235 0.0097 0.0253 0.104 4 0 0.0101 0.0132 0 0.00503 0.0245 0.929 0.0182 5 0.0117 0.228 0.00477 0.00466 0.0537 0.00145 0.542 0.154 6 0 0 0.0587 0.0341 0 -0.0564 0.85 0 7 0.0165 0.13 0.506 0.162 0.0627 0.00977 0.0207 0.0915 8 0.954 0 0.00169 0 0.00723 0.00216 0.00858 0.0256 Table 7. Language Classification Performance for Training Data Language Classification Accuracy Statistics for Training Data Eight State, State Transition Output Model Percent Classified as: Language Error Rate Farsi Spanish Vietnamese Other Farsi 95.5 1.4 0.1 3.0 4.5 Spanish 2.0 91.1 0.1 6.9 8.9 Vietnamese 0.3 1.0 97.8 0.9 2.2 Other 5.4 5.8 0.4 88.4 11.6 Table 8. Language Classification Performance for Test Data Language Classification Accuracy Statistics for Non-Training Data Eight State, State Transition Output Model Percent Classified as: Language Error Rate Farsi Spanish Vietnamese Other Farsi 90.1 2.7 1.0 7.1 9.9 Spanish 2.6 88.8 0.1 8.5 11.9 Vietnamese 0.6 1.6 96.0 1.8 4.0 Other 6.3 6.1 0.4 87.3 12.4 Table 9. Examples of Linguistic Rules Rule English Paraphrase Examples input output N/A PH -> F PH goes to F (everywhere) Phred Stephen Cathy Fred Stefen Kathy ... Colin C -> K/A C goes to K when it precedes A. J->JIHIG/#_ J goes to J, H or G Jimenez Jimenez, Himenez, Borjas when it is word initial. Gimenez Y-> YII/. Y goes to Y or I Bryan Bryan, Brian Yonkers when it is not word initial. Sherry Sherry, Sherri F-> FIV/. F goes to F or V Filip Filip, Vilip Josef when it is not word final. Stefan Stefan, Stevan C -> CIS/_[EI] C goes to C or S when Cespedes Cespedes, Sespedes Carrillo it precedes E or I. Garcia Garcia, Garsia H->H | J/[^CS]_[AEIOU] H goes to H or J when it follows a letter other than C or S, and precedes A,E,I,O, or U. Truhillo Truhillo Trujillo Chacon Sherri T->TID/#_[^R] T goes to T or D when it is word initial and precedes a letter other than R. Tao Tuyet Tao, Dao Tuyet, Duyet Tran Kiet IE -> IELITY/# IE goes to IE, I Vinnie Vinnie, Vinni, Pierson or Y when it is word final Vinny Mier O->OIEIU/S_N# O goes to O, E, Anderson Anderson, Andersen, Andersons or U when it follows S Andersun Anderzon and precedes final N weight, so that the rules could be associated with a plausibility factor. Rules may include a specific context; if a specific environment is not described, the rule applies in all cases. Table 9 shows sample rules and examples of output strings generated by applying the rules. The 'N/A' column gives examples of name strings for which a rule does not apply because the specified context is absent. An example with plausibility weights is also shown. 5.0 PERFORMANCE Although the statistical model building is computationally intensive and time-consuming (several hours), the actual classification procedure is very efficient. The average cpu time to classify a query name was under 200 msec on a VAX-11/780. The rule component that generates spelling variants can process 100 query names in about 2-6 cpu seconds, the difference in time depending on average length of name. As for retrieval performance, in a test of 160 query names (including names known to be in the database and spelling variants not known to be in the database), there were 111 hits (69%) using NYSIIS procedures alone and 141 hits (88%) using the front- end language classifier and linguistic rules and sending the expanded query set to NYSIIS. In recent work, this technique has been extended to include modeling a database of Slavic surnames. Language classification accuracy based on a combined database of 13000 surnames representing Spanish, Farsi, Vietnamese, Slavic and 'other' names, with combined training data (1000 names from each language group to build each language model) and test data (remaining 8000 names), is 96.8% for Vietnamese, 87.7% for Farsi, 86.9% for Spanish, 86.5% for Slavic, and 82.9% for 'other'. 6.0 REFERENCES Ferguson, John D., Ed. 1980 Symposium on the Application of Hidden Markov Models to Text and Speech, Institute for Defense Analyses, Communications Research Division, Princeton, New Jersey. House, Arthur H. and Neuburg, Edward P. 1977 Toward Automatic Identification of the Language of an Utterance, Journal of the Acoustical Society of America, 62 (3):708-713. Li, K. P. and Edwards, Thomas J. 1980 Statistical Models for Automatic Language Identification, Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, Denver, Colorado, 884-887. Moore, Gwendolyn B.; Kuhns, John L.; Trefftzs, Jeffrey L.; and Montgomery, Christine A. 1977 Accessing Individual Records from Personal Data Files Using Non-Unique Identifiers. Computer Science and Technology, National Bureau of Standards Special Publication 500-2, Washington, D.C. Taft, Robert L. 1970 Name Search Techniques. New York State Identification and Intelligence System, Special Report No. 1, Albany, New York."
  },
  {
    "title": "A Finite State and Data-Oriented Method for Grapheme to Phoneme Conversion",
    "abstract": "A finite-state method, based on leftmost longest-match replacement, is presented for segmenting words into graphemes, and for converting graphemes into phonemes. A small set of hand-crafted conversion rules for Dutch achieves a phoneme accuracy of over 93%. The accuracy of the system is further improved by using transformation-based learning. The phoneme accuracy of the best system (using a large rule and a 'lazy' variant of Brill's algorithm), trained on only 40K words, reaches 99%.",
    "content": "1 Introduction Automatic grapheme to phoneme conversion (i.e. the conversion of a string of characters into a string of phonemes) is essential for applications of text to speech synthesis dealing with unrestricted text, where the input may contain words which do not occur in the system dictionary. Furthermore, a transducer for grapheme to phoneme conversion can be used to generate candidate replacements in a (pronunciation-sensitive) spelling correction sys- tem. When given the pronunciation of a misspelled word, the inverse of the grapheme to phoneme trans- ducer will generate all identically pronounced words. Below, we present a method for developing such grapheme to phoneme transducers based on a com- bination of hand-crafted conversion rules, imple- mented using finite state calculus, and automatically induced rules. The hand-crafted system is defined as a two- step procedure: segmentation of the input into a sequence of graphemes (i.e. sequences of one or more characters typically corresponding to a sin- gle phoneme) and conversion of graphemes into (se- quences of) phonemes. The composition of the transducer which performs segmentation and the transducer defined by the conversion rules, is a transducer which converts sequences of characters into sequences of phonemes. Specifying the conversion rules is a difficult task. Although segmentation of the input can in princi- ple be dispensed with, we found that writing con- version rules for segmented input substantially re- duces the context-sensitivity and order-dependence of such rules. We manually developed a grapheme to phoneme transducer for Dutch data obtained from CELEX (Baayen et al., 1993) and achieved a word ac- curacy of 60.6% and a phoneme accuracy of 93.6%. To improve the performance of our system, we used transformation-based learning (TBL) (Brill, 1995). Training data are obtained by aligning the output of the hand-crafted finite state transducer with the correct phoneme strings. These data can then be used as input for TBL, provided that suit- able rule templates are available. We performed sev- eral experiments, in which the amount of training data, the algorithm (Brill's original formulation and 'lazy' variants (Samuel et al., 1998)), and the num- ber of rule templates varied. The best experiment (40K words, using a 'lazy' strategy with a large set of rule templates) induces over 2000 transformation rules, leading to 92.6% word accuracy and 99.0% phoneme accuracy. This result, obtained using a relatively small set of training data, compares well with that of other systems. 2 Finite State Calculus As argued in Kaplan and Kay (1994), Karttunen (1995), Karttunen et al. (1997), and elsewhere, many of the rules used in phonology and morphol- ogy can be analysed as special cases of regular ex- pressions. By extending the language of regular ex- pressions with operators which capture the interpre- tation of linguistic rule systems, high-level linguis- tic descriptions can be compiled into finite state au- tomata directly. Furthermore, such automata can be combined with other finite state automata perform- ing low-level tasks such as tokenization or lexical- lookup, or more advanced tasks such as shallow pars- ing. Composition of the individual components into a single transducer may lead to highly efficient pro- cessing. The system described below was implemented us- ing FSA Utilities,¹ a package for implementing and manipulating finite state automata, which provides possibilities for defining new regular expression oper- ¹www.let.rug.nl/~vannoord/fsa/ [] [R1,..., Rn] {R1,..., Rn} R ignore (A,B) Ax B identity(A) TOU macro (Term, R) the empty string concatenation disjunction optionality ignore: A interspersed with elements of B cross-product: the transducer which maps all strings in A to all strings in B. identity: the transducer which maps each element in A onto itself. composition of the transducers T and U. use Term as an abbreviation for R (where Term and R may contain variables). Figure 1: A fragment of FSA regular expression syntax. A and B are regular expressions denoting recognizers, T and U transducers, and R can be either. ators. The part of FSA's built-in regular expression syntax relevant to this paper, is listed in figure 1. One particular useful extension of the basic syn- tax of regular expressions is the replace-operator. Karttunen (1995) argues that many phonological and morphological rules can be interpreted as rules which replace a certain portion of the input string. Although several implementations of the replace- operator are proposed, the most relevant case for our purposes is so-called 'leftmost longest-match' re- placement. In case of overlapping rule targets in the input, this operator will replace the leftmost target, and in cases where a rule target contains a prefix which is also a potential target, the longer sequence will be replaced. Gerdemann and van Noord (1999) implement leftmost longest-match replacement in FSA as the operator replace (Target, LeftContext, RightContext), where Target is a transducer defining the actual re- placement, and LeftContext and RightContext are regular expressions defining the left and rightcon- text of the rule, respectively. An example where leftmost replacement is use- ful is hyphenation. Hyphenation of (non-compound) words in Dutch amounts to segmenting a word into syllables, separated by hyphens. In cases where (the written form of) a word can in prin- ciple be segmented in several ways (i.e. the se- quence alfabet can be segmented as al-fa-bet, al-fab-et, alf-a-bet, or alf-ab-et), the seg- mentation which maximizes onsets is in general the correct one (i.e. al-fa-bet). This property of hy- phenation is captured by leftmost replacement: macro (hyphenate, replace([) x -, syllable, syllable)). Leftmost replacement ensures that hyphens are in- troduced 'eagerly', i.e. as early as possible. Given a suitable definition of syllable, this ensures that wherever a consonant can be final in a coda or initial in the next onset, it is in fact added to the onset. The segmentation task discussed below makes cru- cial use of longest match. 3 A finite state method for grapheme to phoneme conversion Grapheme to phoneme conversion is implemented as the composition of four transducers: macro (graph2phon, segmentation o mark_begin_end o conversion o clean_up % segment the input % add '#' % apply rules ). % remove markers An example of conversion including the in- termediate steps is given below for the word aanknopingspunt (connection-point). input: aanknopingspunt s: aa-n-k-n-o-p-i-ng-s-p-u-n-t- m: #-aa-n-k-n-o-p-i-ng-s-p-u-n-t-# co: #-a+N+k-n-o-p-I+N+s-p-}+n-t-# cl: aNknopINsp}nt The first transducer (segmentation) takes as its input a sequence of characters and groups these into segments. The second transducer (mark_begin_end) adds a marker ('#') to the be- ginning and end of the sequence of segments. The third transducer (conversion) performs the actual conversion step. It converts each segment into a sequence of (zero or more) phonemes. The final step (clean_up) removes all markers. The output is a list of phonemes in the notation used by CELEX (which can be easily translated into the more com- mon SAMPA-notation). 3.1 Segmentation The goal of segmentation is to divide a word into a sequence of graphemes, providing a convenient input level of representation for the actual grapheme to phoneme conversion rules. While there are many letter-combinations which are realized as a single phoneme (ch, ng, aa, bb, ..), it is only rarely the case that a single letter is mapped onto more than one phoneme (x), or that a letter receives no pronunciation at all (such as word- final n in Dutch, which is elided if it is proceeded by a schwa). As the number of cases where multiple letters have to be mapped onto a single phoneme is relatively high, it is natural to model a letter to phoneme system as involving two subtasks: segmen- tation and conversion. Segmentation splits an input string into graphemes, where each grapheme typ- ically, but not necessarily, corresponds to a single phoneme. Segmentation is defined as: macro(segmentation, replace( [identity(graphemes), () x - ],[],[]) ). The macro graphemes defines the set of graphemes. It contains 77 elements, some of which are: a, aa, au, ai, aai, e, ee, ei, eu, eau, eeu, i, ie, iee, ieu, ij, o, oe, oei,.. Segmentation attaches the marker `-' to each grapheme. Segmentation, as it is defined here, is not context-sensitive, and thus the second and third arguments of replace are simply empty. As the set of graphemes contains many elements which are substrings of other graphemes (i.e. e is a substring of ei, eau, etc.), longest-match is es- sential: the segmentation of beiaardier (caril- lon player) should be b-ei-aa-r-d-ie-r- and not b-e-i-a-a-r-d-i-e-r-. This effect can be ob- tained by making the segment itself part of the tar- get of the replace statement. Targets are identi- fied using leftmost longest-match, and thus at each point in the input, only the longest valid segment is marked. The set of graphemes contains a number of ele- ments which might seem superfluous. The grapheme aai, for instance, translates as aj, a sequence which could also be derived on the basis of two graphemes aa and i. However, if we leave out the segment aai, segmentation (using leftmost longest match) of words such as waaien (to blow) would lead to the segmentation w-aa-ie-n, which is unnatural, as it would require an extra conversion rule for ie. Us- ing the grapheme aai allows for two conversion rules which always map aai to aj and ie goes to i. Segmentation as defined above provides the in- tuitively correct result in almost all cases, given a suitably defined set of graphemes. There are some cases which are less natural, but which do not nec- essarily lead to errors. The grapheme eu, for in- stance, almost always goes to '1', but translates as 'e,j,}' in (loan-) words such as museum and petroleum. One might argue that a segmentation e-u- is therefore required, but a special conver- sion rule which covers these exceptional cases (i.e. eu followed by m) can easily be formulated. Simi- larly, ng almost always translates as N, but in some cases actually represents the two graphemes n-g-, as in aaneengesloten (connected), where it should be translated as NG. This case is harder to detect, and is a potential source of errors. 3.2 The Conversion Rules The g2p operator is designed to facilitate the formu- lation of conversion rules for segmented input: macro (g2p(Target, LtCont, RtCont), replace([Target, - x +], [ignore(LtCont,{+,-}), {-,+}], ignore(RtCont,{+,-}) ). The g2p-operator implements a special purpose ver- sion of the replace-operator. The replacement of the marker '-' by '+' in the target ensures that g2p- conversion rules cannot apply in sequence to the same grapheme.2 Second, each target of the g2p- operator must be a grapheme (and not some sub- string of it). This is a consequence of the fact that the final element of the left-context must be a marker and the target itself ends in '-'. Finally, the ig- nore statements in the left and right context imply that the rule contexts can abstract over the potential presence of markers. An overview of the conversion rules we used for Dutch is given in Figure 2. As the rules are ap- plied in sequence, exceptional rules can be ordered before the regular cases, thus allowing the regular cases to be specified with little or no context. The special vowel rules deal with exceptional trans- lations of graphemes such as eu or cases where i or ij goes to '@'. The short_vowel rules treat sin- gle vowels preceding two consonants, or a word final consonant. One problematic case is e, which can be translated either as 'E' or ''. Here, an ap- proximation is attempted which specifies the con- text where e goes 'E', and subsumes the other case under the general rule for short vowels. The special_consonant rules address devoicing and a few other exceptional cases. The default rules supply a default mapping for a large number of 2Note that the input and output alphabet are not disjoint, and thus rules applying in sequence to the same part of the input are not excluded in principle. graphemes. The target of this rule is a long disjunc- tion of grapheme-phoneme mappings. As this rule- set applies after all more specific cases have been dealt with, no context restrictions need to be speci- fied. Depending somewhat on how one counts, the full set of conversion rules for Dutch contains approxi- mately 80 conversion rules, more than 40 of which are default mappings requiring no context.3 Compi- lation of the complete system results in a (minimal, deterministic) transducer with 747 states and 20,123 transitions. 3.3 Test results and discussion The accuracy of the hand-crafted system was eval- uated by testing it on all of the words wihtout di- acritics in the CELEX lexical database which have a phonetic transcription. After several development cycles, we achieved a word accuracy of 60.6% and a phoneme accuracy (measured as the edit distance between the phoneme string produced by the sys- tem and the correct string, divided by the number of phonemes in the correct string) of 93.6%. There have been relatively few attempts at devel- oping grapheme to phoneme conversion systems us- ing finite state technology alone. Williams (1994) re- ports on a system for Welsh, which uses no less than 700 rules implemented in a rather restricted environ- ment. The rules are also implemented in a two-level system, PC-KIΜΜΟ, (Antworth, 1990), but this still requires over 400 rules. Möbius et al. (1997) report on full-fledged text-to-speech system for German, containing around 200 rules (which are compiled into a weighted finite state transducer) for the grapheme- to-phoneme conversion step. These numbers suggest that our implementation (which contains around 80 rules in total) benefits considerably from the flexibil- ity and high-level of abstraction made available by finite state calculus. One might suspect that a two-level approach to grapheme to phoneme conversion is more appropri- ate than the sequential approach used here. Some- what surprisingly, however, Williams concludes that a sequential approach is preferable. The formulation of rules in the latter approach is more intuitive, and rule ordering provides a way of dealing with excep- tional cases which is not easily available in a two- level system. While further improvements would definitely have been possible at this point, it becomes increasingly difficult to do this on the basis of linguistic knowl- edge alone. That is, most of the rules which have to be added deal with highly idiosyncratic cases (of- ten related to loan-words) which can only be discov- 3It should be noted that we only considered words which do not contain diacritics. Including those is unproblematic in principle, but would lead to a slight increase of the number of rules. ered by browsing through the test results of previ- ous runs. At this point, switching from a linguistics- oriented to a data-oriented methodology, seemed ap- propriate. 4 Transformation-based grapheme to phoneme conversion Brill (1995) demonstrates that accurate part-of- speech tagging can be learned by using a two-step process. First, a simple system is used which as- signs the most probable tag to each word. The re- sults of the system are aligned with the correct tags for some corpus of training data. Next, (context- sensitive) transformation rules are selected from a pool of rule patterns, which replace erroneous tags by correct tags. The rule with the largest benefit on the training data (i.e. the rule for which the number of corrections minus the number of newly introduced mistakes, is the largest) is learned and applied to the training data. This process continues until no more rules can be found which lead to improvement (above a certain threshold). Transformation-based learning (TBL) can be ap- plied to the present problem as well.4 In this case, the base-line system is the finite state transducer de- scribed above, which can be used to produce a set of phonemic transcriptions for a word list. Next, these results are aligned with the correct transcrip- tions. In combination with suitable rule patterns, these data can be used as input for a TBL process. 4.1 Alignment TBL requires aligned data for training and testing. While alignment is mostly trivial for part-of-speech tagging, this is not the case for the present task. Aligning data for grapheme-to-phoneme conversion amounts to aligning each part of the input (a se- quence of characters) with a part of the output (a sequence of phonemes). As the length of both se- quences is not guaranteed to be equal, it must be possible to align more than one character with a single phoneme (the usual case) or a single character with more than one phoneme (the exceptional case, i.e. 'x'). The alignment problem is often solved (Du- toit, 1997; Daelemans and van den Bosch, 1996) by allowing 'null' symbols in the phoneme string, and introducing 'compound' phonemes, such as 'ks' to account for exceptional cases where a single charac- ter must be aligned with two phonemes. As our finite state system already segments the input into graphemes, we have adopted a strategy where graphemes instead of characters are aligned with phoneme strings (see Lawrence and Kaye (1986) for a similar approach). The correspondence 4Hoste et al. (2000b) compare TBL to C5.0 (Quinlan, 1993) on a similar task, i.e. the mapping of the pronunciation of one regional variant of Dutch into another. macro(conversion, special_vowel_rules o short_vowel_rules o special_consonant_rules o default_rules ). macro(special_vowel_rules, g2p([e,u] x [e,j,}], [], m) %% museum o g2p(i x @, [], g) %% moedig(st) o g2p([i,j] x @, 1, k) %% mogelijkheid ). macro(short_vowel_rules, g2p(e x 'E', [], {[t,t], [k,k],x,...}) g2p({ a x 'A', e x @, i x 'I', o x '0', u x '}'}, [], [cons, {cons, #}]) ). macro(special_consonant_rules, g2p(b x p, [], {s,t,#}) o g2p([d,t^] x t, [], {s,g,k,j,v,h,z,#}) o g2p({ f x v, s x z}, [], {b,d}) o g2p(g x 'G', vowel, vowel) o g2p(n x 'N', [], {k,q}) o g2p(n x [], [©],[#]) ). macro(default_rules, g2p({ [a,a] x a, [a,u,i] x [a,j], [a,u] x 'M', [e,a,u] x o, ..., [c,h] x 'x', [s,c,h] x [s,x], [n,g] x 'N', ... [b,b] x b, [d,d] x d, ..., [c,h] x 'x', [s,c,h] x [s,x], [n,g] x 'N', ... }, [], []) ). Figure 2: Conversion Rules between graphemes and phonemes is usually one to one, but it is no problem to align a grapheme with two or more phonemes. Null symbols are only intro- duced in the output if a grapheme, such as word-final 'n', is not realized phonologically. For TBL, the input actually has to be aligned both with the system output as well as with the correct phoneme string. The first task can be solved trivi- ally: since our finite state system proceeds by first segmenting the input into graphemes (sequences of characters), and then transduces each grapheme into a sequence of phonemes, we can obtain aligned data by simply aligning each grapheme with its corre- sponding phoneme string. The input is segmented into graphemes by doing the segmentation step of the finite state transducer only. The corresponding phoneme strings can be identified by applying the conversion transducer to the segmented input, while keeping the boundary symbols '-' and '+'. As a con- sequence of the design of the conversion-rules, the resulting sequence of separated phonemes sequences stands in a one-to-one relationship to the graphemes. An example is shown in figure 3, where GR represents the grapheme segmented string, and SP the (system) phoneme strings produced by the finite state trans- ducer. Note that the final SP cell contains only a boundary marker, indicating that the grapheme 'n' is translated into the null phoneme. For the alignment between graphemes (and, idi- Word aalbessen (currants) GR aa - l- b- e- ss- e- n- SP a + l- b- @ + s + @ + + CP a l b E s @ Figure 3: Alignment rectly, the system output) and the correct phoneme strings (as found in Celex), we used the 'hand- seeded' probabilistic alignment procedure described by Black et al. (1998). From the finite state conver- sion rules, a set of possible grapheme → phoneme se- quence mappings can be derived. This allowables-set was extended with (exceptional) mappings present in the correct data, but not in the hand-crafted sys- tem. We computed all possible alignments between (segmented) words and correct phoneme strings li- cenced by the allowables-set. Next, probabilities for all allowed mappings were estimated on the basis of all possible alignments, and the data was parsed again, now picking the most probable alignment for each word. To minimize the number of words that could not be aligned, a maximum of one unseen map- ping (which was assigned a low probability) was al- lowed per word. With this modification, only one out of 1000 words on average could not be aligned.5 These words were discarded. The aligned phoneme 5Typical cases are loan words (umpires) and letter words (i.e. abbreviations) (abc). method training data phoneme word induced (words) accuracy accuracy rules CPU time (in minutes) Base-line 93.6 60.6 Brill 20K 98.0 86.1 447 162 Brill 40K 98.4 88.9 812 858 lazy(5) 20K 97.6 83.5 337 43 lazy (5) 40K 98.2 87.0 701 190 lazy(5) 60K 98.4 88.3 922 397 lazy(10) 20K 97.7 84.3 368 83 lazy(10) 40K 98.2 87.5 738 335 lazy(10) 60K 98.4 88.9 974 711 lazy(5)+ 20K 98.6 89.8 1225 186 lazy(5)+ 40K 99.0 92.6 2221 603 Figure 4: Experimental Results using training data produced by graph2phon string for the example in figure 3 is shown in the bottom line. Note that the final cell is empty, rep- resenting the null phoneme. 4.2 The experiments For the experiments with TBL we used the µ-TBL- package (Lager, 1999). This Prolog implementation of TBL is considerably more efficient (up to ten times faster) than Brill's original (C) implementa- tion. The speed-up results mainly from using Pro- log's first-argument indexing to access large quanti- ties of data efficiently. We constructed a set of 22 rule templates which replace a predicted phoneme with a (corrected) phoneme on the basis of the underlying segment, and a context consisting either of phoneme strings, with a maximum length of two on either side, or a context consisting of graphemes, with a maximal length of 1 on either side. Using only 20K words (which corresponds to almost 180K segments), and Brill's algorithm, we achieved a phoneme accuracy of 98.0% (see figure 4) on a test set of 20K words of unseen data. Going to 40K words resulted in 98.4% phoneme accuracy. Note, however, that in spite of the relative efficiency of the implementation, CPU time also goes up sharply. The heavy computation costs of TBL are due to the fact that for each error in the training data, all possible instantiations of the rule templates which correct this error are generated, and for each of these instantiated rules the score on the whole training set has to be computed. Samuel et al. (1998) there- fore propose an efficient, 'lazy', alternative, based on Monte Carlo sampling of the rules. For each er- ror in the training set, only a sample of the rules is considered which might correct it. As rules which correct a high number of errors have a higher chance $^6$The statistics for less time consuming experiments were obtained by 10-fold cross-validation and for the more expen- sive experiments by 5-fold cross-validation. of being sampled at some point, higher scoring rules are more likely to be generated than lower scoring rules, but no exhaustive search is required. We ex- perimented with sampling sizes 5 and 10. As CPU requirements are more modest, we managed to per- form experiments on 60K words in this case, which lead to results which are comparable with Brill's al- goritm applied to 40K words. Apart from being able to work with larger data sets, the 'lazy' strategy also has the advantage that it can cope with larger sets of rule templates. Brill's algorithm slows down quickly when the set of rule templates is extended, but for an algorithm based on rule sampling, this effect is much less severe. Thus, we also constructed a set of 500 rule templates, con- taining transformation rules which allowed up to three graphemes or phoneme sequences as left or right context, and also allowed for disjunctive con- texts (i.e. the context must contain an 'a' at the first or second position to the right). We used this rule set in combination with a 'lazy' strategy with sampling size 5 (lazy(5)+ in figure 4). This led to a further improvement of phoneme accuracy to 99.0%, and word accuracy of 92.6%, using only 40K words of training material. Finally, we investigated what the contribution was of using a relatively accurate training set. To this end, we constructed an alternative training set, in which every segment was associated with its most probable phoneme (where frequencies were obtained from the aligned CELEX data). As shown in figure 5, the initial accuracy for such as system is much lower than that of the hand-crafted system. The exper- imental results, for the 'lazy' algorithm with sam- pling size 5, show that the phoneme accuracy for training on 20K words is 0.3% less than for the cor- responding experiment in figure 4. For 40K words, the difference is still 0.2%, which, in both cases, cor- responds to a difference in error rate of around 10%. As might be expected, the number of induced rules --- method training data phoneme word induced (words) accuracy accuracy rules CPU time (in minutes) Base-line 20K 72.9 10.8 691 133 lazy (5) 40K 97.3 81.6 1075 705 lazy(5) 98.0 86.0 Figure 5: Experimental results using data based on frequency. is much higher now, and thus CPU-requirements also increase substantially. 5 Concluding remarks We have presented a method for grapheme to phoneme conversion, which combines a hand-crafted finite state transducer with rules induced by a transformation-based learning. An advantage of this method is that it is able to achieve a high level of accuracy using relatively small training sets. Busser (1998), for instance, uses a memory-based learning strategy to achieve 90.1% word accuracy on the same task, but used 90% of the CELEX data (over 300K words) as training set and a (character/phoneme) window size of 9. Hoste et al. (2000a) achieve a word accuracy of 95.7% and a phoneme accuracy of 99.5% on the same task, using a combination of ma- chine learning techniques, as well as additional data obtained from a second dictionary. Given the result of Roche and Schabes (1997), an obvious next step is to compile the induced rules into an actual transducer, and to compose this with the hand-crafted transducer. It should be noted, how- ever, that the number of induced rules is quite large in some of the experiments, so that the compilation procedure may require some attention. References Evan L. Antworth. 1990. PC-KIMMO: a two-level processor for morphological analysis. Summer In- stitute of Linguistics, Dallas, Tex. R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993. The CELEX Lexical Database (CD-ROM). Linguistic Data Consortium, University of Penn- sylvania, Philadelphia, PA. Alan Black, Kevin Lenzo, and Vincent Pagel. 1998. Issues in building general letter to sound rules. In Proceedings of the 3rd ESCA/COCSADA Work- shop on Speech Synthesis, pages 77-81, Jenolan Caves, Australia. Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21:543-566. Bertjan Busser. 1998. TreeTalk-D: a machine learn- ing approach to Dutch word pronunciation. In Proceedings TSD Conference, pages 3-8, Masaryk University, Czech Republic. W. Daelemans and A. van den Bosch. 1996. Language-independent data-oriented grapheme- to-phoneme conversion. In Progress in Speech Synthesis, pages 77-90, New York. Springer Ver- lag. Thierry Dutoit. 1997. An Introduction to Text-to- Speech Synthesis. Kluwer, Dordrecht. Dale Gerdemann and Gertjan van Noord. 1999. Transducers from rewrite rules with backrefer- ences. In Proceedings of the Ninth Conference of the European Chapter of the Association for Com- putational Linguistics, pages 126-133, Bergen. Veronique Hoste, Walter Daelemans, Erik Tjong Kim Sang, and Steven Gillis. 2000a. Meta-learning of phonemic annotation of corpora. ms., University of Antwerp. Veronique Hoste, Steven Gillis, and Walter Daele- mans. 2000b. A rule induction approach to mod- elling regional pronunciation variation. ms., Uni- versity of Antwerp. Ronald Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computa- tional Linguistics, 20(3). L. Karttunen, J.P. Chanod, G. Grefenstette, and A. Schiller. 1997. Regular expressions for lan- guage engineering. Natural Language Engineering, pages 1-24. Lauri Karttunen. 1995. The replace operator. In 33th Annual Meeting of the Association for Com- putational Linguistics, pages 16-23, Boston, Mas- sachusetts. Torbjörn Lager. 1999. The µ-TBL System: Logic programming tools for transformation- based learning. In Proceedings of the Third In- ternational Workshop on Computational Natu- ral Language Learning (CoNLL '99), pages 33-42, Bergen. S. C. G. Lawrence and G. Kaye. 1986. Alignment of phonemes with their corresponding orthography. Computer Speech and Language, 1(2):153-165. Bernd Möbius, Richard Sproat, Jan van Santen, and Joseph Olive. 1997. The Bell Labs German text- to-speech system: An overview. In Proceedings of the European Conference on Speech Communication and Technology, pages 2443-2446, Rhodes. J. R. Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufmann Publishers, San Ma- teo. --- Emmanuel Roche and Yves Schabes. 1997. Deter- ministic part-of-speech tagging with finite-state transducers. In Emmanuel Roche and Yves Sch- abes, editors, Finite state language processing, pages 205-239. MIT Press, Cambridge, Mass. Ken Samuel, Sandra Carberry, and K. Vijay- Shanker. 1998. Dialogue act tagging with transformation-based learning. In Proceedings of the 17th International Conference on Computa- tional Linguistics (COLING-ACL '98), Montreal. Briony Williams. 1994. Welsh letter-to-sound rules: rewrite rules and two-level rules compared. Com- puter Speech and Language, 8:261-277."
  },
  {
    "title": "Left-To-Right Parsing and Bilexical Context-Free Grammars",
    "abstract": "We compare the asymptotic time complexity of left-to-right and bidirectional parsing techniques for bilexical context-free grammars, a grammar formalism that is an abstraction of language models used in several state-of-the-art real-world parsers. We provide evidence that left-to-right parsing cannot be realised within acceptable time-bounds if the so called correct-prefix property is to be ensured. Our evidence is based on complexity results for the representation of regular languages.",
    "content": "1 Introduction Traditionally, algorithms for natural language pars- ing process the input string strictly from left to right. In contrast, several algorithms have been proposed in the literature that process the input in a bidi- rectional fashion; see (van Noord, 1997; Satta and Stock, 1994) and references therein. The issue of parsing efficiency for left-to-right vs. bidirectional methods has longly been debated. On the basis of experimental results, it has been argued that the choice of the most favourable strategy should depend on the grammar at hand. With respect to grammar formalisms based upon context-free grammars, and when the rules of these formalisms strongly depend on lexical information, (van Noord, 1997) shows that bidirectional strategies are more efficient than left- to-right strategies. This is because bidirectional strategies are most effective in reducing the parsing search space, by activating as early as possible the maximum number of lexical constraints available in the grammar. In this paper we present mathematical arguments in support of the above empirically motivated the- sis. We investigate a class of lexicalized grammars that, in their probabilistic versions, have been widely adopted as language models in state-of-the-art real- world parsers. The size of these grammars usually grows with the square of the size of the working lex- icon, and thus can be very large. In these cases, the primary goal in the design of a parsing algorithm is to achieve asymptotic time performance sublinear in the size of the working grammar and indepen- dent of the size of the lexicon. These desiderata are met by existing bidirectional algorithms (Alshawi, 1996; Eisner, 1997; Eisner and Satta, 1999). In con- trast, we show the following two main results for the asymptotic time performance of left-to-right al- gorithms satisfying the so called correct-prefix prop- erty. • In case off-line compilation of the working gram- mar is not allowed, left-to-right parsing cannot be realised within time bounds independent of the size of the lexicon. • In case polynomial-time, off-line compilation of the working grammar is allowed, left-to-right parsing cannot be realised in polynomial time, and independently of the size of the lexicon, un- less a strong conjecture based on complexity re- sults for the representation of regular languages is falsified. The first result implies that the well known Earley algorithm and related standard parsing techniques that do not require grammar precompilation can- not be directly extended to process the above men- tioned grammars (resp. language models) within an acceptable time bound. The second result provides evidence that well known parsing techniques as left- corner parsing, requiring polynomial-time prepro- cessing of the grammar, also cannot be directly ex- tended to process these formalisms within an accept- able time bound. The grammar formalisms we investigate are based upon context-free grammars and are called bilex- ical context-free grammars. Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (Al- shawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). Our results directly transfer to all these lan- guage models. In a bilexical context-free grammar, possible arguments of a word are always specified along with possible head words for those arguments. Therefore a bilexical grammar requires the grammar writer to make stipulations about the compatibil- ity of particular pairs of words in particular roles, something that was not necessarily true of general context-free grammars. The remainder of this paper is organized as fol- lows. We introduce bilexical context-free grammars in Section 2, and discuss parsing with the correct- prefix property in Section 3. Our results for parsing with on-line and off-line grammar compilation are presented in Sections 4 and 5, respectively. To com- plete the presentation, Appendix A shows that left- to-right parsing in time independent of the size of the lexicon is indeed possible when an off-line com- pilation of the working grammar is allowed that has an exponential time complexity. 2 Bilexical context-free grammars In this section we introduce the grammar formalism we investigate in this paper. This formalism, origi- nally presented in (Eisner and Satta, 1999), is an ab- straction of the language models adopted by several state-of-the-art real-world parsers (see Section 1). We specify a non-stochastic version of the formal- ism, noting that probabilities may be attached to the rewrite rules exactly as in stochastic CFG (Gon- zales and Thomason, 1978; Wetherell, 1980). We assume that the reader is familiar with context-free grammars. Here we follow the notation of (Harrison, 1978; Hopcroft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = (VN, VT, P, S), where VN and Vr are finite, disjoint sets of nonterminal and terminal symbols, respec- tively, SE VN is the start symbol, and P is a finite set of productions having the form A → a, where A E VN and a∈ (VNUVT)*. A \"derives\" relation, written, is associated with a CFG as usual. We use the reflexive and transitive closure of⇒, writ- ten *, and define L(G) accordingly. The size of a CFG G is defined as |G| = Σ(Α+α) EP|Aa). If every production in P has the form A → BC or A → a, for A, B, C E VN, a ∈ Vr, then G is said to be in Chomsky Normal Form (CNF). A CFG G = (VN, VT, P, S[$]) in CNF is called a bilexical context-free grammar if there exists a set VD, called the set of delexicalized nontermi- nals, such that nonterminals from VN are of the form A[a], consisting of A E VD and a ∈ Vr, and every production in P has one of the following two forms: (i) A[a] → B[b] C[c], a ∈ {b,c}; (ii) A[a] → a. A nonterminal A[a] is said to have terminal symbol a as its lexical head. Note that in a parse tree for G, the lexical head of a nonterminal is always \"in- herited\" from some daughter symbol (i.e., from some symbol in the right-hand side of a production). In the sequel, we also refer to the set Vr as the lexicon of the grammar. A bilexical CFG can encode lexically specific pref- erences in the form of binary relations on lexi- cal items. For instance, one might specify Pas to contain the production VP[solve] → V[solve] NP[puzzles] but not the production VP[eat] → V[eat] NP[puzzles]. This will allow derivation of some VP constituents such as \"solve two puzzles\", while forbidding \"eat two puzzles\". See (Eisner and Satta, 1999) for further discussion. The cost of this expressiveness is a very large grammar. Indeed, we have |G| = O(\\VD/³· [VT]²), and in practical applications | VTVD > 1. Thus, the grammar size is dominated in its growth by the square of the size of the working lexicon. Even if we conveniently group lexical items with distributional similarities into the same category, in practical ap- plications the resulting grammar might have several thousand productions. Parsing strategies that can- not work in sublinear time with respect to the size of the lexicon and with respect to the size of the whole input grammar are very inefficient in these cases. 3 Correct-prefix property So called left-to-right strategies are standardly adopted in algorithms for natural language pars- ing. Although intuitive, the notion of left-to-right parsing is a concept with no precise mathematical meaning. Note that in fact, in a pathological way, one could read the input string from left-to-right, storing it into some data structure, and then per- form syntactic analysis with a non-left-to-right strat- egy. In this paper we focus on a precise definition of left-to-right parsing, known in the literature as correct-prefix property parsing (Sippu and Soisalon- Soininen, 1990). Several algorithms commonly used in natural language parsing satisfy this property, as for instance Earley's algorithm (Earley, 1970), tab- ular left-corner and PLR parsing (Nederhof, 1994) and tabular LR parsing (Tomita, 1986). Let Vr be some alphabet. A generic string over Vr is denoted as w = a₁ an, with n≥ 0 and a; ∈ VT (1 ≤ i ≤ n); in case n = 0, w equals the empty string ɛ. For integers i and j with 1 ≤ i ≤ j ≤ n, we write w[i, j] to denote string a¡ai+1... aj; if i > j, we define w[i, j] = ε. Let G = (VN, VT, P, S) be a CFG and let w = a1 an with n ≥ 0 be some string over Vr. A rec- ognizer for the CFG class is an algorithm R that, on input (G, w), decides whether w ∈ L(G). We say that R satisfies the correct-prefix property (CPP) if the following condition holds. Algorithm R processes the input string from left-to-right, \"con- suming\" one symbol ai at a time. If for some i, 0 ≤ i ≤ n, the set of derivations in G having the form S * w[1, i]γ, γ∈ (VNU VT)*, is empty, then R rejects and halts, and it does so before consuming symbol ai+1, if i<n. In this case, we say that R has detected an error at position i in w. Note that the above property forces the recognizer to do rele- vant computation for each terminal symbol that is consumed. We say that w[1, i] is a correct-prefix for a lan- guage L if there exists a string z such that w[1, i]z ∈ L. In the natural language parsing literature, the CPP is sometimes defined with the following condi- tion in place of the above. If for some i, 0 ≤ i ≤ n, w[1, i] is not a correct prefix for L(G), then R rejects and halts, and it does so before consuming symbol ai+1, if i < n. Note that the latter definition asks for a stronger condition, and the two definitions are equivalent only in case the input grammar G is re- duced.¹ While the above mentioned parsing algo- rithms satisfy the former definition of CPP, they do not satisfy the latter. Actually, we are not aware of any practically used parsing algorithm that satisfies the latter definition of CPP. One needs to distinguish CPP parsing from some well known parsing algorithms in the literature that process symbols in the right-hand sides of each gram- mar production from left to right, but that do not exhibit any left-to-right dependency between differ- ent productions. In particular, processing of the right-hand side of some production may be initi- ated at some input position without consultation of productions or parts of productions that may have been found to cover parts of the input to the left of that position. These algorithms may also consult input symbols from left to right, but the processing that takes place to the right of some position i does not strictly depend on the processing that has taken place to the left of i. Examples are pure bottom-up methods, such as left-corner parsing without top- down filtering (Wiren, 1987). Algorithms that do satisfy the CPP make use of some form of top-down prediction. Top-down pre- diction can be implemented at parse-time as in the case of Earley's algorithm by means of the \"predic- tor\" step, or can be precompiled, as in the case of left-corner parsing (Rosenkrantz and Lewis, 1970), by means of the left-corner relation, or as in the case of LR parsers (Sippu and Soisalon-Soininen, 1990), through the closure function used in the construc- tion of LR states. 4 Recognition without precompilation In this section we consider recognition algorithms that do not require off-line compilation of the input grammar. Among algorithms that satisfy the CPP, the most popular example of a recognizer that does 1 A context-free grammar G is reduced if every nonterminal of G can be part of at least one derivation that rewrites the start symbol into some string of terminal symbols. not require grammar precompilation is perhaps Ear- ley's algorithm (Earley, 1970). We show here that methods in this family cannot be extended to work in time independent of the size of the lexicon, in contrast with bidirectional recognition algorithms. The result presented below rests on the follow- ing, quite obvious, assumption. There exists a con- stant c, depending on the underlying computation model, such that in k≥ 0 elementary computation steps any recognizer can only read up to ck pro- ductions from set P. In what follows, and without any loss of generality, we assume c = 1. Apart from this assumption, no other restriction is imposed on the representation of the input grammar or on the access to the elements of sets VN, Vr and P. Theorem 1 Let f be any function of two variables defined on natural numbers. No recognizer for bilexi- cal context-free grammars that satisfies the CPP can run on input (G,w) in an amount of time bounded by f(|VD), wl), where VD is the set of delexicalized nonterminals of G. Proof. Assume the existence of a recognizer R sat- isfying the CPP and running in f(|VD), w) steps or less. We show how to derive a contradiction. Let q≥ 1 be an integer. Define a bilexical CFG Gq = (V, V, P9, A[b1]) where Vf contains q + 2 distinct symbols {b1,..., bq+2} and V = {A[bi] | 1≤ i ≤ q + 1} ∪ {T[b] |b∈ Vi}, and where set Pº contains all and only the following productions: (i) A[bi] A[bi+1] T[bi], 1≤ i ≤ q; (ii) A[bg+1]→T[bg+2] T[bg+1]; (iii) T[b] → b, be VI. Productions in (i) are called bridging productions. Note that there are a bridging productions in Gq. Also, note that V = {A,T} does not depend on the choice of q. Thus, we will simply write VD. Choose q > max{f(\\Vp1, 2), 1}. On input (Gq, bq+2bq+1), R does not detect any error at posi- tion 1, that is after having read the first symbol bq+2 of the input string. This is because A[b1] →* bg+27 with y = T[bg+1]T[bg] T [bq_1]T[b1] is a valid derivation in G. Since R executes no more than f(|VD, 2) steps, from our assumption that reading a production takes unit time it follows that there must be an integer k, 1 ≤ k ≤ q, such that bridging production A(bk] → A[bk+1] T[be] is not read from Gq. Construct then a new grammar G', by replacing in G, the production A[br] → A[bk+1] T[bk] with the new production A[bx] → T[bk] A[bk+1], leaving everything else unchanged. It follows that, on in- put (Gq, bq+2bq+1), R behaves exactly as before and does not detect any error at position 1. But this is a contradiction, since there is no derivation in G' of the form A[b1] ⇒* bq+2γ, γ ∈ (VN ∪ VT)*, as can be easily verified. We can use the above result in the comparison of left-to-right and bidirectional recognizers. The recognition of bilexical context-free languages can be carried out by existing bidirectional algorithms in time independent of the size of the lexicon and without any precompilation of the input bilexical grammar. For instance, the algorithms presented in (Eisner and Satta, 1999) allow recognition in time O(|V<sub>b</sub>||w|<sup>3</sup>)<sup>2</sup>. Theorem 1 states that this time bound cannot be met if we require the CPP and if the input grammar is not precompiled. In the next section, we will consider the possibility that the in- put grammar is in a precompiled form. 5 Recognition with precompilation In this section we consider recognition algorithms that satisfy the CPP and allow off-line, polynomial- time compilation of the working grammar. We focus on a class of bilexical context-free grammars where recognition requires the stacking of a number of un- resolved lexical dependencies that is proportional to the length of the input string. We provide evidence that the above class of recognizers perform much less efficiently for these grammars than existing bidirec- tional recognizers. We assume that the reader is familiar with the notions of deterministic and nondeterministic finite automata. We follow here the notation in (Hopcroft and Ullman, 1979). A nondeterministic finite au- tomaton (FA) is a tuple M = (Q, Σ, δ, q<sub>0</sub>, F), where Q and Σ are finite, disjoint sets of state and alphabet symbols, respectively, q<sub>0</sub> ∈ Q and F ⊆ Q are the ini- tial state and the set of final states, respectively, and δ is a total function mapping Q × Σ to 2<sup>Q</sup>, the power- set of Q. Function δ represents the transitions of the automaton. Given a string w = a<sub>1</sub>…a<sub>n</sub>, n ≥ 0, an accepting computation in M for w is a sequence q<sub>0</sub>, a<sub>1</sub>, q<sub>1</sub>, a<sub>2</sub>, q<sub>2</sub>,…, a<sub>n</sub>, q<sub>n</sub> such that q<sub>i</sub> ∈ δ(q<sub>i−1</sub>, a<sub>i</sub>) for 1 ≤ i ≤ n, and q<sub>n</sub> ∈ F. The language L(M) is the set of all strings in Σ* that admit at least one accepting computation in M. The size of M is de- fined as |M| = Σ<sub>q∈Q,α∈Σ</sub> |δ(q, α)|. The automaton M is deterministic if, for every q ∈ Q and a ∈ Σ, we have |δ(q, a)| = 1. We call quasi-determinizer any algorithm A that satisfies the following two conditions: 1. A takes as input a nondeterministic FA M = (Q, Σ, δ, q<sub>0</sub>, F) and produces as output a device D<sub>M</sub> that, when given a string w as input, de- cides whether w ∈ L(M); and 2. there exists a polynomial p<sub>A</sub> such that every D<sub>M</sub> runs in an amount of time bounded by p<sub>A</sub>(|w|). We remark that, given a nondeterministic FA M specified as above, known algorithms allow simula- tion of M on an input string w in time O(|M||w|) (see for instance (Aho et al., 1974, Thm. 9.5) or (Sippu and Soisalon-Soininen, 1988, Thm. 3.38)). In contrast, a quasi-determinizer produces a device that simulates M in an amount of time independent of the size of M itself. A standard example of a quasi-determinizer is the so called power-set construction, used to convert a nondeterministic FA into a language-equivalent deterministic FA (see for instance (Hopcroft and Ullman, 1979, Thm. 2.1) or (Sippu and Soisalon- Soininen, 1988, Thm. 3.30)). In fact, there exist constants c and c′ such that any deterministic FA can be simulated on input string w in an amount of time bounded by c|w| + c′. This requires function δ to be stored as a |Q| × |Σ|, 2-dimensional array with values in Q. This is a standard representation for automata-like structures; see (Gusfield, 1997, Sect. 6.5) for discussion. We now pose the question of the time efficiency of a quasi-determinizer, and consider the amount of time needed in the construction of D<sub>M</sub>. In (Meyer and Fisher, 1971; Stearns and Hunt, 1981) it is shown that there exist (infinitely many) nonde- terministic FAs with state set Q, such that any language-equivalent deterministic FA must have at least 2<sup>|Q|</sup> states. This means that the power-set con- struction cannot work in polynomial time in the size of the input FA. Despite of much effort, no algo- rithm has been found, up to the authors' knowledge, that can simulate a nondeterministic FA on an input string w in linear time in |w| and independently of |M|, if only polynomial-time precompilation of M is allowed. Even in case we relax the linear-time re- striction and consider recognition of w in polynomial time, for some fixed polynomial, it seems unlikely that the problem can be solved if only polynomial- time precompilation of M is allowed. Furthermore, if we consider precompilation of nondeterministic FAs into \"partially determinized\" FAs that would allow recognition in polynomial (or even exponen- tial) time in |w|, it seems unlikely that the analysis required for this precompilation could consider less than exponentially many combinations of states that may be active at the same time for the original non- deterministic FA. Finally, although more powerful formalisms have been shown to represent some regu- lar languages much more succinctly than FAs (Meyer and Fisher, 1971), while allowing polynomial-time parsing, it seem unlikely that this could hold for reg- ular languages in general. <sup>2</sup>More precisely, the running time for these algorithms is O(|V<sub>b</sub>|<sup>3</sup>|w|<sup>3</sup>min{|V<sub>T</sub>|, |w|}). In cases of practical interest, we always have |w| < |V<sub>T</sub>|. Conjecture There is no quasi-determinizer that works in polynomial time in the size of the input automaton. Before turning to our main result, we need to develop some additional machinery. Let M = (Q, Σ, δ, qo, F) be a nondeterministic FA and let w = a1...an ∈ L(M), where n ≥ 0. Let qo, q1,..., qn be an accepting computation for w in M, and choose some symbol $ ∈ Σ. We can now encode the accepting computation as ($, qo)(a1, q1)...(an, qn) where we pair alphabet symbols to states, prepend- ing $ to make up for the difference in the number of alphabet symbols and states. We now provide a construction that associates M with a bilexical CFG GM. Strings in L(GM) are obtained by pair- ing strings in L(M) with encodings of their accepting computations (see below for an example). Definition 1 Let M = (Q, Σ, δ, qo, F) be a nonde- terministic FA. Choose two symbols $, # ∈ Σ, and let Δ = {(a, q) | a ∈ Σ ∪ {$}, q ∈ Q}. A bilexi- cal CFG GM = (VN, VT, P, C[($, qo)]) is specified as follows: (i) VN = {T[σ] | σ ∈ VT} ∪ {C[σ], C'[σ] | σ ∈ Δ}; (ii) VT = Δ ∪ Σ ∪ {#}; (iii) P contains all and only the following produc- tions: (a) for each σ ∈ VT, T[σ] → σ; (b) for each (a, q), (a', q') ∈ Δ such that q' ∈ δ(q, a'), C[(a, q)] → C'[(a', q')] T[(a, q)]; (c) for each (a, q) ∈ Δ, C'[(a, q)] → T[a] C[(a, q)]; (d) for each (a, q) ∈ Δ such that q ∈ F, C[(a, q)] → T[#] T[(a, q)]. We give an example of the above construc- tion. Consider an automaton M and a string w = a1a2a3 such that w ∈ L(M). Let ($, qo)(a1, q1)(a2, q2)(a3, q3) be the encoding of an accepting computation in M for w. Then the string a1a2a3#(a3, q3)(a2, q2)(a1, q1)($, qo) belongs to L(GM). The tree depicted in Figure 1 represents a derivation in GM of such a string. The following fact will be used below. Lemma 1 For each w ∈ Σ*, w# is a correct-prefix for L(GM) if and only if w ∈ L(M). Outline of the proof. We claim the following fact. For each k > 0, a1, a2,..., ak ∈ Σ and qo, q1,..., qk ∈ Q we have qi ∈ δ(qi−1, ai), for all i (1 ≤ i ≤ k), (i) has random access to data structure C(G) pre- compiled from a bilexical CFG G in polynomial time in |G|, (ii) runs in an amount of time bounded by p(|VD|,|w|), where VD is the set of delexicalized nonterminals of G and w is the input string, and (iii) satisfies the CPP. Proof. Assume there exists a recognizer R that sat- isfies conditions (i) to (iii) in the statement of the theorem. We show how this entails that the conjec- ture about quasi-determinizers is false. We use algorithm R to specify a quasi- determinizer A. Given a nondeterministic FA M, A goes through the following steps. 1. A constructs grammar GM as in Definition 1. 2. A precompiles GM as required by R, producing data structure C(GM). 3. A returns a device DM specified as follows. Given a string w as input, DM runs R on string w#. If R detects an error at any position i, 0 ≤ i ≤ |w|, then DM rejects and halts, oth- erwise DM accepts and halts. From Lemma 1 we have that DM accepts w if and only if w ∈ L(M). Since R runs in time p(|VD|,|w|) and since GM has a set of delexicalized nonterminals independent of M, we have that there exists a poly- nomial pA such that every DM works in an amount of time bounded by pA(|w|). We therefore conclude that A is a quasi-determinizer. It remains to be shown that A works in polyno- mial time in |M|. Step 1 can be carried out in time O(|M|). The compilation at Step 2 takes polynomial time in |GM|, following our hypotheses on R, and hence polynomial time in |M|, since |GM| = O(|M|). Finally, the construction of DM at Step 3 can easily be carried out in time O(|M|) as well. In addition to Theorem 1, Theorem 2 states that, even in case the input grammar is compiled off- line and in polynomial time, we cannot perform CPP recognition for bilexical context-free grammars in time polynomial in the grammar and the input string but independent of the lexicon size. This is true with at least the same evidence that sup- ports the conjecture on quasi-determinizers. Again, this should be contrasted with the time performance of existing bidirectional algorithms, allowing recog- nition for bilexical context-free grammars in time O(|VD|³|w|⁴). In order to complete our investigation of the above problem, in Appendix A we show that, when we drop the polynomial-time restriction on the grammar pre- compilation, it is indeed possible to get rid of any |Vr| factor from the running time of the recognizer. 6 Conclusion Empirical results presented in the literature show that bidirectional parsing strategies can be more time efficient in cases of grammar formalisms whose rules are specialized for one or more lexical items. In this paper we have provided an original mathe- matical argument in favour of this thesis. Our re- sults hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be ex- tended to other language models that properly em- bed bilexical context-free grammars, as for instance the more general history-based models used in (Rat- naparkhi, 1997) and (Chelba and Jelinek, 1998). We leave this for future work. Acknowledgements We would like to thank Jason Eisner and Mehryar Mohri for fruitful discussions. The first author is supported by the German Federal Ministry of Edu- cation, Science, Research and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 V0, and was employed at AT&T Shannon Laboratory during a part of the period this paper was written. The second author is supported by MURST under project PRIN: BioInformatica e Ricerca Genomica and by University of Padua, un- der project Sviluppo di Sistemi ad Addestramento Automatico per l'Analisi del Linguaggio Naturale. References A. V. Aho, J. E. Hopcroft, and J. D. Ullman. 1974. The Design and Analysis of Computer Algorithms. Addison-Wesley, Reading, MA. H. Alshawi. 1996. Head automata and bilingual tiling: Translation with minimal representations. In Proc. of the 34th ACL, pages 167-176, Santa Cruz, CA. E. Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proc. of AAAI-97, Menlo Park, CA. C. Chelba and F. Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proc. of the 36th ACL, Montreal, Canada. M. Collins. 1997. Three generative, lexicalised mod- els for statistical parsing. In Proc. of the 35th ACL, Madrid, Spain. J. Earley. 1970. An efficient context-free parsing al- gorithm. Communications of the Association for Computing Machinery, 13(2):94-102. J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head automa- ton grammars. In Proc. of the 37th ACL, pages 457-464, College Park, Maryland. J. Eisner. 1996. An empirical comparison of proba- bility models for dependency grammar. Technical Report IRCS-96-11, IRCS, Univ. of Pennsylvania. J. Eisner. 1997. Bilexical grammars and a cubic- time probabilistic parser. In Proceedings of the 4th Int. Workshop on Parsing Technologies, MIT, Cambridge, MA, September. R. C. Gonzales and M. G. Thomason. 1978. Syntac- tic Pattern Recognition. Addison-Wesley, Read- ing, MA. D. Gusfield. 1997. Algorithms on Strings, Trees and Sequences. Cambridge University Press, Cam- bridge, UK. M. A. Harrison. 1978. Introduction to Formal Lan- guage Theory. Addison-Wesley, Reading, MA. J. E. Hopcroft and J. D. Ullman. 1979. Introduc- tion to Automata Theory, Languages and Compu- tation. Addison-Wesley, Reading, MA. A. R. Meyer and M. J. Fisher. 1971. Economy of de- scription by automata, grammars and formal sys- tems. In 12th Annual Symp. on Switching and Au- tomata Theory, pages 188-190, New York. IEEE. M.-J. Nederhof and G. Satta. 1996. Efficient tabular LR parsing. In Proc. of the 34th ACL, pages 239- 246, Santa Cruz, CA. M.-J. Nederhof. 1994. An optimal tabular parsing algorithm. In Proc. of the 32nd ACL, pages 117- 124, Las Cruces, New Mexico. A. Ratnaparkhi. 1997. A linear observed time sta- tistical parser based on maximum entropy mod- els. In Second Conference on Empirical Methods in Natural Language Processing, Brown Univer- sity, Providence, Rhode Island. D. J. Rosenkrantz and P. M. Lewis. 1970. Determin- istic left corner parsing. In IEEE Conf. Record 11th Annual Symposium on Switching and Au- tomata Theory, pages 139-152. G. Satta and O. Stock. 1994. Bidirectional context- free grammar parsing for natural language pro- cessing. Artificial Intelligence, 69:123-164. S. Sippu and E. Soisalon-Soininen. 1988. Pars- ing Theory: Languages and Parsing, volume 1. Springer-Verlag, Berlin, Germany. S. Sippu and E. Soisalon-Soininen. 1990. Parsing Theory: LR(k) and LL(k) Parsing, volume 2. Springer-Verlag, Berlin, Germany. R. E. Stearns and H. B. Hunt. 1981. On the equiva- lence and containment problem for unambiguous regular expressions, grammars, and automata. In 22nd Annual Symp. on Foundations of Computer Science, pages 74-81, New York. IEEE. M. Tomita. 1986. Efficient Parsing for Natural Lan- guage. Kluwer, Boston, Mass. G. van Noord. 1997. An efficient implementation of the head-corner parser. Computational Linguis- tics, 23(3):425-456. C. S. Wetherell. 1980. Probabilistic languages: A review and some open questions. Computing Sur- veys, 12(4):361-379. M. Wiren. 1987. A comparison of rule-invocation strategies in parsing. In Proc. of the 3rd EACL, pages 226-233, Copenhagen, Denmark. A Recognition in time independent of the lexicon In Section 5 we have shown that it is unlikely that correct-prefix property parsing for a bilexical CFG can be carried out in polynomial time and indepen- dently of the lexicon size, when only polynomial- time off-line compilation of the grammar is allowed. To complete our presentation, we show here that correct-prefix property parsing in time independent of the lexicon size is indeed possible if we spend ex- ponential time on grammar precompilation. We first consider tabular LR parsing (Tomita, 1986), a technique which satisfies the correct-prefix property, and apply it to bilexical CFGs. Our pre- sentation relies on definitions from (Nederhof and Satta, 1996). Let w∈ V be some input string. A property of LR parsing is that any state that can be reached after reading prefix w[1, j], j ≤ [w], must be of the form goto(goto(...(goto(qin, X1),...), Xm-1), Xm) where qin is the initial LR state, and X1,..., Xm are terminals or nonterminals such that X₁ Xm * w[1, j]. For a bilexical CFG, each Xi is of the form bi or of the form Bi[bi], where b₁,..., bm is some subse- quence of w[1, j). This means that there are at most (2+(VD)\" distinct states that can be reached by the recognizer, apart from qin. In the algorithm, the tab- ulation prevents repeated manipulation of states for a triple of input positions, leading to a time complex- ity of O(n³ VD/\"), where n = |w|. Hence, when we apply precompilation of the grammar, we can carry out recognition in time exponential in the length of the input string, yet independent of the lexicon size. Note however that the precompilation for LR pars- ing takes exponential time. The second algorithm with the CPP we will con- sider can be derived from Earley's algorithm (Ear- ley, 1970). For this new recognizer, we achieve a time complexity completely independent of the size of the whole grammar, not merely independent of the size of the lexicon as in the case of tabular LR parsing. Furthermore, the input grammar can be any general CFG, not necessarily a bilexical one. In terms of the length of the input, the complexity is polynomial rather than exponential. Earley's algorithm is outlined in what follows, with minor modifications with respect to its origi- nal presentation. An item is an object of the form [A → α • β], where A → αβ is a production from the grammar. The recognition algorithm consists in an incremental construction of a (n + 1) × (n + 1), 2-dimensional table T, where n is the length of the input string. At each stage, each entry T[i, j] in the table contains a set of items, which is initially the empty set. After an initial item is added to en- try T[0, 0] in the table, other items in other entries are derived from it, directly or indirectly, using three steps called predictor, scanner and completer. When no more new items can be derived, the presence of a final item in entry T[0, n] indicates whether the input is recognized. The recognition process can be precompiled, based on the observation that for any grammar the set of all possible items is finite, and thereby all po- tential contents of T's entries can be enumerated. Furthermore, the dependence of entries on one an- other is not cyclic; one item in T[i, j] may be derived from a second item in the same entry, but it is not possible that, for example, an item in T[i, j] is de- rived from an item in T[i', j'], with (i, j) ≠ (i', j'), which is in turn derived from an item in T[i, j]. A consequence is that entries can be computed in a strict order, and an operation that involves the combination of, say, the items from two entries T[i, j] and T[j, k] by means of the completer step can be implemented by a simple table lookup. More pre- cisely, each set of items is represented by an atomic state, and combining two sets of items according to the completer step is implemented by indexing a 2-dimensional array by the two states representing those two sets, yielding a third state representing the resulting set of items. Similarly, the scanner and predictor steps and the union operation on sets of items can all be implemented by table lookup. The time complexity of recognition can straight- forwardly be shown to be O(n³), independent of the size of the grammar. However, massive pre- compilation is involved in enumerating all possi- ble sets of items and precomputing the operations on them. The motivation for discussing this algo- rithm is therefore purely theoretical: it illustrates the unfavourable complexity properties that The- orem 2, together with the conjecture about quasi- determinizers, attributes to the recognition problem if the correct-prefix property is to be ensured."
  }
]