{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rnVQKjwJTvc"
      },
      "outputs": [],
      "source": [
        "%%writefile Qwen-2.5-7B_alpacaft_local.py\n",
        "# qwen2.5_alpacaft_local_v5.py\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Fine-tune Qwen-2.5-7B-Instruct with LoRA on local ShareGPT JSON dataset for academic abstract generation.\n",
        "Usage: python llama3_2_alpacaft_local_v5.py --dataset data.json [--output output_dir] [--epochs 4] [--batch-size 3]\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import argparse\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextStreamer\n",
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Fine-tune Qwen 2.5-7B with LoRA\")\n",
        "    parser.add_argument(\"--dataset\", type=str, required=True, help=\"Path to ShareGPT JSON file\")\n",
        "    parser.add_argument(\"--output\", type=str, default=\"outputs\", help=\"Output directory\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=4, help=\"Per-device batch size\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    max_seq_length = 2048\n",
        "    model_name = \"unsloth/Qwen2.5-7B-Instruct\"\n",
        "    output_dir = args.output\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_name,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=8,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0,\n",
        "        bias=\"none\",\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        random_state=3407,\n",
        "    )\n",
        "\n",
        "    tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
        "\n",
        "    system_instruction = (\n",
        "        \"You are an expert academic writer specializing in generating high-quality abstracts for scholarly publications.\\n\\n\"\n",
        "        \"Your task is to produce a concise abstract (150-250 words) based on a research paper's title and content. Please ensure the abstract:\\n\"\n",
        "        \"- Clearly states the research problem or question,\\n\"\n",
        "        \"- Summarizes the methodology or approach,\\n\"\n",
        "        \"- Highlights the key findings,\\n\"\n",
        "        \"- Explains the significance or implications of the results.\\n\\n\"\n",
        "        \"Use formal, objective language appropriate for academic journals. Avoid unnecessary jargon and maintain clarity for a broad scholarly audience.\"\n",
        "    )\n",
        "\n",
        "    with open(args.dataset, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_data = json.load(f)\n",
        "\n",
        "    for item in raw_data:\n",
        "        if \"conversations\" in item:\n",
        "            conv = item[\"conversations\"]\n",
        "            for i, msg in enumerate(conv):\n",
        "                if msg[\"role\"] == \"user\" and (i == len(conv) - 1 or conv[i + 1][\"role\"] != \"assistant\"):\n",
        "                    conv.insert(i + 1, {\"role\": \"assistant\", \"content\": \"\"})\n",
        "\n",
        "    dataset = Dataset.from_list(raw_data)\n",
        "\n",
        "    def apply_chat_template(example):\n",
        "        conv = example[\"conversations\"]\n",
        "        if conv[0][\"role\"] != \"system\":\n",
        "            conv.insert(0, {\"role\": \"system\", \"content\": system_instruction})\n",
        "        return {\"text\": tokenizer.apply_chat_template(conv, tokenize=False)}\n",
        "\n",
        "    dataset = dataset.train_test_split(test_size=0.2, seed=3407)\n",
        "    train_dataset = dataset[\"train\"].map(apply_chat_template, num_proc=4)\n",
        "    val_dataset = dataset[\"test\"].map(apply_chat_template, num_proc=4)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        gradient_accumulation_steps=2,\n",
        "        warmup_ratio=0.03,\n",
        "        num_train_epochs=args.epochs,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
        "        logging_strategy=\"steps\",\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        report_to=\"none\",\n",
        "        dataloader_pin_memory=True,\n",
        "    )\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, pad_to_multiple_of=8),\n",
        "        args=training_args,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    if torch.cuda.is_available():\n",
        "        gpu = torch.cuda.get_device_properties(0)\n",
        "        print(f\"GPU: {gpu.name}, Total Memory: {gpu.total_memory / 1024**3:.2f} GB\")\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    try:\n",
        "        with open(\"test.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "            paper = json.load(f)\n",
        "            if isinstance(paper, list):\n",
        "                paper = paper[0]\n",
        "    except:\n",
        "        paper = {\n",
        "            \"title\": \"Advances in NLP for Scientific Literature\",\n",
        "            \"content\": \"This paper explores transformer-based methods for extracting information from research papers.\"\n",
        "        }\n",
        "\n",
        "    user_prompt = (\n",
        "        \"Generate an academic abstract based on the following paper:\\n\\n\"\n",
        "        f\"Title: {paper['title']}\\n\\n\"\n",
        "        f\"Content: {paper['content']}\\n\\n\"\n",
        "        \"Please ensure the abstract:\\n\"\n",
        "        \"- Is concise (150-250 words),\\n\"\n",
        "        \"- Clearly states the research problem,\\n\"\n",
        "        \"- Summarizes the methodology,\\n\"\n",
        "        \"- Highlights the key findings,\\n\"\n",
        "        \"- Explains the significance or implications,\\n\"\n",
        "        \"- Uses formal academic language suitable for a scholarly journal,\\n\"\n",
        "        \"- Avoids unnecessary jargon while maintaining clarity.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_instruction},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    print(\"Generating abstract:\")\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "    model.generate(input_ids=inputs, streamer=streamer, max_new_tokens=512, temperature=0.7)\n",
        "\n",
        "    print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}