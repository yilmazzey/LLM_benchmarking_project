[
    {
        "title": "integrating_an_unsupervised_transliteration_model_into_statistical_machine_translation",
        "original_abstract": "We investigate three methods for integrat-\ning an unsupervised transliteration model\ninto an end-to-end SMT system. We in-\nduce a transliteration model from parallel\ndata and use it to translate OOV words.\nOur approach is fully unsupervised and\nlanguage independent.\nIn the methods\nto integrate transliterations, we observed\nimprovements from 0.23-0.75 (∆0.41)\nBLEU points across 7 language pairs. We\nalso show that our mined transliteration\ncorpora provide better rule coverage and\ntranslation quality compared to the gold\nstandard transliteration corpora.\n1",
        "generated_abstract": "This paper addresses the challenge of out-of-vocabulary (OOV) words in statistical machine translation (SMT) by integrating an unsupervised transliteration model. The research investigates methods for automatically generating transliterations for OOV words and incorporating them into the translation process. An expectation-maximization (EM) based unsupervised model is employed to induce a transliteration corpus from word-aligned parallel data, which is then used to train a transliteration model. The study explores three distinct methods for integrating transliteration during decoding within the Moses toolkit: post-decoding replacement of OOVs with their best transliteration, n-best transliteration rescoring using language model features, and on-the-fly transliteration phrase-table integration during decoding. Experiments conducted across seven language pairs demonstrate that the proposed integrations yield improvements in translation quality, as measured by BLEU score. Furthermore, the automatically mined transliteration corpus is shown to provide better rule coverage and overall translation quality compared to gold standard transliteration corpora, suggesting that recall is more important than precision for overall MT quality in this context. The unsupervised transliteration miner and its integration into SMT have been made available to the research community via the Moses toolkit, facilitating further research and development in this area."
    },
    {
        "title": "event_temporal_relation_extraction_with_bayesian_translational_model",
        "original_abstract": "Existing models to extract temporal relations\nbetween events lack a principled method to in-\ncorporate external knowledge. In this study, we\nintroduce Bayesian-Trans, a Bayesian learning-\nbased method that models the temporal rela-\ntion representations as latent variables and in-\nfers their values via Bayesian inference and\ntranslational functions. Compared to conven-\ntional neural approaches, instead of performing\npoint estimation to find the best set parame-\nters, the proposed model infers the parameters’\nposterior distribution directly, enhancing the\nmodel’s capability to encode and express un-\ncertainty about the predictions. Experimental\nresults on the three widely used datasets show\nthat Bayesian-Trans outperforms existing ap-\nproaches for event temporal relation extraction.\nWe additionally present detailed analyses on un-\ncertainty quantification, comparison of priors,\nand ablation studies, illustrating the benefits of\nthe proposed approach.1\n1",
        "generated_abstract": "This paper addresses the challenge of event temporal relation extraction, which is often hindered by the wide variety of linguistic expressions and the reliance on unstated commonsense knowledge. To overcome these limitations, a novel Bayesian Translational model (Bayesian-Trans) is proposed. This model leverages the Bayesian learning framework combined with translational models to incorporate external knowledge and mitigate the scarcity of annotated data. The core idea is to model event temporal relations through stochastic translational parameters, which are treated as latent variables in Bayesian inference. Prior distributions for these latent variables are derived from external knowledge graphs, providing the model with the necessary commonsense knowledge to interpret temporal information across different contexts. The model utilizes amortized variational inference to learn the posterior distribution of the translational parameters, refining its belief over the initial prior. Experimental evaluations conducted on three benchmark datasets for event temporal relation extraction demonstrate that Bayesian-Trans achieves state-of-the-art performance. Furthermore, the Bayesian architecture provides an inherent capability to express degrees of uncertainty, avoiding overconfident predictions on out-of-distribution contexts. The results confirm that the combination of translational models and Bayesian learning is particularly beneficial for the detection of event relations, demonstrating the effectiveness of incorporating prior knowledge in a principled manner."
    },
    {
        "title": "Navigating_the_Grey_Area__How_Expressions_of_Uncertainty_and_Overconfidence_Affect_Language_Models",
        "original_abstract": "The increased deployment of LMs for real-\nworld tasks involving knowledge and facts\nmakes it important to understand model episte-\nmology: what LMs think they know, and how\ntheir attitudes toward that knowledge are af-\nfected by language use in their inputs. Here,\nwe study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or\nevidentiality like \"I’m sure it’s\", \"I think it’s\",\nor “Wikipedia says it’s\" affect models, and\nwhether they contribute to model failures. We\ndevelop a typology of epistemic markers and\ninject 50 markers into prompts for question an-\nswering. We find that LMs are highly sensitive\nto epistemic markers in prompts, with accu-\nracies varying more than 80%. Surprisingly,\nwe find that expressions of high certainty re-\nsult in a 7% decrease in accuracy as compared\nto low certainty expressions; similarly, factive\nverbs hurt performance, while evidentials ben-\nefit performance. Our analysis of a popular\npretraining dataset shows that these markers\nof uncertainty are associated with answers on\nquestion-answering websites, while markers of\ncertainty are associated with questions. These\nassociations may suggest that the behavior of\nLMs is based on mimicking observed language\nuse, rather than truly reflecting epistemic un-\ncertainty.\n1",
        "generated_abstract": "The increasing reliance on natural language systems in knowledge-intensive tasks necessitates a deeper understanding of how language models (LMs) interpret human expressions of knowledge, particularly those conveying uncertainty and certainty. This research investigates the impact of injecting both verbal and numerical expressions of uncertainty into prompts for question-answering tasks, utilizing a zero-shot prompting approach across multiple datasets. Contrary to initial hypotheses, the study reveals that LMs are highly sensitive to uncertainty cues, with accuracy varying significantly depending on the epistemic marker used. Surprisingly, expressions of high certainty, such as \"I'm certain it's,\" often lead to lower accuracy compared to expressions of uncertainty. Further analysis indicates that the presence of factive verbs within expressions of certainty contributes to decreased performance. Conversely, the inclusion of evidential markers generally improves accuracy. Examination of pre-training data suggests that LMs may be mirroring human linguistic patterns where certainty is more frequently expressed in questions, while uncertainty is favored in answers. These findings highlight the limitations of current LMs in interpreting nuanced epistemic expressions and underscore the need for improved calibration and contextual understanding in natural language generation. The study contributes a novel typology of uncertainty expressions and offers insights into the potential origins of these unexpected model behaviors."
    },
    {
        "title": "Mirages._On_Anthropomorphism_in_Dialogue_Systems",
        "original_abstract": "Automated dialogue or conversational systems\nare anthropomorphised by developers and per-\nsonified by users. While a degree of anthropo-\nmorphism may be inevitable due to the choice\nof medium, conscious and unconscious design\nchoices can guide users to personify such sys-\ntems to varying degrees. Encouraging users\nto relate to automated systems as if they were\nhuman can lead to high risk scenarios caused\nby over-reliance on their outputs. As a result,\nnatural language processing researchers have\ninvestigated the factors that induce personifi-\ncation and develop resources to mitigate such\neffects. However, these efforts are fragmented,\nand many aspects of anthropomorphism have\nyet to be explored. In this paper, we discuss\nthe linguistic factors that contribute to the an-\nthropomorphism of dialogue systems and the\nharms that can arise, including reinforcing gen-\nder stereotypes and notions of acceptable lan-\nguage. We recommend that future efforts to-\nwards developing dialogue systems take partic-\nular care in their design, development, release,\nand description; and attend to the many linguis-\ntic cues that can elicit personification by users.\n1",
        "generated_abstract": "Automated dialogue systems are increasingly prevalent, raising concerns about anthropomorphism and the potential for users to misinterpret system outputs as human-generated. This paper addresses the risks associated with attributing human characteristics to dialogue systems, ranging from benign personification to potentially harmful reliance on system advice. It argues against gratuitous anthropomorphic features in dialogue system design, drawing upon insights from psychology, linguistics, and human-computer interaction. The paper outlines the psychological mechanisms and linguistic factors that contribute to anthropomorphism, such as self-referential pronoun use and the generation of content that simulates empathy. Consequences of anthropomorphism are discussed, including the potential for increased trust in misinformation, the reinforcement of social stereotypes, and the erasure of marginalized communities. Recommendations are offered to mitigate anthropomorphism, emphasizing the recognition of human tendencies to personify, careful consideration of the appropriateness of anthropomorphic tools, and a reassessment of research goals. The paper concludes by advocating for a more cautious approach to dialogue system design, urging developers to avoid creating \"mirages of humanity\" that could lead to negative societal outcomes."
    },
    {
        "title": "equipping_language_models_with_tool_use_capability_for_tabular_data_analysis_in_finance",
        "original_abstract": "Large language models (LLMs) have exhibited\nan array of reasoning capabilities but face chal-\nlenges like error propagation and hallucination,\nparticularly in specialised areas like finance,\nwhere data is heterogeneous, and precision is\nparamount. We explore the potential of lan-\nguage model augmentation with external tools\nto mitigate these limitations and offload cer-\ntain reasoning steps to external tools that are\nmore suited for the task, instead of solely de-\npending on the LLM’s inherent abilities. More\nconcretely, using financial domain question-\nanswering datasets, we apply supervised fine-\ntuning on a LLAMA-2 13B CHAT model to\nact both as a task router and task solver. The\ntask router dynamically directs a question to\neither be answered internally by the LLM or\nexternally via the right tool from the tool set.\nOur tool-equipped SFT model, RAVEN, demon-\nstrates an improvement of 35.2% and 5.06%\nover the base model and SFT-only baselines,\nrespectively, and is highly competitive with\nstrong GPT-3.5 results.\nTo the best of our\nknowledge, our work is the first that investi-\ngates tool augmentation of language models\nfor the finance domain.1\n1",
        "generated_abstract": "This research addresses the challenge of enhancing Large Language Model (LLM) performance in the finance domain, an area demanding precision and specialized knowledge. The study explores tool augmentation, equipping LLMs with external tools to improve accuracy and reliability in tabular data analysis. A Llama 2 13B Chat model was fine-tuned using parameter-efficient techniques and augmented with a calculator and a SQL engine. The training data consisted of a mixture of financial and generic question-answering datasets, enhanced with instructions for dynamic tool selection.\n\nThe resulting model, RAVEN, demonstrates significant improvements in reasoning over structured data compared to the base model and even outperforms GPT-3.5 on several benchmarks. Results indicate a substantial increase in exact match accuracy on financial datasets, highlighting the advantage of delegating tasks to specialized tools rather than relying solely on the LLM's internal knowledge. The study further analyzes the impact of question complexity and the necessity of tool augmentation for multi-hop reasoning.\n\nThe findings demonstrate the feasibility of effectively equipping LLMs with tool use capabilities through fine-tuning. Augmenting with tools significantly elevates performance, suggesting that this approach is a promising avenue for enhancing LLMs in specialized domains like finance where structured data and numerical reasoning are paramount."
    },
    {
        "title": "multilingual_large_language_models_are_not_(yet)_code-switchers",
        "original_abstract": "Multilingual Large Language Models (LLMs)\nhave recently shown great capabilities in a wide\nrange of tasks, exhibiting state-of-the-art perfor-\nmance through zero-shot or few-shot prompt-\ning methods. While there have been extensive\nstudies on their abilities in monolingual tasks,\nthe investigation of their potential in the context\nof code-switching (CSW), the practice of alter-\nnating languages within an utterance, remains\nrelatively uncharted. In this paper, we provide\na comprehensive empirical analysis of various\nmultilingual LLMs, benchmarking their perfor-\nmance across four tasks: sentiment analysis,\nmachine translation, summarization and word-\nlevel language identification. Our results indi-\ncate that despite multilingual LLMs exhibiting\npromising outcomes in certain tasks using zero\nor few-shot prompting, they still underperform\nin comparison to fine-tuned models of much\nsmaller scales. We argue that current “multilin-\ngualism\" in LLMs does not inherently imply\nproficiency with code-switching texts, calling\nfor future research to bridge this discrepancy.\n1",
        "generated_abstract": "Multilingual Large Language Models (LLMs) have shown promise in various natural language tasks; however, their effectiveness in code-switching scenarios, where speakers intermix languages, remains underexplored. This study investigates the capabilities of existing multilingual LLMs in understanding and processing code-switched text across diverse tasks, languages, model architectures, and prompting methods. The research employs a comprehensive empirical analysis, evaluating models on sentiment analysis, machine translation, summarization, and language identification tasks using publicly available code-switched datasets. The models are assessed through both fine-tuning and prompting strategies, including zero-shot and few-shot learning. Results indicate that while scaling up model size generally improves performance, fine-tuned smaller-scale models significantly outperform even the largest prompted LLMs on code-switching tasks. Publicly available multilingual LLMs demonstrate limited proficiency in code-switching contexts, even with prompting, suggesting that current models do not fully capture the complexities of code-switching. These findings highlight the need for future research to focus on incorporating code-switching objectives during the pre-training and fine-tuning of multilingual LLMs. This work underscores the importance of developing language models that are more inclusive of the linguistic practices of multilingual communities."
    },
    {
        "title": "Harnessing_Black-Box_Control_to_Boost_Commonsense_in_LM’s_Generation",
        "original_abstract": "Large language models (LLMs) such as GPT-3\nhave demonstrated a strong capability to gen-\nerate coherent and contextually relevant text.\nHowever, amidst their successes, a crucial issue\npersists: their generated outputs still lack com-\nmonsense at times. Yet fine-tuning the entire\nLLM towards more commonsensical outputs is\ncomputationally expensive if not infeasible. In\nthis paper, we present a computation-efficient\nframework that steers a frozen Pre-Trained Lan-\nguage Model (PTLM) towards more common-\nsensical generation (i.e., producing a meaning-\nful and plausible output that incorporates a list\nof concepts).\nSpecifically, we first construct a reference-free\nevaluator that assigns a sentence with a com-\nmonsensical score by grounding the sentence\nto a dynamic commonsense knowledge base\nfrom four different relational aspects. We then\nuse the scorer as the oracle for commonsense\nknowledge, and extend the controllable genera-\ntion method called NADO to train an auxiliary\nhead that guides a fixed PTLM to better satisfy\nthe oracle. We test our framework on a series of\nGPT-2-, FLAN-T5- and Alpaca-based language\nmodels (LMs) on two constrained concept-to-\nsentence benchmarks. Human evaluation re-\nsults demonstrate that our method consistently\nleads to the most commonsensical outputs.1\n1",
        "generated_abstract": "Large Pre-trained Language Models (PTLMs) often struggle to generate text that aligns with commonsense knowledge. This paper addresses the challenge of enhancing commonsense reasoning in PTLM-generated sentences, particularly in constrained generation tasks requiring the incorporation of specific concepts. We introduce BOOST, a novel plug-and-play framework that leverages a small auxiliary model to steer a frozen PTLM towards more commonsensical outputs. The framework employs a reference-free commonsense scorer, termed O-Scorer, which extracts tuples of commonsense-related concepts from a sentence and evaluates their plausibility against a dynamic commonsense knowledge base. The O-Scorer's signal is then used to train the auxiliary model, guiding the PTLM during generation. Evaluations on CommonGen and CSK-PN datasets demonstrate that BOOST consistently generates more commonsensical outputs compared to baseline methods, including A*esque Decoding and GeLaTo, across various PTLMs such as GPT-2, Alpaca, and Flan-T5. Notably, the proposed reference-free evaluator achieves performance comparable to reference-based metrics in assessing commonsense. This approach offers a computationally efficient means of improving commonsense reasoning in PTLMs without requiring extensive model fine-tuning."
    },
    {
        "title": "rationaleenhanced_language_models_are_better_continual_relation_learners",
        "original_abstract": "Continual relation extraction (CRE) aims to\nsolve the problem of catastrophic forgetting\nwhen learning a sequence of newly emerging\nrelations. Recent CRE studies have found that\ncatastrophic forgetting arises from the model’s\nlack of robustness against future analogous rela-\ntions. To address the issue, we introduce ratio-\nnale, i.e., the explanations of relation classifica-\ntion results generated by large language models\n(LLM), into CRE task. Specifically, we de-\nsign the multi-task rationale tuning strategy to\nhelp the model learn current relations robustly.\nWe also conduct contrastive rationale replay\nto further distinguish analogous relations. Ex-\nperimental results on two standard benchmarks\ndemonstrate that our method outperforms the\nstate-of-the-art CRE models. Our code is avail-\nable at https://github.com/WeiminXiong/\nRationaleCL\n1",
        "generated_abstract": "Continual relation extraction (CRE) aims to learn new relations between entities while retaining performance on previously learned relations, a task hindered by catastrophic forgetting. This paper investigates the hypothesis that incorporating rationales can enhance CRE models' ability to learn analogous relations, thereby mitigating catastrophic forgetting. A novel approach, RationaleCL, is proposed, which integrates multi-task rationale tuning and contrastive rationale replay into a rehearsal-based framework. Large language models are used to generate rationales, providing explanations for relation classification answers. Multi-task rationale tuning distills rationale knowledge from the large language model into a smaller model, enhancing its reasoning capabilities. Contrastive rationale replay utilizes large language models to differentiate between analogous relations and regenerate corresponding explanations to update the memory. Experiments conducted on FewRel and TACRED datasets demonstrate the effectiveness of RationaleCL, consistently outperforming state-of-the-art CRE models. The results indicate that incorporating rationales improves the model's robustness against analogous relations and mitigates catastrophic forgetting. This work introduces a novel rationale-enhanced framework for CRE, offering a promising direction for future research in continual learning and relation extraction."
    },
    {
        "title": "coordinate_constructions_in_english_enhanced_universal_dependencies_analysis_and_computational_modeling",
        "original_abstract": "In this paper, we address the representation of\ncoordinate constructions in Enhanced Univer-\nsal Dependencies (UD), where relevant depen-\ndency links are propagated from conjunction\nheads to other conjuncts. English treebanks for\nenhanced UD have been created from gold ba-\nsic dependencies using a heuristic rule-based\nconverter, which propagates only core argu-\nments.\nWith the aim of determining which\nset of links should be propagated from a se-\nmantic perspective, we create a large-scale\ndataset of manually edited syntax graphs. We\nidentify several systematic errors in the orig-\ninal data, and propose to also propagate ad-\njuncts. We observe high inter-annotator agree-\nment for this semantic annotation task. Using\nour new manually veriﬁed dataset, we perform\nthe ﬁrst principled comparison of rule-based\nand (partially novel) machine-learning based\nmethods for conjunction propagation for En-\nglish. We show that learning propagation rules\nis more effective than hand-designing heuris-\ntic rules. When using automatic parses, our\nneural graph-parser based edge predictor out-\nperforms the currently predominant pipelines\nusing a basic-layer tree parser plus converters.\n1",
        "generated_abstract": "This paper investigates the correctness and completeness of enhanced Universal Dependencies (UD) representations for coordinate constructions in English, a crucial aspect for natural language understanding. Addressing the limitations of existing rule-based converters that primarily propagate core arguments, this research undertakes a large-scale annotation study on 1,417 sentences from the English Web Treebank containing conjoined verbs. The study identifies instances where adjuncts and other non-core dependents should be propagated based on semantic considerations. The resulting dataset is used to evaluate and improve upon existing methods for enhancing UD representations. Rule-based, Support Vector Machine (SVM)-based, and novel neural network-based conjunction propagation classifiers are compared, revealing that machine learning-based classifiers outperform rule-based converters when operating on gold basic trees. Furthermore, a neural graph-parser is proposed as a superior alternative to pipeline approaches for automatic parsing, demonstrating substantial improvements in propagating links within conjunctions. The findings highlight the importance of semantic considerations in conjunction propagation and demonstrate the effectiveness of graph-based parsing for generating enhanced UD representations. The manually curated dataset and model implementations are publicly available, contributing to ongoing efforts to improve UD treebanks."
    },
    {
        "title": "ViSoBERT__A_Pre-Trained_Language_Model_for_Vietnamese_Social_Media_Text_Processing",
        "original_abstract": "English and Chinese, known as resource-rich\nlanguages, have witnessed the strong devel-\nopment of transformer-based language mod-\nels for natural language processing tasks. Al-\nthough Vietnam has approximately 100M peo-\nple speaking Vietnamese, several pre-trained\nmodels, e.g., PhoBERT, ViBERT, and vELEC-\nTRA, performed well on general Vietnamese\nNLP tasks, including POS tagging and named\nentity recognition. These pre-trained language\nmodels are still limited to Vietnamese social\nmedia tasks.\nIn this paper, we present the\nfirst monolingual pre-trained language model\nfor Vietnamese social media texts, ViSoBERT,\nwhich is pre-trained on a large-scale corpus of\nhigh-quality and diverse Vietnamese social me-\ndia texts using XLM-R architecture. Moreover,\nwe explored our pre-trained model on five im-\nportant natural language downstream tasks on\nVietnamese social media texts: emotion recog-\nnition, hate speech detection, sentiment anal-\nysis, spam reviews detection, and hate speech\nspans detection. Our experiments demonstrate\nthat ViSoBERT, with far fewer parameters, sur-\npasses the previous state-of-the-art models on\nmultiple Vietnamese social media tasks. Our\nViSoBERT model is available4 only for re-\nsearch purposes.\nDisclaimer: This paper contains actual com-\nments on social networks that might be con-\nstrued as abusive, offensive, or obscene.\n1",
        "generated_abstract": "This paper addresses the challenge of processing Vietnamese social media text, which exhibits unique linguistic characteristics not fully captured by existing pre-trained language models (PLMs) trained on general corpora. To overcome this limitation, a novel pre-trained language model, ViSoBERT, was developed specifically for Vietnamese social media. ViSoBERT is based on the transformer architecture and pre-trained on a large-scale dataset of Vietnamese posts and comments extracted from popular social media platforms. A custom tokenizer was also built using SentencePiece to effectively handle the nuances of social media language, including emojis and teencode. The model's performance was evaluated on five downstream tasks: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. ViSoBERT demonstrates state-of-the-art performance on these tasks compared to existing monolingual and multilingual PLMs, including PhoBERT, XLM-R, and TwHIN-BERT. Ablation studies were conducted to assess the impact of masking rate and social media characteristics on model performance. Results confirm the effectiveness of ViSoBERT in capturing the unique characteristics of Vietnamese social media text, highlighting the importance of domain-specific pre-training and custom tokenization for this task. ViSoBERT provides a strong baseline for future research in Vietnamese social media text processing and its applications."
    }
]