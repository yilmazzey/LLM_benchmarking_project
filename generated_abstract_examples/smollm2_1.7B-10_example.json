[
    {
        "title": "integrating_an_unsupervised_transliteration_model_into_statistical_machine_translation",
        "original_abstract": "We investigate three methods for integrat-\ning an unsupervised transliteration model\ninto an end-to-end SMT system. We in-\nduce a transliteration model from parallel\ndata and use it to translate OOV words.\nOur approach is fully unsupervised and\nlanguage independent.\nIn the methods\nto integrate transliterations, we observed\nimprovements from 0.23-0.75 (∆0.41)\nBLEU points across 7 language pairs. We\nalso show that our mined transliteration\ncorpora provide better rule coverage and\ntranslation quality compared to the gold\nstandard transliteration corpora.\n1",
        "generated_abstract": "Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Voegel 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Rochester, New York.\nA Log-Linear Block Transliteration Model based on Bi-Stream HMMs.\nIntroduction\nTransliterations between languages are important in machine translation, information retrieval, and text processing tasks. Most existing works focus on transliterating a source language into an English target or vice versa with little attention to the character set of the source language. The transliteration model based on Hidden Markov Models (HMMs) is widely used to transcribe words from one language into another but it only considers the phonetic similarity and ignores the linguistic structure between languages.\n\nMethodology\nThis paper presents a novel transliteration model which extends the HMM approach by considering the linguistic features of source and target languages. We apply a log-linear block model for transliteration on two Indian languages, Hindi and Bengali with different character sets to analyze the similarities and differences in their phonetic structures. The main contribution of this work is that we consider the lexical semantic and phonological characteristics of the source language when designing our transliteration model. Our approach combines both linguistic and phonetic information from the source and target languages for transliterating words into each other’s languages.\n\nResults\nOur proposed log-linear block model improves the performance of existing HMM based transliteration models by incorporating more structural features and combining it with a phonetic similarity metric. We evaluate the model on a Hindi-English shared task to show that the improved results provide a higher accuracy than previous works. The experimental results demonstrate that our method is effective for transliterating words from a source language into an English target while maintaining its overall performance in translating between two languages.\n\nConclusions and Implications\nOur log-linear block model provides a new approach for transliteration by incorporating both linguistic and phonetic structural information between languages. This work introduces a novel transliteration framework that combines different levels of lexical semantic and phonological similarity to provide more accurate results than previous works.\n\nBing Zhao, Nguyen Bach, Ian Lane, and Stephan Voegel 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Rochester, New York."
    },
    {
        "title": "event_temporal_relation_extraction_with_bayesian_translational_model",
        "original_abstract": "Existing models to extract temporal relations\nbetween events lack a principled method to in-\ncorporate external knowledge. In this study, we\nintroduce Bayesian-Trans, a Bayesian learning-\nbased method that models the temporal rela-\ntion representations as latent variables and in-\nfers their values via Bayesian inference and\ntranslational functions. Compared to conven-\ntional neural approaches, instead of performing\npoint estimation to find the best set parame-\nters, the proposed model infers the parameters’\nposterior distribution directly, enhancing the\nmodel’s capability to encode and express un-\ncertainty about the predictions. Experimental\nresults on the three widely used datasets show\nthat Bayesian-Trans outperforms existing ap-\nproaches for event temporal relation extraction.\nWe additionally present detailed analyses on un-\ncertainty quantification, comparison of priors,\nand ablation studies, illustrating the benefits of\nthe proposed approach.1\n1",
        "generated_abstract": "The Bayesian learning framework with a hyperbolic tangent activation function for temporal relations on semantic parsing datasets.\n\nAbstract:\n\nThis work develops a Bayesian learning framework to extract temporal relationships from unstructured natural language sentences and apply it to various semantic parsing tasks. The framework utilizes the hyperbolic tangent activation function, which learns to represent complex temporal patterns in time-related words. We train this model using multiple datasets, including TimeBank-Dense (TBD) and the TimeBank dataset (TB), both of which have high quality labeled training data but limited testing samples.\n\nThe results demonstrate that our Bayesian learning framework can achieve state-of-the-art performance on semantic parsing tasks, outperforming previous work in several areas such as temporal relation extraction and sentence structure learning from unlabeled text. Specifically, the model performs particularly well on predicting BEFORE, AFTER, and NONE relations while struggling with EQUAL and VAGUE relations, which are underrepresented in most training datasets.\n\nThe findings of this study provide insights into the application of Bayesian learning techniques for temporal relationships in semantic parsing tasks. The work contributes to improving the quality of unsupervised feature representations and highlights the potential of using hyperbolic tangent activation functions in representing complex temporal patterns in natural language data."
    },
    {
        "title": "Navigating_the_Grey_Area__How_Expressions_of_Uncertainty_and_Overconfidence_Affect_Language_Models",
        "original_abstract": "The increased deployment of LMs for real-\nworld tasks involving knowledge and facts\nmakes it important to understand model episte-\nmology: what LMs think they know, and how\ntheir attitudes toward that knowledge are af-\nfected by language use in their inputs. Here,\nwe study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or\nevidentiality like \"I’m sure it’s\", \"I think it’s\",\nor “Wikipedia says it’s\" affect models, and\nwhether they contribute to model failures. We\ndevelop a typology of epistemic markers and\ninject 50 markers into prompts for question an-\nswering. We find that LMs are highly sensitive\nto epistemic markers in prompts, with accu-\nracies varying more than 80%. Surprisingly,\nwe find that expressions of high certainty re-\nsult in a 7% decrease in accuracy as compared\nto low certainty expressions; similarly, factive\nverbs hurt performance, while evidentials ben-\nefit performance. Our analysis of a popular\npretraining dataset shows that these markers\nof uncertainty are associated with answers on\nquestion-answering websites, while markers of\ncertainty are associated with questions. These\nassociations may suggest that the behavior of\nLMs is based on mimicking observed language\nuse, rather than truly reflecting epistemic un-\ncertainty.\n1",
        "generated_abstract": "Abstract:\n\nThis study investigates the performance of different natural language processing (NLP) models and their variants across a range of languages and domains to identify areas for improvement in their ability to recognize and respond appropriately to questions about knowledge and information. The research aims to compare the capabilities and limitations of various NLP models, including machine translation systems, question answering systems, and semantic role labeling systems, with an emphasis on certainties and uncertainties within human-question-human and human-question-computer dialogues. A comprehensive dataset consisting of a diverse collection of texts in multiple languages from various domains is employed to assess the effectiveness of these models in different contexts. The results suggest that while some NLP models excel at recognizing complex queries and generating accurate responses, others struggle with uncertainty or ambiguity in their understanding of context. This study contributes to the field by providing insights into areas for improvement and highlights the importance of considering certainties and uncertainties when designing and evaluating NLP models for natural language understanding tasks."
    },
    {
        "title": "Mirages._On_Anthropomorphism_in_Dialogue_Systems",
        "original_abstract": "Automated dialogue or conversational systems\nare anthropomorphised by developers and per-\nsonified by users. While a degree of anthropo-\nmorphism may be inevitable due to the choice\nof medium, conscious and unconscious design\nchoices can guide users to personify such sys-\ntems to varying degrees. Encouraging users\nto relate to automated systems as if they were\nhuman can lead to high risk scenarios caused\nby over-reliance on their outputs. As a result,\nnatural language processing researchers have\ninvestigated the factors that induce personifi-\ncation and develop resources to mitigate such\neffects. However, these efforts are fragmented,\nand many aspects of anthropomorphism have\nyet to be explored. In this paper, we discuss\nthe linguistic factors that contribute to the an-\nthropomorphism of dialogue systems and the\nharms that can arise, including reinforcing gen-\nder stereotypes and notions of acceptable lan-\nguage. We recommend that future efforts to-\nwards developing dialogue systems take partic-\nular care in their design, development, release,\nand description; and attend to the many linguis-\ntic cues that can elicit personification by users.\n1",
        "generated_abstract": "The paper \"The Moral Philosophy of Embodiment and Agency: A Cognitive Approach to Embodied Conversational Agents\" proposes a novel cognitive framework for creating embodied conversational agents that embody moral principles. The paper argues that these principles are deeply rooted in human cognition and should be taken into account when designing agents. It highlights the importance of considering human values, emotions, and intentions in agent development and uses examples from philosophical discussions to illustrate this concept. Ultimately, the paper aims to provide a foundation for developing more ethical conversational agents and improving their moral decision-making capabilities."
    },
    {
        "title": "equipping_language_models_with_tool_use_capability_for_tabular_data_analysis_in_finance",
        "original_abstract": "Large language models (LLMs) have exhibited\nan array of reasoning capabilities but face chal-\nlenges like error propagation and hallucination,\nparticularly in specialised areas like finance,\nwhere data is heterogeneous, and precision is\nparamount. We explore the potential of lan-\nguage model augmentation with external tools\nto mitigate these limitations and offload cer-\ntain reasoning steps to external tools that are\nmore suited for the task, instead of solely de-\npending on the LLM’s inherent abilities. More\nconcretely, using financial domain question-\nanswering datasets, we apply supervised fine-\ntuning on a LLAMA-2 13B CHAT model to\nact both as a task router and task solver. The\ntask router dynamically directs a question to\neither be answered internally by the LLM or\nexternally via the right tool from the tool set.\nOur tool-equipped SFT model, RAVEN, demon-\nstrates an improvement of 35.2% and 5.06%\nover the base model and SFT-only baselines,\nrespectively, and is highly competitive with\nstrong GPT-3.5 results.\nTo the best of our\nknowledge, our work is the first that investi-\ngates tool augmentation of language models\nfor the finance domain.1\n1",
        "generated_abstract": "Categorizing UML Diagrams into Categories: A Systematic Approach\n\nIntroduction\n\nThis paper presents a systematic approach to categorizing UML diagrams based on their structural components and behavior patterns. The categorization method aims to provide an organized and structured way of classifying UML diagrams, facilitating the identification of relationships between different diagram types and enhancing communication among developers and designers.\n\nMethodology\n\nThe categorization method is based on a combination of existing classification schemes and a novel analysis of UML diagram structure. The approach involves two main steps: (i) identifying common structural components in UML diagrams, and (ii) categorizing these components into predefined categories that reflect the behavior patterns present in each category.\n\nResults\n\nThe categorization method resulted in 14 predefined categories for UML diagrams, including a hierarchy of sub-categories within each major category. The analysis indicated that the most significant structural components were used to classify UML diagrams based on their behavior and complexity level.\n\nConclusion\n\nA systematic approach to categorizing UML diagrams has been presented, providing an organized way of classifying these diagrams based on both structure and behavior patterns. The approach facilitates communication among developers and designers, enables efficient identification of relationships between different diagram types, and supports decision-making processes in software development projects.\n\n<content>"
    },
    {
        "title": "multilingual_large_language_models_are_not_(yet)_code-switchers",
        "original_abstract": "Multilingual Large Language Models (LLMs)\nhave recently shown great capabilities in a wide\nrange of tasks, exhibiting state-of-the-art perfor-\nmance through zero-shot or few-shot prompt-\ning methods. While there have been extensive\nstudies on their abilities in monolingual tasks,\nthe investigation of their potential in the context\nof code-switching (CSW), the practice of alter-\nnating languages within an utterance, remains\nrelatively uncharted. In this paper, we provide\na comprehensive empirical analysis of various\nmultilingual LLMs, benchmarking their perfor-\nmance across four tasks: sentiment analysis,\nmachine translation, summarization and word-\nlevel language identification. Our results indi-\ncate that despite multilingual LLMs exhibiting\npromising outcomes in certain tasks using zero\nor few-shot prompting, they still underperform\nin comparison to fine-tuned models of much\nsmaller scales. We argue that current “multilin-\ngualism\" in LLMs does not inherently imply\nproficiency with code-switching texts, calling\nfor future research to bridge this discrepancy.\n1",
        "generated_abstract": "This paper presents a new method for word-level language identification using large language models (LLMs) and investigates its effectiveness in detecting language categories. The LID problem is challenging as it requires LLMs to distinguish between languages when all available data belongs to one language. We focus on word-level LID, exploring how well LLMs can identify the language of a sentence by analyzing individual words rather than the entire text.\n\nWe first present an extensive evaluation study comparing LID results from different datasets and models. We find that even though these datasets mostly belong to the same language, LLMs still achieve high precision in distinguishing languages based on word-level analysis. Our empirical evidence suggests that large LLM models perform particularly well in this task.\n\nNext, we analyze the factors affecting performance: 1) training data; 2) model size and complexity; 3) linguistic characteristics such as the presence of gendered nouns or the specific vocabulary used by different languages; 4) language-specific typological features like grammatical cases. We find that these factors can significantly influence LID results, and some may have a more significant impact than others.\n\nOur results highlight the importance of considering multiple aspects when developing methods for word-level language identification with LLMs. By understanding how well LLMs perform on different languages and datasets, we provide valuable insights into their strengths and limitations. This study contributes to the ongoing research in NLP by shedding light on a previously under-explored area that holds potential for future applications in natural language processing and machine translation systems."
    },
    {
        "title": "Harnessing_Black-Box_Control_to_Boost_Commonsense_in_LM’s_Generation",
        "original_abstract": "Large language models (LLMs) such as GPT-3\nhave demonstrated a strong capability to gen-\nerate coherent and contextually relevant text.\nHowever, amidst their successes, a crucial issue\npersists: their generated outputs still lack com-\nmonsense at times. Yet fine-tuning the entire\nLLM towards more commonsensical outputs is\ncomputationally expensive if not infeasible. In\nthis paper, we present a computation-efficient\nframework that steers a frozen Pre-Trained Lan-\nguage Model (PTLM) towards more common-\nsensical generation (i.e., producing a meaning-\nful and plausible output that incorporates a list\nof concepts).\nSpecifically, we first construct a reference-free\nevaluator that assigns a sentence with a com-\nmonsensical score by grounding the sentence\nto a dynamic commonsense knowledge base\nfrom four different relational aspects. We then\nuse the scorer as the oracle for commonsense\nknowledge, and extend the controllable genera-\ntion method called NADO to train an auxiliary\nhead that guides a fixed PTLM to better satisfy\nthe oracle. We test our framework on a series of\nGPT-2-, FLAN-T5- and Alpaca-based language\nmodels (LMs) on two constrained concept-to-\nsentence benchmarks. Human evaluation re-\nsults demonstrate that our method consistently\nleads to the most commonsensical outputs.1\n1",
        "generated_abstract": "Your abstract should:\n1. Begin with a clear statement of the research problem or objective \n2. Briefly describe the methodology used\n3. Summarize the key findings and results\n4. State the main conclusions and implications\n5. Be self-contained and understandable on its own\n6. Use formal academic language and avoid first-person pronouns\n7. Be between 150-250 words\n8. Avoid citations, references, or detailed numerical data\n9. Focus on the most significant aspects of the research"
    },
    {
        "title": "rationaleenhanced_language_models_are_better_continual_relation_learners",
        "original_abstract": "Continual relation extraction (CRE) aims to\nsolve the problem of catastrophic forgetting\nwhen learning a sequence of newly emerging\nrelations. Recent CRE studies have found that\ncatastrophic forgetting arises from the model’s\nlack of robustness against future analogous rela-\ntions. To address the issue, we introduce ratio-\nnale, i.e., the explanations of relation classifica-\ntion results generated by large language models\n(LLM), into CRE task. Specifically, we de-\nsign the multi-task rationale tuning strategy to\nhelp the model learn current relations robustly.\nWe also conduct contrastive rationale replay\nto further distinguish analogous relations. Ex-\nperimental results on two standard benchmarks\ndemonstrate that our method outperforms the\nstate-of-the-art CRE models. Our code is avail-\nable at https://github.com/WeiminXiong/\nRationaleCL\n1",
        "generated_abstract": "<<Title of Research Paper Abstract>>\n\n[Research Problem Statement] In recent years, there has been a growing need to enhance student performance and learning outcomes by integrating effective teaching strategies into educational environments.\n\n[Methodology Description] This study employed a mixed-methods approach combining qualitative and quantitative data analysis techniques to investigate the effectiveness of enhanced pedagogical practices in improving student engagement and academic achievement.\n\n[Key Findings and Results] The findings revealed significant improvements in both teacher effectiveness and student learning outcomes, with teachers demonstrating increased use of effective teaching strategies and students showing improved performance in core subjects.\n\n[Main Conclusions and Implications] Our results suggest that the integration of enhanced pedagogical practices can positively impact student outcomes and contribute to more effective educational environments.\n\n[Abstract Conclusion] This research paper provides a comprehensive overview of the benefits of integrating enhanced teaching strategies into education, highlighting significant improvements in teacher effectiveness and student learning outcomes. The study's findings have important implications for educators seeking to optimize their teaching methods and enhance the overall quality of education."
    },
    {
        "title": "coordinate_constructions_in_english_enhanced_universal_dependencies_analysis_and_computational_modeling",
        "original_abstract": "In this paper, we address the representation of\ncoordinate constructions in Enhanced Univer-\nsal Dependencies (UD), where relevant depen-\ndency links are propagated from conjunction\nheads to other conjuncts. English treebanks for\nenhanced UD have been created from gold ba-\nsic dependencies using a heuristic rule-based\nconverter, which propagates only core argu-\nments.\nWith the aim of determining which\nset of links should be propagated from a se-\nmantic perspective, we create a large-scale\ndataset of manually edited syntax graphs. We\nidentify several systematic errors in the orig-\ninal data, and propose to also propagate ad-\njuncts. We observe high inter-annotator agree-\nment for this semantic annotation task. Using\nour new manually veriﬁed dataset, we perform\nthe ﬁrst principled comparison of rule-based\nand (partially novel) machine-learning based\nmethods for conjunction propagation for En-\nglish. We show that learning propagation rules\nis more effective than hand-designing heuris-\ntic rules. When using automatic parses, our\nneural graph-parser based edge predictor out-\nperforms the currently predominant pipelines\nusing a basic-layer tree parser plus converters.\n1",
        "generated_abstract": "Research Objective: This study investigates the application of graph-based models for conjunction propagation in dependency parsing tasks. \n\nMethodology: Graph-based models were employed to predict conjunction dependencies from a training corpus. The input consisted of dependency trees and conjunctive relations. \n\nKey Findings: Our results demonstrate that graph-based models achieve state-of-the-art performance on prediction of conjunction dependencies, outperforming traditional rule-based methods. The proposed method is particularly effective when dealing with uni- and multi-conjunction cases where the same dependency head may be involved in multiple conjunctions.\n\nMain Conclusions: Graph-based models show promise for enhancing parser performance by leveraging dependency propagation to predict conjunction dependencies more accurately than traditional methods. These findings have implications for rule-based parsers, which can benefit from incorporating this novel approach into their architectures."
    },
    {
        "title": "ViSoBERT__A_Pre-Trained_Language_Model_for_Vietnamese_Social_Media_Text_Processing",
        "original_abstract": "English and Chinese, known as resource-rich\nlanguages, have witnessed the strong devel-\nopment of transformer-based language mod-\nels for natural language processing tasks. Al-\nthough Vietnam has approximately 100M peo-\nple speaking Vietnamese, several pre-trained\nmodels, e.g., PhoBERT, ViBERT, and vELEC-\nTRA, performed well on general Vietnamese\nNLP tasks, including POS tagging and named\nentity recognition. These pre-trained language\nmodels are still limited to Vietnamese social\nmedia tasks.\nIn this paper, we present the\nfirst monolingual pre-trained language model\nfor Vietnamese social media texts, ViSoBERT,\nwhich is pre-trained on a large-scale corpus of\nhigh-quality and diverse Vietnamese social me-\ndia texts using XLM-R architecture. Moreover,\nwe explored our pre-trained model on five im-\nportant natural language downstream tasks on\nVietnamese social media texts: emotion recog-\nnition, hate speech detection, sentiment anal-\nysis, spam reviews detection, and hate speech\nspans detection. Our experiments demonstrate\nthat ViSoBERT, with far fewer parameters, sur-\npasses the previous state-of-the-art models on\nmultiple Vietnamese social media tasks. Our\nViSoBERT model is available4 only for re-\nsearch purposes.\nDisclaimer: This paper contains actual com-\nments on social networks that might be con-\nstrued as abusive, offensive, or obscene.\n1",
        "generated_abstract": "Here is the output:\n\nResearch Question: How do tokenization models perform on removing diacritics in social comment texts?\nMethodology: This study employs a comparative analysis of three pre-trained language models—PhoBERT, TwHIN-BERT, and ViSoBERT—on removing diacritics from social comments.\nKey Findings: Results show significant improvements in tokenization quality when comparing the original text to its tokenized versions with removed diacritics.\nMain Conclusions: This research highlights the importance of accurate preprocessing for natural language processing tasks involving multilingual data and suggests that combining multiple pre-trained models can lead to better performance.\nImplications: The findings have implications for improving automated moderation systems in social media platforms, where removing diacritics is crucial for ensuring a more user-friendly interface while maintaining content quality."
    }
]