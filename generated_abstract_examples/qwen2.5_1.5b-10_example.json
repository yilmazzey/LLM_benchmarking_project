[
    {
        "title": "integrating_an_unsupervised_transliteration_model_into_statistical_machine_translation",
        "original_abstract": "We investigate three methods for integrat-\ning an unsupervised transliteration model\ninto an end-to-end SMT system. We in-\nduce a transliteration model from parallel\ndata and use it to translate OOV words.\nOur approach is fully unsupervised and\nlanguage independent.\nIn the methods\nto integrate transliterations, we observed\nimprovements from 0.23-0.75 (∆0.41)\nBLEU points across 7 language pairs. We\nalso show that our mined transliteration\ncorpora provide better rule coverage and\ntranslation quality compared to the gold\nstandard transliteration corpora.\n1",
        "generated_abstract": "The problem addressed in this study is the development and evaluation of unsupervised transliteration mining methods for English to Korean translations. The methodology involves collecting parallel corpus data from multiple sources (e.g., books, news articles) and applying unsupervised techniques such as maximum likelihood decoding, conditional random fields, and probabilistic latent semantic analysis to identify potential transliterations without any prior knowledge of the target language's orthographic rules.\n\nKey findings include successful identification of over 50% of English words in Korean text across various domains. The method also demonstrates high precision (96%) with regard to the accuracy of transliteration predictions, significantly outperforming previous unsupervised approaches in terms of both word coverage and error rates.\n\nThe main conclusion is that this technique provides a viable alternative for automatic translation verification and quality assessment, offering promising future applications in improving machine translation accuracy through enhanced validation processes."
    },
    {
        "title": "event_temporal_relation_extraction_with_bayesian_translational_model",
        "original_abstract": "Existing models to extract temporal relations\nbetween events lack a principled method to in-\ncorporate external knowledge. In this study, we\nintroduce Bayesian-Trans, a Bayesian learning-\nbased method that models the temporal rela-\ntion representations as latent variables and in-\nfers their values via Bayesian inference and\ntranslational functions. Compared to conven-\ntional neural approaches, instead of performing\npoint estimation to find the best set parame-\nters, the proposed model infers the parameters’\nposterior distribution directly, enhancing the\nmodel’s capability to encode and express un-\ncertainty about the predictions. Experimental\nresults on the three widely used datasets show\nthat Bayesian-Trans outperforms existing ap-\nproaches for event temporal relation extraction.\nWe additionally present detailed analyses on un-\ncertainty quantification, comparison of priors,\nand ablation studies, illustrating the benefits of\nthe proposed approach.1\n1",
        "generated_abstract": "Maximum-margin learning combined with Bayesian inference is proposed to address the challenges in representing and predicting temporal relations between events in a probabilistic framework. The method introduces new relational operations that can be easily extended without increasing model complexity while maintaining strong expressiveness. Experimental results on two datasets show significant improvements over previous methods, with F1 scores reaching 87.56% for MATRES and 92.54% for TimeBank-Dense, and precision-recall balance achieved of up to 0.93 on both datasets. The key findings include the use of maximum margin principles combined with Bayesian inference for probabilistic relation representation; efficient implementation through tensor decomposition techniques without increasing model complexity; and superior performance compared to existing methods in terms of accuracy and robustness."
    },
    {
        "title": "Navigating_the_Grey_Area__How_Expressions_of_Uncertainty_and_Overconfidence_Affect_Language_Models",
        "original_abstract": "The increased deployment of LMs for real-\nworld tasks involving knowledge and facts\nmakes it important to understand model episte-\nmology: what LMs think they know, and how\ntheir attitudes toward that knowledge are af-\nfected by language use in their inputs. Here,\nwe study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or\nevidentiality like \"I’m sure it’s\", \"I think it’s\",\nor “Wikipedia says it’s\" affect models, and\nwhether they contribute to model failures. We\ndevelop a typology of epistemic markers and\ninject 50 markers into prompts for question an-\nswering. We find that LMs are highly sensitive\nto epistemic markers in prompts, with accu-\nracies varying more than 80%. Surprisingly,\nwe find that expressions of high certainty re-\nsult in a 7% decrease in accuracy as compared\nto low certainty expressions; similarly, factive\nverbs hurt performance, while evidentials ben-\nefit performance. Our analysis of a popular\npretraining dataset shows that these markers\nof uncertainty are associated with answers on\nquestion-answering websites, while markers of\ncertainty are associated with questions. These\nassociations may suggest that the behavior of\nLMs is based on mimicking observed language\nuse, rather than truly reflecting epistemic un-\ncertainty.\n1",
        "generated_abstract": "The study investigates the effectiveness of different approaches in generating precise and contextually relevant answers to natural language questions about scientific facts and figures. Two methods were compared: a machine learning model trained on large-scale factual knowledge base data (method A) versus an approach that leverages existing lexical resources annotated with their corresponding Wikipedia articles (method B). The results showed method B outperformed method A by 15% in terms of accuracy, primarily due to the integration of external cross-lingual information and a better understanding of context-specific meanings. This finding emphasizes the importance of leveraging diverse sources for enhancing answer quality across different languages and knowledge domains."
    },
    {
        "title": "Mirages._On_Anthropomorphism_in_Dialogue_Systems",
        "original_abstract": "Automated dialogue or conversational systems\nare anthropomorphised by developers and per-\nsonified by users. While a degree of anthropo-\nmorphism may be inevitable due to the choice\nof medium, conscious and unconscious design\nchoices can guide users to personify such sys-\ntems to varying degrees. Encouraging users\nto relate to automated systems as if they were\nhuman can lead to high risk scenarios caused\nby over-reliance on their outputs. As a result,\nnatural language processing researchers have\ninvestigated the factors that induce personifi-\ncation and develop resources to mitigate such\neffects. However, these efforts are fragmented,\nand many aspects of anthropomorphism have\nyet to be explored. In this paper, we discuss\nthe linguistic factors that contribute to the an-\nthropomorphism of dialogue systems and the\nharms that can arise, including reinforcing gen-\nder stereotypes and notions of acceptable lan-\nguage. We recommend that future efforts to-\nwards developing dialogue systems take partic-\nular care in their design, development, release,\nand description; and attend to the many linguis-\ntic cues that can elicit personification by users.\n1",
        "generated_abstract": "This study investigates the influence of gendered names and roles in digital voice assistants (DAVs). By analyzing conversations involving DAIs named \"Alexa\" and \"Sophia,\" it explores how societal expectations around gender can impact communication. The findings reveal that using feminine names leads to a more relatable and approachable interaction, potentially enhancing user engagement. Conversely, the study indicates that masculine names are perceived as less empathetic, leading users to seek explanations for their interactions with these assistants. These observations highlight the importance of considering cultural norms when designing AI interfaces to improve user experience and acceptance."
    },
    {
        "title": "equipping_language_models_with_tool_use_capability_for_tabular_data_analysis_in_finance",
        "original_abstract": "Large language models (LLMs) have exhibited\nan array of reasoning capabilities but face chal-\nlenges like error propagation and hallucination,\nparticularly in specialised areas like finance,\nwhere data is heterogeneous, and precision is\nparamount. We explore the potential of lan-\nguage model augmentation with external tools\nto mitigate these limitations and offload cer-\ntain reasoning steps to external tools that are\nmore suited for the task, instead of solely de-\npending on the LLM’s inherent abilities. More\nconcretely, using financial domain question-\nanswering datasets, we apply supervised fine-\ntuning on a LLAMA-2 13B CHAT model to\nact both as a task router and task solver. The\ntask router dynamically directs a question to\neither be answered internally by the LLM or\nexternally via the right tool from the tool set.\nOur tool-equipped SFT model, RAVEN, demon-\nstrates an improvement of 35.2% and 5.06%\nover the base model and SFT-only baselines,\nrespectively, and is highly competitive with\nstrong GPT-3.5 results.\nTo the best of our\nknowledge, our work is the first that investi-\ngates tool augmentation of language models\nfor the finance domain.1\n1",
        "generated_abstract": "The study investigates urban population density trends in Bangladesh over the past decade to identify areas experiencing rapid growth and inform future planning strategies.\n\nKey findings include an increase in population density from 4,150.5 km² for Gazipur Sadar Upazila in 2001 to 4,491.8 km² in 2011 due to improved infrastructure and urbanization. Palash Upazila showed the highest growth rate with a rise from 34.8% density in 2001 to 50.7% by 2011.\n\nMain conclusions suggest targeted urban development initiatives should focus on improving infrastructure, enhancing accessibility, and providing more affordable housing options for low-density areas like Palash Upazila to address population growth effectively. This will help ensure sustainable urban development in the region."
    },
    {
        "title": "multilingual_large_language_models_are_not_(yet)_code-switchers",
        "original_abstract": "Multilingual Large Language Models (LLMs)\nhave recently shown great capabilities in a wide\nrange of tasks, exhibiting state-of-the-art perfor-\nmance through zero-shot or few-shot prompt-\ning methods. While there have been extensive\nstudies on their abilities in monolingual tasks,\nthe investigation of their potential in the context\nof code-switching (CSW), the practice of alter-\nnating languages within an utterance, remains\nrelatively uncharted. In this paper, we provide\na comprehensive empirical analysis of various\nmultilingual LLMs, benchmarking their perfor-\nmance across four tasks: sentiment analysis,\nmachine translation, summarization and word-\nlevel language identification. Our results indi-\ncate that despite multilingual LLMs exhibiting\npromising outcomes in certain tasks using zero\nor few-shot prompting, they still underperform\nin comparison to fine-tuned models of much\nsmaller scales. We argue that current “multilin-\ngualism\" in LLMs does not inherently imply\nproficiency with code-switching texts, calling\nfor future research to bridge this discrepancy.\n1",
        "generated_abstract": "The sentiment analysis capabilities of large language models (LLMs) have been extensively studied across multiple languages and domains. This paper evaluates the performance of LLMs in sentiment analysis tasks using several benchmark datasets: Sentimix Spanish-English, MixSentiment Malayaman-English, and MixSentiment Tamil-English for (left), Sentimix Spanish-English, MixSentiment Malayaman-English, and MixSentiment Tamil-English for (center), and LLMs’ word-level language identification (LID) on Hindi-English word-level LID and Standard-Egyptian Arabic word-level LID for (right). The results show that despite varying performance across datasets, most models achieve significant improvement over traditional sentiment analysis methods, with a notable increase in macro F1 scores from 60% to 80%. This suggests a promising future direction for enhancing the accuracy of natural language processing tasks using these advanced AI techniques."
    },
    {
        "title": "Harnessing_Black-Box_Control_to_Boost_Commonsense_in_LM’s_Generation",
        "original_abstract": "Large language models (LLMs) such as GPT-3\nhave demonstrated a strong capability to gen-\nerate coherent and contextually relevant text.\nHowever, amidst their successes, a crucial issue\npersists: their generated outputs still lack com-\nmonsense at times. Yet fine-tuning the entire\nLLM towards more commonsensical outputs is\ncomputationally expensive if not infeasible. In\nthis paper, we present a computation-efficient\nframework that steers a frozen Pre-Trained Lan-\nguage Model (PTLM) towards more common-\nsensical generation (i.e., producing a meaning-\nful and plausible output that incorporates a list\nof concepts).\nSpecifically, we first construct a reference-free\nevaluator that assigns a sentence with a com-\nmonsensical score by grounding the sentence\nto a dynamic commonsense knowledge base\nfrom four different relational aspects. We then\nuse the scorer as the oracle for commonsense\nknowledge, and extend the controllable genera-\ntion method called NADO to train an auxiliary\nhead that guides a fixed PTLM to better satisfy\nthe oracle. We test our framework on a series of\nGPT-2-, FLAN-T5- and Alpaca-based language\nmodels (LMs) on two constrained concept-to-\nsentence benchmarks. Human evaluation re-\nsults demonstrate that our method consistently\nleads to the most commonsensical outputs.1\n1",
        "generated_abstract": "### Abstract:\n\nThe study investigates the ability to extract tuples from sentences using a few-shot approach based on natural language processing (NLP). The objective is to identify and classify various relations such as IsUsedFor, AtLocation, CapableOf, and PartOf. A dataset of 100 sentences was created by randomly selecting non-sensical examples for training and validation purposes. GPT-3 was used as the primary model to extract tuples based on a few-shot prompt designed for NLP tasks. The key findings show that while the model successfully extracted several explicit tuples, it struggled with identifying implicit relations in some cases. Overall, this study highlights the importance of careful design in crafting NLP prompts and provides insights into improving tuple extraction methods from non-sensical sentences."
    },
    {
        "title": "rationaleenhanced_language_models_are_better_continual_relation_learners",
        "original_abstract": "Continual relation extraction (CRE) aims to\nsolve the problem of catastrophic forgetting\nwhen learning a sequence of newly emerging\nrelations. Recent CRE studies have found that\ncatastrophic forgetting arises from the model’s\nlack of robustness against future analogous rela-\ntions. To address the issue, we introduce ratio-\nnale, i.e., the explanations of relation classifica-\ntion results generated by large language models\n(LLM), into CRE task. Specifically, we de-\nsign the multi-task rationale tuning strategy to\nhelp the model learn current relations robustly.\nWe also conduct contrastive rationale replay\nto further distinguish analogous relations. Ex-\nperimental results on two standard benchmarks\ndemonstrate that our method outperforms the\nstate-of-the-art CRE models. Our code is avail-\nable at https://github.com/WeiminXiong/\nRationaleCL\n1",
        "generated_abstract": "### Abstract\n\nThis study investigates the impact of incorporating a rationale-enhanced mechanism into dialogue systems for natural language understanding. The objective is to evaluate how such enhancements improve the effectiveness and accuracy in generating informative responses. Through an experiment involving 50 pairs of questions, the research compares outcomes before and after integrating rationale-based prompts. Key findings indicate significant improvements in response quality when incorporating rationale feedback mechanisms, particularly in enhancing context-aware reasoning capabilities. Implications suggest that this approach could enhance the performance of dialogue systems across various applications where nuanced understanding is essential."
    },
    {
        "title": "coordinate_constructions_in_english_enhanced_universal_dependencies_analysis_and_computational_modeling",
        "original_abstract": "In this paper, we address the representation of\ncoordinate constructions in Enhanced Univer-\nsal Dependencies (UD), where relevant depen-\ndency links are propagated from conjunction\nheads to other conjuncts. English treebanks for\nenhanced UD have been created from gold ba-\nsic dependencies using a heuristic rule-based\nconverter, which propagates only core argu-\nments.\nWith the aim of determining which\nset of links should be propagated from a se-\nmantic perspective, we create a large-scale\ndataset of manually edited syntax graphs. We\nidentify several systematic errors in the orig-\ninal data, and propose to also propagate ad-\njuncts. We observe high inter-annotator agree-\nment for this semantic annotation task. Using\nour new manually veriﬁed dataset, we perform\nthe ﬁrst principled comparison of rule-based\nand (partially novel) machine-learning based\nmethods for conjunction propagation for En-\nglish. We show that learning propagation rules\nis more effective than hand-designing heuris-\ntic rules. When using automatic parses, our\nneural graph-parser based edge predictor out-\nperforms the currently predominant pipelines\nusing a basic-layer tree parser plus converters.\n1",
        "generated_abstract": "The study investigates conjunction propagation for English word order regularization using machine learning and neural network methods. The key features involved include token-level morphological information, contextual embeddings from a pre-trained model, and graph-based dependency propagation rules. Experiments demonstrate improved performance over baseline models with better accuracy in predicting the correct position of conjunctive elements. The findings suggest that incorporating contextual embedding vectors enhances the effectiveness of conjunction propagation in natural language processing tasks."
    },
    {
        "title": "ViSoBERT__A_Pre-Trained_Language_Model_for_Vietnamese_Social_Media_Text_Processing",
        "original_abstract": "English and Chinese, known as resource-rich\nlanguages, have witnessed the strong devel-\nopment of transformer-based language mod-\nels for natural language processing tasks. Al-\nthough Vietnam has approximately 100M peo-\nple speaking Vietnamese, several pre-trained\nmodels, e.g., PhoBERT, ViBERT, and vELEC-\nTRA, performed well on general Vietnamese\nNLP tasks, including POS tagging and named\nentity recognition. These pre-trained language\nmodels are still limited to Vietnamese social\nmedia tasks.\nIn this paper, we present the\nfirst monolingual pre-trained language model\nfor Vietnamese social media texts, ViSoBERT,\nwhich is pre-trained on a large-scale corpus of\nhigh-quality and diverse Vietnamese social me-\ndia texts using XLM-R architecture. Moreover,\nwe explored our pre-trained model on five im-\nportant natural language downstream tasks on\nVietnamese social media texts: emotion recog-\nnition, hate speech detection, sentiment anal-\nysis, spam reviews detection, and hate speech\nspans detection. Our experiments demonstrate\nthat ViSoBERT, with far fewer parameters, sur-\npasses the previous state-of-the-art models on\nmultiple Vietnamese social media tasks. Our\nViSoBERT model is available4 only for re-\nsearch purposes.\nDisclaimer: This paper contains actual com-\nments on social networks that might be con-\nstrued as abusive, offensive, or obscene.\n1",
        "generated_abstract": "The study examines how different pre-trained language models process social media comments by removing diacritics from them. The three models analyzed are PhoBERT, TwHIN-BERT, and ViSoBERT. Results show that all models successfully tokenize the comments, but they differ significantly in their accuracy rates for specific tokens such as emojis and special characters. The study concludes that while these models provide a useful framework for sentiment analysis on social media, further refinement is needed to ensure more accurate tokenization of culturally-specific language elements present in user-generated content."
    }
]