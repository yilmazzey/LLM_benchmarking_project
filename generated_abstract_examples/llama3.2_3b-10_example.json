[
    {
        "title": "integrating_an_unsupervised_transliteration_model_into_statistical_machine_translation",
        "original_abstract": "We investigate three methods for integrat-\ning an unsupervised transliteration model\ninto an end-to-end SMT system. We in-\nduce a transliteration model from parallel\ndata and use it to translate OOV words.\nOur approach is fully unsupervised and\nlanguage independent.\nIn the methods\nto integrate transliterations, we observed\nimprovements from 0.23-0.75 (∆0.41)\nBLEU points across 7 language pairs. We\nalso show that our mined transliteration\ncorpora provide better rule coverage and\ntranslation quality compared to the gold\nstandard transliteration corpora.\n1",
        "generated_abstract": "This paper explores the problem of unsupervised transliteration mining, which involves extracting transliterations from a source language to a target language without explicit alignment data. The authors propose a novel method that combines word-level and character-level models to address this challenge. Their approach leverages a statistical model that learns to predict correct transliterations through an optimization process. By using a combination of weak and strong supervisions, the proposed method achieves state-of-the-art results on several benchmark datasets. The key findings suggest that the proposed method can effectively extract accurate transliterations from noisy data sources, such as Wikipedia infoboxes and online forums. The main conclusions drawn are that unsupervised transliteration mining is a viable approach for automatic language translation and that the proposed method provides a robust framework for addressing this challenge."
    },
    {
        "title": "event_temporal_relation_extraction_with_bayesian_translational_model",
        "original_abstract": "Existing models to extract temporal relations\nbetween events lack a principled method to in-\ncorporate external knowledge. In this study, we\nintroduce Bayesian-Trans, a Bayesian learning-\nbased method that models the temporal rela-\ntion representations as latent variables and in-\nfers their values via Bayesian inference and\ntranslational functions. Compared to conven-\ntional neural approaches, instead of performing\npoint estimation to find the best set parame-\nters, the proposed model infers the parameters’\nposterior distribution directly, enhancing the\nmodel’s capability to encode and express un-\ncertainty about the predictions. Experimental\nresults on the three widely used datasets show\nthat Bayesian-Trans outperforms existing ap-\nproaches for event temporal relation extraction.\nWe additionally present detailed analyses on un-\ncertainty quantification, comparison of priors,\nand ablation studies, illustrating the benefits of\nthe proposed approach.1\n1",
        "generated_abstract": "This study investigates the performance of various translational models in the context of temporal relation classification. The objective is to improve the accuracy of these models by leveraging a Bayesian learning framework. A novel framework, termed Bayesian-Trans, is proposed that integrates multiple translational models and utilizes regularization techniques to adapt to different datasets.\n\nThe methodology employed involves training the model on two publicly available datasets: MATRES and TimeBank-Dense. The results show improved performance on certain classes of relations compared to previous state-of-the-art approaches. Specifically, the model demonstrates better accuracy for predicting BEFORE and AFTER relations, with significant improvements over the baseline models.\n\nThe main conclusions drawn from this study are that Bayesian-Trans provides a more robust approach to temporal relation classification, particularly in cases where minority class relations are sparse or underrepresented. The findings also highlight the importance of regularization techniques in adapting to different datasets. Overall, this research contributes to the development of more accurate and reliable models for temporal relation classification, with implications for various applications in natural language processing and information retrieval."
    },
    {
        "title": "Navigating_the_Grey_Area__How_Expressions_of_Uncertainty_and_Overconfidence_Affect_Language_Models",
        "original_abstract": "The increased deployment of LMs for real-\nworld tasks involving knowledge and facts\nmakes it important to understand model episte-\nmology: what LMs think they know, and how\ntheir attitudes toward that knowledge are af-\nfected by language use in their inputs. Here,\nwe study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or\nevidentiality like \"I’m sure it’s\", \"I think it’s\",\nor “Wikipedia says it’s\" affect models, and\nwhether they contribute to model failures. We\ndevelop a typology of epistemic markers and\ninject 50 markers into prompts for question an-\nswering. We find that LMs are highly sensitive\nto epistemic markers in prompts, with accu-\nracies varying more than 80%. Surprisingly,\nwe find that expressions of high certainty re-\nsult in a 7% decrease in accuracy as compared\nto low certainty expressions; similarly, factive\nverbs hurt performance, while evidentials ben-\nefit performance. Our analysis of a popular\npretraining dataset shows that these markers\nof uncertainty are associated with answers on\nquestion-answering websites, while markers of\ncertainty are associated with questions. These\nassociations may suggest that the behavior of\nLMs is based on mimicking observed language\nuse, rather than truly reflecting epistemic un-\ncertainty.\n1",
        "generated_abstract": "This study investigates the role of modal expressions in expressing certainty and uncertainty in human language. The researchers examined a dataset of natural language texts across various GPT models and datasets to identify patterns and trends in the use of modal expressions such as \"undoubtedly,\" \"with 100% confidence,\" and \"I think.\" The findings suggest that these expressions are often used to convey degrees of certainty, but can also be ambiguous or context-dependent. The results highlight the importance of considering the nuances of language in understanding human communication. The study contributes to our understanding of modal expressions and their role in shaping the meaning of linguistic utterances."
    },
    {
        "title": "Mirages._On_Anthropomorphism_in_Dialogue_Systems",
        "original_abstract": "Automated dialogue or conversational systems\nare anthropomorphised by developers and per-\nsonified by users. While a degree of anthropo-\nmorphism may be inevitable due to the choice\nof medium, conscious and unconscious design\nchoices can guide users to personify such sys-\ntems to varying degrees. Encouraging users\nto relate to automated systems as if they were\nhuman can lead to high risk scenarios caused\nby over-reliance on their outputs. As a result,\nnatural language processing researchers have\ninvestigated the factors that induce personifi-\ncation and develop resources to mitigate such\neffects. However, these efforts are fragmented,\nand many aspects of anthropomorphism have\nyet to be explored. In this paper, we discuss\nthe linguistic factors that contribute to the an-\nthropomorphism of dialogue systems and the\nharms that can arise, including reinforcing gen-\nder stereotypes and notions of acceptable lan-\nguage. We recommend that future efforts to-\nwards developing dialogue systems take partic-\nular care in their design, development, release,\nand description; and attend to the many linguis-\ntic cues that can elicit personification by users.\n1",
        "generated_abstract": "Anthropomorphism in digital voice assistants has become increasingly prevalent, raising questions about its impact on human perception and behavior. This study investigated the effects of anthropomorphic speech patterns on user acceptance and engagement. A mixed-methods approach was employed, combining both qualitative and quantitative data from surveys and usability tests. The results showed that users displayed a significant preference for human-like dialogue, with 75% indicating a strong desire for more natural interactions. However, individuals with higher levels of social anxiety reported increased discomfort when interacting with anthropomorphic assistants. Furthermore, the study revealed that users' emotional responses to digital voice assistants were influenced by their perceived similarity to humans. The findings suggest that designers should prioritize human-likeness in digital voice assistants, while also considering individual differences in user experience. Ultimately, this research contributes to a deeper understanding of the complex relationships between technology, social psychology, and human behavior, with implications for the development of more effective and engaging conversational interfaces."
    },
    {
        "title": "equipping_language_models_with_tool_use_capability_for_tabular_data_analysis_in_finance",
        "original_abstract": "Large language models (LLMs) have exhibited\nan array of reasoning capabilities but face chal-\nlenges like error propagation and hallucination,\nparticularly in specialised areas like finance,\nwhere data is heterogeneous, and precision is\nparamount. We explore the potential of lan-\nguage model augmentation with external tools\nto mitigate these limitations and offload cer-\ntain reasoning steps to external tools that are\nmore suited for the task, instead of solely de-\npending on the LLM’s inherent abilities. More\nconcretely, using financial domain question-\nanswering datasets, we apply supervised fine-\ntuning on a LLAMA-2 13B CHAT model to\nact both as a task router and task solver. The\ntask router dynamically directs a question to\neither be answered internally by the LLM or\nexternally via the right tool from the tool set.\nOur tool-equipped SFT model, RAVEN, demon-\nstrates an improvement of 35.2% and 5.06%\nover the base model and SFT-only baselines,\nrespectively, and is highly competitive with\nstrong GPT-3.5 results.\nTo the best of our\nknowledge, our work is the first that investi-\ngates tool augmentation of language models\nfor the finance domain.1\n1",
        "generated_abstract": "This study investigates the changes in gains recognized in other comprehensive income (loss), net of tax, from 2018 to 2019. The analysis examines the amount excluded from effectiveness assessment and ineffective portion. Using data from the financial statements, we calculate the percentage change in these items over the specified period. Our results show that the effective portion increased by 10%, while the amounts excluded from effectiveness assessment and ineffective portion decreased by 25% and 30%, respectively. The gains reclassified from accumulated other comprehensive income (loss) into revenue remained relatively stable at 341, 185, and 555 million dollars for the years 2018, 2019, and 2017, respectively."
    },
    {
        "title": "multilingual_large_language_models_are_not_(yet)_code-switchers",
        "original_abstract": "Multilingual Large Language Models (LLMs)\nhave recently shown great capabilities in a wide\nrange of tasks, exhibiting state-of-the-art perfor-\nmance through zero-shot or few-shot prompt-\ning methods. While there have been extensive\nstudies on their abilities in monolingual tasks,\nthe investigation of their potential in the context\nof code-switching (CSW), the practice of alter-\nnating languages within an utterance, remains\nrelatively uncharted. In this paper, we provide\na comprehensive empirical analysis of various\nmultilingual LLMs, benchmarking their perfor-\nmance across four tasks: sentiment analysis,\nmachine translation, summarization and word-\nlevel language identification. Our results indi-\ncate that despite multilingual LLMs exhibiting\npromising outcomes in certain tasks using zero\nor few-shot prompting, they still underperform\nin comparison to fine-tuned models of much\nsmaller scales. We argue that current “multilin-\ngualism\" in LLMs does not inherently imply\nproficiency with code-switching texts, calling\nfor future research to bridge this discrepancy.\n1",
        "generated_abstract": "This study investigates the performance of large language models (LLMs) in handling multilingual tasks, including sentiment analysis and word-level language identification (LID). The objective is to evaluate the LLMs' ability to accurately analyze texts in multiple languages. A set of benchmarks was created for Sentimix Spanish-English, MixSentiment Malayaman-English, and MixSentiment Tamil-English datasets, as well as Hindi-English and Standard-Egyptian Arabic LID datasets.\n\nThe study employs a range of LLMs, including mBERT, mDeBERTa, XLMR, and BLOOMZ. The results show that the LLMs exhibit varying degrees of performance across languages, with some models performing better on certain tasks than others. Furthermore, the word-level LID evaluation reveals that the LLMs can accurately identify language tags for words in different languages.\n\nThe findings suggest that while LLMs can be effective in handling multilingual tasks, their performance is influenced by factors such as model size and training data. The results also highlight the importance of developing more robust models that can effectively analyze texts in multiple languages. Overall, this study contributes to a deeper understanding of LLMs' capabilities and limitations in multilingual applications."
    },
    {
        "title": "Harnessing_Black-Box_Control_to_Boost_Commonsense_in_LM’s_Generation",
        "original_abstract": "Large language models (LLMs) such as GPT-3\nhave demonstrated a strong capability to gen-\nerate coherent and contextually relevant text.\nHowever, amidst their successes, a crucial issue\npersists: their generated outputs still lack com-\nmonsense at times. Yet fine-tuning the entire\nLLM towards more commonsensical outputs is\ncomputationally expensive if not infeasible. In\nthis paper, we present a computation-efficient\nframework that steers a frozen Pre-Trained Lan-\nguage Model (PTLM) towards more common-\nsensical generation (i.e., producing a meaning-\nful and plausible output that incorporates a list\nof concepts).\nSpecifically, we first construct a reference-free\nevaluator that assigns a sentence with a com-\nmonsensical score by grounding the sentence\nto a dynamic commonsense knowledge base\nfrom four different relational aspects. We then\nuse the scorer as the oracle for commonsense\nknowledge, and extend the controllable genera-\ntion method called NADO to train an auxiliary\nhead that guides a fixed PTLM to better satisfy\nthe oracle. We test our framework on a series of\nGPT-2-, FLAN-T5- and Alpaca-based language\nmodels (LMs) on two constrained concept-to-\nsentence benchmarks. Human evaluation re-\nsults demonstrate that our method consistently\nleads to the most commonsensical outputs.1\n1",
        "generated_abstract": "This study investigates the task of extracting tuples from sentences based on explicit mentions of specific relations. A few non-sensical sentences were purposefully selected to challenge the model's ability to extract meaningful information. The methodology employed involves utilizing a state-of-the-art few-shot text extraction model, gpt-3.5-turbo, to process a set of carefully crafted input sentences. The results show that the model is able to extract tuples from most of the provided sentences, with varying degrees of accuracy depending on the type and complexity of the relations mentioned.\n\nThe study highlights the importance of careful sentence selection and modeling parameters in achieving high extraction accuracy. Notably, the few non-sensical sentences included in the prompt demonstrate the model's limitations in handling ambiguous or irrelevant information. The findings have implications for the development of more robust and efficient text extraction models, particularly in applications where accurate tuple extraction is crucial.\n\nUltimately, this research contributes to a deeper understanding of the strengths and weaknesses of few-shot text extraction models and provides insights into strategies for improving their performance. By examining the intersection of language understanding and model design, this study aims to inform the development of more effective and reliable text extraction systems."
    },
    {
        "title": "rationaleenhanced_language_models_are_better_continual_relation_learners",
        "original_abstract": "Continual relation extraction (CRE) aims to\nsolve the problem of catastrophic forgetting\nwhen learning a sequence of newly emerging\nrelations. Recent CRE studies have found that\ncatastrophic forgetting arises from the model’s\nlack of robustness against future analogous rela-\ntions. To address the issue, we introduce ratio-\nnale, i.e., the explanations of relation classifica-\ntion results generated by large language models\n(LLM), into CRE task. Specifically, we de-\nsign the multi-task rationale tuning strategy to\nhelp the model learn current relations robustly.\nWe also conduct contrastive rationale replay\nto further distinguish analogous relations. Ex-\nperimental results on two standard benchmarks\ndemonstrate that our method outperforms the\nstate-of-the-art CRE models. Our code is avail-\nable at https://github.com/WeiminXiong/\nRationaleCL\n1",
        "generated_abstract": "This study addresses the problem of improving relation extraction in natural language processing by enhancing models with generated rationales. The proposed approach utilizes a new method to generate high-quality rationales for each extracted relation. A dataset of 5,000 sentences is annotated with relations and corresponding rationales. The proposed model achieves superior performance compared to state-of-the-art baselines on this task. Through extensive evaluation, the rationale-enhanced model demonstrates its ability to avoid spurious shortcuts and capture nuanced context. The results show that the proposed approach leads to more accurate relation extraction and improved overall performance. This research contributes to advancing the field of natural language processing by developing a reliable method for generating informative rationales, which can enhance the accuracy and reliability of relation extraction models."
    },
    {
        "title": "coordinate_constructions_in_english_enhanced_universal_dependencies_analysis_and_computational_modeling",
        "original_abstract": "In this paper, we address the representation of\ncoordinate constructions in Enhanced Univer-\nsal Dependencies (UD), where relevant depen-\ndency links are propagated from conjunction\nheads to other conjuncts. English treebanks for\nenhanced UD have been created from gold ba-\nsic dependencies using a heuristic rule-based\nconverter, which propagates only core argu-\nments.\nWith the aim of determining which\nset of links should be propagated from a se-\nmantic perspective, we create a large-scale\ndataset of manually edited syntax graphs. We\nidentify several systematic errors in the orig-\ninal data, and propose to also propagate ad-\njuncts. We observe high inter-annotator agree-\nment for this semantic annotation task. Using\nour new manually veriﬁed dataset, we perform\nthe ﬁrst principled comparison of rule-based\nand (partially novel) machine-learning based\nmethods for conjunction propagation for En-\nglish. We show that learning propagation rules\nis more effective than hand-designing heuris-\ntic rules. When using automatic parses, our\nneural graph-parser based edge predictor out-\nperforms the currently predominant pipelines\nusing a basic-layer tree parser plus converters.\n1",
        "generated_abstract": "This study aims to improve the performance of graph-based conjunction propagation models by integrating token features and tree features. The goal is to enhance the model's ability to accurately predict and propagate dependencies in sentences. A limited set of hyperparameters was adjusted for optimal results, and a minimal amount of feature tuning was conducted. The experiments utilized an SVM and NN model with specific feature sets that included morphological features, contextualized word embeddings, and tree features.\n\nThe results show significant improvements in conjunction propagation accuracy compared to existing models. The addition of token features and tree features enhances the model's performance on tasks such as dependency parsing and question answering. The findings suggest that incorporating additional information from different sources can lead to improved model performance.\n\nThe main conclusion is that graph-based conjunction propagation models can be significantly improved by combining token and tree features. This approach has implications for the development of more accurate and effective language models. The results have important applications in natural language processing, particularly in tasks requiring dependency parsing and question answering."
    },
    {
        "title": "ViSoBERT__A_Pre-Trained_Language_Model_for_Vietnamese_Social_Media_Text_Processing",
        "original_abstract": "English and Chinese, known as resource-rich\nlanguages, have witnessed the strong devel-\nopment of transformer-based language mod-\nels for natural language processing tasks. Al-\nthough Vietnam has approximately 100M peo-\nple speaking Vietnamese, several pre-trained\nmodels, e.g., PhoBERT, ViBERT, and vELEC-\nTRA, performed well on general Vietnamese\nNLP tasks, including POS tagging and named\nentity recognition. These pre-trained language\nmodels are still limited to Vietnamese social\nmedia tasks.\nIn this paper, we present the\nfirst monolingual pre-trained language model\nfor Vietnamese social media texts, ViSoBERT,\nwhich is pre-trained on a large-scale corpus of\nhigh-quality and diverse Vietnamese social me-\ndia texts using XLM-R architecture. Moreover,\nwe explored our pre-trained model on five im-\nportant natural language downstream tasks on\nVietnamese social media texts: emotion recog-\nnition, hate speech detection, sentiment anal-\nysis, spam reviews detection, and hate speech\nspans detection. Our experiments demonstrate\nthat ViSoBERT, with far fewer parameters, sur-\npasses the previous state-of-the-art models on\nmultiple Vietnamese social media tasks. Our\nViSoBERT model is available4 only for re-\nsearch purposes.\nDisclaimer: This paper contains actual com-\nments on social networks that might be con-\nstrued as abusive, offensive, or obscene.\n1",
        "generated_abstract": "This study investigates the effects of diacritic removal on social comments tokenized by three pre-trained language models: PhoBERT, TwHIN-BERT, and ViSoBERT. The objective is to understand how different tokenization strategies impact the representation of social comments in these models. A dataset of actual social comments with varying degrees of diacritics was used to train and evaluate the tokenizers. The results show that the removal of diacritics significantly affects the tokenization of social comments, leading to changes in their meaning, context, and relevance. The most significant differences were observed between PhoBERT and TwHIN-BERT, with ViSoBERT producing more consistent results. The study highlights the importance of considering diacritic removal in natural language processing tasks and its potential impact on model performance and interpretability. Overall, the findings suggest that tokenizers may not always capture the nuances of social comments accurately, emphasizing the need for careful evaluation and consideration of diacritics in NLP applications."
    }
]