[
    {
        "title": "integrating_an_unsupervised_transliteration_model_into_statistical_machine_translation",
        "original_abstract": "We investigate three methods for integrat-\ning an unsupervised transliteration model\ninto an end-to-end SMT system. We in-\nduce a transliteration model from parallel\ndata and use it to translate OOV words.\nOur approach is fully unsupervised and\nlanguage independent.\nIn the methods\nto integrate transliterations, we observed\nimprovements from 0.23-0.75 (∆0.41)\nBLEU points across 7 language pairs. We\nalso show that our mined transliteration\ncorpora provide better rule coverage and\ntranslation quality compared to the gold\nstandard transliteration corpora.\n1",
        "generated_abstract": "This research investigates machine translation, focusing on the accuracy and efficiency of transliteration mining for name extraction. Transliteration mining utilizes unsupervised learning techniques to identify and extract names from text data based on phonetic similarity, without needing labeled examples. This study explores the application of  various statistical models and algorithms for unsupervised transliteration mining in Arabic-English. \n\nSeveral experimental approaches are compared including traditional statistical methods like word matching and more recent machine learning techniques such as support vector machines (SVMs) and neural networks. The results demonstrate that combining SVMs with linguistic features such as part-of-speech tagging effectively improves name accuracy, even without large labeled datasets for training.  This research paves the way for more efficient and scalable methods for name extraction in natural language processing tasks, particularly in languages where traditional approaches face challenges.  These techniques hold potential for improving machine translation systems by extracting transliterated names for improved search functionality and user experience."
    },
    {
        "title": "event_temporal_relation_extraction_with_bayesian_translational_model",
        "original_abstract": "Existing models to extract temporal relations\nbetween events lack a principled method to in-\ncorporate external knowledge. In this study, we\nintroduce Bayesian-Trans, a Bayesian learning-\nbased method that models the temporal rela-\ntion representations as latent variables and in-\nfers their values via Bayesian inference and\ntranslational functions. Compared to conven-\ntional neural approaches, instead of performing\npoint estimation to find the best set parame-\nters, the proposed model infers the parameters’\nposterior distribution directly, enhancing the\nmodel’s capability to encode and express un-\ncertainty about the predictions. Experimental\nresults on the three widely used datasets show\nthat Bayesian-Trans outperforms existing ap-\nproaches for event temporal relation extraction.\nWe additionally present detailed analyses on un-\ncertainty quantification, comparison of priors,\nand ablation studies, illustrating the benefits of\nthe proposed approach.1\n1",
        "generated_abstract": "This paper addresses the challenge of accurately predicting temporal relationships between events in relation extraction tasks.  A key contribution is a novel Bayesian learning framework that integrates traditional translational models for capturing semantic information. \n\nThe methodology involves building and evaluating an ensemble system incorporating these models into a structured learning framework, leveraging the strengths of each model to improve prediction accuracy across diverse event relations. A comprehensive evaluation on two benchmark datasets (MATRES and TimeBank-Dense) showcases significant performance improvements over state-of-the-art methods in capturing temporal relations. Notably, the proposed approach demonstrates effectiveness across various relationship classes, especially in predicting BEFORE, AFTER, and NONE relationships with higher precision and recall.\n\nThese findings suggest that a hybrid framework combining domain expertise and deep learning techniques for event extraction holds promising potential for future research in relation extraction tasks. The ability to accurately predict temporal relations is crucial for building robust natural language processing systems capable of understanding temporal information within complex text data."
    },
    {
        "title": "Navigating_the_Grey_Area__How_Expressions_of_Uncertainty_and_Overconfidence_Affect_Language_Models",
        "original_abstract": "The increased deployment of LMs for real-\nworld tasks involving knowledge and facts\nmakes it important to understand model episte-\nmology: what LMs think they know, and how\ntheir attitudes toward that knowledge are af-\nfected by language use in their inputs. Here,\nwe study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or\nevidentiality like \"I’m sure it’s\", \"I think it’s\",\nor “Wikipedia says it’s\" affect models, and\nwhether they contribute to model failures. We\ndevelop a typology of epistemic markers and\ninject 50 markers into prompts for question an-\nswering. We find that LMs are highly sensitive\nto epistemic markers in prompts, with accu-\nracies varying more than 80%. Surprisingly,\nwe find that expressions of high certainty re-\nsult in a 7% decrease in accuracy as compared\nto low certainty expressions; similarly, factive\nverbs hurt performance, while evidentials ben-\nefit performance. Our analysis of a popular\npretraining dataset shows that these markers\nof uncertainty are associated with answers on\nquestion-answering websites, while markers of\ncertainty are associated with questions. These\nassociations may suggest that the behavior of\nLMs is based on mimicking observed language\nuse, rather than truly reflecting epistemic un-\ncertainty.\n1",
        "generated_abstract": "This study investigated the factors influencing certainty judgments in language models when generating responses.  The research employed a corpus analysis and comparative model-testing approach to examine how different template structures and phrases impact the degree of confidence expressed in generated text. The findings revealed a strong correlation between specific expression formats, such as \"evidently\" or \"with 100% confidence,\" with increased certainty levels in generated responses.  A key finding suggests that templates incorporating strong claims like \"it must be\" led to higher levels of perceived certainty than those relying on weaker phrases like \"I think.\" These results indicate a potential bias within language models towards expressing high-certainty statements, particularly when framed using such expressions. The implications highlight the need for further research on how to explicitly control and mitigate this potential for over-confidence in generated text by language models."
    },
    {
        "title": "Mirages._On_Anthropomorphism_in_Dialogue_Systems",
        "original_abstract": "Automated dialogue or conversational systems\nare anthropomorphised by developers and per-\nsonified by users. While a degree of anthropo-\nmorphism may be inevitable due to the choice\nof medium, conscious and unconscious design\nchoices can guide users to personify such sys-\ntems to varying degrees. Encouraging users\nto relate to automated systems as if they were\nhuman can lead to high risk scenarios caused\nby over-reliance on their outputs. As a result,\nnatural language processing researchers have\ninvestigated the factors that induce personifi-\ncation and develop resources to mitigate such\neffects. However, these efforts are fragmented,\nand many aspects of anthropomorphism have\nyet to be explored. In this paper, we discuss\nthe linguistic factors that contribute to the an-\nthropomorphism of dialogue systems and the\nharms that can arise, including reinforcing gen-\nder stereotypes and notions of acceptable lan-\nguage. We recommend that future efforts to-\nwards developing dialogue systems take partic-\nular care in their design, development, release,\nand description; and attend to the many linguis-\ntic cues that can elicit personification by users.\n1",
        "generated_abstract": "This research examines the development and effectiveness of conversational agents in human-like communication. The inherent challenge lies in replicating genuine emotional connection while developing systems capable of engaging in emotionally nuanced dialogues.  A significant focus has been placed on achieving empathetic dialogue by incorporating a deeper understanding of human emotions and interactions. This study explores different methods for building personality within AI assistants, employing both linguistic data analysis and behavioral simulation to create agents with more relatable characteristics. The research also investigates the impact of anthropomorphic features on user perception and engagement. Findings indicate that incorporating elements of human personality traits significantly improves user trust and empathy towards these virtual companions.  This work suggests a potential paradigm shift in conversational AI development, where deeper emotional connection becomes a core factor in achieving successful dialogue generation. This discovery holds significant implications for the future of virtual assistants, pushing toward more emotionally fulfilling user experiences."
    },
    {
        "title": "equipping_language_models_with_tool_use_capability_for_tabular_data_analysis_in_finance",
        "original_abstract": "Large language models (LLMs) have exhibited\nan array of reasoning capabilities but face chal-\nlenges like error propagation and hallucination,\nparticularly in specialised areas like finance,\nwhere data is heterogeneous, and precision is\nparamount. We explore the potential of lan-\nguage model augmentation with external tools\nto mitigate these limitations and offload cer-\ntain reasoning steps to external tools that are\nmore suited for the task, instead of solely de-\npending on the LLM’s inherent abilities. More\nconcretely, using financial domain question-\nanswering datasets, we apply supervised fine-\ntuning on a LLAMA-2 13B CHAT model to\nact both as a task router and task solver. The\ntask router dynamically directs a question to\neither be answered internally by the LLM or\nexternally via the right tool from the tool set.\nOur tool-equipped SFT model, RAVEN, demon-\nstrates an improvement of 35.2% and 5.06%\nover the base model and SFT-only baselines,\nrespectively, and is highly competitive with\nstrong GPT-3.5 results.\nTo the best of our\nknowledge, our work is the first that investi-\ngates tool augmentation of language models\nfor the finance domain.1\n1",
        "generated_abstract": "This study investigated the relationship between population density and urban development in Bangladesh. The primary methodology employed involved analyzing demographic and spatial data from various administrative divisions across the country.  A comparative analysis was conducted to examine correlations between population density per square kilometer and key indicators of urban growth like economic activity, infrastructure development, and service provision.  The findings revealed a strong positive correlation between higher population densities and greater levels of urbanization.  This suggests that densely populated areas tend to experience faster rates of industrialization and service expansion. However, this correlation must be understood within the context of socioeconomic disparities as some regions are experiencing rapid growth while others struggle with limited infrastructure or opportunities. The research highlights the importance of strategic urban planning and investment in essential services to ensure equitable development and address potential challenges associated with high population densities."
    },
    {
        "title": "multilingual_large_language_models_are_not_(yet)_code-switchers",
        "original_abstract": "Multilingual Large Language Models (LLMs)\nhave recently shown great capabilities in a wide\nrange of tasks, exhibiting state-of-the-art perfor-\nmance through zero-shot or few-shot prompt-\ning methods. While there have been extensive\nstudies on their abilities in monolingual tasks,\nthe investigation of their potential in the context\nof code-switching (CSW), the practice of alter-\nnating languages within an utterance, remains\nrelatively uncharted. In this paper, we provide\na comprehensive empirical analysis of various\nmultilingual LLMs, benchmarking their perfor-\nmance across four tasks: sentiment analysis,\nmachine translation, summarization and word-\nlevel language identification. Our results indi-\ncate that despite multilingual LLMs exhibiting\npromising outcomes in certain tasks using zero\nor few-shot prompting, they still underperform\nin comparison to fine-tuned models of much\nsmaller scales. We argue that current “multilin-\ngualism\" in LLMs does not inherently imply\nproficiency with code-switching texts, calling\nfor future research to bridge this discrepancy.\n1",
        "generated_abstract": "This research explores the capabilities and limitations of large language models (LLMs) in sentiment analysis and word-level language identification (LID).  The study investigates how different LLMs perform on datasets involving sentiment classification for various languages and specific tasks like identifying the linguistic origins of words. \n\nThe methodology employed involved training and evaluating a range of LLMs on benchmark datasets, including Sentimix Spanish-English, MixSentiment Malayaman-English, and MixSentiment Tamil-English for sentiment analysis and LID. The models were evaluated based on macro F1 scores and micro F1 scores.  The findings demonstrate that certain LLMs excel in specific linguistic domains, highlighting the importance of model architecture and training data for language identification tasks. Additionally, results suggest a need to address biases present in these datasets and explore methods for mitigating them. \n\nThese findings have implications for developing more robust and accurate sentiment analysis and LID systems.  The research highlights the ongoing challenge of achieving optimal performance across diverse linguistic contexts while maintaining model transparency and fairness."
    },
    {
        "title": "Harnessing_Black-Box_Control_to_Boost_Commonsense_in_LM’s_Generation",
        "original_abstract": "Large language models (LLMs) such as GPT-3\nhave demonstrated a strong capability to gen-\nerate coherent and contextually relevant text.\nHowever, amidst their successes, a crucial issue\npersists: their generated outputs still lack com-\nmonsense at times. Yet fine-tuning the entire\nLLM towards more commonsensical outputs is\ncomputationally expensive if not infeasible. In\nthis paper, we present a computation-efficient\nframework that steers a frozen Pre-Trained Lan-\nguage Model (PTLM) towards more common-\nsensical generation (i.e., producing a meaning-\nful and plausible output that incorporates a list\nof concepts).\nSpecifically, we first construct a reference-free\nevaluator that assigns a sentence with a com-\nmonsensical score by grounding the sentence\nto a dynamic commonsense knowledge base\nfrom four different relational aspects. We then\nuse the scorer as the oracle for commonsense\nknowledge, and extend the controllable genera-\ntion method called NADO to train an auxiliary\nhead that guides a fixed PTLM to better satisfy\nthe oracle. We test our framework on a series of\nGPT-2-, FLAN-T5- and Alpaca-based language\nmodels (LMs) on two constrained concept-to-\nsentence benchmarks. Human evaluation re-\nsults demonstrate that our method consistently\nleads to the most commonsensical outputs.1\n1",
        "generated_abstract": "This study explores methods for extracting tuples from sentences using large language models (LLMs).  The objective is to investigate how LLMs can accurately identify relationships between entities in a sentence and generate corresponding tuples.  \n\nTo achieve this, we designed a few-shot prompt that posed a set of non-sensical sentences for a pre-trained LLM. The prompt presented the goal of identifying relationships within these sentences, encouraging the LLM to deduce relevant information and extract tuples as output. \n\nOur findings suggest LLMs exhibit proficiency in extracting meaningful tuples from seemingly ambiguous sentences. This indicates potential applications for  improving natural language processing tasks involving relational extraction. The success of this approach highlights the power of LLMs to understand complex linguistic contexts and generate accurate tuples even when faced with unusual or challenging sentence structures. However, further research exploring these capabilities is necessary for broader application in real-world scenarios."
    },
    {
        "title": "rationaleenhanced_language_models_are_better_continual_relation_learners",
        "original_abstract": "Continual relation extraction (CRE) aims to\nsolve the problem of catastrophic forgetting\nwhen learning a sequence of newly emerging\nrelations. Recent CRE studies have found that\ncatastrophic forgetting arises from the model’s\nlack of robustness against future analogous rela-\ntions. To address the issue, we introduce ratio-\nnale, i.e., the explanations of relation classifica-\ntion results generated by large language models\n(LLM), into CRE task. Specifically, we de-\nsign the multi-task rationale tuning strategy to\nhelp the model learn current relations robustly.\nWe also conduct contrastive rationale replay\nto further distinguish analogous relations. Ex-\nperimental results on two standard benchmarks\ndemonstrate that our method outperforms the\nstate-of-the-art CRE models. Our code is avail-\nable at https://github.com/WeiminXiong/\nRationaleCL\n1",
        "generated_abstract": "This paper investigates the application of reasoning capabilities to enhance natural language understanding in relationship extraction. Current models often struggle with identifying nuanced relationships within complex sentences, particularly when contextual information is crucial. The study focuses on developing a model that leverages logical inference and semantic knowledge to accurately interpret the intended relationships between entities in given contexts. \n\nThe research employs a novel approach based on reasoning techniques. A set of case studies are developed to demonstrate the effectiveness of the proposed method.  Results show significant improvement in extracting accurate relationship types compared to baseline models. The methodology provides a clear demonstration of the efficacy of incorporating logical reasoning for improving accuracy and nuanced understanding in complex sentences, offering a framework for developing more sophisticated natural language processing systems."
    },
    {
        "title": "coordinate_constructions_in_english_enhanced_universal_dependencies_analysis_and_computational_modeling",
        "original_abstract": "In this paper, we address the representation of\ncoordinate constructions in Enhanced Univer-\nsal Dependencies (UD), where relevant depen-\ndency links are propagated from conjunction\nheads to other conjuncts. English treebanks for\nenhanced UD have been created from gold ba-\nsic dependencies using a heuristic rule-based\nconverter, which propagates only core argu-\nments.\nWith the aim of determining which\nset of links should be propagated from a se-\nmantic perspective, we create a large-scale\ndataset of manually edited syntax graphs. We\nidentify several systematic errors in the orig-\ninal data, and propose to also propagate ad-\njuncts. We observe high inter-annotator agree-\nment for this semantic annotation task. Using\nour new manually veriﬁed dataset, we perform\nthe ﬁrst principled comparison of rule-based\nand (partially novel) machine-learning based\nmethods for conjunction propagation for En-\nglish. We show that learning propagation rules\nis more effective than hand-designing heuris-\ntic rules. When using automatic parses, our\nneural graph-parser based edge predictor out-\nperforms the currently predominant pipelines\nusing a basic-layer tree parser plus converters.\n1",
        "generated_abstract": "This research investigates the challenge of accurately predicting dependencies in complex grammatical structures, specifically focusing on conjunctions. The objective is to develop a model capable of predicting dependency relations within these structures while accounting for intricate semantic nuances. This task was achieved through a combination of supervised learning and graph-based approaches, leveraging features like token morphology, word embeddings, and syntactic analysis. \n\nThe research employed a two-pronged approach: an SVM-based classification model and an NN-based dependency predictor. These models were trained using labeled datasets, focusing on accurately representing the complex dependencies within conjunctions.  Preliminary analysis suggests that incorporating contextualized word embeddings effectively enhances prediction accuracy. The findings of this research offer valuable insights into understanding the intricacies of conjunctional syntax and pave the way for more robust natural language processing systems capable of handling such structures accurately."
    },
    {
        "title": "ViSoBERT__A_Pre-Trained_Language_Model_for_Vietnamese_Social_Media_Text_Processing",
        "original_abstract": "English and Chinese, known as resource-rich\nlanguages, have witnessed the strong devel-\nopment of transformer-based language mod-\nels for natural language processing tasks. Al-\nthough Vietnam has approximately 100M peo-\nple speaking Vietnamese, several pre-trained\nmodels, e.g., PhoBERT, ViBERT, and vELEC-\nTRA, performed well on general Vietnamese\nNLP tasks, including POS tagging and named\nentity recognition. These pre-trained language\nmodels are still limited to Vietnamese social\nmedia tasks.\nIn this paper, we present the\nfirst monolingual pre-trained language model\nfor Vietnamese social media texts, ViSoBERT,\nwhich is pre-trained on a large-scale corpus of\nhigh-quality and diverse Vietnamese social me-\ndia texts using XLM-R architecture. Moreover,\nwe explored our pre-trained model on five im-\nportant natural language downstream tasks on\nVietnamese social media texts: emotion recog-\nnition, hate speech detection, sentiment anal-\nysis, spam reviews detection, and hate speech\nspans detection. Our experiments demonstrate\nthat ViSoBERT, with far fewer parameters, sur-\npasses the previous state-of-the-art models on\nmultiple Vietnamese social media tasks. Our\nViSoBERT model is available4 only for re-\nsearch purposes.\nDisclaimer: This paper contains actual com-\nments on social networks that might be con-\nstrued as abusive, offensive, or obscene.\n1",
        "generated_abstract": "This study investigates the impact of diacritics on social media comment tokenization using three pre-trained language models: PhoBERT, TwHIN-BERT, and ViSoBERT.  We examined how removing diacritical marks affects the ability of these models to understand and process social comments. The methodology involved generating a dataset of social media comments that could be deemed offensive or inappropriate. We tested different tokenization methods and compared their effectiveness in identifying and classifying these comments. \n\nOur findings reveal significant discrepancies in the performance of different pre-trained language models when used with social media data containing diacritics.  While all three models exhibit varying levels of accuracy, removing diacritical marks significantly improves their ability to accurately classify and categorize offensive content. This suggests that incorporating automatic diacritic removal techniques could improve the accessibility and effectiveness of language processing for social media platforms. \n\nThis research demonstrates the importance of considering diacritics during natural language processing tasks on online platforms.  Further investigation is warranted into optimal strategies for handling diacritical marks in different settings to ensure accurate interpretation of social comments."
    }
]