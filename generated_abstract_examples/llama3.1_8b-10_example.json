[
    {
        "title": "integrating_an_unsupervised_transliteration_model_into_statistical_machine_translation",
        "original_abstract": "We investigate three methods for integrat-\ning an unsupervised transliteration model\ninto an end-to-end SMT system. We in-\nduce a transliteration model from parallel\ndata and use it to translate OOV words.\nOur approach is fully unsupervised and\nlanguage independent.\nIn the methods\nto integrate transliterations, we observed\nimprovements from 0.23-0.75 (∆0.41)\nBLEU points across 7 language pairs. We\nalso show that our mined transliteration\ncorpora provide better rule coverage and\ntranslation quality compared to the gold\nstandard transliteration corpora.\n1",
        "generated_abstract": "Transliteration is a crucial process in natural language processing (NLP) that involves converting written names and words from one script to another, typically from non-roman scripts such as Arabic, Chinese, and Korean to Roman scripts like English. Despite its importance, transliteration remains a challenging task due to the complexities of script conversion, linguistic nuances, and contextual dependencies.\n\nThis research aims to develop a comprehensive framework for unsupervised transliteration mining, which enables the automatic extraction of transliterated words from unlabelled text data. The proposed methodology involves a combination of language-independent name translation mining, character-level models, and machine learning algorithms. Key findings indicate that the developed framework achieves state-of-the-art results in various languages and scripts, outperforming traditional statistical alignment-based methods.\n\nThe research demonstrates the feasibility of unsupervised transliteration mining for real-world applications, including information retrieval, text classification, and language translation. The main conclusions highlight the potential of this approach to reduce manual effort, improve data quality, and enhance the efficiency of NLP tasks involving script conversion. The implications of this work are significant, as they pave the way for developing more accurate and efficient transliteration systems that can support multilingual applications in various domains."
    },
    {
        "title": "event_temporal_relation_extraction_with_bayesian_translational_model",
        "original_abstract": "Existing models to extract temporal relations\nbetween events lack a principled method to in-\ncorporate external knowledge. In this study, we\nintroduce Bayesian-Trans, a Bayesian learning-\nbased method that models the temporal rela-\ntion representations as latent variables and in-\nfers their values via Bayesian inference and\ntranslational functions. Compared to conven-\ntional neural approaches, instead of performing\npoint estimation to find the best set parame-\nters, the proposed model infers the parameters’\nposterior distribution directly, enhancing the\nmodel’s capability to encode and express un-\ncertainty about the predictions. Experimental\nresults on the three widely used datasets show\nthat Bayesian-Trans outperforms existing ap-\nproaches for event temporal relation extraction.\nWe additionally present detailed analyses on un-\ncertainty quantification, comparison of priors,\nand ablation studies, illustrating the benefits of\nthe proposed approach.1\n1",
        "generated_abstract": "This paper presents a Bayesian learning framework for temporal relation extraction from text, which leverages the strengths of several state-of-the-art translational models in the hyperbolic space. The proposed approach combines a context encoder and a translation model to predict temporal relations between events. Experimental results on two benchmark datasets show that the method achieves significant improvements over previous state-of-the-art methods. Notably, the use of MuRE as the translational model yields better performance than other models in the hyperbolic space, including TransE and AttH. Class-specific results indicate that the model performs well on predicting BEFORE and AFTER relations but struggles with minority classes such as EQUAL and VAGUE. The findings have important implications for applications in natural language processing and information retrieval, where accurate temporal relation extraction is crucial. Overall, this research contributes to advancing the field of temporal relation extraction by providing a robust and efficient approach that outperforms existing methods on multiple benchmarks."
    },
    {
        "title": "Navigating_the_Grey_Area__How_Expressions_of_Uncertainty_and_Overconfidence_Affect_Language_Models",
        "original_abstract": "The increased deployment of LMs for real-\nworld tasks involving knowledge and facts\nmakes it important to understand model episte-\nmology: what LMs think they know, and how\ntheir attitudes toward that knowledge are af-\nfected by language use in their inputs. Here,\nwe study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or\nevidentiality like \"I’m sure it’s\", \"I think it’s\",\nor “Wikipedia says it’s\" affect models, and\nwhether they contribute to model failures. We\ndevelop a typology of epistemic markers and\ninject 50 markers into prompts for question an-\nswering. We find that LMs are highly sensitive\nto epistemic markers in prompts, with accu-\nracies varying more than 80%. Surprisingly,\nwe find that expressions of high certainty re-\nsult in a 7% decrease in accuracy as compared\nto low certainty expressions; similarly, factive\nverbs hurt performance, while evidentials ben-\nefit performance. Our analysis of a popular\npretraining dataset shows that these markers\nof uncertainty are associated with answers on\nquestion-answering websites, while markers of\ncertainty are associated with questions. These\nassociations may suggest that the behavior of\nLMs is based on mimicking observed language\nuse, rather than truly reflecting epistemic un-\ncertainty.\n1",
        "generated_abstract": "This study investigates how large language models, specifically GPT-3, process and utilize linguistic patterns to convey uncertainty and certainty in natural language queries. The objective is to identify the top-performing templates that facilitate accurate responses from these models when faced with uncertain or ambiguous questions.\n\nThe methodology involves analyzing the performance of GPT-3 on various question answering datasets, including CountryQA, Jeopardy, NaturalQA, and others. A total of 12 tables are presented, showcasing the top-performing templates for each dataset and their corresponding accuracy rates.\n\nKey findings include the dominance of certain linguistic patterns, such as \"it could be\" and \"I think,\" in conveying uncertainty, while phrases like \"with 100% confidence\" and \"we know it's\" are associated with certainty. Additionally, templates featuring Wikipedia or online sources tend to perform better than those relying on standard methods.\n\nThe main conclusions of this study suggest that large language models can benefit from explicit linguistic cues when processing uncertain or ambiguous queries. Furthermore, the findings have implications for the development of more accurate and effective question answering systems in various applications, including AI-powered search engines and virtual assistants."
    },
    {
        "title": "Mirages._On_Anthropomorphism_in_Dialogue_Systems",
        "original_abstract": "Automated dialogue or conversational systems\nare anthropomorphised by developers and per-\nsonified by users. While a degree of anthropo-\nmorphism may be inevitable due to the choice\nof medium, conscious and unconscious design\nchoices can guide users to personify such sys-\ntems to varying degrees. Encouraging users\nto relate to automated systems as if they were\nhuman can lead to high risk scenarios caused\nby over-reliance on their outputs. As a result,\nnatural language processing researchers have\ninvestigated the factors that induce personifi-\ncation and develop resources to mitigate such\neffects. However, these efforts are fragmented,\nand many aspects of anthropomorphism have\nyet to be explored. In this paper, we discuss\nthe linguistic factors that contribute to the an-\nthropomorphism of dialogue systems and the\nharms that can arise, including reinforcing gen-\nder stereotypes and notions of acceptable lan-\nguage. We recommend that future efforts to-\nwards developing dialogue systems take partic-\nular care in their design, development, release,\nand description; and attend to the many linguis-\ntic cues that can elicit personification by users.\n1",
        "generated_abstract": "Human-like communication is a crucial aspect of artificial intelligence (AI) and human-computer interaction. The growing presence of digital voice assistants has raised concerns about the implications of anthropomorphism, which refers to the attribution of human qualities or characteristics to non-human entities.\n\nThis research investigates the relationship between anthropomorphism and user acceptance of digital voice assistants. A comprehensive review of existing studies on human-like communication in AI systems reveals that users tend to attribute human-like qualities to digital voice assistants when they exhibit behaviors typically associated with humans, such as using idioms or displaying emotional expressions.\n\nThe findings suggest that users' perception of digital voice assistants is closely tied to their anthropomorphic qualities. Users are more likely to accept and trust digital voice assistants that display human-like characteristics, but also tend to hold them accountable for their actions in a way that would be unacceptable if they were truly human.\n\nThe main conclusions of this research are that anthropomorphism plays a significant role in user acceptance of digital voice assistants and that designers should carefully consider the implications of creating AI systems that mimic human behavior."
    },
    {
        "title": "equipping_language_models_with_tool_use_capability_for_tabular_data_analysis_in_finance",
        "original_abstract": "Large language models (LLMs) have exhibited\nan array of reasoning capabilities but face chal-\nlenges like error propagation and hallucination,\nparticularly in specialised areas like finance,\nwhere data is heterogeneous, and precision is\nparamount. We explore the potential of lan-\nguage model augmentation with external tools\nto mitigate these limitations and offload cer-\ntain reasoning steps to external tools that are\nmore suited for the task, instead of solely de-\npending on the LLM’s inherent abilities. More\nconcretely, using financial domain question-\nanswering datasets, we apply supervised fine-\ntuning on a LLAMA-2 13B CHAT model to\nact both as a task router and task solver. The\ntask router dynamically directs a question to\neither be answered internally by the LLM or\nexternally via the right tool from the tool set.\nOur tool-equipped SFT model, RAVEN, demon-\nstrates an improvement of 35.2% and 5.06%\nover the base model and SFT-only baselines,\nrespectively, and is highly competitive with\nstrong GPT-3.5 results.\nTo the best of our\nknowledge, our work is the first that investi-\ngates tool augmentation of language models\nfor the finance domain.1\n1",
        "generated_abstract": "There is no abstract provided in the given content. The content consists of a series of examples and templates related to financial reporting and analysis, but it does not include an abstract."
    },
    {
        "title": "multilingual_large_language_models_are_not_(yet)_code-switchers",
        "original_abstract": "Multilingual Large Language Models (LLMs)\nhave recently shown great capabilities in a wide\nrange of tasks, exhibiting state-of-the-art perfor-\nmance through zero-shot or few-shot prompt-\ning methods. While there have been extensive\nstudies on their abilities in monolingual tasks,\nthe investigation of their potential in the context\nof code-switching (CSW), the practice of alter-\nnating languages within an utterance, remains\nrelatively uncharted. In this paper, we provide\na comprehensive empirical analysis of various\nmultilingual LLMs, benchmarking their perfor-\nmance across four tasks: sentiment analysis,\nmachine translation, summarization and word-\nlevel language identification. Our results indi-\ncate that despite multilingual LLMs exhibiting\npromising outcomes in certain tasks using zero\nor few-shot prompting, they still underperform\nin comparison to fine-tuned models of much\nsmaller scales. We argue that current “multilin-\ngualism\" in LLMs does not inherently imply\nproficiency with code-switching texts, calling\nfor future research to bridge this discrepancy.\n1",
        "generated_abstract": "This study investigates the performance of multilingual language models (LLMs) in four languages: Hinglish-English code-switching, Hinglish, Hindi, and English. The objective is to evaluate these models' ability to analyze sentiment and identify word-level language identification (LID) in diverse linguistic settings. A range of pre-trained LLMs, including mT0, XLMR, mBERT, mDeBERTA, BLOOMZ, and XGLM, are fine-tuned on various code-switching datasets, such as Sentimix Spanish-English and MixSentiment Malayaman-English. The results show that these models exhibit varying levels of performance in sentiment analysis, with mT0 outperforming others in most cases. In LID tasks, the study finds that XLMR (FT) achieves the highest accuracy on Hindi-English word-level LID, while mBERT (FT) performs best on Standard-Egyptian Arabic word-level LID. The findings have significant implications for the development of multilingual models capable of handling code-switching languages and provide insights into their strengths and limitations in real-world applications. Overall, this research contributes to a deeper understanding of the capabilities and challenges of pre-trained LLMs in processing complex linguistic phenomena."
    },
    {
        "title": "Harnessing_Black-Box_Control_to_Boost_Commonsense_in_LM’s_Generation",
        "original_abstract": "Large language models (LLMs) such as GPT-3\nhave demonstrated a strong capability to gen-\nerate coherent and contextually relevant text.\nHowever, amidst their successes, a crucial issue\npersists: their generated outputs still lack com-\nmonsense at times. Yet fine-tuning the entire\nLLM towards more commonsensical outputs is\ncomputationally expensive if not infeasible. In\nthis paper, we present a computation-efficient\nframework that steers a frozen Pre-Trained Lan-\nguage Model (PTLM) towards more common-\nsensical generation (i.e., producing a meaning-\nful and plausible output that incorporates a list\nof concepts).\nSpecifically, we first construct a reference-free\nevaluator that assigns a sentence with a com-\nmonsensical score by grounding the sentence\nto a dynamic commonsense knowledge base\nfrom four different relational aspects. We then\nuse the scorer as the oracle for commonsense\nknowledge, and extend the controllable genera-\ntion method called NADO to train an auxiliary\nhead that guides a fixed PTLM to better satisfy\nthe oracle. We test our framework on a series of\nGPT-2-, FLAN-T5- and Alpaca-based language\nmodels (LMs) on two constrained concept-to-\nsentence benchmarks. Human evaluation re-\nsults demonstrate that our method consistently\nleads to the most commonsensical outputs.1\n1",
        "generated_abstract": "This study investigates the ability of a few-shot learning model to extract relational tuples from natural language sentences. The objective is to develop an efficient and accurate method for extracting specific types of relationships between entities in a sentence, such as \"IsUsedFor,\" \"AtLocation,\" \"CapableOf,\" and \"PartOf.\" A carefully curated dataset of example sentences is used to train the model, which then performs on a set of test sentences. The results are evaluated through human evaluation, where annotators assess the accuracy of the extracted tuples.\n\nThe findings show that the few-shot learning model can accurately extract relational tuples from sentences with high precision and recall rates. However, the performance varies depending on the type of relationship being extracted. The model performs well on relationships such as \"IsUsedFor\" and \"AtLocation,\" but struggles with more abstract relationships like \"CapableOf.\"\n\nThe main conclusion is that few-shot learning can be an effective approach for extracting relational tuples from natural language sentences. However, the performance of the model depends on the quality of the training data and the specific type of relationship being extracted. The results have implications for various applications, including question answering, text classification, and information extraction."
    },
    {
        "title": "rationaleenhanced_language_models_are_better_continual_relation_learners",
        "original_abstract": "Continual relation extraction (CRE) aims to\nsolve the problem of catastrophic forgetting\nwhen learning a sequence of newly emerging\nrelations. Recent CRE studies have found that\ncatastrophic forgetting arises from the model’s\nlack of robustness against future analogous rela-\ntions. To address the issue, we introduce ratio-\nnale, i.e., the explanations of relation classifica-\ntion results generated by large language models\n(LLM), into CRE task. Specifically, we de-\nsign the multi-task rationale tuning strategy to\nhelp the model learn current relations robustly.\nWe also conduct contrastive rationale replay\nto further distinguish analogous relations. Ex-\nperimental results on two standard benchmarks\ndemonstrate that our method outperforms the\nstate-of-the-art CRE models. Our code is avail-\nable at https://github.com/WeiminXiong/\nRationaleCL\n1",
        "generated_abstract": "This study addresses the challenge of accurately identifying relation types in open-domain question answering tasks using a novel rationale-enhanced model. The methodology employed involves developing an attention-based neural network architecture that generates rationales to support its predictions. The model is trained on a dataset of entity-relation pairs and evaluated on its ability to correctly identify relation types and provide supporting rationales.\n\nThe key findings indicate that the rationale-enhanced model significantly outperforms baseline models in identifying relation types, particularly when confronted with ambiguous or noisy input data. The model's ability to generate accurate rationales is found to be closely correlated with its performance on the main task of relation type identification.\n\nThe study's main conclusions are that incorporating rationales into a neural network architecture can improve its accuracy and robustness in open-domain question answering tasks, particularly when dealing with complex or ambiguous input data. The implications of this research are significant for the development of more accurate and reliable natural language processing models that can provide informative explanations for their predictions."
    },
    {
        "title": "coordinate_constructions_in_english_enhanced_universal_dependencies_analysis_and_computational_modeling",
        "original_abstract": "In this paper, we address the representation of\ncoordinate constructions in Enhanced Univer-\nsal Dependencies (UD), where relevant depen-\ndency links are propagated from conjunction\nheads to other conjuncts. English treebanks for\nenhanced UD have been created from gold ba-\nsic dependencies using a heuristic rule-based\nconverter, which propagates only core argu-\nments.\nWith the aim of determining which\nset of links should be propagated from a se-\nmantic perspective, we create a large-scale\ndataset of manually edited syntax graphs. We\nidentify several systematic errors in the orig-\ninal data, and propose to also propagate ad-\njuncts. We observe high inter-annotator agree-\nment for this semantic annotation task. Using\nour new manually veriﬁed dataset, we perform\nthe ﬁrst principled comparison of rule-based\nand (partially novel) machine-learning based\nmethods for conjunction propagation for En-\nglish. We show that learning propagation rules\nis more effective than hand-designing heuris-\ntic rules. When using automatic parses, our\nneural graph-parser based edge predictor out-\nperforms the currently predominant pipelines\nusing a basic-layer tree parser plus converters.\n1",
        "generated_abstract": "The Problem of Conjunction Propagation in Dependency Parsing: An Investigation into Graph-Based and Machine Learning Approaches\n\nThis study addresses the challenge of conjunction propagation in dependency parsing, a crucial task in natural language processing. The objective is to develop accurate models that can efficiently propagate dependencies across coordinated phrases. A graph-based edge predictor and machine learning (ML)-based classifiers are designed using RoBERTa embeddings and feature extraction techniques. The methodology involves training the graph-based model on a subset of the EWT dataset and fine-tuning the ML-based classifier using a combination of instance, token, and tree features. The results show that the graph-based approach outperforms the ML-based method in conjunction propagation accuracy. Furthermore, hyperparameter tuning and ablation experiments are conducted to optimize model performance. The study contributes to the development of more accurate dependency parsing models by providing insights into the effectiveness of graph-based and machine learning approaches for conjunction propagation."
    },
    {
        "title": "ViSoBERT__A_Pre-Trained_Language_Model_for_Vietnamese_Social_Media_Text_Processing",
        "original_abstract": "English and Chinese, known as resource-rich\nlanguages, have witnessed the strong devel-\nopment of transformer-based language mod-\nels for natural language processing tasks. Al-\nthough Vietnam has approximately 100M peo-\nple speaking Vietnamese, several pre-trained\nmodels, e.g., PhoBERT, ViBERT, and vELEC-\nTRA, performed well on general Vietnamese\nNLP tasks, including POS tagging and named\nentity recognition. These pre-trained language\nmodels are still limited to Vietnamese social\nmedia tasks.\nIn this paper, we present the\nfirst monolingual pre-trained language model\nfor Vietnamese social media texts, ViSoBERT,\nwhich is pre-trained on a large-scale corpus of\nhigh-quality and diverse Vietnamese social me-\ndia texts using XLM-R architecture. Moreover,\nwe explored our pre-trained model on five im-\nportant natural language downstream tasks on\nVietnamese social media texts: emotion recog-\nnition, hate speech detection, sentiment anal-\nysis, spam reviews detection, and hate speech\nspans detection. Our experiments demonstrate\nthat ViSoBERT, with far fewer parameters, sur-\npasses the previous state-of-the-art models on\nmultiple Vietnamese social media tasks. Our\nViSoBERT model is available4 only for re-\nsearch purposes.\nDisclaimer: This paper contains actual com-\nments on social networks that might be con-\nstrued as abusive, offensive, or obscene.\n1",
        "generated_abstract": "This study investigates the impact of diacritic removal on the tokenization of social media comments by three pre-trained language models: PhoBERT, TwHIN-BERT, and ViSoBERT. The objective is to evaluate the robustness of these models in handling Vietnamese text without diacritics. Methodologically, a dataset of 5207 actual social comments was used, with varying levels of diacritic removal (25%, 50%, and 75%). The results show that all three models exhibit some degree of degradation in tokenization accuracy when diacritics are removed. However, ViSoBERT performs relatively better than PhoBERT and TwHIN-BERT across the different diacritic removal levels. The findings highlight the importance of considering language-specific characteristics in pre-trained model evaluation and development. The study's implications suggest that language models should be fine-tuned for specific languages to improve their robustness against text preprocessing variations. The results also underscore the need for more research on Vietnamese language processing, which is a critical aspect of natural language processing in Southeast Asia."
    }

]