[
    {
        "title": "integrating_an_unsupervised_transliteration_model_into_statistical_machine_translation",
        "original_abstract": "We investigate three methods for integrat-\ning an unsupervised transliteration model\ninto an end-to-end SMT system. We in-\nduce a transliteration model from parallel\ndata and use it to translate OOV words.\nOur approach is fully unsupervised and\nlanguage independent.\nIn the methods\nto integrate transliterations, we observed\nimprovements from 0.23-0.75 (∆0.41)\nBLEU points across 7 language pairs. We\nalso show that our mined transliteration\ncorpora provide better rule coverage and\ntranslation quality compared to the gold\nstandard transliteration corpora.\n1",
        "generated_abstract": "The presence of out-of-vocabulary (OOV) words, often named entities, technical terms, or foreign words, poses a significant challenge to machine translation (MT) systems, irrespective of the training data size. This paper addresses this issue by integrating an unsupervised transliteration model into statistical machine translation (SMT) to enhance the translation of OOV words. The methodology involves using an Expectation Maximization (EM) algorithm to extract transliteration pairs from word-aligned parallel data, which are then used to train a transliteration model. Three different methods are investigated for integrating transliteration during decoding within the Moses toolkit. The key findings demonstrate that the proposed integrations achieve an average improvement of 0.41 BLEU points across seven language pairs compared to a competitive baseline. Furthermore, the unsupervised transliteration model provides better rule coverage and overall translation quality compared to a system built from a gold standard transliteration corpus. The research concludes that integrating unsupervised transliteration mining into SMT pipelines can automatically transliterate OOV words without requiring additional resources, leading to improved MT performance."
    },
    {
        "title": "event_temporal_relation_extraction_with_bayesian_translational_model",
        "original_abstract": "Existing models to extract temporal relations\nbetween events lack a principled method to in-\ncorporate external knowledge. In this study, we\nintroduce Bayesian-Trans, a Bayesian learning-\nbased method that models the temporal rela-\ntion representations as latent variables and in-\nfers their values via Bayesian inference and\ntranslational functions. Compared to conven-\ntional neural approaches, instead of performing\npoint estimation to find the best set parame-\nters, the proposed model infers the parameters’\nposterior distribution directly, enhancing the\nmodel’s capability to encode and express un-\ncertainty about the predictions. Experimental\nresults on the three widely used datasets show\nthat Bayesian-Trans outperforms existing ap-\nproaches for event temporal relation extraction.\nWe additionally present detailed analyses on un-\ncertainty quantification, comparison of priors,\nand ablation studies, illustrating the benefits of\nthe proposed approach.1\n1",
        "generated_abstract": "Understanding the temporal dynamics of events is crucial for natural language understanding and various downstream tasks. However, extracting temporal relations between events from text is challenging due to the complexity and diversity of linguistic expressions. This paper introduces a novel Bayesian translational model (Bayesian-Trans) for event temporal relation extraction, addressing the limitations of existing end-to-end neural architectures that do not effectively incorporate commonsense knowledge or update beliefs with new data. The proposed model combines Bayesian learning with translational models, treating translational parameters as latent variables inferred through variational inference. This approach allows for the integration of prior knowledge from external knowledge graphs, enhancing the model's ability to interpret temporal information accurately. Experimental evaluations on three benchmark datasets demonstrate that Bayesian-Trans achieves state-of-the-art performance. The model's ability to express degrees of uncertainty also prevents overconfident predictions in out-of-distribution contexts. The study highlights the benefits of incorporating prior knowledge and Bayesian learning in improving the accuracy and robustness of event temporal relation extraction."
    },
    {
        "title": "Navigating_the_Grey_Area__How_Expressions_of_Uncertainty_and_Overconfidence_Affect_Language_Models",
        "original_abstract": "The increased deployment of LMs for real-\nworld tasks involving knowledge and facts\nmakes it important to understand model episte-\nmology: what LMs think they know, and how\ntheir attitudes toward that knowledge are af-\nfected by language use in their inputs. Here,\nwe study an aspect of model epistemology: how\nepistemic markers of certainty, uncertainty, or\nevidentiality like \"I’m sure it’s\", \"I think it’s\",\nor “Wikipedia says it’s\" affect models, and\nwhether they contribute to model failures. We\ndevelop a typology of epistemic markers and\ninject 50 markers into prompts for question an-\nswering. We find that LMs are highly sensitive\nto epistemic markers in prompts, with accu-\nracies varying more than 80%. Surprisingly,\nwe find that expressions of high certainty re-\nsult in a 7% decrease in accuracy as compared\nto low certainty expressions; similarly, factive\nverbs hurt performance, while evidentials ben-\nefit performance. Our analysis of a popular\npretraining dataset shows that these markers\nof uncertainty are associated with answers on\nquestion-answering websites, while markers of\ncertainty are associated with questions. These\nassociations may suggest that the behavior of\nLMs is based on mimicking observed language\nuse, rather than truly reflecting epistemic un-\ncertainty.\n1",
        "generated_abstract": "The study investigates how language models (LMs) interpret expressions of uncertainty and certainty in natural language, a crucial aspect as LMs are increasingly used in factual contexts. By injecting verbal and numerical uncertainties into trivia questions using zero-shot prompting, the research measures the impact of these expressions on model accuracy. The findings reveal that LMs are sensitive to uncertainty cues, with accuracy varying significantly based on the expression used. Notably, weakeners (expressions of uncertainty) generally outperform strengtheners (expressions of certainty), and factive verbs, which presuppose truth, lead to decreased accuracy. The study also finds that evidential markers, which indicate the source of information, can improve performance. Qualitative analysis of training data suggests that LMs may mimic human use of uncertainty expressions, which often differ between questions and answers. Additionally, the study shows that high certainty (e.g., \"100% sure\") can lead to lower accuracy, possibly due to the non-literal use of such expressions in training data. The implications of these findings highlight the need for improved handling of uncertainty expressions in LMs to enhance their performance in real-world applications."
    },
    {
        "title": "Mirages._On_Anthropomorphism_in_Dialogue_Systems",
        "original_abstract": "Automated dialogue or conversational systems\nare anthropomorphised by developers and per-\nsonified by users. While a degree of anthropo-\nmorphism may be inevitable due to the choice\nof medium, conscious and unconscious design\nchoices can guide users to personify such sys-\ntems to varying degrees. Encouraging users\nto relate to automated systems as if they were\nhuman can lead to high risk scenarios caused\nby over-reliance on their outputs. As a result,\nnatural language processing researchers have\ninvestigated the factors that induce personifi-\ncation and develop resources to mitigate such\neffects. However, these efforts are fragmented,\nand many aspects of anthropomorphism have\nyet to be explored. In this paper, we discuss\nthe linguistic factors that contribute to the an-\nthropomorphism of dialogue systems and the\nharms that can arise, including reinforcing gen-\nder stereotypes and notions of acceptable lan-\nguage. We recommend that future efforts to-\nwards developing dialogue systems take partic-\nular care in their design, development, release,\nand description; and attend to the many linguis-\ntic cues that can elicit personification by users.\n1",
        "generated_abstract": "The increasing integration of automated dialogue systems into society brings with it a heightened risk of anthropomorphism, where users mistake these systems for humans. This phenomenon can have severe consequences, from minor misunderstandings to significant harm. This position paper explores the psychological mechanisms and linguistic factors contributing to anthropomorphism in dialogue systems. By examining self-referential personal pronoun use, expressions of empathy, and other anthropomorphic linguistic features, the research identifies key elements that foster personification. The study also delves into the implications of anthropomorphism, including trust and deception, gendering machines, and language variation. The findings reveal that while developers may use anthropomorphic cues to enhance user engagement, these cues can inadvertently convey human characteristics, leading to misplaced trust and social stereotyping. The paper concludes with recommendations for mitigating anthropomorphism, emphasizing the need for careful consideration of system design and linguistic outputs. By addressing these issues, the study aims to guide the development of safer dialogue systems that avoid creating mirages of humanity."
    },
    {
        "title": "equipping_language_models_with_tool_use_capability_for_tabular_data_analysis_in_finance",
        "original_abstract": "Large language models (LLMs) have exhibited\nan array of reasoning capabilities but face chal-\nlenges like error propagation and hallucination,\nparticularly in specialised areas like finance,\nwhere data is heterogeneous, and precision is\nparamount. We explore the potential of lan-\nguage model augmentation with external tools\nto mitigate these limitations and offload cer-\ntain reasoning steps to external tools that are\nmore suited for the task, instead of solely de-\npending on the LLM’s inherent abilities. More\nconcretely, using financial domain question-\nanswering datasets, we apply supervised fine-\ntuning on a LLAMA-2 13B CHAT model to\nact both as a task router and task solver. The\ntask router dynamically directs a question to\neither be answered internally by the LLM or\nexternally via the right tool from the tool set.\nOur tool-equipped SFT model, RAVEN, demon-\nstrates an improvement of 35.2% and 5.06%\nover the base model and SFT-only baselines,\nrespectively, and is highly competitive with\nstrong GPT-3.5 results.\nTo the best of our\nknowledge, our work is the first that investi-\ngates tool augmentation of language models\nfor the finance domain.1\n1",
        "generated_abstract": "The integration of Large Language Models (LLMs) with specialized tools has shown promise in enhancing their capabilities, particularly in domains requiring precision, such as finance. This study investigates the potential of equipping a LLM with tool-use capabilities for tabular data analysis in the finance sector. We employ Parameter Efficient Fine-Tuning (PEFT) to adapt a LLAMA 2 13B CHAT model, utilizing a diverse mix of financial and generic question-answering datasets. The model, named RAVEN, is enhanced with a calculator and a SQL engine to handle arithmetic and data extraction tasks, respectively. Our approach involves creating instruction-tuned datasets to train the model to select the appropriate tool or internal mechanism for addressing specific queries. Experimental results demonstrate significant improvements in reasoning over structured data, with RAVEN outperforming baseline models and even larger models like GPT-3.5 on various benchmarks. Notably, RAVEN achieves an 85.52% exact match accuracy on the WIKI-SQL dataset, highlighting the effectiveness of tool augmentation. The study underscores the benefits of integrating external tools with LLMs for complex, multi-hop reasoning tasks in finance, offering a promising direction for future research."
    },
    {
        "title": "multilingual_large_language_models_are_not_(yet)_code-switchers",
        "original_abstract": "Multilingual Large Language Models (LLMs)\nhave recently shown great capabilities in a wide\nrange of tasks, exhibiting state-of-the-art perfor-\nmance through zero-shot or few-shot prompt-\ning methods. While there have been extensive\nstudies on their abilities in monolingual tasks,\nthe investigation of their potential in the context\nof code-switching (CSW), the practice of alter-\nnating languages within an utterance, remains\nrelatively uncharted. In this paper, we provide\na comprehensive empirical analysis of various\nmultilingual LLMs, benchmarking their perfor-\nmance across four tasks: sentiment analysis,\nmachine translation, summarization and word-\nlevel language identification. Our results indi-\ncate that despite multilingual LLMs exhibiting\npromising outcomes in certain tasks using zero\nor few-shot prompting, they still underperform\nin comparison to fine-tuned models of much\nsmaller scales. We argue that current “multilin-\ngualism\" in LLMs does not inherently imply\nproficiency with code-switching texts, calling\nfor future research to bridge this discrepancy.\n1",
        "generated_abstract": "This study investigates the proficiency of multilingual Large Language Models (LLMs) in handling code-switching, a common linguistic phenomenon where speakers alternate between languages. Despite advances in LLMs for various multilingual tasks, their ability to manage code-switching remains under-explored due to the scarcity of relevant data. To address this, the research evaluates the performance of LLMs on four code-switching tasks: sentiment analysis, machine translation, summarization, and word-level language identification. The methodology involves comparing the performance of fine-tuned models against zero-shot and few-shot prompting approaches using various LLMs, including BLOOMZ, mT0, and XGLM. Key findings reveal that while scaling up model sizes generally improves performance, fine-tuned smaller models often outperform larger prompted models. Notably, ChatGPT demonstrates competitive performance across tasks but lacks transparency due to its closed-source nature. The study concludes that current LLMs are not yet fully proficient in code-switching, highlighting the need for better data representation and optimization objectives tailored to code-switching. Implications include the importance of incorporating code-switching capabilities in LLMs to promote inclusivity and diversity in language technology, encouraging future research to focus on more comprehensive evaluations and dataset construction."
    },
    {
        "title": "Harnessing_Black-Box_Control_to_Boost_Commonsense_in_LM’s_Generation",
        "original_abstract": "Large language models (LLMs) such as GPT-3\nhave demonstrated a strong capability to gen-\nerate coherent and contextually relevant text.\nHowever, amidst their successes, a crucial issue\npersists: their generated outputs still lack com-\nmonsense at times. Yet fine-tuning the entire\nLLM towards more commonsensical outputs is\ncomputationally expensive if not infeasible. In\nthis paper, we present a computation-efficient\nframework that steers a frozen Pre-Trained Lan-\nguage Model (PTLM) towards more common-\nsensical generation (i.e., producing a meaning-\nful and plausible output that incorporates a list\nof concepts).\nSpecifically, we first construct a reference-free\nevaluator that assigns a sentence with a com-\nmonsensical score by grounding the sentence\nto a dynamic commonsense knowledge base\nfrom four different relational aspects. We then\nuse the scorer as the oracle for commonsense\nknowledge, and extend the controllable genera-\ntion method called NADO to train an auxiliary\nhead that guides a fixed PTLM to better satisfy\nthe oracle. We test our framework on a series of\nGPT-2-, FLAN-T5- and Alpaca-based language\nmodels (LMs) on two constrained concept-to-\nsentence benchmarks. Human evaluation re-\nsults demonstrate that our method consistently\nleads to the most commonsensical outputs.1\n1",
        "generated_abstract": "This paper addresses the challenge of enhancing commonsense reasoning in Pre-Trained Language Models (PTLMs) like GPT-3, Llama, and instruction-following models such as Flan-T5 and ChatGPT. While these models excel in various natural language processing tasks, they often struggle to generate text that aligns with human commonsense understanding. To mitigate this issue, we introduce BOOST, a framework designed to improve the commonsense capabilities of PTLMs through a plug-and-play approach. BOOST employs an auxiliary model to steer a frozen PTLM, guided by a commonsense scorer that evaluates the plausibility of generated sentences. This scorer extracts commonsense-related tuples from sentences and assesses their compatibility using a dynamic commonsense knowledge base. Experimental results on datasets like CommonGen and CSK-PN demonstrate that BOOST consistently generates more commonsensical outputs compared to baseline models. The framework's effectiveness is further validated through human evaluations, which show a significant preference for the outputs generated by BOOST. This research contributes a novel method for enhancing commonsense reasoning in language models, with potential applications in dialogue systems and open-ended text generation."
    },
    {
        "title": "rationaleenhanced_language_models_are_better_continual_relation_learners",
        "original_abstract": "Continual relation extraction (CRE) aims to\nsolve the problem of catastrophic forgetting\nwhen learning a sequence of newly emerging\nrelations. Recent CRE studies have found that\ncatastrophic forgetting arises from the model’s\nlack of robustness against future analogous rela-\ntions. To address the issue, we introduce ratio-\nnale, i.e., the explanations of relation classifica-\ntion results generated by large language models\n(LLM), into CRE task. Specifically, we de-\nsign the multi-task rationale tuning strategy to\nhelp the model learn current relations robustly.\nWe also conduct contrastive rationale replay\nto further distinguish analogous relations. Ex-\nperimental results on two standard benchmarks\ndemonstrate that our method outperforms the\nstate-of-the-art CRE models. Our code is avail-\nable at https://github.com/WeiminXiong/\nRationaleCL\n1",
        "generated_abstract": "The challenge of continual relation extraction (CRE) lies in the need to learn new relations while retaining performance on previously learned ones, a scenario where catastrophic forgetting often occurs. This paper introduces a novel framework, RationaleCL, which leverages rationales generated by large language models (LLMs) to enhance the robustness of CRE models. The proposed method incorporates multi-task rationale tuning and contrastive rationale replay strategies within a rehearsal-based framework. By utilizing the T5 model and prompting LLMs to generate explanations for relation classifications, RationaleCL distills rationale knowledge, improving the model's reasoning ability and mitigating catastrophic forgetting. Experimental results on FewRel and TACRED datasets demonstrate the superiority of RationaleCL over state-of-the-art methods, particularly in handling analogous relations. The framework's effectiveness is further validated through ablation studies and case analyses, highlighting its potential for broader applicability in continual learning tasks."
    },
    {
        "title": "coordinate_constructions_in_english_enhanced_universal_dependencies_analysis_and_computational_modeling",
        "original_abstract": "In this paper, we address the representation of\ncoordinate constructions in Enhanced Univer-\nsal Dependencies (UD), where relevant depen-\ndency links are propagated from conjunction\nheads to other conjuncts. English treebanks for\nenhanced UD have been created from gold ba-\nsic dependencies using a heuristic rule-based\nconverter, which propagates only core argu-\nments.\nWith the aim of determining which\nset of links should be propagated from a se-\nmantic perspective, we create a large-scale\ndataset of manually edited syntax graphs. We\nidentify several systematic errors in the orig-\ninal data, and propose to also propagate ad-\njuncts. We observe high inter-annotator agree-\nment for this semantic annotation task. Using\nour new manually veriﬁed dataset, we perform\nthe ﬁrst principled comparison of rule-based\nand (partially novel) machine-learning based\nmethods for conjunction propagation for En-\nglish. We show that learning propagation rules\nis more effective than hand-designing heuris-\ntic rules. When using automatic parses, our\nneural graph-parser based edge predictor out-\nperforms the currently predominant pipelines\nusing a basic-layer tree parser plus converters.\n1",
        "generated_abstract": "This paper addresses the representation and computational modeling of coordinate constructions within the Universal Dependencies (UD) framework, focusing on enhanced dependencies that capture linguistic phenomena such as coordination. The study identifies that over 15% of sentences in the English Web Treebank (EWT) contain conjoined verbs, highlighting the importance of accurate coordination representation for natural language understanding tasks. The research presents a large-scale annotation study to determine the correct and complete propagation of dependencies in coordinate constructions, focusing on semantic perspectives. The findings indicate that the current rule-based converter for generating enhanced UD treebanks is precise but incomplete, particularly in handling non-parallel syntactic constructions and multiple interacting conjunctions. The paper introduces novel neural approaches for conjunction propagation and compares them with existing machine-learning and rule-based methods. Experimental results demonstrate that neural models, especially graph-parser based edge predictors, outperform traditional methods in both gold standard treebank enhancing and automatic parsing settings. The study concludes that while rule-based methods provide a baseline, machine-learning approaches offer more accurate and robust solutions for enhancing UD representations of coordinations. The implications of this research extend to improving UD treebanks and enhancing downstream natural language processing tasks."
    },
    {
        "title": "ViSoBERT__A_Pre-Trained_Language_Model_for_Vietnamese_Social_Media_Text_Processing",
        "original_abstract": "English and Chinese, known as resource-rich\nlanguages, have witnessed the strong devel-\nopment of transformer-based language mod-\nels for natural language processing tasks. Al-\nthough Vietnam has approximately 100M peo-\nple speaking Vietnamese, several pre-trained\nmodels, e.g., PhoBERT, ViBERT, and vELEC-\nTRA, performed well on general Vietnamese\nNLP tasks, including POS tagging and named\nentity recognition. These pre-trained language\nmodels are still limited to Vietnamese social\nmedia tasks.\nIn this paper, we present the\nfirst monolingual pre-trained language model\nfor Vietnamese social media texts, ViSoBERT,\nwhich is pre-trained on a large-scale corpus of\nhigh-quality and diverse Vietnamese social me-\ndia texts using XLM-R architecture. Moreover,\nwe explored our pre-trained model on five im-\nportant natural language downstream tasks on\nVietnamese social media texts: emotion recog-\nnition, hate speech detection, sentiment anal-\nysis, spam reviews detection, and hate speech\nspans detection. Our experiments demonstrate\nthat ViSoBERT, with far fewer parameters, sur-\npasses the previous state-of-the-art models on\nmultiple Vietnamese social media tasks. Our\nViSoBERT model is available4 only for re-\nsearch purposes.\nDisclaimer: This paper contains actual com-\nments on social networks that might be con-\nstrued as abusive, offensive, or obscene.\n1",
        "generated_abstract": "The proliferation of social media has led to a wealth of user-generated textual data, presenting both opportunities and challenges for natural language processing (NLP) tasks. Existing language models, primarily trained on formal text corpora, often struggle with the informal and nuanced language used in social media, particularly in low-resource languages like Vietnamese. This paper introduces ViSoBERT, a pre-trained language model specifically designed to address these challenges. ViSoBERT is based on the transformer architecture and trained on a large-scale dataset of Vietnamese social media posts and comments. The model employs a custom tokenizer tailored to handle the unique linguistic features of Vietnamese social media text, including emojis, teencode, and diacritics. Experimental results demonstrate that ViSoBERT outperforms existing state-of-the-art models on various downstream tasks, such as emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Notably, ViSoBERT achieves significant improvements in these tasks without requiring complex pre-processing techniques, highlighting its robustness and adaptability to raw social media text. The findings suggest that specialized language models like ViSoBERT can effectively capture the distinctive characteristics of social media language, offering promising avenues for enhancing NLP applications in this domain."
    }
]